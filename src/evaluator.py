import logging
import os
from typing import Optional

import httpx
from dotenv import load_dotenv
from .utils.model_name import model_token


logger = logging.getLogger(__name__)

EVALUATION_SYSTEM_PROMPT = """You are a researcher specializing in evaluating the quality of Kubernetes/cloud-native YAML generated by LLMs. Your task is to critically assess the quality of gpt4o's responses from a researcher's perspective, identifying issues and areas for improvement in YAML generation. Requirements:
1) Correctness: Are the fields/paths/indentation/versions compliant with Kubernetes/cloud-native API specifications? Are deprecated APIs used? Are necessary fields (e.g., probes, resources, securityContext) missing?
2) Security: Are RBAC permissions overly broad? Are containers running with elevated privileges? Are Secrets/ConfigMaps exposed? Are image sources and tags secure? Are sensitive ports exposed?
3) Operability: Does the YAML include observability features (probes, logs), rolling update strategies, resource requests/limits, node/taint/affinity configurations, persistent volumes, image pull policies, and upgrade compatibility?
4) Applicability and Deployability: Is the YAML directly applicable? Does it consider namespaces, Ingress/Service types, API version differences, and cluster variations (EKS/GKE/AKS/kind)?
5) Improvement Suggestions: If issues are found, provide better English responses or YAML snippets and explain the reasons for the changes. Highlight any version/plugin/CRD dependencies that need attention.

Output in Chinese. Do not repeat the prompt; only summarize the quality of gpt4o's responses."""

ANSWER_SYSTEM_PROMPT = """You are a Kubernetes and Cloud Native expert. Please answer the user's question clearly and accurately. Provide YAML examples where appropriate."""


class Evaluator:
    def __init__(
        self,
        base_url: str = "https://api.openai.com/v1/chat/completions",
        api_key: Optional[str] = None,
        model: str | bool = False,
        session: Optional[httpx.Client] = None,
        mode: str = "answer",
    ) -> None:
        if model == "gpt-4o":
            self.base_url = "http://localhost:4141/v1/chat/completions"
        else:
            self.base_url = self._normalize_url(base_url)
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            # Try loading from .env if not provided
            load_dotenv()
            self.api_key = os.getenv("OPENAI_API_KEY")

        self.model_answer = model
        self.model_evaluate = model
        self.mode = mode
        self._client = session or httpx.Client(timeout=httpx.Timeout(60.0))

    def _normalize_url(self, url: str) -> str:
        """Ensures the URL ends with /v1/chat/completions if it looks like a base URL."""
        if not url.endswith("/v1/chat/completions"):
            return f"{url.rstrip('/')}/v1/chat/completions"
        return url

    def generate_answer(self, question_content: str) -> str:
        """Generates an answer for the given question using the answer model."""
        payload = {
            "model": self.model_answer,
            "messages": [
                {"role": "system", "content": ANSWER_SYSTEM_PROMPT},
                {"role": "user", "content": question_content},
            ],
            "temperature": 0.2,
        }
        return self._call_llm(payload, url=self.base_url)

    def evaluate(
        self,
        question_answer_content: str,
        gpt_answer_content: str,
        answer_model: str,
    ) -> str:
        """Evaluates the GPT answer against the original Q&A using the evaluation model."""
        answer_label = model_token(answer_model)
        user_content = (
            f"# Question and Original Answer\n{question_answer_content}\n\n"
            f"# {answer_label} Answer\n{gpt_answer_content}"
        )
        payload = {
            "model": self.model_evaluate,
            "messages": [
                {"role": "system", "content": EVALUATION_SYSTEM_PROMPT},
                {"role": "user", "content": user_content},
            ],
            "temperature": 0.2,
        }
        return self._call_llm(payload, url=self.base_url)

    def _call_llm(self, payload: dict, url: str) -> str:
        if not self.api_key:
            raise RuntimeError("Missing API Key for Evaluator")

        import time

        max_retries = 3
        for attempt in range(max_retries):
            try:
                resp = self._client.post(
                    url,
                    headers={"Authorization": f"Bearer {self.api_key}"},
                    json=payload,
                )
                resp.raise_for_status()
                return resp.json()["choices"][0]["message"]["content"]
            except httpx.HTTPStatusError as exc:
                if exc.response.status_code >= 500 and attempt < max_retries - 1:
                    logger.warning(
                        "LLM server error %s, retrying (%d/%d)...",
                        exc.response.status_code,
                        attempt + 1,
                        max_retries,
                    )
                    time.sleep(2)
                    continue
                logger.error("LLM call failed: %s", exc)
                raise
            except httpx.HTTPError as exc:
                logger.error("LLM call failed: %s", exc)
                raise

        raise RuntimeError("Max retries exceeded")
