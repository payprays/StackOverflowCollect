Question ID,Question Title,Question Body,Question Tags,Answer ID,Answer Body,Answer Creation Date,Question Creation Date
79848178,"Traefik Proxy in Kubernetes, can you have unique health checks per service?","Traefik Proxy (v3.6) deployed with native Kubernetes on non-EKS AWS provider, how can I define a unique health-check per back-end service that will be used to verify back-end service availability?

I need to support ingress for both Layer 4 (TCP/TLS) and Layer 7 (HTTP/HTTPS) protocols.  TraefikService complains about ExternalName requirement ('healthCheck allowed only for ExternalName service: ')?

I am using helm with an over-rides values file that mirrors the traefik/values.yaml file from the traefik-helm-chart repo.   This over-rides file specifies the static configuration of the spec.ports and the metadata.annotations, as well as the static endpoint configurations.  I will also be dynamically adding/removing back-end services (listeners) programmatically at run-time that the traefik service will have to pick up and expose (TBD at this time - just informational).

I have followed most/all? of the online docs on Traefik that I could find.  I've added the following load-balancer metadata.annnotations to the traefik service:

```
service.beta.kubernetes.io/aws-load-balancer-id: <arn-of-nlb-to-use>
service.beta.kubernetes.io/aws-load-balancer-namme: <nlb-name>
service.beta.kubernetes.io/aws-load-balancer-tpe: nlb
service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip
service.beta.kubernetes.io/aws-load-balancer-ip-address-type: ipv4
service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
service.beta.kubernetes.io/aws-load-balancer-subnets: <comma separated list of subnetIds>
service.beta.kubernetes.io/aws-load-balancer-load-balancer-target-group-attributes: 'preserve_client_ip.enabled=true,load_balancing.cross_zone.enabled=true'
```

The predefined NLB arn is properly associated with a hosted zone in Route 53 and it is accessible from outside of Kubernetes/AWS deployments.

I have treafik running in it's own namespace and the back-end services and pods running in another namespace.  The traefik pods and service knows about the other namespace. A helm installation of the above config always fails to verify the back-end services??

This configuration is based on the NGINX ingress configuration that is working now, however, we need to validate/authenticate TCP/TLS connectivity to the back-end (and pass-through the client IP to the  back-end pod).  This is why we are trying to move to Traefik.

Using the Traefik dashboard, Traefik finds the back-end services and ports.  From the Traefik dashboard perspective, everything looks good!  All of the ports and services from the service/traefik to the traefik/pods to the back-end services seems the line up as expected and looks correct (dashboard displays the proper IP of the respective pods in the HTTP Services>Servers and TCP Services/Servers view).

Any suggestions or examples of how to get traefik to work as expected when deployed to Kubernetes would be greatly appreciated.

Thanks in advance.
Greg","kubernetes, kubernetes-ingress, traefik, traefik-ingress, kubernetes-health-check",,,,2025-12-16T04:13:44
79844573,API Deployment Failing in Uptime Monitoring App,"I'm building a basic uptime monitoring app (frontend + backend). The application works locally, but I'm running into errors while deploying the API.

I've tried multiple approaches and fixes suggested online (including ChatGPT), but the API deployment still fails with the same error.

What am I doing wrong?

[GitHub repo](https://github.com/rutwikrp/Uptime-Monitoring)","docker, kubernetes, deployment",,,,2025-12-12T05:04:23
79844238,Monitoring for misconfigured Istio VirtualService,"We have an Istio VirtualService with a Destination that referred to a service without a port, which is allowed when that service only has one port:

```
      route:
        - destination:
            host: myservice.mynamespace.svc.cluster.local
```

But then somebody added a second port to the Service, which broke the VirtualService.

We'd like to get some monitoring in that detects this sort of thing. Our first guess was that VirtualService would have a `status` field that we can hook into our ArgoCD health detection, but it looks like it doesn't. (There was an experimental feature to write `status` that was [removed last year](https://github.com/istio/istio/pull/51866).)

Everything I can find about Istio monitoring is about monitoring traffic in the cluster — what's the best way to monitor the actual interpretation of your VirtualServices and other configuration?","kubernetes, istio",,,,2025-12-11T17:49:04
79842830,Google tag gateway in GKE Gateway API,"I'm implementing server side tagging for Google Tag Manager through my GKE Gateway API.

I was successfully able to configure server side tag manager in same domain and GET [https://www.example.com/metrics/healthy](https://www.example.com/metrics/healthy) to return OK. Below is my gateway configuration.

```
kind: Gateway
apiVersion: gateway.networking.k8s.io/v1
metadata:
  name: http-gateway
spec:
  gatewayClassName: gke-l7-global-external-managed
  listeners:
  - name: http
    protocol: HTTP
    port: 80
    allowedRoutes:
      kinds:
      - kind: HTTPRoute
      namespaces:
        from: Selector
        selector:
          matchLabels:
            otherInfra: httpToHttps
  - name: https
    protocol: HTTPS
    port: 443
    tls:
      mode: Terminate
      options:
        networking.gke.io/pre-shared-certs: http-ssl
  addresses:
  - type: NamedAddress
    value: gke-http
---
kind: HTTPRoute
apiVersion: gateway.networking.k8s.io/v1
metadata:
  name: nginx
spec:
  parentRefs:
  - name: http-gateway
  hostnames:
  - ""www.example.com""
  rules:
  - backendRefs:
    - name: nginx
      port: 80
---
kind: HTTPRoute
apiVersion: gateway.networking.k8s.io/v1
metadata:
  name: metrics
spec:
  parentRefs:
  - name: http-gateway
  hostnames:
  - ""www.example.com""
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /metrics
    backendRefs:
    - name: metrics
      port: 8080
```

Above configuration can forward all /metrics traffic to server side tag. Now to serve first-party ""script"" requests to Google servers, I'm following the below document (Serve scripts with a CDN).

[https://developers.google.com/tag-platform/tag-manager/server-side/dependency-serving?option=cdn](https://developers.google.com/tag-platform/tag-manager/server-side/dependency-serving?option=cdn)

It says:

1. Collection path (e.g. /metrics) --> Forwards events to your tagging server
2. Script path (e.g. /scripts) --> Forwards first-party script requests to Google servers

Step 1 is already completed. How can I achieve step 2? I followed the below document to setup Google tag gateway. But my Load Balancer created using GKE Gateway API. I cannot modify it in Google Cloud Console. But I was able to setup backend service (Internet NEG) mentioned in the document.

[https://developers.google.com/tag-platform/tag-manager/gateway/setup-guide?setup=manual#setup-type](https://developers.google.com/tag-platform/tag-manager/gateway/setup-guide?setup=manual#setup-type)

Is there a way I can link this backend with my Gateway API through HTTPRoute or any other resource?

Or is there any other way to achive this? Please help.","kubernetes, google-kubernetes-engine, google-tag-manager, gke-networking",,,,2025-12-10T10:46:45
79842576,Guidance needed for setting up self-managed ClickHouse on AWS EKS,"I’m working on deploying a self-managed ClickHouse setup on AWS EKS and need some help locating the right resources. Specifically, I’m looking for:

1. A build or deployment guide for running ClickHouse on EKS
2. Access to the official container images and Helm charts required for the setup

I’ve looked through the ClickHouse documentation, but I haven’t found a complete end-to-end guide. Any pointers, references, or examples would be greatly appreciated.","kubernetes, containers, kubernetes-helm, amazon-eks, clickhouse",,,,2025-12-10T05:17:11
79842171,ArgoCD applicationset - path and .argocd-source.yaml issues,"I have a test setup where I have a base folder structure. This is working and I create the application for staging with the values and the configmap from the staging folder.

However, I also need to be able to override the Chart.yaml dependency version.
I have tried with the .argocd-source.yaml below but it doesnt seem to work.

- appOne -> base with chart and values.yaml
- appOne -> staging with values.yaml and custom configmap
- appTwo -> base with chart and values.yaml
- appTwo -> staging with values.yaml and custom configmap

Here the .argocd-source.yaml

```
helm:
  chart: .
  version: ""0.0.1""     t
  valueFiles:
    - values.yaml
  parameters:
    - name: dependencies[0].version
      value: ""2.31.2""       # OVERRIDE grafana version
```

The ApplicationSet

```
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: staging-app-set
  namespace: argocd
spec:
  generators:
    - git:
        # Staging generator for staging folders
        repoURL: http://gitea-http.gitea.svc.cluster.local:3000/admin/test-v2.git
        revision: HEAD
        directories:
          - path: ""test-apps/*/staging""
  template:
    metadata:
      name: ""{{path[1]}}-staging""
    spec:
      project: default
      destination:
        namespace: staging-app-set
        server: https://kubernetes.default.svc

      # Multiple sources:
      sources:

        - repoURL: http://gitea-http.gitea.svc.cluster.local:3000/admin/test-v2.git
          path: ""{{path}}/../base""
          targetRevision: HEAD
          helm:
            valueFiles:
              - ../staging/values.yaml
            parameters:
              - name: dependencies[0].version
                value: ""10.3.0""

        - repoURL: http://gitea-http.gitea.svc.cluster.local:3000/admin/rt-v2.git
          path: ""{{path}}""
          targetRevision: HEAD

      syncPolicy:
        automated:
          prune: true
          selfHeal: true
        syncOptions:
          - CreateNamespace=true
```","kubernetes, kubernetes-helm, argocd",,,,2025-12-09T16:36:00
79842158,How to handle database schema migrations in a Kubernetes rolling update without downtime?,"I am designing a CI/CD pipeline for a Python FastAPI application using Kubernetes (EKS) and PostgreSQL.

We are using a Rolling Update strategy for deployment. However, I have an architectural challenge with database migrations using Alembic.

**The Problem:** If I run the migration as part of the container startup command (CMD), multiple pods might attempt to run the migration at the same time as they scale up. This could lead to race conditions or database locks.

If I run the migration as a step in the pipeline (GitHub Actions), the new schema might be applied while the old pods are still running. This can cause the old code to crash if the schema is not compatible with it.

**My Question:** What is the standard architecture pattern for handling database migrations in a Kubernetes environment to ensure zero downtime? Should this be managed via initContainers, Helm Hooks, or a separate Job?",kubernetes,79843752.0,"In a rolling deploy scenario, you will by definition have a period of time where multiple versions of your application will be running at the same time. Therefore the key to a zero downtime migration is less about how you invoke the migration and more about how you write the migration.

As mentioned in a [comment above](https://stackoverflow.com/questions/79842158/how-to-handle-database-schema-migrations-in-a-kubernetes-rolling-update-without#comment140894650_79842158), the goal is to not make any breaking data or schema changes from the perspective of any running code. Some changes may need to be made in multiple deploys. For example, the renaming of a column might be accomplished [as described here](https://stackoverflow.com/questions/41747699/renaming-an-existing-column-without-downtime). Once the rolling deploy completes, the old code is gone and the old column can be safely removed. The last warning here is that code rollbacks are more complicated after you've performed such a migration because older code would expect the older column to exist, so in some scenarios it may be safer to instead find a way to temporarily keep backwards compatibility until it's certain you won't need to roll back.

With all that in mind: init containers, a separate Job kicked off by CI, or just running migrations in your application before you start your FastAPI server are all equally valid strategies, they just have slightly different behavior and implementation complexity.",2025-12-11T08:25:28,2025-12-09T16:19:31
79842158,How to handle database schema migrations in a Kubernetes rolling update without downtime?,"I am designing a CI/CD pipeline for a Python FastAPI application using Kubernetes (EKS) and PostgreSQL.

We are using a Rolling Update strategy for deployment. However, I have an architectural challenge with database migrations using Alembic.

**The Problem:** If I run the migration as part of the container startup command (CMD), multiple pods might attempt to run the migration at the same time as they scale up. This could lead to race conditions or database locks.

If I run the migration as a step in the pipeline (GitHub Actions), the new schema might be applied while the old pods are still running. This can cause the old code to crash if the schema is not compatible with it.

**My Question:** What is the standard architecture pattern for handling database migrations in a Kubernetes environment to ensure zero downtime? Should this be managed via initContainers, Helm Hooks, or a separate Job?",kubernetes,79843709.0,"Instead of having your application pods run migrations, which creates race conditions, you create a separate, single-purpose pod that runs the migration once and then dies. This pod is managed by a Kubernetes ""Job"" resource. The fundamental difference between a Job and a Deployment is that a Job runs to completion and exits, while a Deployment keeps pods running continuously to serve traffic.

### You can use below steps

**Step 1:** Your CI/CD pipeline builds your new Docker image containing both your application code and migration scripts.

**Step 2:** The pipeline creates a Job resource that uses this same image. The Job spins up exactly one pod that runs your migration command, such as alembic upgrade head. When the migration completes, the pod exits and the Job marks itself as complete.

**Step 3:** The pipeline checks whether the migration succeeded or failed by examining the Job's status. If the migration failed, the pipeline stops here and doesn't proceed with deployment.

**Step 4:** Only if the migration succeeded does the pipeline proceed to update your actual application deployment with the new image, triggering a rolling update.

The key insight here is that the Job completes before any new application pods start running. This temporal separation eliminates race conditions entirely because there's never a scenario where multiple pods are attempting to run migrations simultaneously.",2025-12-11T07:31:08,2025-12-09T16:19:31
79841126,How can I control the shutdown order of Spring-managed beans that I don’t create myself?,"I have a Spring Boot application running in Kubernetes. I’m trying to implement a *graceful shutdown* flow using a **readiness probe**:

1. App receives `SIGTERM`.
2. App should start returning `503` from `/health` so Kubernetes stops sending new traffic.
3. Existing requests finish.
4. App shuts down.

I’d like to **record metrics for the last few `/health` calls** (especially the `503` responses during shutdown) using Micrometer’s `MeterRegistry`. However, I’m seeing that the `MeterRegistry` is already closed while the readiness probe is still hitting `/health`.

Relevant k8s config:

```
readinessProbe:
  initialDelaySeconds: 30
  periodSeconds: 1
  failureThreshold: 1
  httpGet:
    path: /health
    port: 9091
```

Controller:

```
import io.micrometer.core.instrument.MeterRegistry;
import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.http.ResponseEntity;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import static org.springframework.http.HttpStatus.SERVICE_UNAVAILABLE;

@RestController
@RequiredArgsConstructor
@Slf4j
public class HealthController {
    private final StatusService statusService;
    private final MeterRegistry meterRegistry;

    @GetMapping(""/health"")
    public ResponseEntity<String> health() {
        Status status = statusService.status();
        ResponseEntity<String> response = status == Status.OK
            ? ResponseEntity.ok(status.name())
            : ResponseEntity.status(SERVICE_UNAVAILABLE).body(status.name());

        int statusCode = response.getStatusCode().value();
        String statusCodeString = String.valueOf(statusCode);
        meterRegistry.counter(""health_check_status"", ""code"", statusCodeString).increment();

        log.debug(""Health check returned: {}. Status code: {}. meterRegistry.isClosed(): {}"",
            response, statusCodeString, meterRegistry.isClosed());
        return response;
    }
}
```

Logs around shutdown:

```
2025-12-08 13:15:24.096  Health check returned: <200 OK OK,OK,[]>. Status code: 200. meterRegistry.isClosed(): false
2025-12-08 13:15:25.092  Health check returned: <200 OK OK,OK,[]>. Status code: 200. meterRegistry.isClosed(): false
2025-12-08 13:15:26.092  Health check returned: <503 SERVICE_UNAVAILABLE Service Unavailable,SHUTTING_DOWN,[]>. Status code: 503. meterRegistry.isClosed(): true
2025-12-08 13:15:26.095  Health check returned: <503 SERVICE_UNAVAILABLE Service Unavailable,SHUTTING_DOWN,[]>. Status code: 503. meterRegistry.isClosed(): true
```

So when my `StatusService` reports `SHUTTING_DOWN` and `/health` starts returning `503`, `meterRegistry.isClosed()` is already `true`, which means I can’t reliably record those `503` metrics.

From what I can see:

- `MeterRegistry` is created and managed by Spring Boot auto-configuration.
- On `SIGTERM`, Spring Boot shuts down and eventually calls `MeterRegistry#close()`.
- I don’t directly control the registry bean (no custom bean definition, no `@DependsOn`, etc.).

**Questions**

1. **Is there a way in Spring Boot to delay or customize when `MeterRegistry.close()` is called during shutdown**, so that I can still write metrics during the last readiness probe calls?
2. If not, what is the recommended pattern to:

  - Keep `/health` responding with accurate status during shutdown, and
  - Still record metrics for those responses, when `MeterRegistry` is managed by Spring Boot?","java, spring, spring-boot, kubernetes, micrometer",,,,2025-12-08T15:43:51
79841115,How do I set up data transfer from one domain to another in ingress k8s?,"Good afternoon, it's tedious for me to configure ingress so that the result from the address [https://s3.animori.tv/animori/public/robots.txt](https://s3.animori.tv/animori/public/robots.txt) issued at the address [https://animori.tv/robots.txt](https://animori.tv/robots.txt) at the moment, ingress looks like this

```
apiVersion: networking.k8s.io/v1 kind: Ingress
metadata:
  name: path-based-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/proxy-body-size: ""0""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/client-body-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-next-upstream-timeout: ""1800""
    nginx.ingress.kubernetes.io/proxy-max-temp-file-size: ""0""
spec:
  rules:
  - host: animori.tv
    http:
      paths:
      - path: /api(/|$)(.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: ani-mori-backend-svc
            port:
              number: 8088
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ani-mori-frontend-svc
            port:
              number: 80

  - host: rabbitmq.animori.tv
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ani-mori-rabbit-mq-sf-svc
            port:
              number: 15672

  - host: minio.animori.tv
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ani-mori-minio-sf-svc
            port:
              number: 9090

  - host: s3.animori.tv
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ani-mori-minio-sf-svc
            port:
              number: 9000

  - host: sentry.animori.tv
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: glitchtip-svc
            port:
              number: 8000I
```

tried the following setup

```

    apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: path-based-ingress
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/proxy-body-size: ""0""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/client-body-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-next-upstream-timeout: ""1800""
    nginx.ingress.kubernetes.io/proxy-max-temp-file-size: ""0""
    \# Для специфических путей используем configuration-snippet
    nginx.ingress.kubernetes.io/configuration-snippet: |
      location = /robots.txt {
        proxy_pass https://s3.animori.tv/
        proxy_set_header Host s3.animori.tv;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
      }
spec:
  rules:
  - host: animori.tv
    http:
      paths:
      - path: /robots.txt
        pathType: ImplementationSpecific
        backend:
          service:
            name: ani-mori-minio-sf-svc
            port:
              number: 9000
      - path: /api(/|$)(.\*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: ani-mori-backend-svc
            port:
              number: 8088
      - path: /
        pathType: Prefix
        backend:
          service:
            name: ani-mori-frontend-svc
            port:
              number: 80
```

but when trying to navigate to [https://animori.tv/robots.txt](https://animori.tv/robots.txt) I'm completing the following route [https://animori.tv/animori/public/robots.txt](https://animori.tv/animori/public/robots.txt) and it gives me a 404, when I try to send it, I get the following

```
    \<html\> \<head\>\<title\>301 Moved Permanently\</title\>\</head\> \<body\> \<center\>\<h1\>301 Moved Permanently\</h1\>\</center\> \<hr\>\<center\>openresty\</center\> \</body\> \</html\>
```","kubernetes, kubernetes-ingress",,,,2025-12-08T15:29:38
79839554,How to add a tty on pod?,"I can exec a bash shell using this command

```
kubectl exec --stdin --tty ftp1-7686766766-8v5s2 -- /bin/bash
bash-4.2#
```

but I want to know why kubectl attach don't work

```
kubectl attach -it ftp1-7686766766-8v5s2
error: Unable to use a TTY - container ftp1 did not allocate one
All commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.
If you don't see a command prompt, try pressing enter.
```

I have tried this way (yaml fails too)

```
kubectl get po ftp1-7686766766-8v5s2 -o json  > ftp.json
```

then edit json

```
    ""tty"": ""true"",
```

but...

```
kubectl apply  -f ftp.json
Warning: resource pods/ftp1-7686766766-8v5s2 is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.

another error

The Pod ""ftp1-7686766766-8v5s2"" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`,`spec.initContainers[*].image`,`spec.activeDeadlineSeconds`,`spec.tolerations` (only additions to existing tolerations),`spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
@@ -127,9 +127,9 @@
    ""TerminationMessagePolicy"": ""File"",
    ""ImagePullPolicy"": ""Never"",
    ""SecurityContext"": null,
-   ""Stdin"": false,
-   ""StdinOnce"": false,
-   ""TTY"": false
+   ""Stdin"": true,
+   ""StdinOnce"": true,
+   ""TTY"": true
   }
  ],
```

If I remove

```
    ""tty"": ""true"",
```

works (but no tty! so attach don't work)

```
kubectl apply  -f ftp.json
pod/ftp1-7686766766-8v5s2 configured
```

any idea? My target is create a pod with tty so attach can work.

EDIT: I solve the first error, I had to delete pod and recreate with kubectl apply -f

but...it create without tty!

```
kubectl attach -it ftp1-7686766766-pslz7
error: Unable to use a TTY - container ftp1 did not allocate one
All commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.
If you don't see a command prompt, try pressing enter.
```

This command confirm tty are missing

```
kubectl get po ftp1-7686766766-pslz7 -o yaml|egrep -i 'tty|stdin'
```",kubernetes,79839576.0,"The issue is that **tty, stdin**, and **stdinOnce** are immutable fields. They can only be set when the Pod is created, not modified afterward. You need to modify the **Deployment/StatefulSet** that creates the Pod.

Edit the Deployment (not the Pod):

`kubectl edit deployment ftp1`

and add these under the containers:

```
spec:
  template:
    spec:
      containers:
      - name: ftp1
        image: your-image
        stdin: true
        stdinOnce: true
        tty: true
```",2025-12-06T08:52:21,2025-12-06T08:10:35
79838905,Turning off buffering in k8s nginx ingress greatly increases client side latency and streaming in downstream,"I have the following architecture:

`Browser App (React)` -> `Nginx K8S Ingress` -> `Streaming Service A (Kotlin, POD)` -> `Streaming Service B (Java,POD)`

From the browser I upload a json array of 500 MB. `Service A`  proxies the request using streaming, `Service B` processes that `InputStream` in chunks of 500 documents.

When request buffering is `ON` in the Nginx ingress, it takes **~10ms** for `Service B` to fetch a chunk of data (500 docs, ~32 megabytes) from the InputStream. Uploading and processing the JSON from the client's perspective takes only a **couple of seconds**.

When request buffering is `OFF` in the Nginx ingress, it takes ~**4.5 seconds** for `Service B` to fetch a chunk of data from the `InputStream` and the entire request takes **minutes** to complete from the client's perspective.

Can you help me understand why there is such a huge slowdown when buffering is off?","kubernetes, nginx, network-programming, streaming, kubernetes-ingress",79847716.0,It's likely a configuration issue rather than a programming one - try Server Fault instead.,2025-12-15T13:33:26,2025-12-05T13:00:07
79838905,Turning off buffering in k8s nginx ingress greatly increases client side latency and streaming in downstream,"I have the following architecture:

`Browser App (React)` -> `Nginx K8S Ingress` -> `Streaming Service A (Kotlin, POD)` -> `Streaming Service B (Java,POD)`

From the browser I upload a json array of 500 MB. `Service A`  proxies the request using streaming, `Service B` processes that `InputStream` in chunks of 500 documents.

When request buffering is `ON` in the Nginx ingress, it takes **~10ms** for `Service B` to fetch a chunk of data (500 docs, ~32 megabytes) from the InputStream. Uploading and processing the JSON from the client's perspective takes only a **couple of seconds**.

When request buffering is `OFF` in the Nginx ingress, it takes ~**4.5 seconds** for `Service B` to fetch a chunk of data from the `InputStream` and the entire request takes **minutes** to complete from the client's perspective.

Can you help me understand why there is such a huge slowdown when buffering is off?","kubernetes, nginx, network-programming, streaming, kubernetes-ingress",79840841.0,"I double checked if the services are bottlenecks but I managed to rule them out.

I deployed a netcat with nginx ingress to the cluster, called it from my machine and I see the same behaviour.

With request buffering on, the 300 mb request finishes in 2-3 seconds and I can see the whole request in the log file.

With buffering off it takes at least a minute.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netcat-service
  namespace: staging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netcat-service
  template:
    metadata:
      labels:
        app: netcat-service
    spec:
      containers:
        - name: netcat
          image: ubuntu:26.04
          command: [""/bin/sh"", ""-c""]
          args:
            - apt update && apt install netcat-openbsd && apt install less && nc -lk -p 8080 > /tmp/requests.log
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: netcat-service
  namespace: staging
spec:
  selector:
    app: netcat-service
  ports:
    - protocol: TCP
      name: http
      port: 80
      targetPort: 8080
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: netcat-ingress
  namespace: staging
  annotations:
    cert-manager.io/cluster-issuer: ""letsencrypt""
    nginx.ingress.kubernetes.io/limit-rps: ""10""
    nginx.ingress.kubernetes.io/proxy-body-size: ""600m""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
spec:
  ingressClassName: nginx
  rules:
    - host: mypublicdns
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: netcat-service
                port:
                  number: 80
  tls:
    - hosts:
        - mypublicdns
      secretName: netcat-service-tls
```",2025-12-08T10:35:24,2025-12-05T13:00:07
79837376,Evaluate Kubernetes CEL Condition locally using Golang,"I am creating a Kubernetes controller that needs to modify resources when specific conditions are met. I would like to define my match conditions using CEL expressions in a configuration file similar to how the ValidatingAdmissionPolicy and MutatingAdmissionPolicy contain a `matchConditions` block.

```
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingAdmissionPolicy
metadata:
  name: ""sidecar-policy.example.com""
spec:
  matchConditions:
    - name: does-not-already-have-sidecar
      expression: ""!object.spec.initContainers.exists(ic, ic.name == \""mesh-proxy\"")"" # CEL Expression
```

```
// custom config file for my controller with rules
{
  ""name"": ""my-container-cpu-overrides"",
  ""description"": ""Override CPU requests and limits for my-container if greater than 1000m"",
  ""matchCondition"": ""quantity(object.spec.template.spec.containers[indexOf('my-container')].resources.requests.cpu).isGreaterThan(quantity(\""1000m\""))]"", # CEL Expression
  ...
}
```

**How can I compile and evaluate the CEL expression locally within in my app using golang so that all existing Kubernetes CEL [libraries](https://kubernetes.io/docs/reference/using-api/cel/#kubernetes-list-library) are automatically imported and setup?** The examples and source code I have come across online are verbose and incorporate lots of complexity to instantiate a compiler + evaluator. [Example](https://github.com/kubernetes/apiextensions-apiserver/blob/5026f3934523aa2ca2da1cb6492f7a9e676aa342/pkg/apiserver/schema/cel/compilation.go#L110C1-L151C2).

For context, I was hoping to find a 2-liner example similar to how the regex package behaves that already bundles in all of the K8s CEL variables + funcs.

```
// Step 1 - Compile string-based expression
r, _ := regexp.Compile(""p([a-z]+)ch"")

// Step 2 - Evaluate + Match
fmt.Println(r.MatchString(""peach""))

// How can I implement these two steps using K8s CEL Expression libraries in golang?
```","go, kubernetes, common-expression-language",,,,2025-12-03T22:29:36
79832167,Resource propagation on multi-cloud environment using Karmada,"As far as i know, resources created on `karmada-api-server` would propagate to member clusters. ( based on propagation policies )

But i need to create a multi cluster synchronization.

> e.g. When a resource is created in member A, it then propagate to all other members. (Full Mesh)

Is it possible using `Karmada`? Or do I need to create a custom `k8s` controller which watches resources and clone them to `karmada-api-server` ?","kubernetes, karmada",,,,2025-11-28T00:51:43
79831797,Microsoft.ML C#: GPU not found in K8s/Docker container,"I have created a .NET app that uses Microsoft.ML.OnnxRuntime.Gpu for interference. Now I'm trying to integrate it with Azure Kubernetes.

We have made the setup with Tesla T4 GPU and we confirmed it's visible:

![enter image description here](https://i.sstatic.net/8IyE4lTK.png)

So we know that T4 is visible under ID = 0.

This is basically my code that **works locally on my Windows machine**:

```
MLContext _mlContext = new();

var estimator = _mlContext.Transforms.ApplyOnnxModel(
    modelFile: _modelFile,
    inputColumnNames: _inputColumnNames,
    outputColumnNames: _outputColumnNames,
    gpuDeviceId: gpuId
);
```

but when deploying to ACR, K8s etc., we are getting an exception:

`System.InvalidOperationException: GPU with ID 0 is not found.`

Bunch of information that I think may help.

- My local machine nvidia-smi log:

![enter image description here](https://i.sstatic.net/mLpTKgMD.png)

- libraries used:

```
    <PackageVersion Include=""Microsoft.ML"" Version=""3.0.1"" />
    <PackageVersion Include=""Microsoft.ML.ImageAnalytics"" Version=""3.0.1"" />
    <PackageVersion Include=""Microsoft.ML.OnnxRuntime"" Version=""1.20.1"" />
    <PackageVersion Include=""Microsoft.ML.OnnxRuntime.Gpu"" Version=""1.20.1"" />
    <PackageVersion Include=""Microsoft.ML.OnnxTransformer"" Version=""3.0.1"" />
```
- Image used in our Dockerfile:

```
nvidia/cuda:12.3.2-runtime-ubuntu22.04
```
- My local `nvcc --version`

`nvcc: NVIDIA (R) Cuda compiler driver`

`Copyright (c) 2005-2024 NVIDIA Corporation`

`Built on Wed_Oct_30_01:18:48_Pacific_Daylight_Time_2024`

`Cuda compilation tools, release 12.6, V12.6.85`

`Build cuda_12.6.r12.6/compiler.35059454_0`","c#, kubernetes, gpu, azure-aks, microsoft.ml",,,,2025-11-27T14:33:41
79830326,Socket with SignalR and kubernetes,"I encountered a problem when deploying a socket application using signalr. I connect with the longpolling method and it still works, but the websocket gives an error:  **Error: Failed to start the transport 'WebSockets': Error: WebSocket failed to connect. The connection could not be found on the server, either the endpoint may not be a SignalR endpoint, the connection ID is not present on the server, or there is a proxy blocking WebSockets. If you have multiple servers check that sticky sessions are enabled.**

I think my ingress configuration has problem, but I still don't find it. Hope anyone can help me.

this's my ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: socket-service-ingress
  namespace: dev
  annotations:
    nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection ""Upgrade"";
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/affinity: cookie
    nginx.ingress.kubernetes.io/session-cookie-name: SIGNALR-AFFINITY
    nginx.ingress.kubernetes.io/session-cookie-hash: sha1
spec:
  rules:
    - host: example.com
      http:
        paths:
          - path: /socket-service/mainHub
            pathType: Prefix
            backend:
              service:
                name: socket-service
                port:
                  number: 80
```",".net, kubernetes, websocket, signalr, kubernetes-ingress",,,,2025-11-26T04:16:42
79829452,Issue with Custom Engine Agent for Copilot – Service Not Responding After Deployment to Kubernetes,"I'm trying to create a Custom Engine Agent for Copilot. I have set up a bot, an app registration, and a manifest file. When I run the service locally using DevTunnel, everything works fine—the bot responds as expected in Teams chat and Copilot chat.

However, after deploying the service to a Kubernetes cluster (running in a pod), I updated the bot configuration endpoint accordingly. The service is accessible from Postman and responds correctly to API calls, so I'm confident it's active. It also works when tested via the *""Test in Web Chat""* feature on the bot configuration page.

The issue is that when I ask questions from Teams chat or Copilot chat, there is no response. I checked the pod logs, and there's no trace of any incoming requests from Teams or Copilot. The same manifest and app registration work perfectly when the service runs locally via DevTunnel.

Why is the bot is not receiving requests from Teams or Copilot after deployment to Kubernetes?

`manifest.json`:

```
{
    ""$schema"": ""https://developer.microsoft.com/json-schemas/teams/v1.22/MicrosoftTeams.schema.json"",
    ""manifestVersion"": ""1.22"",
    ""version"": ""7.0.2"",
    ""id"": """",
    ""developer"": {
        ""name"": """",
        ""websiteUrl"": ""..."",
        ""privacyUrl"": ""..."",
        ""termsOfUseUrl"": ""...""
    },
    ""name"": {
        ""short"": ""..."",
        ""full"": ""...""
    },
    ""description"": {
        ""short"": ""..."",
        ""full"": ""...""
    },
    ""icons"": {
        ""outline"": ""outline.png"",
        ""color"": ""color.png""
    },
    ""accentColor"": ""#FFFFFF"",
    ""bots"": [
        {
            ""botId"": ""correct-id-i-have-checked"",
            ""isNotificationOnly"": false,
            ""supportsFiles"": false,
            ""supportsCalling"": false,
            ""supportsVideo"": false,
            ""scopes"": [
                ""personal"",
                ""copilot"",
                ""team"",
                ""groupChat""
            ],
            ""commandLists"": [
                {
                    ""scopes"": [
                        ""personal"",
                        ""copilot"",
                        ""team"",
                        ""groupChat""
                    ],
                    ""commands"": []
                }
            ]
        }
    ],
    ""composeExtensions"": [],
    ""permissions"": [
        ""identity"",
        ""messageTeamMembers""
    ],
    ""copilotAgents"": {
        ""customEngineAgents"": [
            {
                ""id"": ""correct-bot-id"",
                ""type"": ""bot""
            }
        ]
    }
}
```","kubernetes, microsoft-teams, microsoft-copilot",,,,2025-11-25T08:41:25
79829096,"Does an EC2 instance automatically run my Dockerfile if I copy it onto the server, or do I need to reboot it first?","If I scp my Dockerfile into an EC2 instance, does it automatically become a container, or do I need to reboot the instance first? Also, is there a preferred AZ for this, or will us-east-2b do? I need answers please for me and my team.","azure, kubernetes",,,,2025-11-24T21:00:24
79825907,How does Java containers react when container runtime updates containers cgroup settings,"Kubernetes v1.33+ supports in-place pod resize. I am trying to understand how latest JDK versions react to an in-place pod resize i.e; when container runtime updates containers cgroup settings and updates cpu/memory request and limit, does that get reflected on the JVM automatically?

I couldn't find any doc that confirms the behavior.

Thanks,

Ravi","kubernetes, jvm",,,,2025-11-20T19:39:44
79824169,ActiveProcessorCount is showing &quot;-1&quot; even after specifying CPU limits in Container,"I have my application running on an AL2023 node in EKS cluster.

Below is the snippet of the Container resources in the Pod definition

```
resources:
# memory ~ heap space +10%
# CPU ~ no more than 1/8 of memory
  requests:
    cpu: 2000m
    memory: 22G
# CPU ~ 4x requested
  limits:
    cpu: 4000m
    memory: 24G
```

As per my understanding the ActiveProcessorCount value should be equal to CPU Limits which should 4, but it is showing as -1

```
java -XX:+PrintFlagsFinal -version | grep -E 'ActiveProcessorCount'
     intx ActiveProcessorCount                      = -1                                  {product}
```

Does this mean, it is not able to detect the Container Limits with cgroup v2 even with Container Support enabled? I am running openJDK version 1.8.0_392.

```
cat /proc/self/mounts | grep cgroup
cgroup /sys/fs/cgroup cgroup2 ro,seclabel,nosuid,nodev,noexec,relatime 0 0
```

```
java -XX:+PrintFlagsFinal -version | grep -i container
     bool PreferContainerQuotaForCPUCount           = true                                {product}
     bool UseContainerSupport                       = true                                {product}
```

```
java -version
Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
openjdk version ""1.8.0_392""
OpenJDK Runtime Environment (IcedTea 3.29.0) (Alpine 8.392.08-r0)
OpenJDK 64-Bit Server VM (build 25.392-b08, mixed mode)
```

Also, I have explicitly set JAVA_HEAP as 12000m. Ideally InitialHeapSize and MaxHeapSize should show the values as ""12582912000"" and ""12582912000"" when both -Xms and -Xmx value is set to 12000m. But it is automatically defaulting to the 1/InitialRAMFraction of Mem Limits and 1/MaxRAMFraction of Mem Limits values.","kubernetes, jvm, amazon-eks, cgroups, amazon-linux-2023",79824202.0,"`ActiveProcessorCount = -1` basically means the JVM isn’t picking up the CPU limits from your container. This is a common issue with Java 8 running on cgroup v2, which is what AL2023 uses. Even though container support is turned on, Java 8’s support for cgroup v2 is incomplete, especially in Alpine-based OpenJDK builds. Because of that, Java can’t reliably read CPU and memory limits.

When this happens, the JVM may also ignore your `-Xms` and `-Xmx` settings and instead fall back to its own default memory calculations.

How you can fix it:

1. The best solution is to move to Java 11 or Java 17, which fully support cgroup v2 and correctly detect container limits.
2. If you need to stay on Java 8, you can force the values:

`-XX:ActiveProcessorCount=4`

`-Xms12000m -Xmx12000m`

And if needed, disable container support entirely with `-XX:-UseContainerSupport` so the JVM doesn’t override your settings.

1. Another option is to switch to a non-Alpine Java 8 build, like Temurin, which works better with cgroup v2.

Good Luck!!

Thanks,
Rajat Mishra",2025-11-19T08:23:17,2025-11-19T07:44:58
79822514,java.net.UnknownHostException in a Helm Project,"What are the steps I should take to solve the error:

```
17-11-2025 14:00:00.002 [org.test.ingest.io] ERROR o.s.s.s.TaskUtils$LoggingErrorHandler.handleError - Unexpected error occurred in scheduled task
org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://fileprocessor:8081/fileprocessor/technical/checkAndRestartThread/restartThreads"": fileprocessor
    at org.springframework.web.client.RestTemplate.createResourceAccessException(RestTemplate.java:888)
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:868)
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:764)
    at org.springframework.web.client.RestTemplate.getForObject(RestTemplate.java:378)
    at org.test.ingest.io.technical.service.CheckAndRestartThreadService.restartThreads(CheckAndRestartThreadService.java:108)
    at jdk.internal.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:569)
    at org.springframework.scheduling.support.ScheduledMethodRunnable.run(ScheduledMethodRunnable.java:84)
    at org.springframework.scheduling.support.DelegatingErrorHandlingRunnable.run(DelegatingErrorHandlingRunnable.java:54)
    at org.springframework.scheduling.concurrent.ReschedulingRunnable.run(ReschedulingRunnable.java:96)
    at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)
    at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
    at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
    at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: java.net.UnknownHostException: fileprocessor
    at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:572)
    at java.base/java.net.Socket.connect(Socket.java:633)
    at java.base/java.net.Socket.connect(Socket.java:583)
    at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:183)
    at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:533)
    at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:638)
    at java.base/sun.net.www.http.HttpClient.<init>(HttpClient.java:283)
    at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:386)
    at java.base/sun.net.www.http.HttpClient.New(HttpClient.java:408)
    at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1325)
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1258)
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1144)
    at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:1073)
    at org.springframework.http.client.SimpleBufferingClientHttpRequest.executeInternal(SimpleBufferingClientHttpRequest.java:75)
    at org.springframework.http.client.AbstractBufferingClientHttpRequest.executeInternal(AbstractBufferingClientHttpRequest.java:48)
    at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:66)
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:862)
    ... 15 common frames omitted
```

I use Helm to deploy an app, this app contain an ingest part made of a fileProcessor part and Io part.

i use a common deployment and service file for both parts

here is the service

```
apiVersion: v1
kind: Service
metadata:
  name: ingest-{{ .Values.global.branch }}
  namespace: {{ default .Release.Namespace .Values.global.namespace }}
spec:
  selector:
    app: ingest-{{ .Values.global.branch }}
  ports:
    - name: fileprocessor
      port: {{ .Values.ingest.fileprocessorPort }}
      targetPort: {{ .Values.ingest.fileprocessorPort }}
    - name: io
      port: {{ .Values.ingest.ioPort }}
      targetPort: {{ .Values.ingest.ioPort }}
```

I think i should overwrite what adress they use in order to tell them to talk through the service but everything i tried didn't work.","kubernetes, kubernetes-helm",,,,2025-11-17T15:59:45
79821524,"appsettings.json not read after deploying to Minikube (values become empty, DB connection fails)","I'm running an ASP.NET Core application.

Everything works correctly when using **Docker Compose** — all values from *appsettings.json* load normally, and my services connect to Postgres and RabbitMQ.

But when I deploy the same application to **Minikube (Kubernetes)**, the app stops reading anything from *appsettings.json*.

In logs I see:

```
DEBUG: ConnectionStrings:Default = ''
DEBUG: RabbitMQ = ''
etc
```

And the application throws an error:

```
fail: Microsoft.EntityFrameworkCore.Database.Connection[20004]
An error occurred using the connection to database 'simpchat' on server 'tcp://postgres-service:5432'.
System.Net.Sockets.SocketException: Name or service not known
```

So two problems appear **only in Minikube**:

### 1. All values from appsettings.json become empty

Even though the file exists inside the container, the configuration keys like `ConnectionStrings:Default` and `RabbitMQ, etc` are always empty.

### 2. DNS name `postgres-service` cannot be resolved even if all values empty

The app tries to connect to

`tcp://postgres-service:5432`

but Kubernetes cannot resolve this hostname. But all my values are empty why it even throws?

So I need help with this.","c#, asp.net, kubernetes",,,,2025-11-16T14:22:19
79821111,ArgoCD ApplicationSet and Workflow to create ephemeral environments from GitHub branches,"How would you rate this GitOps workflow idea with ArgoCD + ApplicationSet + PreSync hooks?

In my organization we already use Argo CD for production and staging deployments. We're considering giving developers the ability to deploy any code version to temporary test environments aka ephemeral dev namespaces.

I would like feedback on the overall design and whether I'm reinventing the wheel or missing obvious pitfalls.

**Prerequisites**

- **infrastructure repo** – GitOps home: ArgoCD Applications, Helm charts, default values.
- **deployment-configuration repo** – environment-specific values.yaml files (e.g., image tags).
- ArgoCD Applications load **defaults** from infrastructure repo and **overrides** from deployment-configuration repo.
- All application services are **stateless**. Databases (MySQL/Postgres) are separate ArgoCD apps or external services like RDS.

# Ephemeral environment creation flow

1. Developer pushes code to a branch named dev/{namespace} (or via pull request)
2. GitHub Actions builds the image, pushes it to the registry, uploads assets to CDN, and updates the relevant values.yaml in the deployment-configuration repo with the image tag (e.g. commit sha).
3. ArgoCD ApplicationSet detects the values.yaml and creates a new Application.
4. ArgoCD runs a **PreSync hook** (or triggers an Argo Workflow) that is fully **idempotent**. Steps:

  - Create/update Doppler config, write some secrets, create service token to read this config, configure Doppler operator.
  - Create a database + DB user.
  - Create any external resources not part of the application Helm chart.
  - Wait until Doppler Operator creates the managed secret (it syncs every ~30s, so race conditions are possible).
5. **Sync Wave -2**: create dependencies that must exist before app deploy (Redis, ConfigMaps, etc.).
6. **Sync Wave -1**:

  - If DB is empty: load schema + seed data
  - Run DB migrations and other pre-deployment tasks
7. **Sync**: finally deploy the application.

# Update flow

Pretty much the same flow as create. Thanks to idempotency we can run exactly the same steps:

1. Developer pushes updates to the same branch.
2. GitHub Actions builds and pushes the image, updates values.yaml.
3. PreSync hook runs again but idempotently skips resource creation.
4. Sync Wave -2: update shared resources if needed.
5. Sync Wave -1: run database migrations and other pre-deployment tasks.
6. Sync: update deployment.

# Application deletion

- When the branch is deleted, ApplicationSet removes the Application.
- PostDelete hook cleans up: deletes Doppler config, drops DB, removes RabbitMQ vhosts, etc.

# Namespace recovery options

**Deep Clean**

- Developer manually deletes the ArgoCD Application.
- PostDelete hook removes all resources.
- ApplicationSet recreates the namespace from scratch automatically.

**Soft Clean**

- For instance, a developer wants to have a fresh database
- ..or database is corrupted (e.g., broken database migrations).
- Triggered via GitHub Workflow → Argo event → Argo Workflow.
- Workflow handles: drop DB → restore → reseed.

I am also considering adding simple lifecycle management to avoid hundreds of abandoned dev branches consuming cluster resources:

- Daily GitHub Workflow (cron) scans all dev/{namespace} branches.

  - If a branch has no commits for e.g., 14 days, the workflow commits an update to the corresponding values.yaml to scale replicas down to 0.
  - A new commit instantly bumps replicas back up because the build pipeline updates values.yaml again.
- If a branch has no commits for 30 days, the workflow deletes the branch and values.yaml entirely.

  - ApplicationSet reacts by deleting the namespace and running the PostDelete cleanup.

I'm Looking for feedback from people who have implemented similar workflows:

- Does this design follow common ArgoCD patterns?
- Can you see any major red flags or failure modes I should account for?","kubernetes, github-actions, cicd, argocd, argo-workflows",,,,2025-11-15T20:58:17
79820041,MongoDB 8.0: StaleDbVersion triggers shard-local majority write after step-up; fails with 2 data + 1 arbiter when one data pod is down,"Environment

```
Deployment: Kubernetes (RKE2), Bitnami MongoDB Sharded Helm chart
MongoDB: 8.0.8
mongosh: 2.5.0
Topology:
Config servers: 3 (CSRS)
Mongos: 3
Shards: 3 shards total; each shard is a replica set with 2 data-bearing members + 1 arbiter
Constraint: Topology is fixed (2 data + 1 arbiter per shard). We need resilience to a single data pod (node) outage.
```

When a shard loses one data-bearing member and a step-up occurs (new PRIMARY), the first access to some databases via mongos fails with:

```
MongoServerError[WriteConcernFailed]: waiting for replication timed out
```

Shard logs show that mongos (or the shard) triggers a databaseVersion metadata refresh on the shard (config.cache.databases) to handle StaleDbVersion. That refresh writes with writeConcern: ""majority"" on the shard. In a 2 data + 1 arbiter RS with one data node down, “majority” is 2 but only one data-bearing member can acknowledge, so the refresh times out.

Oddly, some databases “work” (no refresh needed at that moment), while others consistently fail until the shard regains data-bearing majority.

On the affected shard (source DB “credential” below), listCollections via mongos triggers a refresh that fails:

```
{""t"":{""$date"":""2025-11-13T12:45:24.455+00:00""},""s"":""D1"",""c"":""SH_REFR"",""id"":24100,""svc"":""S"",""ctx"":""CatalogCache-23"",""msg"":""Error refreshing cached database entry"",""attr"":{""db"":""credential"",""durationMillis"":60010,""error"":""WriteConcernFailed: waiting for replication timed out""}}
{""t"":{""$date"":""2025-11-13T12:45:24.455+00:00""},""s"":""E"",""c"":""SHARDING"",""id"":6697205,""svc"":""-"",""ctx"":""Sharding-Fixed-14"",""msg"":""Failed database metadata refresh"",""attr"":{""db"":""credential"",""error"":""WriteConcernFailed: waiting for replication timed out""}}
{""t"":{""$date"":""2025-11-13T12:45:24.455+00:00""},""s"":""I"",""c"":""SHARDING"",""id"":22065,""svc"":""S"",""ctx"":""conn725"",""msg"":""Failed to refresh databaseVersion"",""attr"":{""db"":""credential"",""error"":""WriteConcernFailed: waiting for replication timed out""}}
{""t"":{""$date"":""2025-11-13T12:45:24.455+00:00""},""s"":""I"",""c"":""COMMAND"",""id"":51803,""svc"":""S"",""ctx"":""conn725"",""msg"":""Slow query"",""attr"":{""type"":""command"",""ns"":""credential.$cmd"",""command"":{""listCollections"":1,""nameOnly"":true,""databaseVersion"":{""uuid"":{""$uuid"":""c511729b-0757-4e92-a94a-6052482226dc""},""timestamp"":{""$timestamp"":{""t"":1763022815,""i"":2}},""lastMod"":1}},""databaseVersionRefreshDurationMillis"":60011,""ok"":0,""errName"":""WriteConcernFailed"",""errCode"":64,""errMsg"":""waiting for replication timed out""}}
```

On the same shard, hello shows majority cannot advance while single data-bearing member is up:

```
{
  opTime: { ts: Timestamp({ t: 1763039352, i: 1 }), t: Long('12') },
  lastWriteDate: ISODate('2025-11-13T13:09:12.000Z'),
  majorityOpTime: { ts: Timestamp({ t: 1763034662, i: 1 }), t: Long('12') },
  majorityWriteDate: ISODate('2025-11-13T11:51:02.000Z')
}
```

Replica set status reflects the topology:

```
writeMajorityCount: 2
votingMembersCount: 3  // 2 data + 1 arbiter
```

Arbiters vote but do not acknowledge writes; with one data node down, shard-majority writes cannot succeed. Is there a configuration option to set the `writeMajorityCount` in a replica set?","mongodb, kubernetes",,,,2025-11-14T12:45:43
79819874,terraform issue when i add new value in config map,"I’m trying to add a new record to my ConfigMap. It’s the third time I’ve done this operation – I made the previous edits in dev and stage – but in prod, when I add the record, Terraform tries to remove all the records in `data`.

This is the code and the issue in the `tfplan`.

```
 data = {
    ""APP_NAME""             = ""Nuovo prod""
    ""APP_FE_URL""           = ""https://prod.it""
    + ""APP_ENV""              = ""prod""
```

and this is the plan

```
 ~ resource ""kubernetes_config_map_v1"" ""laravel_env"" {
      ~ data        = {
          - ""APP_NAME""               = ""Nuovo prod""
          - ""APP_FE_URL""             = ""https://prod.it""
          - ""APP_ENV""                = prod
.....
Plan: 1 to add, 9 to change, 0 to destroy.
```

there is other 9 resource changed and it's doesent make this errore

```
  # module.viewer.mongodbatlas_database_user.user will be updated in-place
  ~ resource ""mongodbatlas_database_user"" ""user"" {
        id                 = ""YXV""
        # (8 unchanged attributes hidden)

      + scopes {
          + name = ""prod-db-log""
          + type = ""CLUSTER""
        }
```","azure, kubernetes, terraform",79819897.0,"after some test i found the issue, it's generate by a parameter in config map

```
   ""MONGO_LOG_CONNECTION_STRING"" = local.mongo_connection_str_app_log
```

The problem is caused by the string requesting values ​​that will be available with the same apply that's trying to modify the config. That is, I'm creating the database and simultaneously setting it on the config map.

So I wanted to understand if my theory is correct.",2025-11-14T10:10:52,2025-11-14T09:55:42
79819869,Failed to bind properties under &#39;server.address&#39; with helm,"I want to deploy my app on kubernetes with Helm

I have a Ingest Service running with spring boot that is made of two parts

- Fiprocessor
- Io

Both of this part use the same deployment file and the same Service

```
spec:
  replicas: {{ .Values.ingest.replicas }}
  selector:
    matchLabels:
      app: ingest-{{ .Values.global.branch }}
  template:
    metadata:
      labels:
        app: ingest-{{ .Values.global.branch }}
    spec:
      containers:
        - name: fileprocessor
          image: ""{{ .Values.ingest.fileprocessorRepository }}:{{ tpl .Values.ingest.tag $ }}""
          ports:
            - containerPort: {{ .Values.ingest.fileprocessorPort }}
          envFrom:
            - configMapRef:
                name: tryphon-config-{{ .Values.global.branch }}
        - name: io
          image: ""{{ .Values.ingest.ioRepository }}:{{ tpl .Values.ingest.tag $ }}""
          ports:
            - containerPort: {{ .Values.ingest.ioPort }}
          envFrom:
            - configMapRef:
                name: tryphon-config-{{ .Values.global.branch }}
---
apiVersion: v1
kind: Service
metadata:
  name: ingest-{{ .Values.global.branch }}
  namespace: {{ default .Release.Namespace .Values.global.namespace }}
spec:
  selector:
    app: ingest-{{ .Values.global.branch }}
  ports:
    - name: fileprocessor
      port: {{ .Values.ingest.fileprocessorPort }}
      targetPort: {{ .Values.ingest.fileprocessorPort }}
    - name: io
      port: {{ .Values.ingest.ioPort }}
      targetPort: {{ .Values.ingest.ioPort }}
```

i use the value.yaml file to overwrite the server.address for both of them and point it toward the service

```
  FILEPROCESSOR_ADDRESS: ""ingest-{{ .Values.global.branch }}""
  IO_ADDRESS: ""ingest-{{ .Values.global.branch }}""
```

In my properties file for both FileProcessor and Io i used spring variables

```
server.address=${FILEPROCESSOR_ADDRESS}
```

```
server.address=${IO_ADDRESS}
```

Everything work fine in minikube for testing but when i push on the kubernetes my FileProcessor doesn't work anymore

```
 Failed to bind properties under 'server.address' to java.net.InetAddress:
    Property: server.address
    Value: ""fileprocessor""
    Origin: URL [file:/config/fileProcessor.properties] - 6:16
    Reason: failed to convert java.lang.String to java.net.InetAddress (caused by java.net.UnknownHostException: fileprocessor: Name or service not known)
```","kubernetes, kubernetes-helm",79819952.0,"The Spring Boot [`server.address`](https://docs.spring.io/spring-boot/appendix/application-properties/index.html#application-properties.server.server.address) proprety sets the address the main Java process listens on; there is a separate `server.port` setting for the port number.  The IP address in it needs to be an IP address the container actually has, which is hard to determine statically.  In particular, `server.address` can't be set to the name of a Service because no individual Pod will have the Service's cluster-internal IP address, each Pod has its own address.

In a container context the address almost always needs be set to the special ""all interfaces"" address 0.0.0.0.  This is the default setup (as it is for most non-development servers) and you can just delete the `spring.address` setting entirely, along with the machinery that provides unique names for it.",2025-11-14T11:06:29,2025-11-14T09:53:06
79817800,TLS Handshake disappearing when modifying it with eBPF (TC egress),"I have two k8s pods that communicate using TLS. I am loading an eBPF TC code on the egress of the sender pod. This code adds 28 bytes to the optional space of the TCP headers after TCP options. If I add these bytes only to TLS Application Data, everything works fine. Instead, when I add the bytes to TLS Handshake packets, the packets are correctly modified by the eBPF and released (return TC_ACT_OK;), but I cannot observe them with wireshark coming out of the pod. Notice that in both cases, TCP Option is 12 bytes long in the original packet. This is the code:

```
int egress_packet(struct __sk_buff *skb){
    //...
                int flag = 0;
                if(data + payload_offset + 1 > data_end){
                    if(data_end - data < skb->len){
                        ret = bpf_skb_pull_data(skb, skb->len);
                        flag = 1;
                        if(ret < 0)
                            return TC_ACT_SHOT;
                        }
                    } else {
                        // there is no non-linear part
                    }

                //...

// modify the packet
                struct ethhdr eth_copy;
                struct iphdr ip_copy;
                struct tcphdr tcp_copy;
                struct tag_hdr tag_copy;
                long inner_ret;

                if(data + sizeof(*eth) + sizeof(*ip) + sizeof(*tcp) + 12 > data_end){ /* 12 is opts length */
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_load_bytes(skb, 0, &eth_copy, sizeof(eth_copy));
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_load_bytes(skb, sizeof(*eth), &ip_copy, sizeof(ip_copy));
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_load_bytes(skb, sizeof(*eth) + sizeof(*ip), &tcp_copy, sizeof(tcp_copy));
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                u8 opts[12];
                ret = bpf_skb_load_bytes(skb, sizeof(*eth) + sizeof(*ip) + sizeof(*tcp), &opts[0], 12);
                if(ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_adjust_room(skb, 28, BPF_ADJ_ROOM_MAC, BPF_F_ADJ_ROOM_FIXED_GSO);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                int tmp = bpf_ntohs(ip_copy.tot_len);
                tmp += 28;
                ip_copy.tot_len = bpf_htons(tmp);

                tmp = tcp_copy.doff;
                tmp += 7;
                tcp_copy.doff = tmp;

                /* need recasting after bpf_skb_adjust_room */
                data = (void *)(long)skb->data;
                data_end = (void *)(long)skb->data_end;

                if(data + sizeof(*eth) + sizeof(*ip) + sizeof(*tcp) + 12 + 28 > data_end){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_store_bytes(skb, 0, &eth_copy, sizeof(eth_copy), 0);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_store_bytes(skb, sizeof(*eth), &ip_copy, sizeof(ip_copy), 0);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_store_bytes(skb, sizeof(*eth) + sizeof(*ip), &tcp_copy, sizeof(tcp_copy), 0);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                inner_ret = bpf_skb_store_bytes(skb, sizeof(*eth) + sizeof(*ip) + sizeof(*tcp), &opts, sizeof(opts), 0);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                uint8_t new[28];
                //
                // Loading new vector
                //

                // Storing new vector
                inner_ret = bpf_skb_store_bytes(skb, sizeof(*eth) + sizeof(*ip) + sizeof(*tcp) + sizeof(opts), new, sizeof(new), 0);
                if(inner_ret){
                    return TC_ACT_SHOT;
                }

                /* fix checksum */
                data = (void *)(long)skb->data;
                data_end = (void *)(long)skb->data_end;

                if(data + sizeof(*eth) > data_end){
                    return TC_ACT_SHOT;
                }
                eth = data;
                if(data + sizeof(*eth) + sizeof(*ip) > data_end){
                    return TC_ACT_SHOT;
                }
                ip = data + sizeof(*eth);

                ip->check = 0;
                __u64 csum = 0;
                ipv4_csum(ip, sizeof(*ip), &csum);
                ip->check = csum;

                if(data + sizeof(*eth) + sizeof(*ip) + sizeof(*tcp) > data_end){
                    return TC_ACT_SHOT;
                }
                tcp = data + sizeof(*eth) + sizeof(*ip);

                return TC_ACT_OK;
```

The only differences I noticed between Handshake and Application data packets are that:

1. Handshake packets are way bigger than Application Data packets (~1500 bytes vs ~150)
2. Handshake packets have non-linear data whereas Application Data packets don't

I believe the problem lays in adjusting a packet that has non-linear data. In fact, with Handshake packets, the Client Hello gets sent out of the pod if I return before calling bpf_skb_adjust_room, but it doesn't if I return after adjusting and updating ip_copy.tot_len and tcp_copy.doff.

I am loading the program to TC egress as such (using BCC from Python):

```
f_out = b.load_func(""egress_packet"", BPF.SCHED_CLS)
ipr = pyroute2.IPRoute()
eth = ipr.link_lookup(ifname=""eth0"")[0]
ipr.tc(""add"", ""clsact"", eth)
ipr.tc(""add-filter"", ""bpf"", eth, "":1"", fd=f_out.fd, name=f_out.name, parent=""ffff:fff3"", classid=1, direct_action=True)
```

Why am I having this issue and how could I solve it?

I am running my code on Ubuntu 24.02, kernel 6.14.0-35-generic with BCC version 0.29.1.

Thanks!!","kubernetes, linux-kernel, ebpf, tls1.3, bcc-bpf",,,,2025-11-12T13:36:18
79815411,YQ: load a text file as array and use it for an operation,"I have a yaml file with an arbitrary amount of documents, and I'm trying to replace all missing namespaces for namespaceable resources with an arbitrary input one.

Getting the non-namespaceable resources is easy:

```
kubectl api-resources --namespaced=false --no-headers | awk '{print $NF}' > /tmp/bad_resources.yaml
```

The problem is using this list in YQ (mike farah's).

This code works for hardcoded resources:

```
      NAMESPACE=""$NAMESPACE"" yq  '
        select(.kind != ""Namespace"" and .kind != ""CustomResourceDefinition"") |
        .metadata.namespace = (.metadata.namespace // strenv(NAMESPACE))
      ' ""$INPUT"" > ""$OUTPUT""
```

How can I replace this hardcoded list with the list I generated via `kubectl`?

I'm kind of going crazy with this, even LLMs utterly fail at this and keep mistaking `yq` versions and suggesting input arguments that don't exist.

Sample yaml:

```
---
kind: Namespace
metadata:
  name: test
---
kind: ConfigMap
metadata:
  name: test1
  namespace: asd
---
kind: ConfigMap
metadata:
  name: test2
```

In this example, it should be able to add the namespace to the Configmap `test2`, but not change `test1`, nor add it to `Namespace`, because `Namespace` is not a namespaceable resource. The output should be the same, except for the added namespace, so the last resource should have a new `metadata.namespace` field with the input namespace.

The `kubectl` list of resources looks like this:

```
Namespace
Node
PersistentVolume
```

Given that I'm generating it with the command I posted above, I can manipulate this, so it could also be a yaml array.","bash, kubernetes, yq",79815454.0,"Use `load_str` to load a text file, `/` to split by lines, and `all_c` to check against all items:

```
NAMESPACE=""nsp"" goyq '
  (load_str(""list.txt"") / ""\n"") as $list
  | select(.kind as $kind | $list | all_c(. != $kind))
    .metadata.namespace |= . // strenv(NAMESPACE)
' sample.yaml
```

```
---
kind: Namespace
metadata:
  name: test
---
kind: ConfigMap
metadata:
  name: test1
  namespace: asd
---
kind: ConfigMap
metadata:
  name: test2
  namespace: nsp
```

using [mikefarah/yq](https://github.com/mikefarah/yq) v4.32+

(Replacing `/ ""\n""` with `| split(""\n"")` will make it work with v4.18+)",2025-11-10T10:01:52,2025-11-10T09:37:46
79813245,Why is container.cpu.usage metric from kubeletstats receiver bigger than k8s.pod.cpu.usage metric from the same receiver,"I have deployed opentelemetry-collector-contrib 0.139.0 to a Kubernetes cluster as a DaemonSet. The Kubernetes cluster is deployed on AWS EKS with a Node group of two EC2 Nodes. Kubernetes version is currently 1.34. The DaemonSet YAML of the collector(extracted using `kubectl get daemonset -o yaml`):

```
apiVersion: apps/v1
kind: DaemonSet
metadata:
  annotations:
    deprecated.daemonset.template.generation: ""1""
  creationTimestamp: ""2025-11-07T08:19:48Z""
  generation: 1
  labels:
    app: otel-collector
  name: otel-collector
  namespace: observability-namespace
  resourceVersion: ""22385414""
  uid: 28c2f297-7404-439c-90d3-eec673c92e0e
spec:
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
      - command:
        - /otelcol-contrib
        - --config
        - /conf/otel-collector-config.yaml
        env:
        - name: K8S_NODE_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: spec.nodeName
        image: otel/opentelemetry-collector-contrib:0.139.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 1
          httpGet:
            path: /
            port: 13133
            scheme: HTTP
          periodSeconds: 30
          successThreshold: 1
          timeoutSeconds: 5
        name: otel-collector
        ports:
        - containerPort: 4318
          protocol: TCP
        - containerPort: 13133
          protocol: TCP
        resources: {}
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /conf
          name: otel-collector-config-vol
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: otel-account
      serviceAccountName: otel-account
      terminationGracePeriodSeconds: 30
      volumes:
      - configMap:
          defaultMode: 420
          items:
          - key: otel-collector-config
            path: otel-collector-config.yaml
          name: otel-config
        name: otel-collector-config-vol
  updateStrategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
status:
  currentNumberScheduled: 2
  desiredNumberScheduled: 2
  numberAvailable: 2
  numberMisscheduled: 0
  numberReady: 2
  observedGeneration: 1
  updatedNumberScheduled: 2
```

The otel collector uses this configuration put inside `otel-config` ConfigMap:

```
exporters:
  prometheusremotewrite:
    auth:
      authenticator: sigv4auth/metrics
    endpoint: https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/xxxxxxxxxxxxxxxx/api/v1/remote_write
    resource_to_telemetry_conversion:
      enabled: true
extensions:
  health_check:
    endpoint: 0.0.0.0:13133
  sigv4auth/metrics:
    region: eu-central-1
    service: aps
receivers:
  kubeletstats:
    auth_type: serviceAccount
    collection_interval: 20s
    endpoint: https://${env:K8S_NODE_NAME}:10250
service:
  extensions:
    - sigv4auth/metrics
    - health_check
  pipelines:
    metrics:
      exporters:
        - prometheusremotewrite
      receivers:
        - kubeletstats
```

I have a Grafana deployment on AWS with Prometheus, also using the AWS managed service. When I query metrics using Grafana for my cluster, I see two metrics: `container.cpu.usage` and `k8s.pod.cpu.usage`. On this same Kubernetes cluster, I'm deploying a Python bot with Docker.

I understand the difference between `container.cpu.usage` and `k8s.pod.cpu.usage`, as a pod can contain multiple containers and a container name can belong to multiple pods(when new versions are deployed). For this particular Python bot deployment, its pod only has one container.

I did an average query over the last month in Grafana by `k8s_container_name` and `k8s_pod_name` labels for both metrics(`k8s.pod.cpu.usage` only by `k8s_pod_name` since it doesn't have a `k8s_container_name`). I see that in the last month, my Python bot had only one pod name for both metrics.
Yet `container.cpu.usage` reported 0.8 average value at one time in the last month, while `k8s.pod.cpu.usage` metric never reported more than 0.02 for any pod, including the previous Python bot pod, in the whole month.

How can that be? How can `container.cpu.usage` be way bigger than `k8s.pod.cpu.usage`? Maybe I'm misunderstanding these two metrics?

Here is the Python bot Kubernetes deployment file(got it through `kubectl get deploy -o yaml`)

```
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""28""
  creationTimestamp: ""2025-08-20T07:58:23Z""
  generation: 28
  labels:
    app: xxxxx
  name: xxxxx
  namespace: xxxx-namespace
  resourceVersion: ""22191731""
  uid: d972b210-d072-4a0d-94a0-4bd26cf05566
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: xxxxxxxx
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: xxxxxx
    spec:
      containers:
      - image: xxx-imageUri
        imagePullPolicy: IfNotPresent
        name: xxxxxxx
        resources: {}
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsGroup: 1000
          runAsNonRoot: true
          runAsUser: 1000
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: {}
      serviceAccount: xxxx-service-account
      serviceAccountName: xxxx-service-account-name
      terminationGracePeriodSeconds: 30
status:
  availableReplicas: 1
  conditions:
  - lastTransitionTime: ""2025-11-06T09:04:12Z""
    lastUpdateTime: ""2025-11-06T09:04:12Z""
    message: Deployment has minimum availability.
    reason: MinimumReplicasAvailable
    status: ""True""
    type: Available
  - lastTransitionTime: ""2025-08-20T07:58:23Z""
    lastUpdateTime: ""2025-11-06T15:44:37Z""
    message: ReplicaSet ""xxxxxxxxxxx"" has successfully progressed.
    reason: NewReplicaSetAvailable
    status: ""True""
    type: Progressing
  observedGeneration: 28
  readyReplicas: 1
  replicas: 1
  updatedReplicas: 1
```

This is the Dockerfile of the Python bot:

```
# Dockerfile
# --- Builder stage ---
FROM python:3.14.0-alpine3.22 AS builder

WORKDIR /install

COPY requirements.txt .
RUN pip install --prefix=/install/deps --no-cache-dir -r requirements.txt

# --- Runtime stage ---
FROM python:3.14.0-alpine3.22

ENV PYTHONDONTWRITEBYTECODE=1
ENV PYTHONUNBUFFERED=1

WORKDIR /app

# Copy only the installed dependencies from builder
COPY --from=builder /install/deps /usr/local

# Copy each python file
# our python bot is small composed of only 3 files
# the actual names is different and not important
COPY file1.py .
COPY file2.py .
COPY file3.py .

CMD [""python"", ""file1.py""]
```","kubernetes, metrics, open-telemetry-collector",,,,2025-11-08T14:00:48
79812926,Error in V1Binding when using a custom scheduler,"I encountered the error

> ```
> V1_binding.py"", line 156, in target
>     raise ValueError(""Invalid value for `target`, must not be `None`"")
> ValueError: Invalid value for `target`, must not be `None`
> ```

I saw that this issue already existed back in 2018, but I’m still experiencing it. From what I can tell, the workaround is to mask the exception because, even though the exception occurs, the pod actually gets assigned to a node and executes. I have verified that the node assignment does happen.

However, it is unclear whether the assignment is actually done by my custom scheduler that triggered the error. When I inspect the pod, it doesn’t show that the assignment was performed by my custom scheduler. One proposed solution is to add _preload_content=False to the v1.create_namespaced_binding call.

The _preload_content=False option prevents the object from being serialized and causes the exception. The question is: if this bug has existed since 2018 and has not been resolved, why is it marked as closed? Because when I inspect the pod that should theoretically be assigned by my scheduler, it is clear that it wasn’t.

The possibilities seem to be:
a) The pod is assigned by my custom scheduler, but when using _preload_content=False, the pod’s metadata is not updated after assignment, and it still shows that the default scheduler assigned the pod to the node where it runs.
b) It never worked, and the default scheduler always assigns the pod, even if the pod YAML explicitly specifies my custom scheduler.

Can anyone clarify this? How can I determine who actually assigned the node for my pod?

After running some tests, I observed that the Pod is indeed assigned to a node:

> kubectl -n test-scheduler get pod test-pod -w
>
>
> NAME       READY   STATUS            RESTARTS   AGE test-pod   0/1
>
> Pending           0          0s test-pod   0/1     ContainerCreating 0
> 0s test-pod   1/1     Running           0          3s

By reviewing the events in the namespace where the Pod is located and the logs of my-scheduler, we can confirm that it is indeed my custom scheduler (my-scheduler) that assigned the node to the Pod.

> kubectl -n test-scheduler get events --field-selector
> involvedObject.name=test-pod --sort-by='.metadata.creationTimestamp'
>
>
> LAST SEEN   TYPE     REASON    OBJECT         MESSAGE 53s
>
> Normal   Pulled    pod/test-pod   Container image
> ""registry.k8s.io/pause:3.9"" already present on machine 53s
>
> Normal   Created   pod/test-pod   Created container pause 53s
>
> Normal   Started   pod/test-pod   Started container pause

And:

> kubectl -n kube-system logs -f my-scheduler-6fbbc9c795-pfxw9 [polling]
> scheduler starting… name=my-scheduler Bound test-scheduler/test-pod ->
> sched-lab-control-plane","python-3.x, kubernetes, scheduler",,,,2025-11-08T01:53:26
79812107,Open resty kubernetes frontend,"User-->Load abalancer-Frontend VPC-Openresty Nginx --->Router-->Priv LB -->Kubernetes

I want to ask about Openresty in Kubernetes. I am using it now as reverse Proxy as Tasks in AWS and then goes into ECS and apps in AWS.

So now everything is in Tasks in AWS ECS.

And have to go into EKS or kubernetes.

So how to deploy openresty Nginx in frontend VPC in kubernetes? Just as a container with a service and then the traffic goes by to the Kubernetes where apps are running","amazon-web-services, kubernetes, nginx-reverse-proxy, nginx-ingress, openresty",,,,2025-11-07T08:54:32
79809902,Java 21+ container support: active processor count,"In containers running Java 21+, I would like to have the active processor count reflect the actual available hardware cores, regardless of configured CPU shares.

CSR [JDK-8281571](https://bugs.openjdk.org/browse/JDK-8281571) ""Do not use CPU Shares to compute active processor count"" explains the problem with using Docker/Kubernetes CPU resource limits to determine the active processor count with `-XX:+UseContainerSupport` and I understand it to have changed this.

In Java 21 however, the deprecated flags controlling this (`UseContainerCpuShares` and `PreferContainerQuotaForCPUCount`) are gone, and I cannot find documentation on what the situation is supposed to be wrt container CPU limits.

The behavior in Kubernetes, with resource limits like this:

```
   resources:
      limits:
         cpu: 1000m
```

is that the active processor count is limited to `1` with all the problems described in JDK-8281571 (`Runtime.getRuntime().availableProcessors()` returns `1`).

I could override this explicitly with `-XX:ActiveProcessorCount=n`, but this requires explicitly configuring the concrete processor count, which might change with node updates and tends to be missed during updates.

I tried setting `-XX:ActiveProcessorCount=-1`, however this does not do anything.","java, kubernetes, jvm",,,,2025-11-05T10:17:56
79808800,dockerhub.io officially removed openjdk:8-jre from thier repository and what is he best alternative for it,"While building the image for one of our application, I observed that I am unable to pull openjdk:8-jre from official docker.io

error: [ERROR]: [#3 ERROR: docker.io/library/openjdk:8-jre: not found]

I have searched the dockerhub as well and can see the deprecation notice however, no confirmation.","docker, kubernetes, docker-registry",79808845.0,"You got this error because the official docker image of the openjdk is deprecated.So the specific image you tried is not present in the official library anymore

Some examples of other Official Image alternatives

- [`amazoncorretto`](https://hub.docker.com/_/amazoncorretto)
- [`eclipse-temurin`](https://hub.docker.com/_/eclipse-temurin)
- [`ibm-semeru-runtimes`](https://hub.docker.com/_/ibm-semeru-runtimes)
- [`ibmjava`](https://hub.docker.com/_/ibmjava)
- [`sapmachine`](https://hub.docker.com/_/sapmachine)

Please verify that your app working fine with new  image

checkout this **DEPRECATION NOTICE**

[https://hub.docker.com/_/openjdk](https://hub.docker.com/_/openjdk)",2025-11-04T11:26:40,2025-11-04T10:20:38
79808473,KubernetesPodOperator - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1006),"I’m running Apache Airflow inside a Docker container and trying to use the KubernetesPodOperator to run a simple “hello world” pod in an external Kubernetes cluster (not the same one where Airflow runs).

Here’s the task definition:

```
task = KubernetesPodOperator(
    task_id=""teste_k8s"",
    name=""teste-pod"",
    namespace=""test-kube-operator"",
    image=""python:3.11"",
    cmds=[""echo""],
    arguments=[""hello from airflow""],
    in_cluster=False,
    hostnetwork=True,
    config_file=""/home/airflow/.kube/config"",
    get_logs=True,
)
```

To access this cluster, I need a VPN.
Initially, I thought the VPN might be the issue, but I can successfully run `kubectl`
commands from inside the same Airflow container, using the same kubeconfig file.

I also tested connecting with Python code (using the official Kubernetes client), and it worked once I disabled SSL verification.
So it doesn’t seem to be a kubeconfig or network issue.

I’ve also tried:

- Disabling SSL verification (works in pure Python but not via the operator)
- Changing Airflow and Python versions
- Using different Kubernetes provider versions
- Also tried with kubernetes conn

But I still get the same connection errors when running the KubernetesPodOperator.

Is there any specific configuration or environment variable required for the KubernetesPodOperator to connect to an external cluster (with VPN access)?
Why would kubectl work inside the container, but not the operator?","kubernetes, ssl, airflow, kubernetespodoperator",,,,2025-11-04T02:31:37
79804562,How to translate caddy to ingress nginx controller,"I'm having this config from Caddy and I want to migrate it to ingress nginx controller

```
    @restrictAccess {
        path /path1/loc1/*
        path /path2/loc3/*
    }
    route @restrictAccess {
        forward_auth check-auth:1221 {
            uri /review/request
            copy_headers Cookie
            @deniedAccess status 403
            handle_response @deniedAccess {
                respond ""Access denied!"" 403
            }
        }

        @pathOrigin header Origin *
        header @pathOrigin {
            +Vary ""Origin""
            +Access-Control-Allow-Credentials ""true""
            +Access-Control-Allow-Origin ""{http.request.header.Origin}""
        }
    }
```

What I'm having right now for ingress is:
(LE with the solution maybe will help someone else)

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://check-auth.default.svc.cluster.local:1221//review/request
    nginx.ingress.kubernetes.io/auth-snippet: |
       if ( $request_uri !~ ^/path1/loc1/ ) {
         return 200;
       }
    nginx.ingress.kubernetes.io/configuration-snippet: |
       if ( $request_uri ~ ^/path1/loc1/ ) {
         more_set_headers ""Access-Control-Allow-Origin: $http_origin"";
         more_set_headers ""Access-Control-Allow-Credentials: true"";
         more_set_headers ""Vary: Origin"";
         more_set_headers ""Cookie: $http_cookie"";
       }
  name: ingress-1
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: page
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
```

but don't know how to actually finish this.

Any help is more than welcome.","kubernetes, nginx, nginx-ingress, caddy, caddyfile",79805658.0,"Founded the solution.

Use of `auth-url` and `auth-snippet` will do the trick

The end result will look like:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://check-auth.default.svc.cluster.local:1221//review/request
    nginx.ingress.kubernetes.io/auth-snippet: |
       if ( $request_uri !~ ^/path1/loc1/ ) {
         return 200;
       }
    nginx.ingress.kubernetes.io/configuration-snippet: |
       if ( $request_uri ~ ^/path1/loc1/ ) {
         more_set_headers ""Access-Control-Allow-Origin: $http_origin"";
         more_set_headers ""Access-Control-Allow-Credentials: true"";
         more_set_headers ""Vary: Origin"";
         more_set_headers ""Cookie: $http_cookie"";
       }
  name: ingress-1
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: page
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
```",2025-10-31T10:24:37,2025-10-30T09:06:22
79801711,How to overwrite an env value with Helm,"I want to deploy an app on Kubernetes with Helm. This app is composed of multiple parts, 2 of them are a Spring backend and a Mongo database.

I want to deploy theme in 2 pods and have them talk with each other, so I set up a service to allow my DB and my backend to talk.

Here is my service:

```
apiVersion: v1
kind: Service
metadata:
  name: mongo-{{ .Values.global.branch }}
  namespace: {{ default .Release.Namespace .Values.global.namespace }}
spec:
  selector:
    app: mongo-{{ .Values.global.branch }}
  ports:
    - port: {{ .Values.mongo.port }}
      targetPort: {{ .Values.mongo.port }}
```

Here is some of my `values.yaml` file

```
global:
  namespace: """"
  branch: ""poc-cicd""

backend:
  repository: backcicd
  tag: ""{{ .Values.global.branch }}""
  replicas: 1
  port: 8080

mongo:
  repository: mongocicd
  tag: ""{{ .Values.global.branch }}""
  port: 27017
  pullPolicy: IfNotPresent
```

Here is where I override the URL of my Mongo pod in the `backend-deployment.yaml` file:

```
          env:
            - name: MONGO_URL
              value: ""mongodb://root:pass@mongo-{{ .Values.global.branch }}:{{ .Values.mongo.port }}""
```

Everything works when I try it in Minikube but when I push to test on the real cluster I get this error:

```
Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongo:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongo}, caused by {java.net.UnknownHostException: mongo}}]
```

here is the top of the error

```
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'app': Unsatisfied dependency expressed through field 'sched': Error creating bean with name 'schedulerFactoryBean' defined in class path resource [org/poc/backend/app/scheduler/QuartzConfig.class]: Error while initializing the indexes
```","mongodb, kubernetes, kubernetes-helm",79836288.0,"You shold use the FQDN and PORT of the service for the Mongo URL, which is composed of `<service-name>.<namespace>.svc.cluster.local:<port>`

Considering you are deploying on the `default` namespace it should be:

```
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-{{ .Values.global.branch }}.default.svc.cluster.local:{{ .Values.mongo.port }}""
```

More information here [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services)

Also make sure to verify your variables are getting correctly replaced by doing a helm template:

`helm template <release-name> <chart-path-or-name> -f values.yaml`

Or test harcoding the MONGO_URL for now to discard any issue with variable replacement:

```
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-poc-cicd.default.svc.cluster.local:27017""
```",2025-12-02T20:03:09,2025-10-27T10:37:26
79798908,How can I support multiple OIDC authentication providers?,"I have built a Blazor standalone webassembly in .NET 9 for a new application that we are building in our company and have successfully integrated it into our Okta system for authentication of the user.

The requirements have now changed and I need to support 2 different Okta integrations, one for internal users on our network and one for external users that will access the application over the internet (the two instances need to be kept separate for security reasons).

The code that I currently have is:

```
services.AddOidcAuthentication(options =>
{
    options.ProviderOptions.ResponseType = ""code"";
    options.ProviderOptions.Authority = configuration[""Okta:Authority""];
    options.ProviderOptions.ClientId = configuration[""Okta:ClientId""];
    options.ProviderOptions.RedirectUri = Path.Combine(configuration[""HubWeb""], ""callback"");
    options.ProviderOptions.DefaultScopes.Add(""email"");
});
```

I have not been able to find a way in which multiple providers can be configured without writing all of the code myself and having to handle retrieving the tokens etc.

The reason that I am using a standalone webassembly is because my hosting model is to use ROSA (RedHat OpenShift for AWS) and have the Blazor app running as a container, and I have not been able to find a way of sharing the authentication state between the instances of the container.

I am looking for help with understanding if I could use a Blazor web app instead of the standalone webassembly, and if so how to share the authentication state between the instances so that the app continues working if the pod gets restarted due to instability.","kubernetes, blazor, openid-connect, .net-9.0",79798993.0,"Integrating two different IdP directly in your application is probably not what you want. As you said, you would have to write a lot of code by yourself and ensure everything is correctly implemented, you'll have to deal with two different token providers, you can't use standard mechanisms like token/scope validation, etc. Also, if you have a backend as well, and this will also be secured by OAuth tokens, from which of the two instances will you retrieve that token?

I suggest you think about whether IdP Federation would be suitable for you in this case. Means you setup a THIRD Okta instance. This third instance is the one you connect to your Blazor application and also to your backends. This is the only instance you're talking to from your code, so you can use all the standard OAuth/OIDC mechanisms + libraries.

Now you configure this third instance in a way that it doesn't support any local accounts, but only federated accounts. Then you go to the place in Okta where you configure external IdPs (normally used for Social Login, but works also between Okta instances) and add two IdPs: Your two Okta Instances for your internal and external users. In each of these two instances you need a ClientID + ClientSecret that you will put there in that configuration in the third instance.

Then you can tweak the UI. Your Login/Registration Landingpage of your third instance won't ask for username/password, since this is disabled. It should only have two buttons that you can name ""Internal users"" and ""external users"" via the configuration, redirecting to the two other Instances. You can configure your Okta instance for internal users with any security mechanisms you need, e.g. that it will only run in your company network.

From perspective of your Blazor Application, you won't see anything of that. Your third Okta instance will still act like the OIDC black box and takes care of translating the tokens, etc. However, there should be an additional claim in the Token that tells you from which of the two main instances your user is coming from. That you can use for further business logic, if required.",2025-10-24T16:44:07,2025-10-24T15:07:17
79798328,Tilt Port Forwarding to specific pod in Kubernetes Resource,"We use Tilt to manage our local development Kube clusters.  I've integrated the `langfuse` helm chart in the `Tiltfile`, which functions properly, however I'm having trouble forwarding a port to the `web` pod in this Langfuse workload. I use `k8s_resource` to refer to the workload and `extra_pod_selectors` to select the specific pod, but it always ends up selecting a random pod to forward to.

This is the portion of `Tiltfile` that applies the helm chart:

```
helm_repo(""langfuse-repo"", ""https://langfuse.github.io/langfuse-k8s"", labels=[""langfuse""])
helm_resource(
  ""langfuse"",
  ""langfuse-repo/langfuse"",
  resource_deps=[""langfuse-repo""],
  namespace=""langfuse"",
  labels=[""langfuse""],
  deps=[""configurations/langfuse.yaml""],
  flags=[
      ""--create-namespace"",
      ""--values=configurations/langfuse.yaml"",
  ],
)
```

From a terminal, I can see that one pod should be selectable using the following label:

```
> kubectl get pods -l app=web -n langfuse
NAME                            READY   STATUS    RESTARTS   AGE
langfuse-web-5897965c8c-drwcn   1/1     Running   0          4h37m
```

Then I add this after the `helm_resource` call, to attempt to forward the port:

```
k8s_resource(
    ""langfuse"",
    port_forwards=3000,
    extra_pod_selectors=[{""app"": ""web""}],
    labels=[""langfuse""],
)
```

This does then create a `PortForward` in tilt, but it randomly applies it to one of the pods created by the Langfuse helm chart. I.e. `clickhouse` or `worker` or `zookeeper`:

```
> tilt get PortForward
NAME                                             CREATED AT
langfuse-langfuse-clickhouse-shard0-0            2025-10-17T02:41:07Z
```

Are there suggestions on how to properly select the single `web` pod when applying the port forward?","kubernetes, kubernetes-helm, tilt",,,,2025-10-24T01:55:22
79797988,Helm subchart uses baseline values.yaml instead of merged values.yaml + values-dev.yaml when deployed via parent chart,"Here is my helm chart structure:

```
app/
 ├── Chart.yaml
 ├── values.yaml
 ├── values-dev.yaml
 └── templates/

app-test/
 ├── Chart.yaml
 ├── values.yaml
 ├── values-dev.yaml
 └── charts/
      └── app-1.0.0.tgz
```

I want the subchart app to use a combination of `values.yaml` + `values-dev.yaml` for certain environments when deploying app-test. Values in app-test are symbolic links to values in app.
I'm running the app with:

```
helm upgrade app-test ./app-test -f values.yaml -f values-dev.yaml
```

If I install only app, the combination of `values.yaml` + `values-dev.yaml` works correctly.
So the behavior of the subchart changes depending on whether it’s deployed standalone or as a dependency.

How can I make the subchart app use the dev values (merged with baseline `values.yaml`) when deploying the parent chart (app-test)?
Do I need to merge values manually when packaging the subchart, or is there a recommended Helm way to do this?","kubernetes, kubernetes-helm",79798056.0,"In fact, the Helm values are processed differently if a chart is deployed as an independent chart *vs.* if it is a dependency of another chart.  There is some discussion of this in the Helm documentation in [the general description of Helm values](https://docs.helm.sh/docs/topics/charts/#scope-dependencies-and-values), with a further example in [Subcharts and Global Values](https://docs.helm.sh/docs/chart_template_guide/subcharts_and_globals/#overriding-values-from-a-parent-chart).

If the chart is a top-level chart, then its settings are at the top level of the Helm values

```
appSpecificValue: something
```

But if it is a dependency of another chart, then its settings are under a key with the chart's name.

```
app:
  appSpecificValue: something
```

In both cases, the chart code sees `.Values` as the settings for this chart specifically, so if it is as a subchart, `.Values.appSpecificValue` sees the value under `app: { appSpecificValue: }`.  You can create a [`global:` top-level key](https://docs.helm.sh/docs/topics/charts/#global-values) that will be visible to all subcharts, but this probably doesn't fit your use case.

You don't describe how `app` and `app-test` are related.  If `app-test` just provides some extra Kubernetes artifacts to the application (a data-loading Job; a debugging Deployment/Service; an in-cluster database StatefulSet/Service) then the easiest approach will be to use two separate Helm releases for them.

```
helm upgrade app ./app -f values.yaml -f values-dev.yaml
helm upgrade app-test ./app-test -f values.yaml -f values-dev.yaml
```

With this setup `app-test` would not directly have `app` as a dependency.  You might need to pass `app`'s Helm release name as a value to `app-test`.

It also could make sense to move the `app-test` content directly into the `app` chart and have it controlled by Helm values.

```
{{-/* This was in app-test, but we can make it conditional in the main chart */-}}
{{- if .Values.debugService.enabled -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include ""chart.fullname"" . }}-debug
...
{{- end -}}
```

```
debugService:
  enabled: false
```

But there isn't a way for a chart to be used as both a top-level chart and a subchart, and also for identical `helm install -f extras.yaml` files to have the same effects on both; the YAML layout is different for subcharts.",2025-10-23T17:50:51,2025-10-23T15:56:05
79792444,Why does KEDA create a second Selenium Node for the second Chrome session instead of using available slots on the first node,"I'm running Selenium Grid in Minikube using the official Docker Selenium Helm chart, with KEDA enabled for autoscaling. My goal is to scale Chrome nodes based on session demand, where each node can handle up to 3 parallel sessions. However, when I launch a second identical test job, KEDA spins up a second node instead of assigning the session to one of the available slots on the first node. I'm trying to figure out why this happens.

Kubernetes: Minikube (latest version, single-node cluster).
Helm Install Command:

```
helm install selenium-grid docker-selenium/selenium-grid --values val.yaml --namespace default
```

val.yaml

```
seleniumGrid:
  nodeMaxSessions: 3

autoscaling:
  enabled: true
  scalingType: deployment
```

I launch sessions via Python Selenium script in Kubernetes Jobs. The script creates a single Chrome session and keeps it open for testing.

Python code (in a Docker image, run as K8s Job):

```
import os
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
chrome_options.add_argument(""--headless"")

SELENIUM_REMOTE_URL = os.getenv(""SELENIUM_REMOTE_URL"", ""http://selenium-hub.default.svc.cluster.local:4444/wd/hub"")

driver = webdriver.Remote(
command_executor=SELENIUM_REMOTE_URL,
options=chrome_options
)

driver.get(""https://www.example.com"")
time.sleep(60)
driver.quit()
```

K8s Job:

```
apiVersion: batch/v1
kind: Job
metadata:
name: selenium-test-job-1  # Or -2 for the second
spec:
template:
spec:
containers:
- name: selenium-test
  image: my-selenium-client:latest
  env:
  - name: SELENIUM_REMOTE_URL
    value: ""http://selenium-hub.default.svc.cluster.local:4444/wd/hub""
  restartPolicy: Never
  backoffLimit: 0
```

I apply the first Job (kubectl apply -f job1.yaml), wait for it to run, then apply the second (kubectl apply -f job2.yaml with a different name).

This is what happening

- First Job:

KEDA scales up to 1 Chrome node (`selenium-grid-selenium-node-chrome` Deployment replicas=1). Hub assigns the session to this node (1/3 slots used).
- Second Job:

Instead of assigning to the existing node (which has 2/3 slots free), KEDA immediately scales to 2 replicas. New node starts, and the second session goes there.

###

I tried increasing the pooling interval for the scaled object from 20 seconds to 60 (I suspected that a new application instance wouldn't have time to connect to an existing node).
I tried running jobs with a longer interval, about 2-3 minutes.
I changed the scaling type for autoscaling from deployment to jobs.","kubernetes, selenium-grid, keda",,,,2025-10-16T18:08:58
79792371,ArgoCD database patching,"We have a namespace consisting of 3 applications:

- Database patcher
- API
- Web site

Currently we are using ArgoCD app-of-apps pattern to roll this out. Each application above is therefore it's own ArgoCD application.

I want the database patcher to be rolled out before the other two applications and I want the entire deployment of all applications to halt if the database patcher fails.

Is this possible with app-of-apps pattern?","kubernetes, argocd",,,,2025-10-16T16:27:18
79791764,Handling cleanup for tasks which might be OOMKilled,"I have some Python code running in k8s which in some cases is being OOMKilled and not leaving me time to cleanup which is causing bad behavior.

I've tried multiple approaches but nothing seems quite right... I feel like I'm missing something.

I've tried creating a soft limit in the code to:
`resource.setrlimit(resource.RLIMIT_RSS, (-1, cgroup_mem_limit // 100 * 95)`
but sometimes my code still gets killed by the OOMKiller before I get a memory error.
(When this happens it's completely reproducible)

What I've found that works is limiting by `RLIMIT_AS` instead of `RLIMIT_RSS` but this gets me killed much earlier as AS is much higher than RSS (sometimes >100MB higher) I'd like to avoid wasting so much memory. (100MB x hundreds of replicas adds up)

I've tried using a sidecar for the cleanup but (at least the way I managed to implement it) this means both containers need an API which together cost more than 100MB as well, so didn't really help.

Why am I surpassing my memory limit? My system often handles very large loads with lots of tasks which could be either small or large (and there's no way to know ahead of time, think uncompressing) so in order to take best advantage of our resources we try each task with a pod which has little memory (which allows for high replica count) and if the task fails we bump it up to a new pod with more memory.

Is there a way to be softly terminated before being OOMKilled while still looking at something which more closely corresponds to my real usage? Or is there something wrong with my design? Is there a better way to do this?","python, kubernetes, memory",79803839.0,"When a container is **OOMKilled**, it’s terminated immediately with a hard `SIGKILL`. That means the process doesn’t get a chance to run any cleanup logic, not even lifecycle hooks like `preStop`. If cleanup is important, it has to happen *outside* of the process that might get killed.

**1. Understand the limitation**

An OOM kill is abrupt. The container doesn’t get to react or run any shutdown routines. Only sibling containers in the same Pod or external controllers can observe and respond to the event.

**2. Handle cleanup externally**

**Sidecar aproach:** Run a lightweight helper container in the same pod with a separate memory limit. It can monitor the main container’s status and perform cleanup if it dies with an OOMKilled reason.

**Controller or operator:** Use a higher-level controller that watches Pod status and triggers cleanup Jobs or routines when it sees OOMKilled events.

**Startup recovery:** If the container restarts, make it detect annd repair partial state on startup rather than relying on in-process cleanup at shutdown.

**3. Make your workloads resilient**

Use **ephemeral storage** like `emptyDir` so Kubernetes automatically deletes leftovers when the Pod goes away.

Design your work to be idempotent, so rerunning it is safe even after a crash.

Use leases, heartbeats, or TTLs to detect abandoned work and reclaim it.

For external systems, give each task a unique namespace or prefix so cleanup jobs can find and remove orphaned resources later.

**4. Reduce the likelihood of OOM kills**

Set realistic resourc requests and limits, monitor actual memory use, and tune your workloads to avoid runaway memory growth. Smaller work chunks and backpressure mechanisms can also help.",2025-10-29T13:18:22,2025-10-16T04:05:42
79790596,Helm doesn&#39;t remove initContainers from Deployment,"I've created deployment template using helm (v3.14.3) with support for setting initContainers. Last time I realized one of initContainers removed from values.yaml is still present in cluster. I tried various fixes, but I'm not able to force helm to remove it.

The way how I deploy chart is:

```
helm template site-wordpress ./web-chart \
  -f ./values-prod.yaml \
  --set image.tag=prod-61bdfc674d25c376f753849555ab74ce0b01a0dea617a185f8a7a5e33689445e
```

Can someone advise me on the issue? Here's the template:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include ""app.fullname"" . }}
  labels:
    {{- include ""app.labels"" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  {{- with .Values.strategy }}
  strategy:
    {{- toYaml . | nindent 4 }}
  {{- end }}
  selector:
    matchLabels:
      {{- include ""app.selectorLabels"" . | nindent 6 }}
      app.kubernetes.io/component: app
  template:
    metadata:
      annotations:
        # This will change whenever initContainers config changes (including when removed)
        checksum/initcontainers: {{ .Values.initContainers | default list | toYaml | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
      labels:
        {{- include ""app.labels"" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        app.kubernetes.io/component: app
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include ""app.serviceAccountName"" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      {{- if .Values.initContainers }}
      initContainers:
        {{- range .Values.initContainers }}
        - name: {{ .name }}
          image: ""{{ $.Values.image.repository }}:{{ $.Values.image.tag | default $.Chart.AppVersion }}""
          imagePullPolicy: {{ $.Values.image.pullPolicy }}
          {{- if .command }}
          command: {{ toJson .command }}
          {{- end }}
          {{- if .args }}
          args: {{ toJson .args }}
          {{- end }}
        {{- end }}
      {{- end }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: ""{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}""
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          {{- if .Values.command }}
          command: {{ toJson .Values.command }}
          {{- end }}
          ports:
            - name: http
              containerPort: {{ .Values.service.port }}
              protocol: TCP
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          {{- end }}
      {{- end }}
```","kubernetes, kubernetes-helm, amazon-eks",79836294.0,"As others commented, the way this chart works is too generic and might not be the best idea to allow any initContainer from the values.

That being said, the error you are having might be due to the new deployment not working and so the POD not getting really replace, which would explain why you still see the initContainer. Can you confirm that when you deploy the hel mchart without any init container it does replace the latest deployment and POD? you can see the kubectl events to verify there isn't any error and also describe the deployment.

`kubectl get events --sort-by=.metadata.creationTimestamp`

Replace <deployment_name> below:

`kubectl describe deployment <deployment_name>`

Also confirm that the initContaienr you see is not coming from other configurations (for example some tools as Istio inject initContainers to every POD)

If this doesn't work please share the values.yaml and the deployment describe yaml.",2025-12-02T20:17:56,2025-10-14T20:21:04
79781744,K8s Leader election with Micronaut,"I'm trying to follow [this doc](https://micronaut-projects.github.io/micronaut-kubernetes/snapshot/guide/#kubernetes-operator) from Micronaut to implement the leader election to lock on the scheduler, so that I can make sure only scheduler run for only one pod. (I know we have shred lock, but team don't want to use that so we decided to go with leader election).

But the documentation is not enough, most of the examples I can find on Google they use Spring. So not sure if we have any example to do leader election to lock on scheduler in Micronaut.","kubernetes, micronaut",,,,2025-10-03T11:57:31
79781392,Spatial join without Apache Sedona,"currently I'm working in a specific version of Apache Spark (3.1.1) that cannot upgrade. Since that I can't use Apache Sedona and the version 1.3.1 is too slow. My problem is the following code that works for standalone pure python but in cluster mode returns Null for all data.

I suspect that broadcast variables could be the source of the problem but I can't find the problem.

Any suggestion?

```
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType
from pyspark import SparkContext, Broadcast

from shapely import wkt
from shapely.geometry import Point
from rtree import index

spark = SparkSession.builder \
    .appName(""GeoUDFExample"") \
    .master(""local[*]"") \
    .getOrCreate()

sc = spark.sparkContext

geometries_df = spark.read.parquet(""db_geometrias.geoparquet"")
geometries = geometries_df.select(""id"", ""geometry"").collect()

rtree_idx = index.Index()
geom_dict = {}

for row in geometries:
    geom_id = row[""id""]
    geom_wkt = row[""geometry""]
    geom_obj = wkt.loads(geom_wkt)
    rtree_idx.insert(geom_id, geom_obj.bounds)
    geom_dict[geom_id] = geom_wkt

rtree_broadcast: Broadcast = sc.broadcast(rtree_idx)
geom_dict_broadcast: Broadcast = sc.broadcast(geom_dict)

def point_to_geom_id(lat, lon):
    pt = Point(lon, lat)

    candidates = list(rtree_broadcast.value.intersection((lon, lat, lon, lat)))
    for int_id in candidates:
        geom_wkt = geom_dict_broadcast.value[int_id]
        geom = wkt.loads(geom_wkt)
        if geom.contains(pt):
            return int_id
    return None

geo_udf = udf(point_to_geom_id, IntegerType())

points_df = spark.createDataFrame([
    (1, -78.5, -0.2),
    (2, -78.3, -0.1),
    (3, -78.6, -0.25)
], [""point_id"", ""lon"", ""lat""])

result_df = points_df.withColumn(""geom_id"", geo_udf(""lat"", ""lon""))
result_df.show()
```

I tried to debug the problem with fixed return but the same result and change the variables to global but use too much memory.","apache-spark, kubernetes, apache-spark-sql, geospatial",,,,2025-10-03T01:35:49
79779172,My django API “next” link uses a stale hostname only when receiving requests from GKE services (Like Cloud Run),"**What are the details of your problem?**

I have a application in Django that is deployed using GKE. It uses an ingress to deploy it...
Those `manifests.yml` that are used for deploying applications on Google Cloud...

The application deploys it successfully, I'm able to login, navigate, and do most of my tasks.
The problem is, whenever I try to request a API route that returns a list, I get the correct results but the `next` pagination link is built with a 'stale hostname'...

It should appear at like something like this: [https://www.my-api-url.com/api/stores/?page=1&active=true](https://www.my-api-url.com/api/stores/?page=1&active=true)
But the hostname is being built like this: [https://api-back.my-api-url.com/api/stores/?active=true&page=2](https://api-back.my-api-url.com/api/stores/?active=true&page=2)
This 'api-back' is actually the hostname of my API Container from Google Cloud... The Docker Container on Kubernetes.

But the thing is: I already configured in Django Settings the correct hosts, and I checked the Environment Variables... They all point to the correct URLs.
I also searched the entire codebase, Kubernetes manifests, and ingress configs and I can’t find this 'api-back.my-api-url.com' anywhere.

I found this related question stating DRF uses the request hostname for the paginator, but I still can’t figure out where the stale hostname is coming from: [How to change the host in next key in a paginated URL in django rest framework?](https://stackoverflow.com/questions/62421753/how-to-change-the-host-in-next-key-in-a-paginated-url-in-django-rest-framework)

**What did you try and what were you expecting?**

I expected my DRF and Django Settings to build next with the API URL defind in my settings and my enviroment variables.
Checklist of what I saw to see if it's OK or not:

- `ALLOWED_HOSTS` variable from `django.settings` contains the correct host.
- Tested with both `USE_X_FORWARDED_HOST` settings (True and False).
- I als configured `SECURE_PROXY_SSL_HEADER to ('HTTP_X_FORWARDED_PROTO', 'https')`.
- I double-checked my ingress configurations to see that it uses the correct public host... It did...
- Saw no hard-coded references to `api-back.my-api-url.com` in the repo or K8s manifests.

Despite that, the **`next`** link still shows the stale hostname.

What could be causing this behaviour?","django, kubernetes, django-rest-framework, pagination, nginx-ingress",79809027.0,"DRF builds pagination links using `request.build_absolute_uri()`, which depends on the `Host` header it receives.

If your app is behind a GKE Ingress or Load Balancer, it’s likely not forwarding the original host —

so Django sees your internal service name like `api-back.my-api-url.com`.

1. In your **Django settings.py**:

```
USE_X_FORWARDED_HOST = True
SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
```

2.In your **Ingress annotations**, preserve the original host:

```
nginx.ingress.kubernetes.io/use-forwarded-headers: ""true""
nginx.ingress.kubernetes.io/configuration-snippet: |
  proxy_set_header Host $host;
  proxy_set_header X-Forwarded-Host $host;
```

3.If needed, override DRF’s pagination link generator:

```
from rest_framework.pagination import PageNumberPagination

class FixedHostPagination(PageNumberPagination):
    def get_next_link(self):
        url = super().get_next_link()
        if url:
            return url.replace('api-back.my-api-url.com', 'www.my-api-url.com')
        return None
```

Why it happens:

Your ingress or proxy rewrites the `Host` header to the internal service name.

DRF uses that to build links, so you end up with stale internal URLs.",2025-11-04T14:30:03,2025-09-30T13:47:22
79778601,Layers of timeout in an Istio+k8s managed cluster,"I have a cluster of microservices. UI calls API1 (assuming it goes through ingress gateway, correct me if I am wrong), API1 calls API2 via RestTemplate.

The API2 process is bulky and takes roughly 1.5 minutes to complete, however there are no errors or exceptions in the process itself. For testing purposes, I called API2 directly via Bruno (whcih is set with a sufficiently large timeout value) which gives a socket hangup around 1 minute which is expected as the AWS LB connection idle timeout is 1 min. But from Chrome's network tab I see the timing of the call to succesfully complete with waiting for server time to be 1.5 minutes.

I understand istio has a default of 2 retries for connection failures, and timeout is disabled. My question is that why when calling from the UI it is successful after 1.5 minutes rather than waiting for 3 minutes and failing. Are the pods behaving in a way that I don't understand? My understanding is that after 1 minute socket gets closed and a retry kicks off, but that should also fail and kickoff the 2nd retry. Again, that should also fail after the next minute. Is the socket somehow reopened within the total time of 3x1=3 minutes and the call is successful because it takes less than 3 minutes?

P.S. I am a Junior developer who is just getting into the devops world of cluster orchestration, service mesh, etc. Any clarification is deeply appreciated.

I changed the LB connection idle timeout to a higher value and the call was successful in Bruno as expected. I put arbritrarily large sleep times in the process and altered the idle time out values to get expected results.
But I don't understand the difference I am seeing in Chrome(UI->API1->API2) vs calling (Bruno->API2).
I read the docs and scoured google with no satisfactory answer.","java, kubernetes, istio, istio-gateway",79785206.0,"Multiple timeout layers (load balancer, ingress, Istio sidecars, HTTP client) can each cut off the call, it’s not that socket “reopens.” To fix this, extend or disable the timeout at each layer, or break the long-running operation into an async or polling pattern.",2025-10-08T08:05:39,2025-09-29T23:32:14
79774756,Flux with SOPS doesn&#39;t decrypt stringData,"I'm trying to use fluxcd on my kubernetes cluster, which is working perfectly fine.

Now I wanted to add prometheus/grafana stack and use sops to decrypt the basic_auth part in the scrapingConfig from prometheus.

The issue is, I encrypted the secret and added the private pgp key as sops-gpg to the cluster. The secret is used but the content remains encrypted (starting with ENC[...).

What am I missing?

`gotk-sync.yaml`:

```
# This manifest was generated by flux. DO NOT EDIT.
---
apiVersion: source.toolkit.fluxcd.io/v1
kind: GitRepository
metadata:
  name: iac
  namespace: flux-system
spec:
  interval: 1m0s
  ref:
    branch: k8s
  secretRef:
    name: iac
  url: ssh://git@git.example.com/infrastruktur/iac
---
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: iac
  namespace: flux-system
spec:
  interval: 10m0s
  path: ./
  prune: true
  sourceRef:
    kind: GitRepository
    name: iac
  validation: client
  decryption:
    provider: sops
    secretRef:
      name: sops-gpg
```

The `prometheus/additional-scrape-configs.sops.yaml`:

```
apiVersion: v1
kind: Secret
metadata:
    name: additional-scrape-configs
    namespace: prometheus
type: Opaque
stringData:
    additional-scrape-configs.yaml: ENC[...]
sops:
    lastmodified: ""2025-09-24T12:13:55Z""
    mac: ENC[...]
    pgp:
        - created_at: ""2025-09-24T12:13:55Z""
          enc: |-
            -----BEGIN PGP MESSAGE-----

            ...
            -----END PGP MESSAGE-----
          fp: ...
    encrypted_regex: ^(data|stringData)$
    version: 3.10.2
```

The `prometheus/release.yaml`:

```
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: prometheus-stack
  namespace: prometheus
spec:
  interval: 5m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: ""56.5.0""
      sourceRef:
        kind: HelmRepository
        name: prometheus-community
        namespace: prometheus
  install:
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  values:
    grafana:
      ingress:
        enabled: true
        ingressClassName: ""traefik""
        hosts:
          - grafana.example.com
    prometheus:
      prometheusSpec:
        additionalScrapeConfigSecret:
          name: additional-scrape-configs
          key: additional-scrape-configs.yaml
```

The `.sops.yaml`

```
creation_rules:
  - path_regex: *.sops.yaml
    encrypted_regex: '^(data|stringData)$'
    pgp: >-
      ...
```

`kustomization.yaml`:

```
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - prometheus
  - traefik
```

`prometheus/kustomization.yaml`:

```
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - namespace.yaml
  - release.yaml
  - repository.yaml
  - additional-scrape-configs.sops.yaml
```

I already fixed my repo names like stated in [Flux not decrypting using SOPS](https://stackoverflow.com/questions/73725389/flux-not-decrypting-using-sops)

Edit:

- I fixed the `.sops.yaml` and changed `path_regex` to `.sops.yaml`
- I also tried `sops -d prometheus/additional-scrape-configs.sops.yaml`, which works fine.
- Removed `validation: client` and `secretsRef: iac` from `gotk-sync.yaml`","kubernetes, prometheus, fluxcd, mozilla-sops",79776114.0,"I fixed it by changing stringData to data and provide the content as base64 string.

Also had to rename the Repo name back to flux-system.

And had to add the decryption section with the flux command:

```
flux create kustomization flux-system \
--source=flux-system \
--path=./ \
--prune=true \
--interval=10m \
--decryption-provider=sops \
--decryption-secret=sops-gpg
```",2025-09-26T15:15:01,2025-09-25T11:31:35
79771240,Render webpage that sets X-Frame-Options and redirects to OAuth flow in IFrame,"I have a requirement where I need to render a web app that uses oauth to login in an iframe.  The iframe setup is for tutorials, where the tutorial content is in one column, and the web app is in another column.  The web app is rendered in an iframe.  The web apps are deployed by me, I can't really change them but I do run them in kubernetes so I do proxy their requests/responses and can modify request/response headers.  It's easy enough remove X-Frame-Options headers, and modify CSB headers to allow the web apps to run in an iframe.  Now I have a new web app that I need to display, it uses oauth so the user gets redirected to the oauth login page, and back via the redirect link passed at login time.  I can't unfortunately easily proxy the requests/reponses from the oauth provider.  It is techincally possible I believe, but I'm reaching out here to find out if anyone has any other ideas on how to meet this requirement.

I've tried [https://github.com/Rob--W/cors-anywhere](https://github.com/Rob--W/cors-anywhere), which works more or less the same as the route proxy above, and breaks down as soon as i hit the oauth endpoint.

Only working solution I have so far is a docker image running VNC, noVnc, IceWM and firefox.  It's great, fast and all the redirects work without any extra configuration.  Negatives here are memory usage, the container requires 1GB+ of RAM and even then it crashes occasionally.  We could have 200-300 users at one time doing the tutorials.  While I do have the RAM available, it's obviously not efficient. And the RAM usuage multiplies when I need more than 1 web app in an iframe.","javascript, kubernetes, iframe, cors, reverse-proxy",79771247.0,"I've encountered the popup-based authentication flow with Microsoft Entra: [https://learn.microsoft.com/en-us/entra/identity-platform/scenario-spa-sign-in?tabs=react](https://learn.microsoft.com/en-us/entra/identity-platform/scenario-spa-sign-in?tabs=react)

If your OAuth identity provider supports a popup-based authentication flow then you might be able to use that to avoid loading the login page inside the iframe.",2025-09-22T05:10:58,2025-09-22T04:50:03
79769761,Python Socket.IO with FastAPI in Kubernetes 499 and 426 error,"I am trying to setup a socketio server, but having issues with client connection either getting 499 or 426 error -

I have tried port forwarding from the pod to my local and run curl command that also don't seem to complete, they get stuck indefinitely not sure what I am doing wrong using this same ingress to deploy a fastapi app on port 8000 in a different pod but on the same domain and that seems to work completely fine, but socket connection are failing.

ingress.yaml

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: websocket-headers
  namespace: duck-backend
data:
  Connection: ""$connection_upgrade""
  Upgrade: ""$http_upgrade""
  Host: ""$host""
  X-Real-IP: ""$remote_addr""
  X-Forwarded-For: ""$proxy_add_x_forwarded_for""
  X-Forwarded-Proto: ""$scheme""
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: duck-backend-ingress
  namespace: duck-backend
  annotations:
    kubernetes.io/ingress.class: 'nginx'
    cert-manager.io/cluster-issuer: 'letsencrypt-prod'

    nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-set-headers: 'duck-backend/websocket-headers'

spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - example.com
      secretName: race1-tls
  rules:
    - host: example.com
      http:
        paths:
          - path: /socket.io
            pathType: Prefix
            backend:
              service:
                name: socketio-service
                port:
                  number: 8001
          - path: /
            pathType: Prefix
            backend:
              service:
                name: fastapi-service
                port:
                  number: 8000
```

socketio.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: socketio
  namespace: duck-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: socketio
  template:
    metadata:
      labels:
        app: socketio
    spec:
      containers:
      - name: socketio
        image: duck-backend:latest
        command: [""./docker/entrypoint-k8s.sh"", ""socket-io""]
        ports:
        - containerPort: 8001
        envFrom:
        - configMapRef:
            name: duck-backend-config
        - secretRef:
            name: duck-backend-secrets
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""100m""
          limits:
            memory: ""1Gi""
            cpu: ""1000m""
---
apiVersion: v1
kind: Service
metadata:
  name: socketio-service
  namespace: duck-backend
spec:
  selector:
    app: socketio
  ports:
  - port: 8001
    targetPort: 8001
  type: ClusterIP
```

main.py

```
app = FastAPI(title=""Socket.IO Server"")

app.add_middleware(
    CORSMiddleware,
    allow_origins=[""*""],
    allow_credentials=True,
    allow_methods=[""*""],
    allow_headers=[""*""],
)

sio = socketio.AsyncServer(
    async_mode='asgi',
    cors_allowed_origins='*',
    client_manager=manager,
    logger=True,
    engineio_logger=True
)

@sio.on('*')
def any_event(event, sid, data):
     print('*', event, sid)

@sio.event
def connect(sid, environ, auth):
    print('connect ', sid)

@sio.event
def disconnect(sid, reason):
    print('disconnect ', sid, reason)

uvicorn.run(socketio.ASGIApp(sio, app), host=""0.0.0.0"", port=8001)
```

sysout

```
Starting service: socket-io
Starting Socket.IO service on port 8001...
Server initialized for asgi.
2025-09-19 16:20:48,983 - app.api.v1.socket_events.balance_update - INFO - Starting Socket.IO server...
INFO:     Started server process [1]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)
2025-09-19 16:21:25,682 - engineio.server - INFO - TQFnIo5NlfhS4P9bAAAA: Sending packet OPEN data {'sid': 'TQFnIo5NlfhS4P9bAAAA', 'upgrades': ['websocket'], 'pingTimeout': 20000, 'pingInterval': 25000, 'maxPayload': 1000000}
TQFnIo5NlfhS4P9bAAAA: Sending packet OPEN data {'sid': 'TQFnIo5NlfhS4P9bAAAA', 'upgrades': ['websocket'], 'pingTimeout': 20000, 'pingInterval': 25000, 'maxPayload': 1000000}
```

In output it mostly just shows 426 upgrade required, but the request from the browser shows connection and upgrade headers being in request

client.html

```
<!DOCTYPE html>
<html>
<head>
  <title>Socket.IO Test Client</title>
  <script src=""https://cdn.socket.io/4.7.5/socket.io.min.js""></script>
</head>
<body>
  <h2>Socket.IO Test</h2>
  <div id=""status"">Connecting...</div>
  <script>
    // Replace with your actual endpoint and port
    const socket = io(""https://example.com"", {
      path: ""/socket.io/"",
      transports: ['websocket', 'polling'],
      secure: true
    });

    socket.on(""connect"", () => {
      document.getElementById(""status"").innerText = ""Connected: "" + socket.id;
    });

    socket.on(""disconnect"", () => {
      document.getElementById(""status"").innerText = ""Disconnected"";
    });

    // Example: Listen for custom events
    socket.on(""your_event"", (data) => {
      console.log(""Received:"", data);
    });

    // Example: Emit a test event
    socket.emit(""test"", { msg: ""Hello from client!"" });
  </script>
</body>
</html>
```

It seems like either the headers are not making to he app or the app is rejecting requests no idea why","python, kubernetes, socket.io, kubernetes-ingress, nginx-ingress",,,,2025-09-19T16:43:35
79767805,How to update the pods from a StatefulSet after changing its image?,"I have an EKS cluster with a StatefulSet. I had to update the image of this StatefulSet, so I ran:

```
$ kubect set image statefulsets liferay-default liferay-default=liferay/dxp:2025.q3.1
```

I expected it to automatically start to replace the old pod with a new one, but no new pod was created.

How should I update the StatefulSet image then so the the pods are upgraded?","kubernetes, statefulset",79767806.0,"The command above should have worked, in principle. StatefulSets by default use a `RollingUpdate` strategy, and they update the pods sequentially: once a new pod is ready, an old one is discarded.

The problem in our case is that the previous pod was not ready:

```
$ k get pod
NAME                         READY   STATUS             RESTARTS      AGE
liferay-default-0       0/1     CrashLoopBackOff   3 (22s ago)   76s
```

Unlike deployments, StatefulSets do not update their pods if they are not in a `Running` or `Ready` state. I had then to delete the pod manually to update it.

(Of course, you may have other reasons. For example, if you have `spec.updateStrategy.type` set to `OnDelete`, you will have to delete the pods, too. If the new image was not fetched, it naturally will lead to issues. But it is good to have in mind that failing pods are not automatically updated by `StatefulSets`)",2025-09-17T20:58:49,2025-09-17T20:58:49
79767130,Kubernetes HPA algorithm,"i have 2 questions:

1 - i'd like to know if i can set my hpa to compare the limits resources instead of the requests resources with the target utilization i choose for memory and cpu(i'm using normal metrics, not external or custom ones).

2 - i encountered the problem where i created a hpa that should scale up my deployment if memory or cpu hit 80%, now the app itself baseline usage is around 65% avg memory. so at peak times the hpa create another pod which is fine. the problem starts when scale down cant occur because of the calculation the hpa use to determine the desired replicas(it turns 1.1 >= to 2 pods) so for scale down to occur the avg memory should be around 50% which cant be. i tried to change the resources a bit and couldnt get fine results.

i'd be happy to get some advices","kubernetes, resources, openshift, scale, hpa",79767284.0,"1. No, you can't. HPA's built-in resource metrics always use requests, not limits.

If you need limits-based scaling, use custom metrics instead.

2. Quick fix - Increase resource requests:

Alternative approaches:

Lower HPA target from 80% to 70%

Tune scale-down behavior:

```
spec:
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

Your requests should be set so baseline usage = ~50-60% of requests, giving HPA room to scale down properly.

if you want to scale at 80% but need scale-down at 50%, set requests = baseline_usage / 0.5. So 650MB baseline needs ~1.3GB requests.",2025-09-17T11:47:24,2025-09-17T09:53:08
79767068,Why does kubectl create secret fail with an error stating it is not logged in?,"When I try to run `kubectl create secret generic` from my pipeline it fails with the error:

```
error: You must be logged in to the server (the server has asked for the client to provide credentials)
```

Here is my Github Actions pipeline..

```
# Sign into AWS
- name: Configure AWS credentials (OIDC)
  uses: aws-actions/configure-aws-credentials@v4
  with:
    role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
    aws-region: ${{ env.AWS_REGION }}
    output-credentials: true

# Update Kube config
- name: Update KubeCTL
  run: |
    aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ env.CLUSTER_NAME }}
```

For the remainder of this script I can do things on my cluster e.g.

```
- name: Test Commands
  run: |
    kubectl config current-context
    kubectl config get-contexts
    kubectl auth can-i create secret -n my-app
```

These all work perfectly with the last command reporting `yes` that I have permission to create secrets.

However when I try to actually create the secret I get the error above.

My Github deployment role has the `AmazonEKSClusterAdminPolicy` access policy.

What am I doing incorrectly?","amazon-web-services, kubernetes, amazon-eks, kubectl",79767917.0,"This usually happens because your GitHub Actions IAM role isn’t mapped to a Kubernetes RBAC user in EKS. To fix it, confirm which IAM identity is being used with `aws sts get-caller-identity` and `kubectl whoami`, then check the `aws-auth` ConfigMap with `kubectl get configmap aws-auth -n kube-system -o yaml`. If your GitHub Actions IAM role isn’t listed there, add it. That’s the step that wires up authentication properly.",2025-09-18T01:21:34,2025-09-17T08:47:37
79766427,Kubernetes: driver-py: not found and Missing application resource error,"I'm trying to run my pyspark file in a k8s environment using a custom image.
It used to work fine with our company provided image but it has spark v2.4.4 so I wanted an upgrade.

First I got this error:
`/opt/entrypoint.sh: line 128: exec: driver-py: not found`

Figured this was due to the driver-py being removed from the entrypoint.sh file in v3+ and so modified my docker image to add the last line.

```
FROM apache/spark:3.5.1-java17-python3

ENV SPARK_HOME=/opt/spark
ENV PATH=$SPARK_HOME/bin:$PATH

# # Set working directory
WORKDIR ${SPARK_HOME}

# Install extra Python packages as root
USER root

RUN pip install --no-cache-dir numpy pandas smbprotocol openpyxl pyspark==3.5.1

ADD mssql-jdbc-12.10.1.jre11.jar /opt/spark/jars/
ADD ojdbc11.jar /opt/spark/jars/

## added later
RUN ln -s /opt/spark/bin/spark-submit /usr/bin/driver-py
```

Now Im getting this:

```
Error: Missing application resource.

Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]
Usage: spark-submit run-example [options] example-class [example args]
```

Here is my yaml:

```
apiVersion: ""sparkoperator.k8s.io/v1beta2""
kind: SparkApplication
metadata:
  name: juni-test
  namespace: hpesbo
spec:
  type: Python
  sparkVersion: 3.5.1
  mode: cluster
  image: **********************:spark3.5-test
  imagePullPolicy: Always

  mainApplicationFile: ""file:///maprfs-csi/test.py""

  restartPolicy:
    type: Never
  imagePullSecrets:
    - imagepull

  sparkConf:
    spark.mapr.user.secret: long-live-spark-secret
    spark.driver.extraClassPath: ""local:///maprfs-csi/snowflake-jdbc-3.13.30.jar:local:///maprfs-csi/spark-snowflake_2.12-2.12.0-spark_3.4.jar""
    spark.executor.extraClassPath: ""local:///maprfs-csi/snowflake-jdbc-3.13.30.jar:local:///maprfs-csi/spark-snowflake_2.12-2.12.0-spark_3.4.jar""

  deps:
    jars:
      - local:///maprfs-csi/snowflake-jdbc-3.13.30.jar
      - local:///maprfs-csi/spark-snowflake_2.12-2.12.0-spark_3.4.jar
  volumes:
    - name: maprfs-volume
      persistentVolumeClaim:
        claimName: spark-pvc
    - name: rsa-key-volume
      secret:
        secretName: snowflake-rsa-key
    - name: log-volume
      persistentVolumeClaim:
        claimName: spark-pvc-logs

  driver:
    cores: 1
    coreLimit: ""1000m""
    memory: ""8g""
    labels:
      version: 3.5.1
    annotations:
      sidecar.istio.io/inject: ""false""
    volumeMounts:
      - name: maprfs-volume
        mountPath: /maprfs-csi
      - name: rsa-key-volume
        mountPath: /keys
        readOnly: true
      - name: log-volume
        mountPath: /log-csi

  executor:
    cores: 1
    coreLimit: ""1000m""
    instances: 3
    memory: ""16g""
    labels:
      version: 3.5.1
    annotations:
      sidecar.istio.io/inject: ""false""
    volumeMounts:
      - name: maprfs-volume
        mountPath: /maprfs-csi
      - name: rsa-key-volume
        mountPath: /keys
        readOnly: true
      - name: log-volume
        mountPath: /log-csi
```

I tried changing from `mainApplicationFile: ""local:///maprfs-csi/test.py""` to
`mainApplicationFile: ""file:///maprfs-csi/test.py""` and `mainApplicationFile: ""/maprfs-csi/test.py""` all of them are throwing the same error. The file is definitely present in the PVC.

I was able to do a describe pod before it failed if it helps with anything:

```
Name:             spark-test-driver
Namespace:        <your-namespace>
Priority:         0
Service Account:  <your-service-account>
Node:             <node-name>/<node-ip>
Start Time:       Tue, 16 Sep 2025 19:52:31 +0530
Labels:           spark-app-selector=<uid>
                  spark-role=driver
                  sparkoperator.com/app-name=spark-test
                  sparkoperator.com/launched-by-spark-operator=true
                  sparkoperator.com/submission-id=<submission-id>
                  version=3.5.1
Annotations:      cni.projectcalico.org/podIP: <pod-ip>/32
                  sidecar.istio.io/inject: false
Status:           Failed
IP:               <pod-ip>
Controlled By:    SparkApplication/spark-test

Containers:
  spark-kubernetes-driver:
    Container ID:  docker://<container-id>
    Image:        <your-docker-repo>:spark3.5-test
    Image ID:     docker-pullable://<repo-digest>
    Ports:        7078/TCP, 7079/TCP, 4040/TCP
    Args:
      driver-py
      --properties-file
      /opt/spark/conf/spark.properties
      --class
      org.apache.spark.deploy.PythonRunner
    State:          Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Tue, 16 Sep 2025 19:52:50 +0530
      Finished:     Tue, 16 Sep 2025 19:52:55 +0530
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     1
      memory:  11468Mi
    Requests:
      cpu:     1
      memory:  11468Mi
    Environment:
      SPARK_DRIVER_BIND_ADDRESS:  (v1:status.podIP)
      SPARK_LOCAL_DIRS:           /var/data/spark-<uid>
      PYSPARK_PRIMARY:            /maprfs-csi/test.py
      PYSPARK_MAJOR_PYTHON_VERSION:  2
      SPARK_CONF_DIR:             /opt/spark/conf
      NEW_RELIC_METADATA_KUBERNETES_CLUSTER_NAME:  <cluster-name>
      NEW_RELIC_METADATA_KUBERNETES_NODE_NAME:     (v1:spec.nodeName)
      NEW_RELIC_METADATA_KUBERNETES_NAMESPACE_NAME: <your-namespace>
      NEW_RELIC_METADATA_KUBERNETES_POD_NAME:      spark-test-driver
      NEW_RELIC_METADATA_KUBERNETES_CONTAINER_NAME: spark-kubernetes-driver
    Mounts:
      /keys from rsa-key-volume (ro)
      /log-csi from log-volume (rw)
      /maprfs-csi from maprfs-volume (rw)
      /opt/spark/conf from spark-conf-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from <serviceaccount-token> (ro)

Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True

Events:
  Type     Reason                  Age   From               Message
  ----     ------                  ----  ----               -------
  Normal   Scheduled               25s   default-scheduler  Successfully assigned <namespace>/spark-test-driver to <node>
  Warning  FailedMount             25s   kubelet            MountVolume.SetUp failed for volume ""spark-conf-volume"" : configmap ""spark-test-<id>-driver-conf-map"" not found
  Normal   Pulling                 9s    kubelet            Pulling image ""<your-docker-repo>:spark3.5-test""
  Normal   Pulled                  7s    kubelet            Successfully pulled image ""<your-docker-repo>:spark3.5-test""
  Normal   Created                 7s    kubelet            Created container spark-kubernetes-driver
  Normal   Started                 7s    kubelet            Started container spark-kubernetes-driver
```

I found a few issues reported in the git for the same errors but none of them have a solution. What am I missing here?","python, docker, apache-spark, kubernetes, pyspark",,,,2025-09-16T15:41:19
79765689,Add job name and build number to Jenkins agent provided by Kubernetes plugin,"I would like to add the job name and build number as a pod label of the agent pod that is spawned by the kubernetes plugin.

For example, if I have a job called ""buildMyApp"" and I start build #10 I would like the k8s pod to have a label similar to this

```
metadata:
  labels:
    com.jenkins.jobName: ""buildMyApp""
    com.jenkins.buildNumber: ""10""
```

I am aware this is possible by doing something like this:

```
pipeline {
  agent {
    kubernetes {
      inheritFrom ""defaultLabel""
      yamlMergeStrategy merge()
      yaml """"""
      metadata:
        labels:
          com.jenkins.jobName: ""buildMyApp""
          com.jenkins.buildNumber: ""10""
"""""" }
<rest of the pipeline>
```

1. It requires editing all jobs, which is unfeasible in my case (we have 20k+ pipelines, some as Jenkinsfiles, some saved directly in Jenkins)
2. It causes the agent pod name to be very long and dependent on job name, while I would like to keep it as its default format (<label>-<randomId>)

Is there another way, possibly at global level, that doesn't have these two drawbacks?","kubernetes, jenkins, jenkins-plugins, kubernetes-jenkins-plugin",,,,2025-09-16T00:09:51
79762856,Ingress-NGINX consistent-hash for WebSocket routing across multiple controller pods is inconsistent (hash by query param),"I’m trying to route WebSocket connections deterministically to the same backend pod(in a k8s deployment) based on a query parameter (room/league id). This works with a single ingress-nginx pod, but becomes inconsistent once I scale the ingress controller (multiple nginx pods).

Topology

Kubernetes (EKS)
Ingress: ingress-nginx (multiple controller/nginx pods)
One Deployment exposes two ports behind one Service with two ingresses:
REST (list rooms)
WebSocket (/websocket) that creates/joins a room stored in-memory on the selected pod
Goal

All clients connecting with the same league_id should land on the same backend pod (room state is in-memory).
Current behavior

With 1 ingress-nginx pod: zero latency; creator and joiners always land on the same backend pod.
With >1 ingress-nginx pods: the second user often lands on a different backend pod and connection is failed; we added client retries to “eventually” hit the right one, causing latency. Sometimes even the first user experiences latency.
Minimal Ingress (WebSocket) config

Using consistent hashing by query param via upstream-hash-by. Separate Ingress for WebSockets to avoid rewrite issues.

```
  annotations:
    nginx.ingress.kubernetes.io/affinity: cookie
    nginx.ingress.kubernetes.io/affinity-mode: persistent
    nginx.ingress.kubernetes.io/large-client-header-buffers: 4 32k
    nginx.ingress.kubernetes.io/proxy-buffer-size: 32k
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""3950""
    nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3950""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3950""
    nginx.ingress.kubernetes.io/session-cookie-max-age: ""3600""
    nginx.ingress.kubernetes.io/session-cookie-name: myservice-session
**    nginx.ingress.kubernetes.io/upstream-hash-by: $arg_league_id
**....
```

Repro

1. User A connects: wss://api.example.com/websocket?league_id=123 → room created on Pod X.
2. User B connects: wss://api.example.com/websocket?league_id=123 → intermittently routed to Pod Y when multiple ingress-nginx pods exist; after retries, eventually reaches Pod X.

What I’ve tried:

- nginx.ingress.kubernetes.io/load-balance: hash
- nginx.ingress.kubernetes.io/upstream-hash-by: $arg_league_id
- Separate Ingress object for WebSockets (no rewrite)
- Verified the Service exposes the correct ws port
- Observed that single ingress replica fixes the issue; multiple replicas reintroduce inconsistency.
- Change nginx versions
- Change deployment into statefulset (didn't helped, it was just an nonsense attempt, i ran out of ideas)
- removed the affinity annotation didn't work as well
- remove cookie

Questions:

1. Is upstream-hash-by expected to be consistent across multiple ingress-nginx replicas, or is it only deterministic within a single nginx instance?
2. How can I guarantee identical upstream selection across all ingress-nginx pods?

- Is there a way to enforce stable upstream peer ordering so the hash mapping matches on every ingress pod?
- Do I need to avoid service-level load-balancing (e.g., ensure service-upstream=false) or set any specific annotation to keep hashing at the pod endpoint level?
- Should I enable the “consistent” hash ring behavior (if supported) or use upstream-hash-by-subset annotations?

1. If this can’t be made truly consistent at the ingress layer, is the recommended approach to externalize room state (e.g., Redis and reverse proxy for the websocket) to make another smart routing in the back?

Environment:
EKS: 1.31 .
ingress-nginx controller: 1.13.2 (also 1.9.5 wasn't successful).
NLb .
backend: nodejs .","kubernetes, websocket, load-balancing, nginx-ingress, consistent-hashing",79762866.0,"`nginx.ingress.kubernetes.io/upstream-hash-by` is deterministic, but it only works consistently **inside a single NGINX instance**. Once you scale ingress-nginx to multiple controller pods, each pod builds its own upstream server list from the Kubernetes endpoints API, and the **ordering of those backends is not guaranteed to be identical** across replicas.

That’s why you see it “just work” with one ingress pod, but as soon as you add more, the same hash value may map to different pods depending on how each controller ordered the upstream list.

### Why this happens

- `upstream-hash-by` hashes the key (your `league_id`) → picks a backend index.
- If ingress A has upstream list `[pod1, pod2, pod3]` and ingress B has `[pod2, pod3, pod1]`, the same hash will point to different pods.
- Kubernetes doesn’t guarantee stable ordering of endpoints, so across replicas the mapping diverges.

### What you can do

**Short-term workarounds**

1. **Try `nginx.ingress.kubernetes.io/service-upstream: ""true""`**

This makes ingress point to the Service ClusterIP instead of embedding every pod in the upstream block. Sometimes this produces more consistent routing, but test carefully: you lose some pod-level health awareness.
2. **Cookie/session affinity**

Works if you only need the *same client* to reconnect to the same pod, but won’t group multiple clients with the same `league_id`.
3. **Subset / consistent hashing features**

In newer ingress-nginx versions, there are annotations like `upstream-hash-by-subset`. They can reduce churn, but they don’t fully solve cross-replica upstream ordering issues.

**Robust solution (what most production systems do):**

- **Externalize the room state** into Redis, DynamoDB, or similar.

Then it doesn’t matter which backend pod a user lands on — they all pull/join state from the shared store. This completely removes the need for deterministic sticky routing and scales much better.

If you really must keep in-memory room state, your only options are:

- Run a single ingress controller (sacrifices HA), or
- Introduce a dedicated routing layer (Envoy/Traefik/etc.) that supports cluster-wide consistent hashing with a shared control plane.

### Answers to your questions

- **Is upstream-hash-by consistent across replicas?** → No, only within a single ingress pod.
- **Can you guarantee identical upstream selection?** → Not with multiple ingress-nginx replicas; the endpoint list ordering is not stable.
- **Stable upstream peer ordering?** → Not guaranteed by Kubernetes.
- **Should you avoid service-level load balancing?** → You can try `service-upstream: ""true""`, but test carefully.
- **Enable “consistent” hash ring?** → Only helps if the upstream lists are identical.
- **If not possible, what’s recommended?** → Externalize room state (Redis, etc.).",2025-09-12T11:26:59,2025-09-12T11:13:05
79759113,Conditional Argo Workflow Execution,"i have a simple workflow with dag, it runs the first job and depending on the output of that job, it will run either one or both of the following jobs named `optional-job-one` or `optional-job-two`. here is the part of my yaml file that does this:

```
workflowSpec:
  serviceAccountName: ""{{ .Values.serviceAccountName }}""
  entrypoint: mother
  templates:
    - name: mother
      dag:
        tasks:
          - name: main-job
            template: main-job-step

          - name: optional-job-one
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION1 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-one-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""

          - name: optional-job-two
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION2 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-two-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""
```

Now, i want to add a new step. This new step will run if either one or both of the `optional-job-one` or `optional-job-two` have ran and finished successfully. how do i do it? i asked the AI chatbots for help and i got this but it doesnt work and im lost (im super new to k8 and argo)!

```
- name: optional-job-three
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: false
    error: false
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```

Thank you in advance for your help!","kubernetes, argo-workflows",79767555.0,"Thanks to the previous response i came across this solution that works perfectly, its a bit ugly but it does the job:

```
- name: optional-job-three
  depends: ""(optional-job-one.Succeeded && optional-job-two.Skipped) || (optional-job-one.Skipped && optional-job-two.Succeeded) || (optional-job-one.Succeeded && optional-job-two.Succeeded)""
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```",2025-09-17T15:52:31,2025-09-08T15:52:17
79759113,Conditional Argo Workflow Execution,"i have a simple workflow with dag, it runs the first job and depending on the output of that job, it will run either one or both of the following jobs named `optional-job-one` or `optional-job-two`. here is the part of my yaml file that does this:

```
workflowSpec:
  serviceAccountName: ""{{ .Values.serviceAccountName }}""
  entrypoint: mother
  templates:
    - name: mother
      dag:
        tasks:
          - name: main-job
            template: main-job-step

          - name: optional-job-one
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION1 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-one-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""

          - name: optional-job-two
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION2 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-two-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""
```

Now, i want to add a new step. This new step will run if either one or both of the `optional-job-one` or `optional-job-two` have ran and finished successfully. how do i do it? i asked the AI chatbots for help and i got this but it doesnt work and im lost (im super new to k8 and argo)!

```
- name: optional-job-three
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: false
    error: false
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```

Thank you in advance for your help!","kubernetes, argo-workflows",79764130.0,"instead of doing .status directly in `when`, you should combine it with `dependencies`  because conditions don’t support checking `.status` directly. also you should use `continueOn.failed: true` to avoid aborting the task when one dependent fails :

```
  dependencies: [optional-job-one, optional-job-two]
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: true
    error: true
```",2025-09-14T06:45:47,2025-09-08T15:52:17
79757637,How to forward /api using frontendconfig to / in kubernetes ingress?,"I am using helm to create kubernetes resouces and manifest is as below

```
---
# Source: project-app/charts/backend/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: backend-sa
  namespace: dev
  annotations:
    iam.gke.io/gcp-service-account: project-k8s-backend-pod-sa@project-dev.iam.gserviceaccount.com
---
# Source: project-app/charts/backend/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: backend-secret
  namespace: dev
  labels:
    app: backend
data:
  OPENAI_API_KEY: c2stcHJvai0XNoWTZhdHVoZVRONHRCVDdXUjU1QzBnMEE=
---
# Source: project-app/charts/backend/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: backend-config
  labels:
    app: backend
data:
  EMBEDDING_MODEL: ""text-embedding-ada-002""
---
# Source: project-app/charts/backend/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: backend-svc
  namespace: dev
  labels:
    app: backend
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
spec:
  selector:
    app: backend
  ports:
    - port: 80
      targetPort: 8000
  type: ClusterIP
---
# Source: project-app/charts/backend/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend
  namespace: dev
  labels:
    app: backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      serviceAccountName: backend-sa
      containers:
        - name: backend
          image: ""us-central1-docker.pkg.dev/project-dev/project-docker-registry/project-backend:latest""
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
          envFrom:
            - configMapRef:
                name: backend-config
            - secretRef:
                name: backend-secret
          readinessProbe:
            httpGet:
              path: /healthz
              port: 8000
            initialDelaySeconds: 5
            periodSeconds: 10
---
# Source: project-app/templates/ingressclass.yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: gce
  annotations:
    ingressclass.kubernetes.io/is-default-class: ""true""
spec:
  controller: k8s.io/ingress-gce
---
# Source: project-app/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: project-ingress
  namespace: dev
  annotations:
    networking.gke.io/v1beta1.FrontendConfig: project-ingress-fc
    kubernetes.io/ingress.class: gce
    kubernetes.io/ingress.global-static-ip-name: ""project-ip""
spec:
  ingressClassName: gce
  defaultBackend:
    service:
      name: backend-svc
      port:
        number: 80
  rules:
    - http:
        paths:
          - path: /api
            pathType: Prefix
            backend:
              service:
                name: backend-svc
                port:
                  number: 80
          - path: /
            pathType: Prefix
            backend:
              service:
                name: backend-svc
                port:
                  number: 80
---
# Source: project-app/templates/frontendconfig.yaml
apiVersion: networking.gke.io/v1beta1
kind: FrontendConfig
metadata:
  name: project-ingress-fc
  namespace: dev
spec:
  redirectToHttps:
    enabled: false
  requestPath:
    pathRules:
    - path: /api
      service: backend-svc
      backendPath: /
```

I am expecting to get same response for `http://<project-ip>/` and `http://<project-ip>/api` because both are pointing to same service.

While the `http://<project-ip>/` works, but `http://<project-ip>/api` gives error for 404.

How to make sure both works ?","kubernetes, url-rewriting, kubernetes-ingress",,,,2025-09-06T15:16:16
79756618,Custom Airflow Image,"I have Airflow running on Kubernetes.
So, as you know, every task is spun up as it's own pod by the KubernetesExecutor.
I have tried to override the individual container images with slim python images, but the pods are failing. Apparently the Airflow image must be present for the system to interact with the pod properly?
But the Airflow image is heavy and overkill for a single task, as it also contains the API server, the Scheduler, etc..

Is there a way to build a ""Slim Airflow"" Image, with just the bare necessities to get it working, with the minimum of Airflow that the pods need to run, but no extra baggage?","docker, image, kubernetes, airflow",79756658.0,There is a slim image available: [https://airflow.apache.org/docs/docker-stack/index.html](https://airflow.apache.org/docs/docker-stack/index.html),2025-09-05T10:13:42,2025-09-05T09:37:00
79755504,HTTP-01 Challenge Fails on Bare-Metal Kubernetes with cert-manager and Let’s Encrypt,"I am running a bare-metal Kubernetes cluster (no cloud provider) with cert-manager and Let’s Encrypt to issue TLS certificates. I bought a domain from Namecheap and configured a basic Nginx ingress controller. However, my certificates are stuck in a pending/errored state.

**Setup**
Kubernetes cluster: 1 control plane + 3 workers
CI/CD: GitHub Runner with ArgoCD
Ingress: Nginx
LoadBalancer: MetalLB with a local IP assigned to the Nginx controller
Domain: A record points to my public IP

Here is what my conf is (I have remove my domain and email info.):

**K8s**

- Ingress YAML

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: private-test-deployment-api
  namespace: default
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - 'my-domain'
      secretName: my-domain-tls
  rules:
    - host: 'my-domain'
      http:
        paths:
          - path: /grades
            pathType: Prefix
            backend:
              service:
                name: private-test-deployment-api
                port:
                  number: 80
```

- ClusterIssuer YAML

```
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: letsencrypt-prod
spec:
  acme:
    email: 'my-email'
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-prod
    solvers:
      - http01:
          ingress:
            class: nginx
            ingressTemplate:
              metadata:
                annotations:
                  nginx.ingress.kubernetes.io/ssl-redirect: ""false""
                  nginx.ingress.kubernetes.io/force-ssl-redirect: ""false""
```

- MikroTik NAT Rules

```
 0    ;;; PPPoer Connection !
      chain=srcnat action=masquerade out-interface=pppoe-out1 log=no log-prefix=""""

 2    ;;; Forward HTTP to worker0 ingress
      chain=dstnat action=dst-nat to-addresses=192.168.88.39 to-ports=80 protocol=tcp in-interface=pppoe-out1 dst-port=80
      log=no log-prefix=""""

 3    ;;; Forward HTTPS to worker0 ingress
      chain=dstnat action=dst-nat to-addresses=192.168.88.39 to-ports=443 protocol=tcp in-interface=pppoe-out1 dst-port=443
      log=no log-prefix=""""
```

- Mikrotik Firewall Rules

```
0    ;;; Allow HTTP to ingress
      chain=forward action=accept protocol=tcp dst-address=192.168.88.39 in-interface=pppoe-out1 dst-port=80

 1    ;;; Allow HTTPS to ingress
      chain=forward action=accept protocol=tcp dst-address=192.168.88.39 in-interface=pppoe-out1 dst-port=443

 2    ;;; Allow Access From Trusted IPs
      chain=input action=accept src-address-list=Trusted-IPs log=no log-prefix=""""

 3    ;;; Accept Established & Related Connections
      chain=input action=accept connection-state=established,related log=no log-prefix=""""

 4    ;;; Accept ICMP Request From LAN
      chain=input action=accept tcp-flags="""" protocol=icmp in-interface=ether2 icmp-options=8:0-255 log=no log-prefix=""""

 5    ;;; Accept DNS-UDP
      chain=input action=accept protocol=udp in-interface=ether2 dst-port=53 log=no log-prefix=""""

 6    ;;; Accept DNS-TCP
      chain=input action=accept protocol=tcp in-interface=ether2 dst-port=53 log=no log-prefix=""""

 7    ;;; Drop Everything Else
      chain=input action=drop log=no log-prefix=""""
```

**Tests**

- Inside Cluster

`kubectl -n default run -it --rm --image=curlimages/curl curltest -- curl -sv http://private-test-deployment-api.default/.well-known/acme-challenge/test`

**Returns: error: timed out waiting for the condition**

- Outside Cluster

`curl -v http://test.icptokens.net/.well-known/acme-challenge/test`

**Returns: 404 (Not Found)**

**Observation and Question**
I suspect the issue is external HTTP traffic not reaching the solver pod, but I’m not sure if it’s my MikroTik NAT rules, firewall, or ingress configuration.

Has anyone successfully run HTTP-01 on a bare-metal cluster behind a MikroTik router?
I would appreciate guidance on:

- Correct NAT and firewall setup for HTTP-01
- Any additional ingress annotations or tweaks needed
- Ways to debug why Let’s Encrypt cannot reach the challenge path

Thanks in advance for any advice!","kubernetes, kubernetes-ingress, lets-encrypt, cert-manager",79755641.0,"I know that `Namecheap` is not a natively supported issuer for `cert-manager` but in your situation I would drop firewall/NAT rules in favor of [webhook issuer](https://cert-manager.io/docs/configuration/acme/dns01/webhook/).

This method will allow you to generate certificate via placing `txt` record in your DNS provider and generate any certificate without need to expose your internal cluster.

Example [webhook](https://github.com/kelvie/cert-manager-webhook-namecheap) for your DNS.",2025-09-04T11:14:20,2025-09-04T09:14:07
79755186,How to see untruncated kubernetes pod logs? kubectl logs truncating long logs,"When using `kubectl logs` to log a given Kubernetes pod’s messages ([link](https://kubernetes.io/docs/reference/kubectl/)), the logs shown are truncated when there are too many logs, it seems.

Even when adding a flag such as `--since`:

```
kubectl logs -f <pod name> --since=1h
```

The logs shown by `kubectl logs` are truncated at about 106K characters, which is too little for what I need.

Ideally, I want to see *all* the logs of the past hour, regardless of how many characters/lines.

Isn't that possible?",kubernetes,79756022.0,"You can use [--tail](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/#:%7E:text=%23%20Display%20only%20the%20most%20recent%2020%20lines%20of%20output%20in%20pod%20nginx%0A%20%20kubectl%20logs%20%2D%2Dtail%3D20%20nginx) flag in `kubectl logs` specifying the maximum number of lines to return. Setting it to a large number (e.g., 1000) that may fetch more logs, potentially bypassing truncation.

```
kubectl logs <pod-name> --since=1h --tail=1000
```

You can also use [-f](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_logs/#:%7E:text=%2Df%2C%20%2D%2Dfollow,should%20be%20streamed.) and redirect to a file. The `-f flag` streams logs redirecting to a file that may ensure that all logs are saved without being limited by terminal.

```
kubectl logs -f <pod-name> --since=1h > pod_logs.txt
```

For more logs flag you can refer to this [documentation](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs).",2025-09-04T17:13:16,2025-09-04T00:23:45
79754763,URL getting rewritten when using Kubernetes ingress alongside Flask,"I want to access two services with my Kubernetes ingress, and both services are based upon a Flask app. Each Flask app is made of a `main.py` script and a `index.html` web page, which is rendered using `render_template` library. Each service should let me get an answer from a Large Linguage Model (LLM) using Groq, and each service differs from the other only because of the model it uses, so I will show you the code of just one of them.

`main.py`

```
from flask import Flask, render_template, request
from groq import Groq

model_id=""qwen/qwen3-32b""
groq_api_key = ""<--my API key :)-->""

# Initialize the Groq client with the API key
client = Groq(api_key=groq_api_key)

app = Flask(__name__)

# Home route to display
@app.route('/')
def index():
    return render_template('index.html')

# Route to handle form submission
@app.route('/answer', methods=['POST'])
def answer():
    input_text = request.form.get('input_text')
    if not input_text:
        return ""Please provide input text."", 400
    try:
        completion = client.chat.completions.create(
            model=model_id,
            messages=[
                {""role"": ""system"", ""content"": ""User chatbot""},
                {""role"": ""user"", ""content"": input_text}
            ],
            temperature=1,
            max_tokens=1024,
            top_p=1,
            stream=True,
            stop=None,
        )
        # Collect the streamed response
        result = """"
        for chunk in completion:
            result += chunk.choices[0].delta.content or """"
    except Exception as e:
        return f""An error occurred: {e}"", 500
    # Render the index.html template with the results
    return render_template('index.html', input_text=input_text, result=result)

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=5000)
```

`index.html`

```
<form action=""{{ url_for('answer') }}"" method=""POST"">
  <label for=""input_text"">Enter Input Text:</label>
  <br>
  <textarea id=""input_text"" name=""input_text"" rows=""4"" cols=""50"" required></textarea>
  <button type=""submit"">Submit</button>
</form>
{% if result %}
  <div class=""result-container"">
    <p><strong>Input Text:</strong> {{ input_text }}</p>
    <p><strong>Result:</strong> {{ result }}</p>
  </div>
{% endif %}
```

Similarly, for each Flask app there is a Deployment and a Service. The Deployment uses a simple Docker image with the Flask app and its dependencies inside. Since the two Deployments and the two Services are similar, I will show you the code of just one of them (each).

Deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: groq-app
spec:
  selector:
    matchLabels:
      app: groq-app
  template:
    metadata:
      labels:
        app: groq-app
    spec:
      containers:
      - name: groq-app
        image: <--my DockerHub username :)-->/groq-test:v2
        ports:
        - containerPort: 5000
```

Service:

```
apiVersion: v1
kind: Service
metadata:
  name: groq-app-service
spec:
  type: NodePort
  selector:
    app: groq-app
  ports:
  - name: http
    protocol: TCP
    port: 8080
    targetPort: 5000
    nodePort: 30008
```

Now the fun part: the ingress. And yes, I have an Ingress Controller (nginx) and it works fine.

Ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-groq-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: test.com
    http:
      paths:
      - path: /qwen
        pathType: Prefix
        backend:
          service:
            name: groq-app-service
            port:
              number: 8080
      - path: /llama
        pathType: Prefix
        backend:
          service:
            name: llama-app-service
            port:
              number: 9090
```

The problem I need help for: when I try to access one of the two services via its URL, for example `http://test.com/qwen`, everything is fine, but when I type the input and press Submit, what I get is the URL `http://test.com/answer` and, obviously, a 404 NOT FOUND error. The URL I'd like to see, with the corresponding web page, is `http://test.com/qwen/answer`. Obviously, I want something similar for `http://test.com/llama`.

What does the rewrite, the Ingress or Flask? And how I fix it?

What I tried & I can tell so far:

- Flask app works well when executed in any other way: using `python` or `flask` commands from both master and worker nodes, while inside a Docker container using `docker run`, while inside the pod accessing it directly and while inside the pod accessing it through the nodePort. The only problem is the damn URL rewrite.
- *I tried* to use `nginx.ingress.kubernetes.io/rewrite-target` and regular expressions, but I don't know the right way to do it. I don't even know if it's because of the ingress!
- *I tried* to modify the subpath used in URLs by the Flask app, but without results. I don't even know if it's because of Flask!","kubernetes, flask, nginx, url, kubernetes-ingress",79761625.0,"I've found the solution: a middleware inside my `main.py` based on [this class](https://gist.github.com/m4ce/9fbd438007950135fa54184f60572667). The full code of my `main.py` is now the following.

```
from flask import Flask, render_template, request
from groq import Groq

# App configuration
model_id = ""qwen/qwen3-32b""
groq_api_key = ""<--my API key :)-->""
PREFIX = ""/qwen""

# PrefixMiddleware auxiliary class
class PrefixMiddleware:
    def __init__(self, app, prefix):
        self.app = app
        self.prefix = prefix

    def __call__(self, environ, start_response):
        path = environ.get('PATH_INFO', '')
        if path.startswith(self.prefix):
            environ['SCRIPT_NAME'] = self.prefix
            environ['PATH_INFO'] = path[len(self.prefix):] or '/'
        return self.app(environ, start_response)

# App definition
app = Flask(__name__)
app.wsgi_app = PrefixMiddleware(app.wsgi_app, PREFIX)
client = Groq(api_key=groq_api_key)

# Flask routes
@app.route('/')
    def index():
    return render_template('index.html')

@app.route('/answer', methods=['POST'])
    def answer():
    input_text = request.form.get('input_text')
    if not input_text:
        return ""Please provide input text."", 400
    try:
        completion = client.chat.completions.create(
            model=model_id,
            messages=[
                {""role"": ""system"", ""content"": ""User chatbot""},
                {""role"": ""user"", ""content"": input_text}
            ],
            temperature=1,
            max_tokens=1024,
            top_p=1,
            stream=True,
            stop=None,
        )
        result = """"
        for chunk in completion:
            result += chunk.choices[0].delta.content or """"
    except Exception as e:
        return f""An error occurred: {e}"", 500

    return render_template('index.html', input_text=input_text, result=result)

# __main__
if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=5000)
```

In the Kubernetes Ingress, I also removed the annotation. The other files are the same.",2025-09-11T07:48:53,2025-09-03T14:50:46
79752789,Performance regression in a Kubernetes deployment that does not occur locally,"I have a docker image and an EC2. When I run this image on my EC2, it takes `x` seconds to finish. When I run the app natively, it also takes `x` seconds.

But if I deploy the exact image in a container in a multi-tenant Kubernetes environment, it takes `3x - 4x` seconds to complete. For context,

- Both my EC2 and the nodegroup that k8s uses underneath are the same exact instance type and size.
- I specify taints and affinities so that nothing else gets scheduled to the node. I can verify that with `kubectl` within my namespace at least.
- I request 90% vCPUs and set a limit of 95% vCPUs. The node has some orchestration / metric agents that always get scheduled that I cannot control, so I leave 5% CPU units for them.
- My workload is entirely CPU driven -- there is no IO / network activity.

When I run my application locally and monitor it with `htop` I can see maxed out CPU usage across all cores. When I run my app in k8s, I can see that it uses only 1/3rd of the threads that it should.

Kubernetes is managed by a separate team and I don't have complete transparency into how they do things. Any ideas why this might happen?","performance, kubernetes, cpu",79753572.0,"Based on your given observations, you're most likely having [CPU throttling](https://www.groundcover.com/blog/kubernetes-cpu-throttling) in your containers, causing the performance regression. Locally, there's no quota enforcement, so your app can freely use 100% of the available vCPUs across all cores without interruption, as what you can see in your `htop` monitoring. Meanwhile, Kubernetes uses [Linux Completely Fair Scheduler (CFS)](https://medium.com/@noman_bajwa/process-scheduling-completely-fair-scheduler-cfs-part-i-f23d1812572e) that restricts your container’s cgroup CPU usage (that you set at 95% of your node's vCPUs) [within short periods of time (default is at 100ms)](https://docs.kernel.org/scheduler/sched-bwc.html#:%7E:text=The%20default%20values,cpu.cfs_period_us%3D100ms).

Even though the limit you set is high, your multi-threaded workload can exceed the allocated CPU time in these brief intervals, triggering throttling even when the node has spare capacity. This causes threads to pause, increase context switching, and reduce effective CPU utilization, which explains the 3-4x slowdown in Kubernetes compared to your local or EC2 deployments.

Also refer to this [article](https://dev.to/causely/tackling-cpu-throttling-in-kubernetes-for-better-application-performance-1dko) that might be useful to you.",2025-09-02T13:47:11,2025-09-01T17:50:02
79750332,Assigning unique tokens to starting pods,"I'm trying to find an (elegant) solution to the following problem: I have a Deployment that I want to be able to scale from 2 - 4 pods.

I have a pool of 4 unique tokens to access a resource, external to the cluster. Is there a way to (automatically) assign one of the 4 tokens to the each of the running pods from the available pool? When a pod goes down, its token needs to be returned to the pool so a new pod starting up can get it assigned.

Initially, the token we use here is not a secret, but I'm looking for a solution in case secrets are involved as well.

Thanks for any help!

I could set up an additional 'DHCP' pod to do this assignment, but does Kubernetes have any built-in solution for this?","kubernetes, token",79750652.0,"Kubernetes has a built in solution that provides `tokens for authentication`, namely service account tokens, which also have built in expiry and renewal.

The solution you want is a `token for authorization` which you would need to build on top of Kubernetes. To do so, one option is to build a `token service` utility API:

- Deploy all authorization tokens with the token service.
- To get an authorization token, your pods send an authentication token to the token service.
- The token service downloads token signing public keys from the Kubernetes JWKS URI and validates the authentication token , then returns an authorization token.

This is an established pattern in cloud native OAuth deployments. You can read more about it in the WIMSE documents - [this early draft](https://www.ietf.org/archive/id/draft-ietf-wimse-workload-identity-bcp-02.html) is quite short and best summarizes the pattern.

**POD IDS**

Each service account token has a pod ID but pod IDs are randomly generated in a Deployment. You could consider using a StatefulSet so that pods have known pod suffixes like `-0` and `-1`. Pod IDs are also included in service account tokens.

A simpler option than imposing extra pod requirements might be for the token service to issue authorization tokens on demand and keep them short lived, rather than deploying longer lived fixed tokens. That is the pattern from the WIMSE docs.",2025-08-29T18:17:40,2025-08-29T13:22:54
79748776,How to utilize header based routing approach in latest kubernetes and community nginx version i.e. k8s = 1.32.3 ngnix =1.12.4,"I am trying to convert some logic written in kubernetes ingress configuration yaml files.

In latest versions of k8s, keywords that are with `""-snippets""` values are identified as risky one (e.g. configuration-snippet, auth-snippet, etc).

I have one use case, where am using header based approach to navigate to service, only if specific header is present in the request (this was earlier written using ingress object).

I tried out approach using Gateway API and `HTTPRoute` objects which supports header based routing, but for that i had to add explicit CRT's via helm and those doesn't come directly with k8s.

I don't want to go with this approach though, and also paths are not supposed to be changed.

How to fix this using classic ingress approach such that no any path changes will be needed.

Please find the configuration I was trying out

```
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: i-nginx-gateway
  namespace: my-namespace
spec:
  gatewayClassName: nginx
  listeners:
    - name: http
      protocol: HTTP
      port: 80
      allowedRoutes:
        namespaces:
          from: All
```

```
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: i-app-route
  namespace: my-namespace
spec:
  parentRefs:
    - name: i-nginx-gateway
      namespace: my-namespace
  hostnames:
    - something.com
  rules:
    # Rule 1: Authorization header present → go through asvc
    - matches:
        - path:
            type: PathPrefix
            value: /i/(.*)
          headers:
            - name: authtype
              type: Exact
              value: my-jwt
      backendRefs:
        - name: asvc
          port: 8000

    # Rule 2: No Authorization header → go directly to i
    - matches:
        - path:
            type: PathPrefix
            value: /i/(.*)
      backendRefs:
        - name: i
          port: 8000
```

Tried to use canary ingress approach, but somehow request are hitting primary ing only and not canary one.
Added config that i am trying out.

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: i-direct-ingress
  namespace: mynamespace
spec:
  ingressClassName: nginx
  rules:
  - host: something.com
    http:
      paths:
      - path: /i
        pathType: Prefix
        backend:
          service:
            name: i
            port:
              number: 8000
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: i-canary-ingress
  namespace: mynamespace
  annotations:
    nginx.ingress.kubernetes.io/canary: ""true""
    nginx.ingress.kubernetes.io/canary-by-header: ""authtype""
    nginx.ingress.kubernetes.io/canary-by-header-value: ""myjwt""
spec:
  ingressClassName: nginx
  rules:
  - host: something.com
    http:
      paths:
      - pathType: Prefix
        path: /i
        backend:
          service:
            name: asvc
            port:
              number: 8000
```","kubernetes, nginx, ingress-nginx",79750401.0,"To utilize header-based routing in your Kubernetes setup using NGINX Ingress Controller, you can use the built-in [canary deployment](https://kubernetes.github.io/ingress-nginx/examples/canary/) feature via [annotations](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary). This works by allowing routing to a secondary (canary) backend based on HTTP headers. When the specified header matches exactly your defined value, all matching traffic routes to the canary backend. While the traffic that doesn't match (either with a missing header or having a different value) is sent to the primary backend.

- You'll create two Ingress objects for the same host and path prefix (
`/i`
). One is the primary Ingress (routing to service i), and the other is the canary ingress (routing to service `asvc` when the header matches).
- Use [nginx.ingress.kubernetes.io/canary-by-header](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary:%7E:text=true%22%20is%20set%3A-,nginx.ingress.kubernetes.io/canary%2Dby%2Dheader,-%3A%20The%20header%20to:%7E:text=true%22%20is%20set%3A-,nginx.ingress.kubernetes.io/canary%2Dby%2Dheader,-%3A%20The%20header%20to) for your header name (authtype) and [nginx.ingress.kubernetes.io/canary-by-header-value](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary:%7E:text=rules%20by%20precedence.-,nginx.ingress.kubernetes.io/canary%2Dby%2Dheader%2Dvalue,-%3A%20The%20header%20value) for the exact value (my-jwt).
- To preserve your original path matching, set [nginx.ingress.kubernetes.io/rewrite-target](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#canary:%7E:text=Set%20the%20annotation-,nginx.ingress.kubernetes.io/rewrite%2Dtarget,-to%20the%20path) annotation on both your ingresses.",2025-08-29T14:17:11,2025-08-28T07:42:48
79747587,KEDA ScaledJob with MongoDB _id fails to pass JOB_ID to Python script,"I'm trying to use KEDA ScaledJob to process jobs from a MongoDB collection. Each document has a _id (ObjectId) and a jobStatus field. My Python script requires the job ID as an argument:

python downloader.py <job_id>

[https://github.com/Nahin-sajjad/filesure-devops-starter/blob/main/worker/downloader.py](https://github.com/Nahin-sajjad/filesure-devops-starter/blob/main/worker/downloader.py)

```
- name: MONGO_URI
            valueFrom:
              secretKeyRef:
                name: filesure-secrets
                key: MONGO_URI
          - name: AZURE_BLOB_CONN
            valueFrom:
              secretKeyRef:
                name: filesure-secrets
                key: AZURE_BLOB_CONN
          - name: AZURE_CONTAINER
            value: ""documents""
          - name: JOB_ID
            valueFrom:
              fieldRef:
                fieldPath: metadata.labels['keda.sh/job-id']
          resources:
            requests:
              memory: ""100Mi""
              cpu: ""200m""
            limits:
              memory: ""512Mi""
              cpu: ""500m""
        restartPolicy: Never
    backoffLimit: 2
    completions: 1
    ttlSecondsAfterFinished: 300
  pollingInterval: 30
  successfulJobsHistoryLimit: 5
  failedJobsHistoryLimit: 5
  maxReplicaCount: 10
  triggers:
  - type: mongodb
    metadata:
      connectionStringFromEnv: MONGO_URI
      dbName: filesure
      collection: jobs
      query: '{""jobStatus"": ""pending""}'
This is my mongodb jobs
{""_id"":{""$oid"":""68a9fb3835b62fc6b0eb250f""},""cin"":""L83203AO1996HTR262495"",""companyName"":""CORE SERVICES LIMITED"",""jobStatus"":""pending"",""processingStages"":{""documentDownload"":{""status"":""pending"",""totalDocuments"":{""$numberInt"":""39""},""downloadedDocuments"":{""$numberInt"":""13""},""pendingDocuments"":{""$numberInt"":""26""},""lastUpdated"":{""$date"":{""$numberLong"":""1755970747528""}}}},""createdAt"":{""$date"":{""$numberLong"":""1755970360712""}},""updatedAt"":{""$date"":{""$numberLong"":""1755970747528""}}}
Using queryValue: ""1"" → doesn’t correctly pass the ObjectId.
How can I make KEDA pass the MongoDB _id of a pending job to my Python script?

I’d prefer the solution where each pod fetches a single pending job and runs:
python downloader.py 68a9fb3835b62fc6b0eb250f
```","python, mongodb, kubernetes, keda",,,,2025-08-27T05:58:28
79743233,Increased latency in tidb read queries,"I'm running a TiDB 7.x cluster with TiFlash nodes. I am some queries to slow down or fail intermittently. and I'm encountering the following error in the TiFlash logs:

`[2025/08/22 09:30:07.938 +00:00] [ERROR] [LocalAdmissionController.cpp:122] [""Code: 49, e.displayText() = DB::Exception: Check added_tokens >= 0 failed, e.what() = DB::Exception, Stack trace: ...""] [source=LocalAdmissionController]`

And store size of tiflash increasing.
Cluster details:

- TiDB version: 7.x
- TiFlash nodes: 3, running in Kubernetes
- The issue started recently without obvious cluster changes.

What I've tried:

- Restarting TiFlash, PD, TiDB pods

Any insight or recommendations would be appreciated!","kubernetes, tidb",,,,2025-08-22T09:46:56
79741383,Restart Debezium by clicking a button on the web application In the Kubernetes environment,"I have 3 pods running in a Kubernetes environment. The three pods are as follows:

```
(1) Web application pod
(2) Oracle database
(3) Debezium
```

The web application is connected to an Oracle database, and Debezium uses this Oracle database for CDC (Change Data Capture) operations. This is a Kubernetes environment. Users can log in to the web application and select the tables that need CDC from time to time. Whenever a user changes the table selection from the web application UI, I want to restart Debezium. Please guide me on how to handle this situation. From Debezium’s side, a restart is required whenever there is a change in the supplemental logging configuration or the list of tables for CDC.","kubernetes, web-applications, debezium, change-data-capture",,,,2025-08-20T17:17:48
79740865,KEDA + Celery: Need Immediate Pod Scaling for Each Queued Task (Zero Queue Length Goal),"I have KEDA + Celery setup working, but there's a timing issue with scaling. I need immediate pod scaling when tasks are queued - essentially maintaining zero pending tasks at all times by spinning up a new pod for each task that can't be immediately processed.

What Happens Now:

1. Initial state: 1 pod running (minReplicaCount=1), queue=0
2. Add task 1: Pod picks it up immediately, queue=0, pods=1 ✅
3. Add task 2: Task goes to queue, queue=1, pods=1 (no scaling yet) ❌
4. Add task 3: queue=2, pods=1 → KEDA scales to 2 pods
5. New pod starts: Picks task 2, queue=1, pods=2
6. Result: Task 3 still pending until another task is added

What I Want:

1. Add task 1: Pod picks it up immediately, queue=0, pods=1 ✅
2. Add task 2: Task queued → Immediately scale new pod, new pod picks it up ✅
3. Add task 3: Task queued → Immediately scale another pod, pod picks it up ✅
4. Result: Zero tasks pending in queue at any time

Is there a KEDA configuration to achieve ""zero queue length"" scaling?

```
# Worker deployment (relevant parts)
containers:
- name: celery-worker
  command:
    - /home/python/.local/bin/celery
    - -A
    - celeryapp.worker.celery
    - worker
    - --concurrency
    - ""1""
    - --prefetch-multiplier
    - ""1""
    - --optimization
    - ""fair""
    - --queues
    - ""celery""

kind: ScaledObject
metadata:
  name: celery-worker-scaler
spec:
  scaleTargetRef:
    kind: Deployment
    name: celery-worker
  pollingInterval: 5
  cooldownPeriod: 120
  maxReplicaCount: 10
  minReplicaCount: 1
  triggers:
    - type: redis
      metadata:
        host: redis-master.namespace.svc
        port: ""6379""
        listName: celery
        listLength: ""1""
```","kubernetes, redis, django-celery, keda",,,,2025-08-20T09:09:36
79738705,How to access keys with a period in the name?,"I'm working on a Helm deployment of an app that I've written. I thought a neat way of managing the configuration was to specify it in `values.yaml` in the following format:

```
configFiles:
  file.yaml:
    example:
      foo: bar
```

This makes the file name and content very clear, but I'm struggling to use it in the config map template:

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include ""example.fullname"" . }}
  labels:
    {{- include ""example.labels"" . | nindent 4 }}
data:
  file.yaml: |-
    {{- .Values.configFiles.file.yaml | toYaml | toString | nindent 4 }}
```

It seems like it doesn't resolve `file.yaml` as a key, but `yaml` as a subkey of `file`, which is obviously empty and I get the following error:

```
Error: template: ...: executing ""..."" at <.Values.configFiles.file.yaml>: nil pointer evaluating interface {}.yaml
```

I've tried escaping the `.` with backslash (`\.`) and putting quotes around the key `.Values.configFiles.""file.yaml""`, but that just makes it complain about bad characters.

How do I use the value of a key that has a period in it, in a Helm template?","kubernetes, kubernetes-helm",79738727.0,"There's a get method that does this:

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include ""example.fullname"" . }}
  labels:
    {{- include ""example.labels"" . | nindent 4 }}
data:
  file.yaml: |-
    {{- get .Values.configFiles ""file.yaml"" | toYaml | toString | nindent 4 }}
```",2025-08-18T12:43:46,2025-08-18T12:29:46
79734436,How to check if an optional struct field is set using &#39;x-kubernetes-validations&#39; and CEL expressions?,"I have the following go struct that is part of a bigger struct which is converted into a Kubernetes CustomResourceDefinition:

```
type RoleRef struct {
    // Name is the name of the role or cluster role to bind to the subjects.
    // +kubebuilder:validation:MinLength=1
    Name string `json:""name""`

    // Namespace is the namespace of the role to bind to the subjects.
    // It must be set if the kind is 'Role' and may not be set if the kind is 'ClusterRole'.
    // +optional
    Namespace string `json:""namespace,omitempty""`

    // Kind is the kind of the role to bind to the subjects.
    // It must be 'Role' or 'ClusterRole'.
    // +kubebuilder:validation:Enum=Role;ClusterRole
    Kind string `json:""kind""`
}
```

I would like to use CEL expressions to ensure that if the `kind` is `Role`, `namespace` is non-empty, and if it is `ClusterRole`, `namespace` is empty.

It seems to be a problem that the `namespace` field is optional, though.
No matter how I try to check if it is set, it always results in an error:

### Try 1

```
rule: self.kind == 'Role' && (has(self.namespace) && self.namespace != '')
```

```
ERROR: <input>:1:28: undefined field 'namespace'
```

It does not make a difference whether I do it this way or use the `<if> ? <then> : <else>` syntax, the `has(self.namespace)` already produces the error.

### Try 2

```
rule: self.kind == 'Role' && (('namespace' in self) && self.namespace != '')
```

```
ERROR: <input>:1:38: found no matching overload for '@in' applied to '(string, selfType837108858)'
```

### Try 3

```
rule: self.kind == 'Role' && (self.?namespace.or('') != '')
```

```
ERROR: <input>:1:29: undefined field 'namespace'
```

`self` in the above examples refers to the aforementioned `RoleRef` struct, which has the following schema:

```
description: |-
  RoleRef defines a reference to a (cluster) role that should be bound to the subjects.
properties:
  kind:
    description: |-
      Kind is the kind of the role to bind to the subjects.
      It must be 'Role' or 'ClusterRole'.
    enum:
    - Role
    - ClusterRole
    type: string
  name:
    description: Name is the name of the role or cluster
      role to bind to the subjects.
    minLength: 1
    type: string
  namespace:
    description: |-
      Namespace is the namespace of the role to bind to the subjects.
      It must be set if the kind is 'Role' and may not be set if the kind is 'ClusterRole'.
    type: string
required:
- kind
- name
type: object
x-kubernetes-validations:
- message: namespace must be set if kind is 'Role'
  rule: # this is the rule from the examples above
```

What is the correct way to verify that the namespace is non-empty if `kind` is `Role` and that it is empty when `kind` is `ClusterRole` without making the `namespace` field mandatory?

According to the documentation [here](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation-rules), `has(self.field)` should work, but that always results in an error in my use-case.","go, kubernetes, kubernetes-custom-resources, common-expression-language",,,,2025-08-13T14:51:01
79733868,How to filter recent logs with kubectl logs for a specific pattern?,"I’m working with Kubernetes and I often need to check my application logs for specific keywords, but only within the most recent log entries.

For example, I can get the logs from my pod using:

```
kubectl logs my-pod-name
```

I know I can search for a pattern using grep, like:

```
kubectl logs my-pod-name | grep ""ERROR""
```

However, this gives me all matching lines from the logs, not just the most recent ones.

What I want is something like:

- Only fetch the last few minutes or last few lines of logs
- Filter those logs for a keyword (e.g., ""ERROR"" or ""timeout"")

I’ve seen --since=5m and --tail=100 options in kubectl logs, but I’m not sure how to combine them effectively with pattern matching while still being efficient.

How can I use kubectl to only get logs from, say, the last 5 minutes and filter for a keyword?

Is there a built-in way in kubectl to do this without relying entirely on grep?

If not, what’s the best practice for efficiently doing this in production environments?","kubernetes, kubectl",79746061.0,+1 For the `Loki` recommendation. It is nice being able to query the Loki data in the Grafana UI. You can tail live logs from your pod using the label selector or pick a specific time range that you are interested in.,2025-08-25T17:42:48,2025-08-13T04:55:39
79733868,How to filter recent logs with kubectl logs for a specific pattern?,"I’m working with Kubernetes and I often need to check my application logs for specific keywords, but only within the most recent log entries.

For example, I can get the logs from my pod using:

```
kubectl logs my-pod-name
```

I know I can search for a pattern using grep, like:

```
kubectl logs my-pod-name | grep ""ERROR""
```

However, this gives me all matching lines from the logs, not just the most recent ones.

What I want is something like:

- Only fetch the last few minutes or last few lines of logs
- Filter those logs for a keyword (e.g., ""ERROR"" or ""timeout"")

I’ve seen --since=5m and --tail=100 options in kubectl logs, but I’m not sure how to combine them effectively with pattern matching while still being efficient.

How can I use kubectl to only get logs from, say, the last 5 minutes and filter for a keyword?

Is there a built-in way in kubectl to do this without relying entirely on grep?

If not, what’s the best practice for efficiently doing this in production environments?","kubernetes, kubectl",79734178.0,"Yes, if you’re only using kubectl, you can combine `--since` or `--tail` with grep to filter the relevant log lines.

like

```
# Last 200 lines + keyword
kubectl logs POD -n NS --tail=200 | grep -Ei 'error|timeout'
# Follow from last 5 minutes
kubectl logs POD -n NS --since=5m -f | grep -Ei 'error|timeout'
# All containers in pod
kubectl logs POD -n NS --since=5m --all-containers | grep -Ei 'error|timeout'
```

There are also external tools for working with Kubernetes logs — [stern](https://github.com/stern/stern) and [kail](https://github.com/boz/kail)  which let you conveniently view logs from multiple pods and filter them in real time. These tools can be safely used in production systems for faster troubleshooting.

Alternatively, you can use centralized log storage solutions like the **ELK stack** or **Loki** for more advanced searching and analysis.",2025-08-13T10:28:58,2025-08-13T04:55:39
79727761,MongoDB Compass connects to secondary node instead of primary in Replicasets,"I have a MongoDB replica set deployed in a Kubernetes cluster using the MongoDB Kubernetes Operator. I can connect to the database using mongosh from within the cluster, but when I try to connect using MongoDB Compass, it connects to a secondary node, and I cannot perform write operations (insert, update, delete).

In Compass, I get the following error:

```
single connection to server type : secondary is not writeable
```

I am unsure why Compass connects to a secondary node despite specifying readPreference=primary. The same URI connects successfully via CLI with write access.

I can connect below command in local cli or terminal ubuntu

```
kubectl exec --stdin --tty mongodb-0 -n mongodb -- mongosh ""mongodb://test:xxxxxx@mongodb-0.mongodb-svc.mongodb.svc.cluster.local:27017,mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017,mongodb-2.mongodb.svc.mongodb.svc.cluster.local:27017/test?replicaSet=mongodb&ssl=false""
```

Compass connects but in read-only mode

```
mongodb://test:xxxxxx@<external-ip>:27017/test?replicaSet=mongodb&readPreference=primary
```

Even with readPreference=primary, Compass shows I’m connected to a secondary node

Tried with directConnection:

```
mongodb://test:xxxxxx@<external-ip>:27017/test?directConnection=true&readPreference=primary
```

Fails to connect entirely.

Tried exposing all 3 MongoDB pods separately

```
mongodb-0-external -> <ip1>
mongodb-1-external -> <ip2>
mongodb-2-external -> <ip3>
```

Then tested

```
mongodb://test:xxxxxx@<ip1>:27017,<ip2>:27017,<ip3>:27017/test?replicaSet=mongodb&readPreference=primary
```

not connecting

Do i need change this also inside mongodb shell (i didnt change below because im not sure will this help or not)

```
cfg = rs.conf()
cfg.members[0].host = ""xxxxxx.251:27017""
cfg.members[1].host = ""xxxxxx.116:27017""
cfg.members[2].host = ""xxxxxx.541:27017""
rs.reconfig(cfg, { force: true })
```

I'm running a MongoDB replica set inside a Kubernetes cluster using the MongoDB Kubernetes Operator. I’m able to connect to the database using `mongosh` from within the cluster and perform read/write operations.

However, when I try to connect using **MongoDB Compass**, it connects to a **secondary node**, and I receive the error: single connection to server type : secondary is not writeable

Even though I’ve set `readPreference=primary` in the connection string, Compass still connects to a secondary node. I need Compass to connect to the **primary** node so I can write to the database.

Current replica set configuration (`rs.conf()`):

```
{
  _id: 'mongodb',
  version: 1,
  term: 27,
  members: [
    {
      _id: 0,
      host: 'mongodb-0.mongodb-svc.mongodb.svc.cluster.local:27017',
    },
    {
      _id: 1,
      host: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
    },
    {
      _id: 2,
      host: 'mongodb-2.mongodb-svc.mongodb.svc.cluster.local:27017',
      arbiterOnly: false,
    }
  ]
}
```

Below is shows that primary is mongodb-1

```
mongodb [primary] admin> rs.status()
{
  set: 'mongodb',
  date: ISODate('2025-08-06T17:33:17.598Z'),
  members: [
    {
      _id: 0,
      name: 'mongodb-0.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      syncSourceHost: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
    },
    {
      _id: 1,
      name: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
    },
    {
      _id: 2,
      name: 'mongodb-2.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      syncSourceHost: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
```

**What I'm trying to understand / solve:**

- Why does Compass always connect to a secondary node, even with `readPreference=primary`?

- How can I make Compass connect directly to the primary node for full read/write access?

Here is the
db.hello()

```
{
  topologyVersion: {
    processId: ObjectId('687fcd8688ee5b26ef882551'),
    counter: Long('13')
  },
  hosts: [
    'mongodb-0.mongodb-svc.mongodb.svc.cluster.local:27017',
    'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
    'mongodb-2.mongodb-svc.mongodb.svc.cluster.local:27017'
  ],
  setName: 'mongodb',
  setVersion: 1,
  isWritablePrimary: true,
  secondary: false,
  primary: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
  me: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
  electionId: ObjectId('7fffffff000000000000001b'),
  lastWrite: {
    opTime: { ts: Timestamp({ t: 1754585107, i: 2 }), t: Long('27') },
    lastWriteDate: ISODate('2025-08-07T16:45:07.000Z'),
    majorityOpTime: { ts: Timestamp({ t: 1754585107, i: 2 }), t: Long('27') },
    majorityWriteDate: ISODate('2025-08-07T16:45:07.000Z')
  },
  maxBsonObjectSize: 16777216,
  maxMessageSizeBytes: 48000000,
  maxWriteBatchSize: 100000,
  localTime: ISODate('2025-08-07T16:45:08.606Z'),
  logicalSessionTimeoutMinutes: 30,
  connectionId: 560323,
  minWireVersion: 0,
  maxWireVersion: 25,
  readOnly: false,
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1754585107, i: 2 }),

  },
  operationTime: Timestamp({ t: 1754585107, i: 2 })
```

Here is full rs.status()

```
ongodb [primary] admin> rs.status()
{
  set: 'mongodb',
  date: ISODate('2025-08-07T16:47:00.259Z'),
  myState: 1,
  term: Long('27'),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long('2000'),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 3,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
    lastCommittedWallTime: ISODate('2025-08-07T16:46:59.222Z'),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
    appliedOpTime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
    durableOpTime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
    writtenOpTime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
    lastAppliedWallTime: ISODate('2025-08-07T16:46:59.222Z'),
    lastDurableWallTime: ISODate('2025-08-07T16:46:59.222Z'),
    lastWrittenWallTime: ISODate('2025-08-07T16:46:59.222Z')
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1754585189, i: 8 }),
  electionCandidateMetrics: {
    lastElectionReason: 'stepUpRequestSkipDryRun',
    lastElectionDate: ISODate('2025-07-24T07:23:54.068Z'),
    electionTerm: Long('27'),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1753341831, i: 1 }), t: Long('26') },
    lastSeenWrittenOpTimeAtElection: { ts: Timestamp({ t: 1753341831, i: 1 }), t: Long('26') },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1753341831, i: 1 }), t: Long('26') },
    numVotesNeeded: 2,
    priorityAtElection: 1,
    electionTimeoutMillis: Long('10000'),
    priorPrimaryMemberId: 0,
    numCatchUpOps: Long('0'),
    newTermStartDate: ISODate('2025-07-24T07:23:54.098Z'),
    wMajorityWriteAvailabilityDate: ISODate('2025-07-24T07:23:54.196Z')
  },
  electionParticipantMetrics: {
    votedForCandidate: true,
    electionTerm: Long('26'),
    lastVoteDate: ISODate('2025-07-24T07:19:15.483Z'),
    electionCandidateMemberId: 0,
    voteReason: '',
    lastWrittenOpTimeAtElection: { ts: Timestamp({ t: 1753341555, i: 4 }), t: Long('25') },
    maxWrittenOpTimeInSet: { ts: Timestamp({ t: 1753341555, i: 4 }), t: Long('25') },
    lastAppliedOpTimeAtElection: { ts: Timestamp({ t: 1753341555, i: 4 }), t: Long('25') },
    maxAppliedOpTimeInSet: { ts: Timestamp({ t: 1753341555, i: 4 }), t: Long('25') },
    priorityAtElection: 1
  },
  members: [
    {
      _id: 0,
      name: 'mongodb-0.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1243052,
      optime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeDurable: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeWritten: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeDate: ISODate('2025-08-07T16:46:59.000Z'),
      optimeDurableDate: ISODate('2025-08-07T16:46:59.000Z'),
      optimeWrittenDate: ISODate('2025-08-07T16:46:59.000Z'),
      lastAppliedWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastDurableWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastWrittenWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastHeartbeat: ISODate('2025-08-07T16:47:00.258Z'),
      lastHeartbeatRecv: ISODate('2025-08-07T16:46:58.641Z'),
      pingMs: Long('0'),
      lastHeartbeatMessage: '',
      syncSourceHost: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
      syncSourceId: 1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 27
    },
    {
      _id: 1,
      name: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 1379070,
      optime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeDate: ISODate('2025-08-07T16:46:59.000Z'),
      optimeWritten: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeWrittenDate: ISODate('2025-08-07T16:46:59.000Z'),
      lastAppliedWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastDurableWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastWrittenWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1753341834, i: 1 }),
      electionDate: ISODate('2025-07-24T07:23:54.000Z'),
      configVersion: 1,
      configTerm: 27,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 2,
      name: 'mongodb-2.mongodb-svc.mongodb.svc.cluster.local:27017',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1242222,
      optime: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeDurable: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeWritten: { ts: Timestamp({ t: 1754585219, i: 8 }), t: Long('27') },
      optimeDate: ISODate('2025-08-07T16:46:59.000Z'),
      optimeDurableDate: ISODate('2025-08-07T16:46:59.000Z'),
      optimeWrittenDate: ISODate('2025-08-07T16:46:59.000Z'),
      lastAppliedWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastDurableWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastWrittenWallTime: ISODate('2025-08-07T16:46:59.222Z'),
      lastHeartbeat: ISODate('2025-08-07T16:47:00.077Z'),
      lastHeartbeatRecv: ISODate('2025-08-07T16:46:58.913Z'),
      pingMs: Long('1'),
      lastHeartbeatMessage: '',
      syncSourceHost: 'mongodb-1.mongodb-svc.mongodb.svc.cluster.local:27017',
      syncSourceId: 1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 27
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1754585219, i: 8 }),
    signature: {
    }
  },
```

Please remember that im still getting mongodb connected but when i click on collection it show secondary so sometime it get primary with DB name if we try after 10mint or 20 mint if try to connect it show secondary","mongodb, kubernetes, mongodb-query, mongodb-compass, mongodb-replica-set",79747194.0,"I deployed an additional service that connects to `mongodb-0`, which is currently the primary. After that, I connected using MongoDB Compass with the connection string, and it worked — it showed as primary instead of secondary.

The only remaining concern is that if `mongodb-0` restarts, then `mongodb-1` would become the new primary. In that case, I would still need to find a proper solution.

For now, my issue is resolved since this is a **Dev Environment**, and the chances of `mongodb-0` restarting are very low.",2025-08-26T18:23:56,2025-08-06T19:13:22
79727312,Why does kubectl logs --follow show duplicate output when used in a loop to watch Kubernetes Job logs?,"I'm running a Kubernetes Job from a Jenkins pipeline and want to stream its logs until completion.

I currently use this pattern in a Bash script:

```
job_status_cmd_complete=""kubectl get job ${kube_job_name} --namespace=${kube_namespace} -o jsonpath={..status.conditions[?(@.type=='Complete')].status}""
job_status_cmd_failed=""kubectl get job ${kube_job_name} --namespace=${kube_namespace} -o jsonpath={..status.conditions[?(@.type=='Failed')].status}""

function jobIsFinished {
  if [[ $($job_status_cmd_complete) == 'True' || $($job_status_cmd_failed) == 'True' ]]; then
    echo 'True'
  else
    echo 'False'
  fi
}

# First log stream — immediately after pod is ready
kubectl logs --tail=-1 --follow --selector=job-name=$job_name --namespace=$namespace -c $job_name

# Loop to check for completion
while [[ $(jobIsFinished) != 'True' ]]; do
  # Second log stream to catch more output in case the job is still running
  kubectl logs --tail=100 --follow --selector=job-name=$job_name --namespace=$namespace -c $job_name
done
```

Why I do this:

1. The first kubectl logs --follow is to get logs as soon as the pod is
ready.
2. The second one in the loop is a safeguard to continue watching if
the job is long-running or restarts.
3. This was intended to ensure I don't miss logs if the first stream
gets disconnected or if the pod is slow to start logging.

Issue:

1. This approach sometimes causes duplicate logs, especially in Jenkins output.
2. It's not clear whether --follow maintains log offset or restarts from the beginning in each call.

Is `kubectl logs --follow` stateless and prone to duplication if used multiple times?","bash, kubernetes, jenkins, logging, kubectl",,,,2025-08-06T12:54:02
79726462,vLLM + Ray multi-node tensor-parallel deployment completely blocked by pending placement groups and raylet heartbeat failures,"**Environment:**

- Ray version: 2.x
- vLLM version: 0.9.2
- Python version: 3.9
- OS / Container base: Linux (CentOS-based UBI8 in Kubernetes)
- Cloud / Infrastructure: AWS based Kubernetes cluster (pods scheduled on Tesla T4 GPU nodes)
**Other libs/tools:**
- CUDA 11.x
- NCCL for cross-node GPU communication

Expected
Actual

`--tensor-parallel-size 2` should shard my 16 GB Mamba-Codestral model across two 16 GB GPUs (one per pod) and launch successfully.
Ray logs show a **pending placement group** for two 1-GPU bundles (one pinned to the head, one “anywhere”), never scheduling the second bundle. vLLM then errors out.

Worker pod joins the Ray cluster and stays Alive.
The worker’s **raylet** process is repeatedly “marked dead” due to missed heartbeats (even on generous CPU/memory requests), then crashes its core-worker processes.

I set `VLLM_DISTRIBUTED_EXECUTOR_CONFIG='{""placement_group_options"":{""strategy"":""SPREAD""}}'` in env to force SPREAD placement.
Ray still uses the default **PACK** strategy, trying to place both shards on one node → placement group unsatisfiable → vLLM blocks.

HEAD Pod Snippet:
*NOTE* : Pipeline Parallelism is not supported for the mamba2 arch

```
ray start --head \
    --disable-usage-stats \
    --include-dashboard=false \
    --port=6379 \
    --node-ip-address=$VLLM_HOST_IP \
    --node-manager-port=6380 \
    --object-manager-port=6381

export RAY_ADDRESS=$VLLM_HOST_IP:6379

# Wait until Ray sees 2 GPUs, then:
python3 -m vllm.entrypoints.openai.api_server \
    --model /model-cache/Mamba-Codestral-7B-v0.1 \
    --tensor-parallel-size 2 \
    --distributed-executor-backend ray
```

WORKER Pod Snippet:

```
export RAY_ADDRESS=vllm-head:6379

ray start --disable-usage-stats \
    --include-dashboard=false \
    --address=$RAY_ADDRESS \
    --node-ip-address=$VLLM_HOST_IP \
    --node-manager-port=6380 \
    --object-manager-port=6381 \
    --block

sleep infinity
```

KEY LOGS:

```
Warning: The number of required GPUs exceeds the total number of available GPUs in the placement group. specs=[{'node:10.42.22.33':0.001,'GPU':1.0},{'GPU':1.0}]
...
Total Demands:
 {'GPU':1.0,'node:10.42.22.33':0.001} * 1,
 {'GPU':1.0} * 1 (PACK): 1+ pending placement groups
```

**Steps I’ve tried:**

1. Exported `VLLM_DISTRIBUTED_EXECUTOR_CONFIG='{""placement_group_options"":{""strategy"":""SPREAD""}}'` in both head & worker pods.
2. Increased CPU/memory requests for the worker’s raylet.
3. Tuned Ray’s health-check timeouts (`health_check_period_ms`, `health_check_timeout_ms`, etc.)

Despite all this, the second TP bundle never schedules (still PACK), and the worker raylet eventually dies of missed heartbeats.

Has anyone successfully run vLLM 0.9.2 + Ray 2.x in pure tensor-parallel multi-node mode? Any help is appreciated!","kubernetes, distributed-computing, ray, multi-gpu, vllm",,,,2025-08-05T17:38:16
79718907,Is GKE&#39;s 15s pod termination grace period always guaranteed on preemptible node shutdown?,"I'm running Pods on GKE preemptible nodes, and I've observed some inconsistencies with graceful shutdown behavior.

According to [GKE documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#graceful-shutdown):

> When Compute Engine needs to reclaim the resources used by preemptible
> VMs, a preemption notice is sent to GKE. Preemptible VMs terminate 30
> seconds after receiving a termination notice.
>
>
> By default, clusters use graceful node shutdown. The kubelet notices
> the termination notice and gracefully terminates Pods that are running
> on the node. If the Pods are part of a managed workload, such as a
> Deployment, the controller creates and schedules new Pods to replace
> the terminated Pods.
>
>
> On a best-effort basis, the kubelet grants a graceful termination
> period of 15 seconds for non-system Pods, after which system Pods
> (with the system-cluster-critical or system-node-critical
> priorityClasses) have 15 seconds to gracefully terminate. During
> graceful node termination, the kubelet updates the status of the Pods
> and assigns a Failed phase and a Terminated reason to the terminated
> Pods.
>
>
> The VM shuts down 30 seconds after the termination notice is sent even
> if you specify a value greater than 15 seconds in the
> terminationGracePeriodSeconds field of your Pod manifest.

My understanding is: non-system pods are expected to get 15 seconds to shut down gracefully, on a best-effort basis.

However, I'm observing that:

1. Some of my Pods do not appear to shut down gracefully — they don't seem to receive any signal, and logs that normally indicate shutdown (like Caught SIGTERM or Shutting down...) are missing.
2. I have a sidecar container that traps signals (SIGTERM, SIGINT, etc.) and logs them. It looks like this:

```
- name: signal-logger
  image: bash:5.2
  command: [""/usr/local/bin/bash"", ""-c""]
  args:
    - |
      echo ""Signal logger started on $(hostname) at $(date)""
      trap 'echo ""$(date) - $(hostname) - Caught SIGTERM"";' SIGTERM
      trap 'echo ""$(date) - $(hostname) - Caught SIGINT"";' SIGINT
      trap 'echo ""$(date) - $(hostname) - Caught SIGHUP"";' SIGHUP
      trap 'echo ""$(date) - $(hostname) - Caught SIGQUIT"";' SIGQUIT
      trap 'echo ""$(date) - $(hostname) - Caught SIGUSR1"";' SIGUSR1
      trap 'echo ""$(date) - $(hostname) - Caught SIGUSR2"";' SIGUSR2
      while true; do
        echo ""$(date) - $(hostname) - Alive""
        sleep 10
      done
  resources:
    limits:
      cpu: 100m
      memory: 64Mi
    requests:
      cpu: 10m
      memory: 16Mi
```

This works well most of the time, but sometimes it logs nothing — not even the signal. It's as if the container is terminated abruptly without any signal delivery.

# My questions:

Is it guaranteed that user pods always receive a SIGTERM and have up to 15s for graceful shutdown, even on preemptible node shutdowns?

Are there any known scenarios where this best-effort period is skipped or shortened (e.g. under load, node problems, shutdown method)?

How can I diagnose if kubelet failed to deliver the SIGTERM or the Pod didn’t get time to shut down?

Any advice on improving reliability or further troubleshooting this would be appreciated.","kubernetes, google-kubernetes-engine",79719150.0,"> Is it guaranteed that user pods always receive a SIGTERM and have up to 15s for graceful shutdown, even on preemptible node shutdowns?

No, it is not guaranteed that user pods will always receive a `SIGTERM` signal or be given the full 15 second graceful shutdown window during preemptible node shutdowns in GKE. This states that a 15 second termination period for non-system Pods is provided on a best-effort basis. The `kubelet` tries to send a `SIGTERM` to non-system pods and allows up to 15 seconds for them to shut down, followed by another 15 seconds for system pods with `system-cluster-critical` or `system-node-critical` priority classes. However, this process is not guaranteed, particularly in situations involving resource constraints, node overload, or rapid VM termination.

> Are there any known scenarios where this best-effort period is skipped or shortened (e.g. under load, node problems, shutdown method)?

Yes, there are possible scenarios where the best-effort 15 second graceful termination period may be skipped. These include :

- Compute Engine enforces a 30 second window for preemptible VM termination. If the kubelet’s graceful shutdown process such as `SIGTERM` delivery and pod cleanup takes longer than expected, the VM may be forcibly terminated before all pods finish their graceful shutdown.
- System pod prioritization with `system-cluster-critical` or `system-node-critical` priority classes are given priority during the second 15 second window of the 30 second shutdown period. If these pods consume significant resources or time, non-system pods may receive less than the intended 15 seconds.
- If the node is under heavy CPU, memory, high resource usage by pods or system processes the `kubelet` may struggle to process pod terminations promptly.
- If the `kubelet` is overloaded, misconfigured or crashes during the shutdown process, `SIGTERM` delivery may be skipped entirely. This could happen due to bugs, misconfigurations, or resource exhaustion.

> How can I diagnose if kubelet failed to deliver the SIGTERM or the Pod didn’t get time to shut down?

- You can check the pod or node events. Use `kubectl describe pod <pod name\>` / `kubectl describe node <node name\> `to inspect events related to the pod termination.
- You can inspect the kubelet logs access the kubelet logs on the affected node (if still available) to check for errors or warnings during the shutdown process. Look for messages about `SIGTERM`.
- Check GKE node logs or Container Runtime Logs.
- Monitor Node Preemption Metrics.

For further information and reference you can refer to these documentations :

- [Kubernetes best practices: terminating with grace](https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace)
- [Kubernetes Rolling Update and Termination Grace Periods](https://medium.com/brexeng/kubernetes-rolling-update-and-termination-grace-periods-d922c6b84d88#:%7E:text=Kubernetes%20Termination%20Period,been%20updated%20to%3A)
- [Graceful shutdown in Kubernetes](https://linkerd.io/2.18/tasks/graceful-shutdown/#graceful-shutdown-in-kubernetes)",2025-07-29T19:43:38,2025-07-29T15:57:20
79718397,How to retrieve the logs of the init containers which are still initializing?,"Using the C# Kubernetes Client, I'm trying to retrieve the logs of the init containers which are still initializing.

My current implementation look something like this:

```
foreach (V1Container container in pod.Spec.InitContainers)
{
  try
  {
    Stream logStream = await Client.CoreV1.ReadNamespacedPodLogAsync(pod.Name(), pod.Namespace(), container: container.Name, pretty: true);
    using (MemoryStream memoryStream = new MemoryStream())
    {
        await logStream.CopyToAsync(memoryStream);
        logs[container.Name] = Encoding.UTF8.GetString(memoryStream.ToArray());
    }
  } catch (HttpOperationException ex)
  {
    _logger.LogError(ex, ""Couldn't read logs of container {}.{}.{}"", pod.Namespace(), pod.Name(), container.Name);
    logs[container.Name] = null;
  }
}
```

I know the init containers aren't ran in parallel, so it's possible that they haven't started yet which would throw the HttpOperationException and is fine by me.

But the Kubernetes Api returns a bad request even when I'm sure the container is actually running:

```
  ""Exception"": ""k8s.Autorest.HttpOperationException: Operation returned an invalid status code 'BadRequest', response body {
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""container ""myinit"" in pod ""mypod"" is waiting to start: PodInitializing"",
  ""reason"": ""BadRequest"",
  ""code"": 400
```

So I was think that maybe there's a specific init container log endpoint or flag that has to be set, but when retrieving the logs with kubectl using verbose output, I don't see anything particular out of the ordinairy:

```
$> kubectl logs -c myinit mypod --since=""1s"" --tail=-1 -v=8
I0729 10:17:51.044577   20956 loader.go:402] Config loaded from file:  C:\Users\user\.kube\config
I0729 10:17:51.047262   20956 cert_rotation.go:141] ""Starting client certificate rotation controller"" logger=""tls-transport-cache""
I0729 10:17:51.047262   20956 envvar.go:172] ""Feature gate default state"" feature=""ClientsAllowCBOR"" enabled=false
I0729 10:17:51.047262   20956 envvar.go:172] ""Feature gate default state"" feature=""ClientsPreferCBOR"" enabled=false
I0729 10:17:51.047782   20956 envvar.go:172] ""Feature gate default state"" feature=""InformerResourceVersion"" enabled=false
I0729 10:17:51.047803   20956 envvar.go:172] ""Feature gate default state"" feature=""InOrderInformers"" enabled=true
I0729 10:17:51.047803   20956 envvar.go:172] ""Feature gate default state"" feature=""WatchListClient"" enabled=false
I0729 10:17:51.090637   20956 helper.go:105] ""Request Body"" body=""""
I0729 10:17:51.090637   20956 round_trippers.go:527] ""Request"" verb=""GET"" url=""https://127.0.0.1:60136/api/v1/namespaces/default/pods/mypod"" headers=<
        Accept: application/json, */*
        User-Agent: kubectl.exe/v1.33.0 (windows/amd64) kubernetes/60a317e
 >
I0729 10:17:51.105826   20956 round_trippers.go:632] ""Response"" status=""200 OK"" headers=<
        Audit-Id: c81e6ebe-7083-4a67-9368-6f52976960d3
        Cache-Control: no-cache, private
        Content-Type: application/json
        Date: Tue, 29 Jul 2025 08:17:51 GMT
        X-Kubernetes-Pf-Flowschema-Uid: 6595fa6f-4fc5-4093-a9ee-075cb3fae3cf
        X-Kubernetes-Pf-Prioritylevel-Uid: 4e69e2a5-e2db-42f0-9fe0-d08eca5e32ef
 > milliseconds=15
I0729 10:17:51.110826   20956 helper.go:105] ""Response Body"" body=""{\""kind\"":\""Pod\"",\""apiVersion\"":\""v1\"",\""metadata\"":{\""name\"":\""mypod\"",[truncated 16716 chars]""
I0729 10:17:51.119827   20956 logs.go:469] ""Request Body"" body=""""
I0729 10:17:51.119827   20956 round_trippers.go:527] ""Request"" verb=""GET"" url=""https://127.0.0.1:60136/api/v1/namespaces/default/pods/mypod/log?container=myinit&sinceSeconds=1"" headers=<
        Accept: application/json, */*
        User-Agent: kubectl.exe/v1.33.0 (windows/amd64) kubernetes/60a317e
 >
I0729 10:17:51.125939   20956 round_trippers.go:632] ""Response"" status=""200 OK"" headers=<
        Audit-Id: 21de526a-a9e2-422b-bdbe-03f423401593
        Cache-Control: no-cache, private
        Content-Type: text/plain
        Date: Tue, 29 Jul 2025 08:17:51 GMT
 > milliseconds=6
```

The request gets sent to `/api/v1/namespaces/default/pods/mypod/log` without anything specific to notify that it's an init container and gets a 200Ok response. I would expect that the C# Client to call the exact same endpoint when executing `CoreV1.ReadNamespacedPodLogAsync` though I haven't figured out how to get the exact endpoint that the client calls.

`kubectl version` gives the following information:

```
Client Version: v1.33.0
Kustomize Version: v5.6.0
Server Version: v1.33.1
```

So in short: How do I get the logs of an init container of which I'm sure is running?","c#, kubernetes",,,,2025-07-29T09:42:36
79714626,Why k8s resource doesn&#39;t increase apiVersion?,"I have a `CustomResourceDefinition`:

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  labels:
    app.kubernetes.io/name: applicationsets.argoproj.io
    app.kubernetes.io/part-of: argocd
  name: applicationsets.argoproj.io
....
```

In this CRD I have `goTemplateOptions` attribute defined.

It's here: [https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml](https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml)

But when I download the crd from helm repo: [https://artifacthub.io/packages/helm/argo/argo-cd/5.31.0?modal=template&template=crds/crd-applicationset.yaml](https://artifacthub.io/packages/helm/argo/argo-cd/5.31.0?modal=template&template=crds/crd-applicationset.yaml)

I can see that both `apiVersion` equal, but API themselves are different: no `goTemplateOptions` attribute defined in Helm.

Why do we have `apiVersion` equal with API different?

I presumed that version must have been different, through semantic versioning","kubernetes, kubernetes-helm",79714720.0,"Kubernetes doesn't use semantic versioning for its API objects.  Within the core object set, it's fairly common for fields to be *added* between releases without changing the `apiVersion:`.

There's some discussion of this in the documentation about [API groups and versioning](https://kubernetes.io/docs/concepts/overview/kubernetes-api/#api-groups-and-versioning):

> Versioning is done at the API level rather than at the resource or field level....
>
>
> In general, new API resources and new resource fields can be added often and frequently. Elimination of resources or fields requires following the [API deprecation policy](https://kubernetes.io/docs/reference/using-api/deprecation-policy/).

There is also a discussion of [API versioning](https://kubernetes.io/docs/reference/using-api/#api-versioning).  The most important thing here is that the compatibility rules for ""alpha"", ""beta"", and GA API versions are different: `v1alpha2` is allowed to make arbitrary changes (including removing objects and fields) from `v1alpha1`, ""beta"" versions may have breaking changes but need to clearly document them, and GA versions only have additive changes.

In the context of a Helm chart, for core Kubernetes API objects, you can at least use [`.Capabilities`](https://docs.helm.sh/docs/chart_template_guide/builtin_objects/) to check if the cluster is new enough to have some field

```
{{ if or (gt .Capabilities.KubeVersion 1) (ge .Capabilities.KubeVersion 32) }}
...
{{ end }}
```

For an extension type, this is harder to check, and you probably need a control in your chart to specify whether to use the newly-added field.  Many charts simply pass through more complex options from Helm values, and another possibility is to document in a comment in the chart's `values.yaml` that there's a version dependency and omit the field if it's not specified, leaving it up to the system administrator to not supply the value when it won't work.",2025-07-25T12:29:27,2025-07-25T11:08:38
79714201,How do you list all pods of a specific type using kubectl?,"In the past I would use something like this

```
kubectl get svc --field-selector spec.type=LoadBalancer
kubectl get svc --field-selector spec.type=ClusterIP
```

However this stopped working when I updated kubectl. I found documentation here [https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/#list-of-supported-fields](https://kubernetes.io/docs/concepts/overview/working-with-objects/field-selectors/#list-of-supported-fields) that explains the supported types for field-selector, and spec.type is not one of them.

I know one option is to use grep e.g. `kubectl get svc | grep LoadBalancer` but this seems incorrect. Currently I am using a workaround where I add a label named type and use `kubectl get svc --selector type=LoadBalancer` but again this seems like an improper method and we would have to add this tag to all pods just to make this work.","kubernetes, kubectl",79717622.0,"AFAIK, there is no official way but few work-arounds like:

- Using grep on the output:   `kubectl get svc | grep LoadBalancer`

It's a text filter not true selector.
- Adding a label to services and then selecting by label:

`kubectl get svc --selector type=LoadBalancer`",2025-07-28T16:19:31,2025-07-25T04:46:43
79712661,"WebSocket Connection to KubeVirt Console Established, but No TTY Output","I'm currently facing an issue when trying to connect to the KubeVirt VirtualMachineInstance (VMI) console serial using a WebSocket client.

### Implementation:

The WebSocket is initialized as follows:

```
const targetUrl = `${apiServer}/apis/subresources.kubevirt.io/v1alpha3/namespaces/${namespace}/virtualmachineinstances/${vmi}/console`;

const ws = new WebSocket(targetUrl, {
  headers: {
    Authorization: `Bearer ${token}`,
    ""Sec-WebSocket-Protocol"": ""base64.channel.kubevirt.io"",
  },
  rejectUnauthorized: false,
});
proxySocket.on(""open"", () => {
  socket.emit(""data"", ""Connected To VM WS"");
  console.log(""KubeVirt WS connected"");
});

proxySocket.on(""message"", (data) => {
  console.log(""Received data:"", data);
});
```

### WebSocket Behavior:

- WebSocket connection successfully opens (onopen is triggered).
- However, no data is received after the connection is established.
- No TTY output appears in the frontend/client. (after implement with xtermjs)
- The client remains silent, even after sending user input (such as \n).

If anyone's run into this before or has any idea what I might be missing, I'd really appreciate the help. Thanks a bunch!","kubernetes, websocket",79713747.0,"After conducting further research, I was able to resolve the issue independently. The solution involved converting the string into an ASCII array before sending it through the WebSocket.

```
        const enc = new TextEncoder().encode(data);
        proxySocket.send(enc);
```",2025-07-24T17:19:27,2025-07-24T00:15:48
79712495,Non existing field in persistentVolumeClaim helm template,"I was reviewing a Prometheus Helm chart and noticed that the following `prometheus/templates/alertmanager/pvc.yaml` template includes the `volumeBindingMode` field:

```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  {{- if .Values.alertmanager.persistentVolume.annotations }}
  annotations:
{{ toYaml .Values.alertmanager.persistentVolume.annotations | indent 4 }}
  {{- end }}
  labels:
    {{- include ""prometheus.alertmanager.labels"" . | nindent 4 }}
  name: {{ template ""prometheus.alertmanager.fullname"" . }}
{{ include ""prometheus.namespace"" . | indent 2 }}
spec:
  accessModes:
{{ toYaml .Values.alertmanager.persistentVolume.accessModes | indent 4 }}
{{- if .Values.alertmanager.persistentVolume.storageClass }}
{{- if (eq ""-"" .Values.alertmanager.persistentVolume.storageClass) }}
  storageClassName: """"
{{- else }}
  storageClassName: ""{{ .Values.alertmanager.persistentVolume.storageClass }}""
{{- end }}
{{- end }}
{{- if .Values.alertmanager.persistentVolume.volumeBindingMode }}
  volumeBindingMode: ""{{ .Values.alertmanager.persistentVolume.volumeBindingMode }}""
{{- end }}
```

As far as I know, the `volumeBindingMode` field is only valid in a `StorageClass` resource, not in a `PersistentVolumeClaim`.

However, this chart is widely used and published, so I am wondering:
Is `volumeBindingMode` ever supported in a `PVC` (e.g., in newer Kubernetes versions)?
Is it a mistake in the Helm chart?

I checked the official PVC documentation and couldn't find any mention of `volumeBindingMode`.

**About the chart**:

- Repository: [https://prometheus-community.github.io/helm-charts](https://prometheus-community.github.io/helm-charts)
- Version: 15.8.7

Any clarification is appreciated.

Thank you!","kubernetes, kubernetes-helm, azure-aks, kubernetes-pvc",,,,2025-07-23T19:56:01
79711345,attach to kubernetes pod fails in vscode as context does not exist in the session,"We are connecting to kubernetes cluster hosted in AWS.
I am able to do it via cli

```
aws sso login --profile 234234293-prj_ns_qa_manager

#set the context to appropriate sandbox cluster

kubectl config use-context qasandbox1-qa_manager
```

After this i am to install helm chart in the kubernetes cluster.
Also i am able to access pod via cli.

I have install vscode kubernetes extension. I am able to see the cluster and pod .When i attach vscode it opens in another session where this kubernetes context and cluster not found so it throws error as below.

```
[660 ms] Start: Run: kubectl exec -it my-debug-proj-cli-w7vng --context qasandbox1-qa_manager --namespace qa-manager --container proj-cli -- env VSCODE_REMOTE_CONTAINERS_SESSION=abb319e7-e9f2-4f8f-889c-b1183738e93f1753247958353 /bin/sh
[681 ms] Start: Run in container: echo $PATH
[815 ms] Shell server terminated (code: 1, signal: null)

error: context ""qasandbox1-qa_manager"" does not exist
```

How to attach to container in a pod with proper context? or is there a way to open container in the same session where i have configured context?","amazon-web-services, visual-studio-code, kubernetes, vscode-extensions",79739940.0,"I found the answer recently , I am adding this for other's benefit

Go to Files-> Preferences -> settings -> Vs-kubernetes -> Edit in settings.json

[![enter image description here](https://i.sstatic.net/8RHnJTKZ.png)](https://i.sstatic.net/8RHnJTKZ.png)

add kubectl path

```
""vs-kubernetes.kubectl-path"": ""/usr/local/bin/kubectl"",
```

I am using wsl ,so also enable below settings

[![enter image description here](https://i.sstatic.net/M6sOWUap.png)](https://i.sstatic.net/M6sOWUap.png)

After this I am able to attach to a pod",2025-08-19T13:09:30,2025-07-23T05:28:27
79703741,K8 gateway issue and the gateway api routes does not work,"GKE k8 gateway api route not working saying ""no healthy upstream"" even when the gateway and routes are created healthy .Below are the configs

```
apiVersion: networking.gke.io/v1
kind: GCPGatewayPolicy
metadata:
  name: monitoring-gateway-policy
spec:
  default:
    allowGlobalAccess: true
  targetRef:
    group: gateway.networking.k8s.io
    kind: Gateway
    name: monitoring-gateway

kind: Gateway
apiVersion: gateway.networking.k8s.io/v1beta1
metadata:
  name: monitoring-gateway
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    kubernetes.io/tls-acme: ""true""
spec:
  gatewayClassName: gke-l7-rilb
  listeners:
  - name: https
    protocol: HTTPS
    hostname: abc.com
    port: 443
    tls:
      mode: Terminate
      certificateRefs:
      - kind: Secret
        group: """"
        name: monitoring-ingress-tls
    allowedRoutes:
      namespaces:
        from: All

kind: HTTPRoute
apiVersion: gateway.networking.k8s.io/v1beta1
metadata:
  name: monitoring-routes
spec:
  parentRefs:
  - kind: Gateway
    name: monitoring-gateway
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /grafana
    backendRefs:
    - name: internal-grafana
      namespace: monitoring
      port: 3000
  - matches:
    - path:
        type: PathPrefix
        value: /chronograf
    backendRefs:
    - name: chronograf
      namespace: monitoring
      port: 80
  - backendRefs:
    - name: jaeger-query
      port: 80
    matches:
    - path:
        type: PathPrefix
        value: /jaeger
```

even the gateway and routes are healthy and pointing to the service , which has endpoints to the pods .
abc.com/grafana , abc.com/chronograf , abc.com/jaeger  does not work and says ""no healthy upstream""","kubernetes, google-kubernetes-engine, kubernetes-gateway-api",79706611.0,"The `""no healthy upstream""` error can be caused by several factors, most commonly related to failing health checks, missing or unhealthy endpoints, port mismatches, or general misconfigurations that prevent back-ends services from being reachable.

- Start by verifying the health check endpoints for `chronograf`, `internal-grafana` and `jaeger-query` in the Google Cloud Console and ensure they return HTTP 200.
- Service port mismatch, check the `HTTPRoute` specified ports for the backend services `3000` for internal-grafana, `80` for chronograf and `80` for jaeger-query. If these ports don’t match the actual ports exposed by the Service or the pod’s container, the gateway cannot route traffic correctly.

Run the following commands to validate:

```
kubectl get pods -n monitoring
kubectl describe svc <service-name> -n monitoring
kubectl describe endpoints <service-name> -n monitoring
```

Confirm that the Endpoints object for each service lists valid pod IPs. Ensure the backend pods are running and reachable.

Use the command below to confirm that each pod is listening on the correct port:

```
kubectl describe pod <grafana-pod-name> -n monitoring
```

Ensure the pod’s container is listening on the specified port example `Grafana` listens on `3000`, `Chronograf` on `80`, etc.

I also highly recommend to include the output of these `kubectl` commands in your post or comments to assist with further debugging.

For further reference you can refer to [Understanding No Healthy Upstream Error](https://uptrace.dev/blog/no-healthy-upstream).",2025-07-18T18:53:54,2025-07-16T16:31:48
79703261,How to send grpc request to the leader controller pod,"I have a k8s controller using leader-replica mode, so there're 2 pods, and they are behind a Service which exposes a grpc port.

Now I need to send grpc requests to the leader controller pod from another program.
If I just send to the Service's domain name, 50% of requests will reach the replica pod and won't reach the leader, which is not wanted.
How can I make sure the leader pod can always receive the request?

I can think of several ways, for example,

1. Query pod ip by the Service and broadcast requests to all of them
2. Query Lease to get the holder identity and then get the pod info
3. Let the leader add a label to itself and make the Service to match the label
4. ...

What is the best practice/ usual way of doing this?","kubernetes, grpc, kubernetes-service",,,,2025-07-16T10:14:23
79702386,How to deliver asynchronous image processing results via WebSocket in Kubernetes with horizontal scaling?,"I have a PWA where users upload images, and after asynchronous processing, results need to be sent back to them in real-time via WebSocket.

Problem:

- Users establish WebSocket connections to my backend to receive their processing results.
- WebSocket connections are tied to the memory of the specific backend pod that accepted them.
- When I scale horizontally in Kubernetes, processing results may arrive at a different pod that does not hold the user’s WebSocket connection, making it impossible to send the result to the correct user.

Question:

How can I design my architecture to:

1. Reliably deliver these asynchronous results to the user via WebSocket
2. Support horizontal scaling in Kubernetes
3. Handle pod restarts without losing pending result deliveries

Additional context:

- Connections are not authenticated; users are identified by a generated processing token.
- I want to avoid a single pod design for production scalability.

What I’ve considered:

- Sticky sessions (won’t work as processing Webhooks come from external services)
- pub/sub with a dedicated WebSocket service
- Using managed WebSocket gateways (AWS API Gateway WebSocket API)

What is the best architectural approach for this use case?","kubernetes, websocket, redis-cluster",,,,2025-07-15T16:31:55
79699411,ArgoCD ApplicationSet not deploying manifests from nested folder structure,"I'm trying to use ArgoCD ApplicationSet to deploy all manifests stored in my output/ folder in a Git repository.
Here is my folder structure:

```
output/
├── app1/
│   ├── deployment/
│   │   └── manifest1.yml
│   │   └── manifest2.yml
│   ├── service/
│   │   └── manifest.yml
│   └── serviceaccount/
│       └── manifest1.yml
│       └── manifest2.yml
├── app2/
│   ├── deployment/
│   │   └── manifest.yml
│   └── service/
│       └── manifest1.yml
│       └── manifest2.yml
└── app3/
    └── deployment/
        └── manifest.yml
```

And here the code of the appset:

```
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: output-appset
  namespace: my-namespace
spec:
  generators:
  - git:
      repoURL: https://gitlab.com/gitlab/repo.git
      revision: HEAD
      directories:
      - path: output/*
  template:
    metadata:
      name: '{{path.basenameNormalized}}'
      namespace: my-namespace
    spec:
      project: my-project
      source:
        repoURL: https://gitlab.com/gitlab/repo.git
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: my-namespace
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

The ApplicationSet creates successfully an Application for each app folder (app1, app2, app3), but each of them returns error:

```
'Lua returned an invalid health status'
```

None of my manifests gets deployed and when I try to open the Application in Argo I get error:

```
'Unable to load data: permission denied'
```

I want ArgoCD to automatically discover and deploy all manifests from the nested folder structure, ideally creating one Application per app folder (app1, app2, app3), but not necessarily. Ultimately I only need to deploy all the manifests found in the app's subfolders, I've already validated them by successfully running:

```
kubectl apply -f *.yaml -n my-namespace
```

How should I configure the ApplicationSet generator to handle this nested folder structure where manifests are located in subfolders within each application directory? I'd be also okay to use an Application instead.","kubernetes, argocd",79709423.0,"The solution was using a different ArgoCD project.

```
    spec:
      project: my-project
```

My-project was not allowing the creation of Application and ApplicationSet objects, so I had to use a different AppProject which doesn't explicitly deny the creation of these resources.",2025-07-21T17:16:12,2025-07-12T16:06:38
79699411,ArgoCD ApplicationSet not deploying manifests from nested folder structure,"I'm trying to use ArgoCD ApplicationSet to deploy all manifests stored in my output/ folder in a Git repository.
Here is my folder structure:

```
output/
├── app1/
│   ├── deployment/
│   │   └── manifest1.yml
│   │   └── manifest2.yml
│   ├── service/
│   │   └── manifest.yml
│   └── serviceaccount/
│       └── manifest1.yml
│       └── manifest2.yml
├── app2/
│   ├── deployment/
│   │   └── manifest.yml
│   └── service/
│       └── manifest1.yml
│       └── manifest2.yml
└── app3/
    └── deployment/
        └── manifest.yml
```

And here the code of the appset:

```
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: output-appset
  namespace: my-namespace
spec:
  generators:
  - git:
      repoURL: https://gitlab.com/gitlab/repo.git
      revision: HEAD
      directories:
      - path: output/*
  template:
    metadata:
      name: '{{path.basenameNormalized}}'
      namespace: my-namespace
    spec:
      project: my-project
      source:
        repoURL: https://gitlab.com/gitlab/repo.git
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: my-namespace
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

The ApplicationSet creates successfully an Application for each app folder (app1, app2, app3), but each of them returns error:

```
'Lua returned an invalid health status'
```

None of my manifests gets deployed and when I try to open the Application in Argo I get error:

```
'Unable to load data: permission denied'
```

I want ArgoCD to automatically discover and deploy all manifests from the nested folder structure, ideally creating one Application per app folder (app1, app2, app3), but not necessarily. Ultimately I only need to deploy all the manifests found in the app's subfolders, I've already validated them by successfully running:

```
kubectl apply -f *.yaml -n my-namespace
```

How should I configure the ApplicationSet generator to handle this nested folder structure where manifests are located in subfolders within each application directory? I'd be also okay to use an Application instead.","kubernetes, argocd",79705144.0,"You can set directory recursive = true for application/applicationset

Refer - [https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection](https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection)

```
apiVersion: argoproj.io/v1alpha1
kind: Application
spec:
  source:
    directory:
      recurse: true
```",2025-07-17T16:39:38,2025-07-12T16:06:38
79699234,why Argoworkflows multi-app-docker-build image with kaniko fails,"new to argworkflows ** I am trying to create a workflow that takes the name of the application that we are going to build the docker image for and push it to an ECR .**

this is my workflow.yaml:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env3
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env3
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]
          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            dependencies: [clone-and-check]
    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/application-$APP
            cd /workspace/application-$APP/$PATH_INSIDE_REPO
            ls -l
            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            cat /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo
            cat /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace
    - name: debug-list-files
      inputs:
        parameters:
          - name: name
          - name: path_inside_repo
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - ls -l /workspace
      volumeMounts:
        - name: workspace
          mountPath: /workspace
    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
          - name: path_inside_repo
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/application-{{inputs.parameters.name}}/{{inputs.parameters.path_inside_repo}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace

  volumes:
    - name: workspace
      emptyDir: {}
```

I my kaniko step fails with this error  :
[![kaniko pod error](https://i.sstatic.net/itMYRCaj.png)](https://i.sstatic.net/itMYRCaj.png)

although i did the cat in the previous step and i did add the dockerfile to the shared volume i can't understand why it can't find the dockerfile .
this is the previous step of logs :
[![git-checkout](https://i.sstatic.net/BOQgjDrz.png)](https://i.sstatic.net/BOQgjDrz.png)","docker, kubernetes, amazon-ecr, argo-workflows, argo",79726984.0,"As @[Thomas Delrue](https://stackoverflow.com/users/4958265/thomas-delrue) pointed out, the issue was caused by using an `emptyDir` volume. However, instead of switching to a PersistentVolume (PV), I initially intended to use artifacts .

Here's my updated Argo Workflow file:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]

          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            dependencies: [clone-and-check]

    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            # Clone to the artifact path
            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/source
            cd /workspace/source/$PATH_INSIDE_REPO

            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""

    - name: debug-list-files
      inputs:
        parameters:
          - name: name
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - |
            echo ""=== Listing /workspace/source ===""
            ls -la /workspace/source
            echo ""=== Listing application directory ===""
            ls -la /workspace/source/*/
            echo ""=== Finding Dockerfiles ===""
            find /workspace/source -name ""Dockerfile"" -type f

    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/source/{{inputs.parameters.name}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_SESSION_TOKEN
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SESSION_TOKEN
          - name: AWS_SDK_LOAD_CONFIG
            value: ""true""
```",2025-08-06T08:00:40,2025-07-12T11:34:10
79699234,why Argoworkflows multi-app-docker-build image with kaniko fails,"new to argworkflows ** I am trying to create a workflow that takes the name of the application that we are going to build the docker image for and push it to an ECR .**

this is my workflow.yaml:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env3
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env3
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]
          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            dependencies: [clone-and-check]
    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/application-$APP
            cd /workspace/application-$APP/$PATH_INSIDE_REPO
            ls -l
            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            cat /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo
            cat /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace
    - name: debug-list-files
      inputs:
        parameters:
          - name: name
          - name: path_inside_repo
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - ls -l /workspace
      volumeMounts:
        - name: workspace
          mountPath: /workspace
    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
          - name: path_inside_repo
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/application-{{inputs.parameters.name}}/{{inputs.parameters.path_inside_repo}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace

  volumes:
    - name: workspace
      emptyDir: {}
```

I my kaniko step fails with this error  :
[![kaniko pod error](https://i.sstatic.net/itMYRCaj.png)](https://i.sstatic.net/itMYRCaj.png)

although i did the cat in the previous step and i did add the dockerfile to the shared volume i can't understand why it can't find the dockerfile .
this is the previous step of logs :
[![git-checkout](https://i.sstatic.net/BOQgjDrz.png)](https://i.sstatic.net/BOQgjDrz.png)","docker, kubernetes, amazon-ecr, argo-workflows, argo",79717889.0,"I think your problem is using an `emptyDir` volume for sharing between tasks. The tasks themselves are different pods which might not even run on the same node, and not different containers sharing the same pod.

See GH issue on Argo Workflow project: [https://github.com/argoproj/argo-workflows/issues/3533](https://github.com/argoproj/argo-workflows/issues/3533)

Can't you use a persistent volume instead? Check the documentation for clear examples: [https://argo-workflows.readthedocs.io/en/latest/walk-through/volumes/](https://argo-workflows.readthedocs.io/en/latest/walk-through/volumes/)

If not, then try with an `emptyDir` and node affinity to make sure the tasks are on the same node, as suggested in the linked GH issue",2025-07-28T21:19:44,2025-07-12T11:34:10
79698910,I cannot get my pod to scale from 1 to 2 instances,"I have a strategy that i'd like to implement that has a consumer(background worker) pod that uses keda to scale from 0 - 5 replicas.

The source of the scaling is kafka topic with a lagThreashold of 1:

```
{{- if .Values.keda.enabled }}
apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: job-consumer-scaledobject
  namespace: {{ .Release.Namespace }}
  annotations:
    # Enable debug annotations for troubleshooting
    scaledobject.keda.sh/transfer-hpa-labels: ""true""
spec:
  scaleTargetRef:
    name: job-consumer-app
  pollingInterval: {{ .Values.keda.pollingInterval }}
  minReplicaCount: {{ .Values.keda.minReplicas }}
  maxReplicaCount: {{ .Values.keda.maxReplicas }}
  idleReplicaCount: {{ .Values.keda.idleReplicas }}
  cooldownPeriod: {{ .Values.keda.cooldownPeriod }}
  triggers:
  - type: kafka
    metadata:
      bootstrapServers: ""{{ .Values.kafka.serviceName }}.{{ .Release.Namespace }}.svc.cluster.local:{{ .Values.kafka.servicePort }}""
      consumerGroup: ""{{ .Values.keda.consumerGroup }}""
      topic: ""{{ .Values.keda.topic }}""
      lagThreshold: ""{{ .Values.keda.lagThreshold }}""
      offsetResetPolicy: latest
      # Allow scaling from zero when consumer group doesn't exist yet
      allowIdleConsumers: ""false""
      # Enable scaling from zero by checking topic lag even without active consumers
      scaleToZeroOnInvalidOffset: ""false""
      # Add debug logging
      logLevel: ""debug""
{{- end }}
```

The problem i am having is that it will scale from 0 - 1 just fine.. But it will not scale from 1 - 2 no matter what the current lag is.

Here are the keda-operator logs showing that it is querying the keta topic correctly:

```
2025-07-11T22:28:59Z    DEBUG   kafka_scaler    Kafka scaler: Providing metrics based on totalLag 500, topicPartitions 1, threshold 1   {""type"": ""ScaledObject"", ""namespace"": ""default"", ""name"": ""job-consumer-scaledobject""}
```

But when communicating the metric to the hpa it always sends 1:

```
2025-07-11T22:28:47Z    DEBUG   grpc_server     Providing metrics       {""scaledObjectName"": ""job-consumer-scaledobject"", ""scaledObjectNamespace"": ""default"", ""metrics"": ""&ExternalMetricValueList{ListMeta:{   <nil>},Items:[]ExternalMetricValue{ExternalMetricValue{MetricName:s0-kafka-jobs-topic,MetricLabels:map[string]string{},Timestamp:2025-07-11 22:28:47.980947591 +0000 UTC m=+236.787074315,WindowSeconds:nil,Value:{**{1000 -3}** {<nil>}  DecimalSI},},},}""}
```

```
kubectl describe hpa keda-hpa-job-consumer-scaledobject

Reference:                                       Deployment/job-consumer-app
**Metrics:                                         ( current / target )
  ""s0-kafka-jobs-topic"" (target average value):  1 / 1**
Min replicas:                                    1
Max replicas:                                    5
Deployment pods:                                 1 current / 1 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from external metric s0-kafka-jobs-topic(&LabelSelector{MatchLabels:map[string]string{scaledobject.keda.sh/name: job-consumer-scaledobject,},MatchExpressions:[]LabelSelectorRequirement{},})
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:           <none>
```

Im pulling my hair out over hear trying to understand why my target stays at 1 when the lag is clear over 500.  Any thoughts?  Thanks!","kubernetes, apache-kafka, keda",79699027.0,"```
topicPartitions 1
```

It is impossible to scale this consumer group; you're limited to one consumer process within a group, **per partition**

Even if your Deployment did scale, you'd have 4 idle pods after a consumer group rebalance and lag would therefore remain the same",2025-07-12T05:26:02,2025-07-11T23:22:01
79698891,Volume mounting issues with Docker-in-Docker in GitLab Runner Kubernetes executor,"Problem Summary
I'm running a GitLab CI pipeline with a Kubernetes runner that uses Docker-in-Docker (privileged mode). When I try to mount a volume from the GitLab Runner's filesystem into a Docker container, the files exist on the host but are not accessible inside the container.
Environment
GitLab Runner: Kubernetes executor with privileged mode
Docker: Running in privileged container with Docker socket mounted
**GitLab Runner Config:**

```
config: |
    [[runners]]
      id = 0
      output_limit = 100000
      [runners.kubernetes]
        namespace = ""{{.Release.Namespace}}""
        image_pull_secrets = [""my-registry-secret""]
        memory_request = ""1048Mi""
        cpu_request = ""500m""
        logs_section_max_size = 52428800  # 50 MB
        timeout = 3600
        privileged = true
        helper_cpu_request = ""500m""
        helper_memory_request = ""168Mi""
        helper_image = ""docker.io/gitlab/gitlab-runner-helper:x86_64-v17.0.0""
        pull_policy = ""if-not-present""
        image = ""xxxxxxxxxxxxxx""
        # Enable shared storage for docker in docker
        builds_dir = ""/builds""
        cache_dir = ""/cache""
        [runners.kubernetes.volumes]
          [[runners.kubernetes.volumes.host_path]]
            name = ""docker""
            mount_path = ""/var/run/docker.sock""
            read_only = false
            host_path = ""/var/run/docker.sock""
          [[runners.kubernetes.volumes.host_path]]
            name = ""cache""
            mount_path = ""/cache""
            read_only = false
            host_path = ""/tmp/cache""
          [[runners.kubernetes.volumes.host_path]]
            name = ""builds""
            mount_path = ""/builds""
            read_only = false
            host_path = ""/tmp/gitlab-runner/builds""
          [[runners.kubernetes.volumes.host_path]]
            name = ""runner-secrets""
            mount_path = ""/etc/gitlab-runner/certs""
            read_only = true
            host_path = ""/etc/ssl/certs""
```

**Current Code**

```
container_monorepo_path = ""/workspace""
  docker_command = (
    f'docker run --rm '
    f'-v {monorepo_root}:{container_monorepo_path} '
    f'-w {container_monorepo_path}/{relative_project_path} '
    f'--user root '
    f'private-registry.company.com/docker-adv-all/docker-agent-playwright '
    f'sh -c ""'
    f'npm config set strict-ssl false && '
    f'npm install -g pnpm nx && '
    f'pnpm nx run {project_name}:{task}""'
  )
```

Actual Error Output

```
[2025-07-11 22:24:36] - [INFO]: running command => docker run --rm -v /builds/t3_zrKwM/0/550016304/cp-monorepo:/workspace -w /workspace/packages/frontend/actions --user root private-registry.company.com/docker-agent-playwright:latest sh -c ""npm config set strict-ssl false && npm install -g pnpm nx && pnpm nx run actions:test-component""
[2025-07-11 22:24:46] - [INFO]: added 125 packages in 9s
[2025-07-11 22:24:46] - [INFO]: 28 packages are looking for funding
[2025-07-11 22:24:46] - [INFO]: run `npm fund` for details
[2025-07-11 22:24:46] - [ERROR]: npm notice
[2025-07-11 22:24:46] - [ERROR]: npm notice New major version of npm available! 9.5.1 -> 11.4.2
[2025-07-11 22:24:46] - [ERROR]: npm notice Changelog: <https://github.com/npm/cli/releases/tag/v11.4.2>
[2025-07-11 22:24:46] - [ERROR]: npm notice Run `npm install -g npm@11.4.2` to update!
[2025-07-11 22:24:46] - [ERROR]: npm notice
[2025-07-11 22:24:46] - [INFO]: ERR_PNPM_NO_IMPORTER_MANIFEST_FOUND  No package.json (or package.yaml, or package.json5) was found in ""/workspace/packages/frontend/actions"".
[2025-07-11 22:24:47] - [ERROR]: ❌ test-component failed
```","docker, kubernetes, gitlab-ci-runner, docker-in-docker",,,,2025-07-11T22:41:12
79698488,Restricting Kafka K8s Apps to Topics,"We are looking for restricting a k8s app, so it can only produce/consume to/from defined kafka topics.
The communications are already encrypted with SSL Certificates (no mTLS yet).

Our thoughts are:

- Either using Kafka SASL_SSL (PLAIN) Method. But dynamic user/password changes seems not to work. Solution could be: using a custom implementation of ""sasl.server.callback.handler""
- Using k8s network policies to restrict app communication to kafka listener (using different ports). Inside kafka use listener name to build the Principal. Applying Kafka ACLs.
- Use mTLS

In case of any sort of credential: it should be updatedable without restarting the kafka nodes (but k8s apps can be restarted).

Any experience ?","kubernetes, security, apache-kafka",79728320.0,"We decided to go the mTLS way.
Because:

- Changing client/server certificate is already implemented (k8s cert-manager/acme.sh and kafka's dynamic key-/truststore reload mechanism)
- Replacing a CA works well because you can have multiple CAs in Kafka's truststore
- Long running ssl-connections are not interrupted by changes to key-/truststore
- well supported by most kafka client libraries",2025-08-07T08:53:06,2025-07-11T14:28:45
79697905,The ocp application service cannot be connected correctly,"The route is set and the pod is started.

But when you enter [https://www.test.gov.tw/my-apps](https://www.test.gov.tw/my-apps)
an error page will appear.
[ocp error page](https://i.sstatic.net/xyN3CniI.png)

The following issues have already been checked:

The Service is not correctly mapped to the corresponding Pod: It's possible that the Service's selector or target port is misconfigured, preventing the request from being properly forwarded to the application running inside the Pod.

The Pod is running, but the application has not started correctly or is not bound to the expected port: Please ensure that the application is listening on the correct port and has completed its startup process.

Route configuration issues: Although the Route exists, it may be pointing to an incorrect Service name or path. Additionally, the Route path might not match the actual context path of the application service, leading to routing errors.

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app: my-apps
  name: my-apps
  namespace: test-systems
  resourceVersion: ""48338578""
  uid: 36692e79-0f08-4416-8242-cdb0087900da
spec:
  clusterIP: 172.26.110.30
  clusterIPs:
  - 172.26.110.30
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: http
  - name: actuator
    port: 5678
    protocol: TCP
    targetPort: actuator
  selector:
    app: my-apps
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app: my-apps
  name: my-apps-qbrjf
  namespace: test-systems
  resourceVersion: ""48406718""
  uid: 47e6b206-a15a-4b51-9853-a11d4d1243d4
spec:
  host: www.test.gov.tw
  path: /my-apps
  port:
    targetPort: http
  to:
    kind: Service
    name: my-apps
    weight: 100
  wildcardPolicy: None
```","kubernetes, routes, openshift",79697914.0,"The YAML snippet you provided is the TLS configuration section of an OpenShift Route. It defines how HTTPS/TLS connections are handled.

Client → HTTPS → Router (TLS termination) → HTTP → Pod

If the client tries to access via HTTP, the router sends a 302 redirect to the HTTPS URL.

```
tls:
  insecureEdgeTerminationPolicy: Redirect
  termination: edge
```

[https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes](https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes)",2025-07-11T06:25:18,2025-07-11T06:13:28
79697768,Trying to pass Kafka message bodies as environment variables through an Argo Sensor that triggers jobs,"I am attempting to create a Kubernetes cluster that uses a Kafka eventsource that has an argo sensor trigger a job.  I am trying to get the content event body set as an environment variable in my job so that i can use it durring processing.   The problem is that I am getting an empty environment variable.  Here is my helm chart for my argo sensor:

```
apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: kafka-job-sensor
spec:
  dependencies:
    - name: kafka-job-dep
      eventSourceName: kafka
      eventName: jobs-topic
  triggers:
    - template:
        name: job-trigger
        k8s:
          group: batch
          version: v1
          resource: jobs
          operation: create
          source:
            resource:
              apiVersion: batch/v1
              kind: Job
              metadata:
                generateName: job-consumer-job-
              spec:
                backoffLimit: 0
                template:
                  spec:
                    containers:
                    - name: job-consumer-app
                      image: jobconsumerapp:latest
                      imagePullPolicy: IfNotPresent
                      env:
                      - name: EVENT_BODY
                        value: ""{{ .Input }}""
                    restartPolicy: Never
```

Any pointers in the right direction would be appreciated.

Ive tried all of the following without any luck:

```
env:
- name: EVENT_BODY
  value: {{ .Input }}
- name: DEBUG_DATA
  value: {{ .Data }}
- name: DEBUG_BODY
  value: {{ .Body }}
```","kubernetes, kubernetes-helm, argo-events",79697799.0,"Assuming those are supposed to be event variables, when you are using Helm, you must escape the template brackets that Helm is interpreting for values first

```
""{{`{{ .Input }}`}}""
```

Personally, I'd recommend using a long running Kafka consumer pod with KEDA for scaling, rather than starting a new pod per event",2025-07-11T03:46:27,2025-07-11T02:29:54
79697039,can&#39;t run linter goanalysis_metalinter: failed to load package environment,"If golangci-lint gives you an error like this, what can I do to fix it?

```
ERRO Running error: can't run linter goanalysis_metalinter
buildir: failed to load package environment: could not load export data: no export data for ""k8s.io/apiserver/pkg/cel/environment""
```

I see no additional hints in the output of golangci-lint","go, kubernetes, golangci-lint, common-expression-language, cluster-api",79697040.0,"Be sure that the dependencies you use, use the same version of CEL.

In my case I could solve it like this:

I download go.mod from cluster-api. Be sure to use the version which you use in your `go.mod` file.

Then update your `go.mod` file.

Finally the versions should be equal:

```
❯ grep cel  go.mod ../cluster-api/go.mod
go.mod: cel.dev/expr v0.18.0 // indirect
go.mod: github.com/google/cel-go v0.22.0 // indirect
../cluster-api/go.mod:  github.com/google/cel-go v0.22.0
../cluster-api/go.mod:  cel.dev/expr v0.18.0 // indirect
```

Then run `go mod tidy`.

After that the error of goanalysis_metalinter was gone.",2025-07-10T12:17:22,2025-07-10T12:17:22
79693471,How are Kubernetes control plane components started when they run as static Pods?,"I've been diving deep on my learning Kubernetes clusters. I want to know how `kubeadm` starts up each control plane component and in what order?

After reading the Kubernetes documents, I wasn't sure. Doing some more reading I started to understand that it may be the `kubelet` service that does it by picking up the static Pod manifests in `/etc/kubernetes/manifests`. I think that sounds about right, since the kubelet is the only service that is running on the host machine (and not as a Pod in the cluster); and it handles static Pods.

I didn't want to waste time going down the wrong path so then I asked AI (Google, and MS CoPilot) to see if I could get the warm fuzzies and the pat on the back. They both said No, because it doesn't. Then they started to lecture me by droning on about how the `kubelet` has other responsibilities, see snippet here:

[![enter image description here](https://i.sstatic.net/4aS8CsvL.png)](https://i.sstatic.net/4aS8CsvL.png)

The AI seems to be explaining the difference in each component and missed what I was asking here. But I guess this could be right. It would be great if the Kubernetes documentation laid out the manual steps for setting up a cluster. I find it to be **very** helpful in the learning process to better comprehend the architecture.","kubernetes, process, boot, kubeadm",79695435.0,"Your assumptions are correct and AI is hallucinating. Static pods ARE being started by kubelet and providing the `kube-apiserver` is deployed as a static pod, then yes it is indeed started by kubelet (as any other static/non-static pod). There are cases and k8s distros where api-server can be provided as a service. In this case the api-server doesn't run as a pod in the cluster.

Check [https://docs.aws.amazon.com/eks/latest/best-practices/control-plane.html](https://docs.aws.amazon.com/eks/latest/best-practices/control-plane.html) for example. Also tools like kind or k3d doesn't run the `kube-apiserver` as a dedicated pod.

To answer your question, you can review the `kubeadm` [Implementation details](https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/). Which lay out exactly how it does it in good details.

These are from the official Kubernetes documentation. You can navigate to it by following the breadcrumbs **Kubernetes Documentation > Reference > Setup tools > Kubeadm > Implementation details**.  There they provide this hint [Generate static Pod manifests for control plane components](https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/#generate-static-pod-manifests-for-control-plane-components) which cements the idea that the `kube-apiserver` (starts as a static Pod) with a manifest that `kubeadm` generates and places at `/etc/kubernetes/manifests/kube-apiserver.yaml`.

Furthermore, the installing `kubeadm` process [seen here](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#installing-kubeadm-kubelet-and-kubectl); instructs you to install the `kubelet` package as a service of systemd (if using systemd). Where you can optionally enable the `kubelet` service, but not to start it at that point. `kubeadm` will start it at the right time. The implementation details also state ""The kubelet watches this directory for Pods to be created on startup"". Thus verifying that this is how the `kube-apiserver` pod is started.",2025-07-09T09:57:27,2025-07-07T22:33:46
79692778,How to host a simple REST service as a Kubernetes pod,"I have been working in Kubernetes. I am well aware of all the components like pod, deployment, service etc.

But I am not able to get the whole picture and hence need help.

Suppose I want to create a new pod only for this small Java program:

```
package com.test;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;

@Path(""/hello"")
public class HelloWorldRestService {

    @GET
    @Produces(MediaType.TEXT_PLAIN)
    public String getMessage(){
        return ""Hello World"";
    }
}
```

In that case what all do I need to launch this pod in the Kubernetes cluster? REST clients outside the cluster should be able to access this URL.

So I need:

1. The image and the image should be hosted in some registry
2. The pod yaml file
3. The deployment yaml file
4. Route table
5. Service
6. Gateway
7. Load balancer

But what should be the configuration in each of them and how they will be related to each other?

I have followed many tutorials etc, but everyone is just explaining the concepts, not sharing the YAML code. How can I achieve the same through coding?","kubernetes, cloud",79692889.0,"Here are the rough steps (code snippets are untested, but hopefully give you the basic idea):

1. **Build your app**. First, you need to build your app into some sort of deployable artifact. In the Java world, the most common way to do this is to create a `.jar` file. One option is to do this manually (this assumes your Java code is in the `src` folder):

```
javac -d ./build src/*.java
jar cvf app.jar ./build/*
```

That said, a more realistic option would be to use a build system such as [Gradle](https://gradle.org/) or [Maven](https://maven.apache.org/) to manage your build and dependencies.
2. **Package your app as a Docker image**. Next, you need to package your app artifact (the `.jar` file) as a Docker image.

```
# Use OpenJDK 17 as base image
FROM openjdk:17-jdk-slim

# Set working directory
WORKDIR /app

# Copy source code
COPY src/ ./src/

# Create directory for compiled classes
RUN mkdir -p build

# Create jar file. If you use Gradle or Maven, run those here instead.
RUN javac -d ./build src/*.java
RUN jar cvf app.jar ./build/*

# Set default command to run the JAR file
CMD [""java"", ""-jar"", ""app.jar""]
```

To build the Docker image:

```
docker build -t my-app:v1 .
```
3. **Push to a Docker registry**. You now have a Docker image, but it's only on your own computer. Your Kubernetes cluster won't be able to access it there, so you need to push the image to a registry that is accessible to the cluster. For example, you might use [Docker Hub](https://hub.docker.com/) as a registry. You can use the web UI in Docker Hub to create a user for yourself named `username` and a new repository under that user named `my-app`. You can then login to Docker hub:

```
docker login
```

This will allow you to login via your web browser. Once authenticated, tag your Docker image with your Docker Hub username and repo name, and push the image:

```
docker tag my-app:v1 username/my-app:v1
docker push username/my-app:v1
```
4. **Create a `Deployment`**. There are many ways to deploy apps in Kubernetes. One option is to create a `Deployment`, which is a declarative way to manage an application in Kubernetes. The Deployment allows you to declare which Docker images to run, how many copies of them to run (replicas), a variety of settings for those images (e.g., CPU, memory, port numbers, environment variables), and so on, and the Deployment will then work to ensure that the requirements you declared are always met. Here's the YAML for a basic `Deployment`:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: sample-app-pods
    spec:
      containers:
        - name: sample-app
          # Specify the Docker image to deploy from your Docker registry
          image: username/my-app:v1
          ports:
            # Specify the port your app listens on for HTTP requests
            - containerPort: 8080
  selector:
    matchLabels:
      app: sample-app-pods
```

Note that if your app is in a private Docker registry, you'll have to [give your Kubernetes cluster a way to authenticate to that registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).

You can use [kubectl](https://kubernetes.io/docs/reference/kubectl/) to create this `Deployment`. First, you need to [authenticate to your Kubernetes cluster](https://kubernetes.io/docs/reference/access-authn-authz/authentication/). How you do this depends on the cluster. For example, if you're using the local [Kubernetes cluster built into Docker Desktop](https://docs.docker.com/desktop/features/kubernetes/), you can authenticate to it as follows:

```
kubectl config use-context docker-desktop
```

If the YAML for the `Deployment` is in a file called `deployment.yml`, you can create it as follows:

```
kubectl apply -f deployment.yml
```
5. **Create a `Service`**. A `Deployment` will get your app running in the cluster, but it won't make it available to other services over the network. To expose your app to the outside world, you can create a `Service`:

```
apiVersion: v1
kind: Service
metadata:
  name: sample-app-loadbalancer
spec:
  type: LoadBalancer
  selector:
    app: sample-app-pods
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

If the YAML for this `Service` is in `service.yml`, you can create it as follows:

```
kubectl apply -f service.yml
```
6. **Test**. It'll take a minute or two for everything to deploy. To see the status of your `Deployment`:

```
kubectl describe deployment sample-app-deployment
```

To see the status of your `Service`:

```
kubectl describe service sample-app-loadbalancer
```

If everything is working, that last command should output a `LoadBalancer Ingress` field, which shows you the URL to use for the load balancer. You can then test that URL:

```
curl http://<<URL>
```

If everything is working, you should see ""Hello, World.""

This is a minimal deployment just for learning. I've glossed over many details. For a lot more info, including working & tested code examples, check out the Container Orchestration section of [How to manage your apps using orchestration tools](https://books.gruntwork.io/books/fundamentals-of-devops/deploying-apps-orchestration-vms-containers-serverless) (an article I wrote).",2025-07-07T12:57:43,2025-07-07T11:23:56
79690752,hudson.remoting.ProxyException: java.nio.file.AccessDeniedException: /var/jenkins_home,"**Jenkins or the container process tried to write to `/var/jenkins_home`, but it lacked the required file system permissions**.

Jenkins pipelines on Kubernetes using a shared `podTemplate` that dynamically defines containers and volumes for CI/CD jobs. One of the stages involves using the `withCredentials` Jenkins step to bind a GCP service account key file (of type `Secret file`) to authenticate with Google Cloud using `gcloud auth activate-service-account --key-file=$GCLOUD_KEY`.

Despite properly mounting `/var/jenkins_home` as an `emptyDirVolume`, the pipeline fails with

```
 hudson.remoting.ProxyException: java.nio.file.AccessDeniedException: /var/jenkins_home
```

error when trying to access `$GCLOUD_KEY`. Logs reveal that the `credentials-binding` plugin attempts to write the key file to `$JENKINS_HOME`, which defaults to `/var/jenkins_home`, but the file creation silently fails due to insufficient write permissions.

Attempts to scope the pipeline within `dir(""${WORKSPACE}"")` and define a writable path failed, as Jenkins still targets `$JENKINS_HOME` internally. The container runs as root, but permission issues persist due to volume mount ownership.

I don't want to add a `securityContext` to the pod template, setting `runAsUser`, `runAsGroup`, and `fsGroup` all to `0` because I don't want all processes to run as root, as its not a good practice

below is the withCredentials code line -

```
withCredentials([file(credentialsId: 'key-sa-prd', variable: 'GCLOUD_KEY')]) {
                        key_json = readFile(""${GCLOUD_KEY}"")
                    }
                    writeFile file: 'key.json', text: key_json
                    sh ""cat key.json""
                    sh ""gcloud auth activate-service-account --key-file=key.json""
                    sh ""gcloud auth configure-docker ${registry}""
```

and for the above code getting below error.

```
14:49:50  Also:   hudson.remoting.ProxyException: hudson.remoting.Channel$CallSiteStackTrace: Remote call to JNLP4-connect connection from 172.29.193.183/172.29.193.183:51850
14:49:50        at hudson.remoting.Channel.attachCallSiteStackTrace(Channel.java:1916)
14:49:50        at hudson.remoting.UserRequest$ExceptionResponse.retrieve(UserRequest.java:384)
14:49:50        at hudson.remoting.Channel.call(Channel.java:1108)
14:49:50        at hudson.FilePath.act(FilePath.java:1207)
14:49:50        at hudson.FilePath.act(FilePath.java:1196)
14:49:50        at hudson.FilePath.write(FilePath.java:2492)
14:49:50        at hudson.FilePath.copyFrom(FilePath.java:1125)
14:49:50        at PluginClassLoader for credentials-binding//org.jenkinsci.plugins.credentialsbinding.impl.FileBinding.write(FileBinding.java:54)
14:49:50        at PluginClassLoader for credentials-binding//org.jenkinsci.plugins.credentialsbinding.impl.FileBinding.write(FileBinding.java:42)
14:49:50        at PluginClassLoader for credentials-binding//org.jenkinsci.plugins.credentialsbinding.impl.AbstractOnDiskBinding.bindSingle(AbstractOnDiskBinding.java:38)
14:49:50        at PluginClassLoader for credentials-binding//org.jenkinsci.plugins.credentialsbinding.Binding.bind(Binding.java:149)
14:49:50        at PluginClassLoader for credentials-binding//org.jenkinsci.plugins.credentialsbinding.impl.BindingStep$Execution2.doStart(BindingStep.java:132)
14:49:50        at PluginClassLoader for workflow-step-api//org.jenkinsci.plugins.workflow.steps.GeneralNonBlockingStepExecution.lambda$run$0(GeneralNonBlockingStepExecution.java:77)
14:49:50        at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
14:49:50        at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
14:49:50        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
14:49:50        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
14:49:50        at java.base/java.lang.Thread.run(Unknown Source)
14:49:50  Also:   hudson.remoting.ProxyException: org.jenkinsci.plugins.workflow.actions.ErrorAction$ErrorId: 77d61711-f1e1-4ac0-bb85-39d615986317
14:49:50  hudson.remoting.ProxyException: java.nio.file.AccessDeniedException: /var/jenkins_home
14:49:50    at java.base/sun.nio.fs.UnixException.translateToIOException(Unknown Source)
14:49:50    at java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)
14:49:50    at java.base/sun.nio.fs.UnixException.rethrowAsIOException(Unknown Source)
14:49:50    at java.base/sun.nio.fs.UnixFileSystemProvider.createDirectory(Unknown Source)
14:49:50    at java.base/java.nio.file.Files.createDirectory(Unknown Source)
14:49:50    at java.base/java.nio.file.Files.createAndCheckIsDirectory(Unknown Source)
14:49:50    at java.base/java.nio.file.Files.createDirectories(Unknown Source)
14:49:50    at Jenkins v2.504.2//hudson.FilePath.mkdirs(FilePath.java:3731)
14:49:50    at Jenkins v2.504.2//hudson.FilePath$WritePipe.invoke(FilePath.java:2501)
14:49:50    at Jenkins v2.504.2//hudson.FilePath$WritePipe.invoke(FilePath.java:2495)
14:49:50    at Jenkins v2.504.2//hudson.FilePath$FileCallableWrapper.call(FilePath.java:3593)
14:49:50    at hudson.remoting.UserRequest.perform(UserRequest.java:225)
14:49:50    at hudson.remoting.UserRequest.perform(UserRequest.java:50)
14:49:50    at hudson.remoting.Request$2.run(Request.java:391)
14:49:50    at hudson.remoting.InterceptingExecutorService.lambda$wrap$0(InterceptingExecutorService.java:81)
14:49:50    at java.base/java.util.concurrent.FutureTask.run(Unknown Source)
14:49:50    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
14:49:50    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
14:49:50    at hudson.remoting.Engine$1.lambda$newThread$0(Engine.java:138)
14:49:50    at java.base/java.lang.Thread.run(Unknown Source)
14:49:50  Finished: FAILURE
```

I used the pod template YAML shown below, but I'm still encountering the same error. I don't want to define the `securityContext` to run as root. However, when I configure it to run as the `jenkins` user, the container fails to start—because it's expected to start as root.

```
apiVersion: v1
kind: Pod
metadata:
    labels:
        ecosystem: ${ecosystem}
        job_name: ${job_name}
spec:
  containers:
    - name: ${containerName}
      image: ${dImage}
      command:
        - dockerd
      tty: true
      securityContext:
        privileged: true
      volumeMounts:
        - name: jenkins-home
          mountPath: /var/jenkins_home
          readOnly: false
  securityContext:
    fsGroup: 1000
  initContainers:
    - name: init-jenkins-home
      image: alpine
      command:
        - sh
        - -c
        - ""addgroup -g 1000 jenkins && adduser -D -u 1000 -G jenkins jenkins && chown -R 1000:1000 /var/jenkins_home && chmod -R 777 /var/jenkins_home""
      volumeMounts:
        - name: jenkins-home
          mountPath: /var/jenkins_home
  enableServiceLinks: ""false""
  volumes: ${volumes}
  tolerations: ${tolerations}
  nodeSelector:
    ${nodeSelector}
```","docker, kubernetes, jenkins, groovy, dsl",,,,2025-07-05T02:06:35
79690486,Correct Url to contact selenium in a Kubernetes cluster,"I am working on trying to use selenium as a sidecar container for an application. In the application code I have tried the following:

```
 URL remoteUrl = new URL(""http://localhost:4444/wd/hub"");

 ChromeOptions options = new ChromeOptions();
 options.addArguments(""--headless"", ""--disable-gpu"", ""--no-sandbox"", ""--disable-dev-shm-usage"");
 log.info(""starting web driver"");
 WebDriver driver = new RemoteWebDriver(remoteUrl, options);
```

I have also tried

```
 URL remoteUrl = new URL(""http://remote-chrome-webdriver.default.svc.cluster.local:4444/wd/hub"");
```

and

```
URL remoteUrl = new URL(""http://remote-chrome-webdriver:4444/wd/hub"");
```

And all of these options have yielded a ConnectException.

My yaml defines the selenium container like this:

```
      initContainers:
        - name: wait-for-chrome
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl -f http://remote-chrome-webdriver:4444/wd/hub/status; do echo ""Waiting fro remote-chrome-webdriver...""; sleep 5; done;']
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
        - name: remote-chrome-webdriver
          image: xxxxxxxxxx/selenium/standalone-chrome:4.23.1
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 4444
              protocol: TCP
          restartPolicy: Always
          env:
            - name: ""xxxxx_APPLICATION_CREDENTIALS""
              value: {{ .Values.env.xxxxxxxxxxx}}
          envFrom:
            - secretRef:
                name: xxxxxx
          volumeMounts:
            - name: xxxxxx
              readOnly: true
              mountPath: ""/etc/xxxxx""
          resources:
            {{- toYaml .Values.resources | nindent 12}}
```

Where I have redacted some sensitive information.

What is the proper syntax to use in java code to allow the application to connect to the selenium sidecar?","java, kubernetes, selenium-webdriver",79696072.0,"The correct syntax in the code to reach the selenium standalone running in the kubernetes cluster was

[http://remote-chrome-webdriver.default.svc:80/wd/hub](http://remote-chrome-webdriver.default.svc:80/wd/hub)

Also we had to create a Kubernetes service to expose this selenium to other workloads in the cluster

```
apiVersion: v1
kind: Service
metadata:
  name: remote-chrome-webdriver
  labels:
    app: remote-chrome-webdriver
spec:
  selector:
    app: remote-chrome-webdriver
  ports:
    - protocol: TCP
      port: 80
      targetPort: 4444
  type: LoadBalancer
```",2025-07-09T18:06:17,2025-07-04T16:46:52
79689017,Istio ServiceEntry for connections to RMI server,"I have a microservice that is connecting to mydomain.server.com on port 1119 to establish RMI connection. I have an ServiceEntry for that connection and it works properly.

However after connection is established, microservice starts to connect to different port - I guess this is how tRMI server works by dynamically assigning port per connection.

What's werid, I see that another app, from another namespace is making connections to the RMI server using same high value ports - is this possible? Can it be specified on side of RMI server?

```
apiVersion: networking.istio.io/v1
kind: ServiceEntry
metadata:
  annotations:
  name: myserv127
  namespace: istio-system
spec:
  hosts:
  - myserv127.mydomain.dev
  location: MESH_EXTERNAL
  ports:
  - name: tcp-rmi
    number: 1119
    protocol: TCP
  resolution: DNS
```

Logs from first microservice:

```
k logs -n default first-rmi-microservice-78d8f6dd-tbz6m -c istio-proxy | grep BlackHoleCluster|  grep -o downstream_local_address\"":\""[0-9.]*:[0-9]*  | sort | uniq

downstream_local_address"":""10.48.11.127:33823
downstream_local_address"":""10.48.11.127:33825
```

Logs from second microservice:

```
k logs -n sandbox second-rmi-microservice-5b6d5c85f9-2r885 -c istio-proxy | grep BlackHoleCluster|  grep -o downstream_local_address\"":\""[0-9.]*:[0-9]*  | sort | uniq

downstream_local_address"":""10.48.11.127:33823
downstream_local_address"":""10.48.11.127:33825
```

Only soultion that I currently have is to use `excludeIPRanges` in my istio config, however I wonder if there is a way for example to open all ports in ServiceEntry (if thats even make senese)","kubernetes, istio, rmi",,,,2025-07-03T14:32:44
79688743,kubectl RBAC roles for scaling down the deployment,"I've a requirement to provide RBAC roles for a cluster role so that they can scale up or scale down the replicas. The role already has 'get' 'list' & 'watch' verbs and I've added the below code to the yaml for the additional access.

```
  - apiGroups:
    - ""apps""
    resources:
      - deployments/scale
    verbs:
      - update
      - patch
```

I'm able to edit the deployment file using `kubectl edit deployment deplName` and adjust the replica count but I'm getting denied with permission error when I try to use the `kubectl scale deploy deplName --replicas` command. Below is the error when I tried to scale the replicas using `kubectl scale`

```
Error from server (Forbidden): deployments.apps ""Name"" is forbidden: User ""system:serviceaccount:xyz"" cannot patch resource ""deployments/scale"" in API group ""apps"" in the namespace ""namespace""
```

Am I missing anything here or do I need to provide any other roles?

Below is the output from ""kubectl describe clusterrole RoleName""

```
deployments.extensions                []                 []              [get list watch]
  deployments.apps                      []                 []              [list watch get patch]
  deployments.apps/status               []                 []              [list watch get]
  deployments.apps                      []                 []              [list watch get]
  deployments                           []                 []              [list watch get]
  deployments.apps.apps/status          []                 []              [list watch get]
  deployments.apps.apps                 []                 []              [list watch get]
  deployments.apps.batch/status         []                 []              [list watch get]
  deployments.apps.batch                []                 []              [list watch get]
  deployments.batch                     []                 []              [list watch get]
  deployments.apps/scale                []                 []              [update patch]
```","kubernetes, k8s-cluster-role",79693196.0,"The error you're seeing indicating that the patch action is forbidden suggests that the RBAC  settings may not be correctly configured. The `ClusterRole` must explicitly allow the `update` or `patch` verbs on the `deployments/scale` subresource in the apps API group.

Here is an example of a corrected [ClusterRole YAML configuration](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#clusterrole-example):

```
rules:
- apiGroups: [""apps""]
  resources: [""deployments"", ""deployments/scale""]
  verbs: [""get"", ""list"", ""watch"", ""patch"", ""update""]
```

- `get, list, watch,` and `patch` permissions on deployments for general operations and editing (such as kubectl edit deployment).
- `get, update,` and `patch` permissions on `deployments/scale` for scaling actions (such as kubectl scale).

After applying these changes, verify the `ClusterRole` . And confirm that the `deployments.apps/scale` resource has both update and patch verbs.

```
kubectl describe clusterrole RoleName
```

You can also test the permission using [kubectl auth can-i](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_auth/kubectl_auth_can-i/). The output should be `yes` otherwise `no`:

```
kubectl auth can-i patch deployments/scale --as=system:serviceaccount:xyz -n <namespace>
```

If the issue persists after applying changes above. Verify that the `RoleBinding `or `ClusterRoleBinding` properly associates the service account `xyz` in the specified namespace with the `ClusterRole`:

```
kubectl describe clusterrolebinding role-name-binding
```",2025-07-07T17:01:51,2025-07-03T11:12:59
79688483,Unable to login to Dependency Track using Microsoft Entra ID (Azure AD) account,"I have deployed the latest version of Dependency Track using Helm chart on my Kubernetes cluster and configured all the variables related to enabling authentication via OpenID connect for Microsoft Entra ID (Azure AD) account as explained [here](https://docs.dependencytrack.org/getting-started/openidconnect-configuration/).

After creating an enterprise app and app registration on MS Entra ID (Azure AD) as explained [here](https://docs.dependencytrack.org/getting-started/openidconnect-configuration/#microsoft-entra-id-app-registration), I set the following environment variables via default values file of the Helm chart:

```
apiServer:
  extraEnv:
    # -- Microsoft Entra ID (Azure AD) Configuration
    - name: ALPINE_OIDC_ENABLED
      value: ""true""
    - name: ALPINE_OIDC_CLIENT_ID
      value: ""_REDACTED_""
    - name: ALPINE_OIDC_ISSUER
      value: ""https://login.microsoftonline.com/_REDACTED_/v2.0""
    - name: ALPINE_OIDC_USERNAME_CLAIM
      value: ""preferred_username""
    - name: ALPINE_OIDC_USER_PROVISIONING
      value: ""true""
    - name: ALPINE_OIDC_TEAMS_CLAIM
      value: ""groups""
        - name: ALPINE_OIDC_TEAM_SYNCHRONIZATION
      value: ""true""
frontend:
  extraEnv:
    # -- Microsoft Entra ID (Azure AD) Configuration
    - name: OIDC_CLIENT_ID
      value: ""_REDACTED_""
    - name: OIDC_ISSUER
      value: ""https://login.microsoftonline.com/_REDACTED_/v2.0""
```

When I try login, I get the following error: `Invalid username or password and Unauthorized (401)`.

How to fix this?

I have followed all the guidelines mentioned in the Dependency-Track documentation.","authentication, kubernetes, azure-active-directory, microsoft-entra-id, owasp-dependency-track",79688523.0,"I have figured it out. Following are the changes I did in the environment variables in the default values file of Helm chart of Dependency Track:

- Set `ALPINE_OIDC_USERNAME_CLAIM` to `email`.
- Removed `ALPINE_OIDC_TEAMS_CLAIM` and `ALPINE_OIDC_TEAM_SYNCHRONIZATION`.

Now, you will be able to login but you see blank screen (empty dashboard) with a lot of `Forbidden (403)` errors. Basically, you need to login to Dependency-Track as admin using username and password, create a team in it (Administration > Access Management > Teams) by the name of `Viewers` and then select the created team and add all `View_*` related permissions which are as follows:

- `VIEW_BADGES`
- `VIEW_POLICY_VIOLATION`
- `VIEW_PORTFOLIO`
- `VIEW_VULNERABILITY`

Then, you need to set the following environment variable like before:

- Set `ALPINE_OIDC_TEAMS_DEFAULT` to `Viewers`.

Now, upon first login using OpenID connect for Microsoft (MS) Entra ID (Azure AD), `Viewers` team will be assigned by default and then you can see things.",2025-07-03T08:23:58,2025-07-03T08:03:38
79687835,"Spring Boot application running in Kubernetes does not receive X-Forwarded-For header, but works with docker run","I'm facing an issue where my Spring Boot application does not receive the `X-Forwarded-For:` header when running inside a Kubernetes cluster, even though it works correctly when running the same Docker image locally with `docker run`.

I'm using Spring Boot version 3.5.3 with its embedded Tomcat server.  I'm calling the Pod directly from inside the Kubernetes cluster, either using the Pod's IP address or using `kubectl exec` to `curl localhost` from inside the Pod directly.

Code of `HeaderController.java`:

```
package com.example.headerdemo.controller;

import org.springframework.web.bind.annotation.*;

import jakarta.servlet.http.HttpServletRequest;
import java.util.*;

@RestController
@RequestMapping(""/headers"")
public class HeaderController {

    @GetMapping
    public Map<String, String> getAllHeaders(HttpServletRequest request) {
        Map<String, String> headers = new HashMap<>();

        Enumeration<String> headerNames = request.getHeaderNames();
        if (headerNames != null) {
            while (headerNames.hasMoreElements()) {
                String headerName = headerNames.nextElement();
                String headerValue = request.getHeader(headerName);
                headers.put(headerName, headerValue);
                System.out.printf(""Header: %s = %s%n"", headerName, headerValue);
            }
        }

        return headers;
    }
}
```

`Dockerfile`:

```
# Stage 1: Build the application
FROM maven:3.9.6-eclipse-temurin-17 as builder
WORKDIR /app
COPY pom.xml .
COPY src ./src
RUN mvn clean package -DskipTests

# Stage 2: Create a minimal runtime image
FROM eclipse-temurin:17-jdk-alpine
WORKDIR /app
COPY --from=builder /app/target/*.jar app.jar

# Expose port
EXPOSE 8080

# Run the Spring Boot app
ENTRYPOINT [""java"", ""-jar"", ""app.jar""]
```

When running with **docker run** locally:

```
docker run -p 8080:8080 myapp
curl -H ""X-Forwarded-For: 1.1.1.1"" http://localhost:8080/headers
```

The header `X-Forwarded-For` is received and printed in the controller.

When running in Kubernetes:

```
kubectl exec -it <my-pod> -- curl -H ""X-Forwarded-For: 1.1.1.1"" http://localhost:8080/headers
```

or `curl` to IP of `my-pod` from another pod in cluster, the application does not receive the `X-Forwarded-For` header (it’s missing in `HttpServletRequest#getHeaderNames()`).

I call the pod IP directly, not through a Kubernetes Service or Ingress.  No proxy or sidecar (e.g., Istio, Linkerd) is involved.  No custom filters in my Spring Boot app.

Other custom headers are received just fine — only `X-Forwarded-For` is dropped.","spring-boot, kubernetes, x-forwarded-for",79689389.0,"I was able to reproduce this issue with a pod inside a microk8s cluster.

I solved it by following the [Spring docs](https://docs.spring.io/spring-boot/how-to/webserver.html#howto.webserver.use-behind-a-proxy-server):

> If the proxy adds the commonly used `X-Forwarded-For` and `X-Forwarded-Proto` headers, setting `server.forward-headers-strategy` to `NATIVE` is enough to support those. With this option, the Web servers themselves natively support this feature; you can check their specific documentation to learn about specific behavior.
>
>
> If this is not enough, Spring Framework provides a [ForwardedHeaderFilter](https://docs.spring.io/spring-framework/reference/6.2/web/webmvc/filters.html#filters-forwarded-headers) for the servlet stack and a [ForwardedHeaderTransformer](https://docs.spring.io/spring-framework/reference/6.2/web/webflux/reactive-spring.html#webflux-forwarded-headers) for the reactive stack. You can use them in your application by setting `server.forward-headers-strategy` to `FRAMEWORK`.

I chose the latter strategy and added this to *application.yml*:

```
server:
  forward-headers-strategy: framework
```

and used `HttpServletRequest#remoteAddr` in controller.

**Example controller (Kotlin)**

```
import jakarta.servlet.http.HttpServletRequest
import org.springframework.web.bind.annotation.GetMapping
import org.springframework.web.bind.annotation.RequestMapping
import org.springframework.web.bind.annotation.RestController

@RestController
@RequestMapping(""/hello"")
class HelloWorldController {
    @GetMapping
    fun hello(request: HttpServletRequest): String {
        val clientIp = request.remoteAddr
        return ""Hello World, $clientIp!""
    }
}
```

I also tested this with a Ktor-app, and the `X-Forwarded-For` header came through:

```
call.request.header(""X-Forwarded-For"")
```",2025-07-03T19:37:35,2025-07-02T17:15:59
79686119,MinIO in Kubernetes fails with &quot;Rename across devices&quot; error when using PersistentVolumeClaim (works with hostPath),"I'm setting up MinIO and MLflow in a Minikube Kubernetes cluster and encountering persistent volume issues. The deployment works correctly when using hostPath, but fails with a ""Rename across devices"" error when using PersistentVolumeClaims.

### Configuration Details:

1. Persistent Volume & Claim (volume.yaml):

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: minio-pv
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /minio
  nodeAffinity:
    required:
      nodeSelectorTerms:
        - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - minikube
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: minio-pv
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 1Gi
  storageClassName: local-storage
```

1. MinIO Deployment:

```
apiVersion: apps/v1
kind: ReplicaSet
metadata:
name: minio
spec:
template:
  spec:
    containers:
      - name: minio
        image: quay.io/minio/minio:latest
        command: [""/bin/bash"", ""-c""]
        args: [""minio server /data --console-address :9090""]
        volumeMounts:
          - mountPath: /data
            name: minio-data
            subPath: minio
    volumes:
      - name: minio-data
        persistentVolumeClaim:
          claimName: minio-pv
```

### Error Encountered:

When applying the deployment, MinIO fails with:

```
Error: unable to rename (/data/.minio.sys/tmp -> /data/.minio.sys/tmp-old/...) Rename across devices not allowed
FATAL Unable to initialize backend: Rename across devices not allowed
```

### Working Alternative:

The deployment works when using hostPath instead:

```
volumes:
  - name: localvolume
    hostPath:
      path: /mnt/disk1/data
      type: DirectoryOrCreate
```

### Investigation Attempts:

- Tried deleting .minio.sys both inside and outside Minikube
- Verified PV/PVC binding is successful
- Confirmed storage path exists on the node
- Added subPath to volumeMounts

Questions:

- Why does MinIO work with hostPath but fail with PVC?
- Is there a configuration issue with my PersistentVolume setup?
- Could this be related to how Minikube handles local volumes?
- Are there known solutions for this ""Rename across devices"" error with MinIO in Kubernetes?

Environment:

- Minikube version: v1.35.0
- Kubernetes version: v1.30.2
- MinIO version: latest (quay.io/minio/minio)
- Host OS: Linux OS

Any insights or suggestions would be greatly appreciated. Thank you!","kubernetes, minikube, minio, persistent-volumes",,,,2025-07-01T13:17:32
79684702,io.netty.channel.unix.Errors$NativeIoException: Connection reset by peer in Spring Cloud Gateway with WebFlux routing to GraphQL backend in Kubernetes,"I'm experiencing a `Connection reset by peer` exception in my Spring Boot 3.4.4 application that uses Spring Cloud Gateway with WebFlux to serve a web application. The entire stack is deployed in a **Kubernetes cluster** using Helm charts with multiple interconnected services.

## Kubernetes Architecture Overview

- **Cluster**: Azure Kubernetes Service (AKS)
- **Services deployed via Helm charts**:
  - **webclient** (Spring Cloud Gateway): Main application with OAuth2/JWT security
  - **webclient-graphql** (Node.js Apollo Server): GraphQL API layer on port 4000
  - **webclient-nginx** (Nginx): Frontend proxy serving React application

**Network Configuration:**

- **Service Discovery**: Kubernetes ClusterIP services
- **Load Balancing**: Built-in Kubernetes service load balancing
- **Network Policies**: Configured to allow traffic from public nginx namespace

## Request Flow in Kubernetes

External Traffic → Spring Cloud Gateway (fwoweb) → GraphQL Service (Node.js) → Backend APIs

**Inter-service Communication:**

- Services communicate via Kubernetes internal DNS (e.g., `fwo-webclient-graphql.vsmds-flashware-npr.svc:4000`)
- All services run with security contexts (non-root users, privilege escalation disabled)
- Resource limits and requests configured for CPU/memory

## Current Configuration

**Dockerimage**

```
ENTRYPOINT [""java"", ""-Dreactor.netty.pool.maxIdleTime=30000"", ""-Dreactor.netty.pool.maxLifeTime=60000"",\
    ""-javaagent:/datadog/agent/javaagent.jar"", ""-XX:+HeapDumpOnOutOfMemoryError"", ""-XX:HeapDumpPath=/tmp/heap/"", \
    ""-cp"", ""."", ""org.springframework.boot.loader.launch.JarLauncher""]
```

**Spring Cloud Gateway HTTP Client Pool Settings:**

```
spring:
  cloud:
    gateway:
      httpclient:
        pool:
          max-idle-time: 30s
          max-life-time: 60s
```

**Key Dependencies:**

- Spring Boot 3.4.4
- Spring Cloud 2024.0.1
- Spring Cloud Gateway
- Spring WebFlux
- Spring Security OAuth2
- Redis for session management
- Netty (via WebFlux)

## Error Details

The exception occurs during GraphQL requests routed through the gateway in the Kubernetes environment:

```
io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer
Suppressed: The stacktrace has been enhanced by Reactor, refer to additional information below:
Error has been observed at the following site(s):
*__checkpoint ⇢ ...
*__checkpoint ⇢ ...
*__checkpoint ⇢ ...[DefaultWebFilterChain]
*__checkpoint ⇢ HTTP POST ""/graphql"" [ExceptionHandlingWebHandler]
Original Stack Trace:
```

## Questions

1. **Configuration Conflict**: I have Reactor Netty pool settings configured in **both** places:

  - **Dockerfile ENTRYPOINT**: `-Dreactor.netty.pool.maxIdleTime=30000 -Dreactor.netty.pool.maxLifeTime=60000`
  - **Spring YAML**: `max-idle-time: 30s` and `max-life-time: 60s`

Could this **double configuration** be causing connection management issues? Which takes precedence?
2. **Kubernetes Service Discovery**: Could the connection resets be related to Kubernetes service discovery or pod-to-pod communication issues? The GraphQL service runs on a separate pod with its own ClusterIP service.
3. **General**: What else can cause the issue?
4. **DNS and Network Policies**: Could the `single-request-reopen` DNS configuration or network policies be interfering with persistent connections between the gateway and GraphQL services?","kubernetes, spring-webflux, netty, spring-cloud-gateway, connection-reset",,,,2025-06-30T11:43:51
79680098,draining a node - surge replica up and not down,"When draining I node, I can specificy a pod-disruption-budget telling the cluster how far it can dip down with the replicas

[https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pdb-example](https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pdb-example)

All the examples and configs point towards ""when I have 2 replicas, I can tell k8s to allow it to go down to 1 and guarantee uptime""

What I have not found thus far is how I can allow the cluster to surge upwards.

Lets say I have 1 replica. I tolerate that I have multiple replicas during upgrades of the service:

I have a RollingUpdate strategy where I instruct the cluster to got +1 replicas (1 is running, another pod is added, we are now at 2 instances, when the new pod is ready, first instance is taken down)

```
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
```

How can I tell the cluster to go with the same strategy when draining a node? Try first to get another pod on another node, then get that pod running first, only then tear down the pod on the node I want to drain

a) am I missing something?
b) is there a reason this is not working for planned distruption, but is an available feature for upgrading an application?",kubernetes,79682505.0,"You are not missing anything, this is just by default how Kubernetes handles upgrades and node drains differently. The [RollingUpdate](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#rolling-update-deployment) strategy with `maxSurge` works as by design, Kubernetes increases the number of replicas temporarily as part of the upgrade process. Meanwhile during a [node drain](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service), Kubernetes primarily focuses on evicting pods safely. While the PDB is there to ensure availability during disruption, it doesn't automatically scale up the replicas during the drain process.

To achieve your desired behavior, manually scale your replicas up temporarily first before draining the node. Once the node is drained, scale the replicas back down.

For further reference, you can also check related [thread](https://github.com/kubernetes/kubernetes/issues/114877).",2025-06-27T21:19:23,2025-06-26T07:55:40
79679927,Spring WebFlux WebClient to Okta JWKs Fails with &quot;recvAddress(..) failed: Connection reset by peer&quot; in Kubernetes,"I'm building a Spring WebFlux application that integrates with Okta for JWT token verification. The app is deployed in Kubernetes (AKS), and uses a WebClient with custom Netty configuration to fetch JWKs from this endpoint:

https://{domain}/oauth2/default/v1/keys

I am getting this error - io.netty.channel.unix.Errors$NativeIoException: recvAddress(..) failed: Connection reset by peer
Suppressed: io.netty.handler.ssl.StacklessSSLHandshakeException: Connection closed while SSL/TLS handshake was in progress
at io.netty.handler.ssl.SslHandler.channelInactive(Unknown Source) ~[netty-handler-4.1.118.Final.jar:4.1.118.Final]

This is my security file -

object InsecureWebClient {

```
private const val TIMEOUT = 60000L // 60 seconds
fun getInstance(): WebClient {
    val provider = ConnectionProvider.builder(""custom"")
        .maxConnections(500)
        .maxIdleTime(Duration.ofSeconds(20))
        .maxLifeTime(Duration.ofSeconds(60))
        .pendingAcquireTimeout(Duration.ofSeconds(60))
        .evictInBackground(Duration.ofSeconds(120))
        .build()

    val sslContext = SslContextBuilder.forClient()
        .trustManager(InsecureTrustManagerFactory.INSTANCE)
        .build()

    val httpClient = HttpClient.create(provider)
        .secure { it.sslContext(sslContext) }
        .option(ChannelOption.CONNECT_TIMEOUT_MILLIS, TIMEOUT.toInt())
        .responseTimeout(Duration.ofMillis(TIMEOUT))
        .doOnConnected { conn ->
            conn.addHandlerLast(ReadTimeoutHandler(TIMEOUT, TimeUnit.MILLISECONDS))
            conn.addHandlerLast(WriteTimeoutHandler(TIMEOUT, TimeUnit.MILLISECONDS))
        }
        .wiretap(""reactor.netty.http.client.HttpClient"", LogLevel.DEBUG, AdvancedByteBufFormat.TEXTUAL)

    return WebClient.builder()
        .clientConnector(ReactorClientHttpConnector(httpClient))
        .build()
}
```

}","spring, kubernetes, ssl, spring-webflux",,,,2025-06-26T04:57:50
79679241,"After Once mutating webhook modifies object, subsequent validating webhook calls get empty object","I have a mutating webhook which applies label on a CustomResource when it gets created.
This part is working fine.

I also have a validating webhook for the same CR but for detele call.
When I delete the CR, call to webhook fails with

```
Error from server: admission webhook denied the request: Failed to serialize CR: unexpected end of JSON input
```

When I prtinted the object in my logs, I found that the object was empty.

Anyone knows what might be going on?

Mutating webhook logic:

```
func mutateMyCR(ctx context.Context,
    req admission.Request) admission.Response {

    newCR := v1alpah1.NewCR{}
    if err := json.Unmarshal(req.Object.Raw, &newCR); err != nil {
        return admission.Errored(http.StatusInternalServerError, err)
    }

    if newCR.Labels == nil {
        newCR.Labels = make(map[string]string)
    }
    newCR.Labels[""test""] = ""true""

    newRawCR, err := json.Marshal(newCR)
    if err != nil {
        return admission.Errored(http.StatusInternalServerError, err)
    }

    return admission.PatchResponseFromRaw(req.Object.Raw, newRawCR)
}
```","kubernetes, webhooks, kubernetes-custom-resources",79679395.0,The problem was with my validating webhook where for Delete call I was taking req.Object.Raw instead of req.OldObject.Raw.,2025-06-25T16:47:22,2025-06-25T14:50:54
79678424,springboot3.2.5 restclient occasionally delays about 3 seconds,"When I use springboot3.2.5 restclient to call an API, occasionally there is a gap of about 3 seconds from the time the request is sent to the time the request is received by the receiver.This is the RestClient configuration:

```
import org.yu.feign.GatewayRestService;
import io.micrometer.observation.ObservationRegistry;
import jakarta.annotation.Resource;
import org.apache.hc.client5.http.classic.HttpClient;
import org.apache.hc.client5.http.config.RequestConfig;
import org.apache.hc.client5.http.impl.classic.HttpClientBuilder;
import org.apache.hc.client5.http.impl.io.PoolingHttpClientConnectionManager;
import org.apache.hc.client5.http.socket.ConnectionSocketFactory;
import org.apache.hc.client5.http.socket.PlainConnectionSocketFactory;
import org.apache.hc.client5.http.ssl.SSLConnectionSocketFactory;
import org.apache.hc.core5.http.URIScheme;
import org.apache.hc.core5.http.config.RegistryBuilder;
import org.apache.hc.core5.util.Timeout;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.http.client.HttpComponentsClientHttpRequestFactory;
import org.springframework.web.client.RestClient;
import org.springframework.web.client.support.RestClientAdapter;
import org.springframework.web.service.invoker.HttpServiceProxyFactory;

@Configuration
public class RestClientConfig {
    @Resource
    private ObservationRegistry observationRegistry;

    public HttpClient myHttpClient() {
        PoolingHttpClientConnectionManager connectionManager = new PoolingHttpClientConnectionManager(
                RegistryBuilder.<ConnectionSocketFactory>create()
                        .register(URIScheme.HTTP.id, PlainConnectionSocketFactory.getSocketFactory())
                        .register(URIScheme.HTTPS.id, SSLConnectionSocketFactory.getSystemSocketFactory())
                        .build());
        connectionManager.setDefaultMaxPerRoute(50);
        connectionManager.setMaxTotal(100);
        RequestConfig requestConfig = RequestConfig.custom()
                .setResponseTimeout(Timeout.ofSeconds(300))
                .build();
        return HttpClientBuilder.create()
                .useSystemProperties()
                .setDefaultRequestConfig(requestConfig)
                .setConnectionManager(connectionManager)
                .build();
    }

    @Bean(""gatewayRestService"")
    public GatewayRestService gatewayRestService() {
        RestClient restClient = RestClient.builder()
                .observationRegistry(observationRegistry)
                .requestFactory(new HttpComponentsClientHttpRequestFactory(this.myHttpClient()))
                .baseUrl(""http://gateway.namespace.svc:8080/rest/"")
                .build();
        final RestClientAdapter adapter = RestClientAdapter.create(restClient);
        final HttpServiceProxyFactory factory = HttpServiceProxyFactory.builderFor(adapter).build();
        return factory.createClient(GatewayRestService.class);
    }
}
```

This is GatewayRestService.class:

```
import org.springframework.web.bind.annotation.RequestBody;
import org.springframework.web.service.annotation.GetExchange;

public interface GatewayRestService {
    @GetExchange(value = ""getData"")
    String getData(@RequestBody RequestOfGetData request);
}
```

This is the code that calls the API:

```
@Resource
private GatewayRestService gatewayRestService;

final RequestOfGetData request = new RequestOfGetData();
String response = gatewayRestService.getData(request);
```

As shown in the code, I have configured the HTTP client pool with maxTotal set to 100 and defaultMaxPerRoute set to 50. In theory, as long as the number of concurrent requests does not exceed 50, there should be no queuing or waiting. I confirmed that during the occurrence of the delay, the number of in-flight requests was under 10. Therefore, the time between sending the request and the receiving of the request by the peer service should not experience a 3-second delay.

My service is deployed in Kubernetes, and the peer service is a gateway service deployed in another namespace. The API calls use the Kubernetes cluster communication method, i.e., via [http://service.namespace.svc:port](http://service.namespace.svc:port).

Unfortunately, such delays appear to be occasional, and I can't consistently reproduce them.I don't know if it has something to do with Kubernetes.

[Here is the zipkin trace of the call when the delay occurs](https://i.sstatic.net/LRvb2wMd.png)","rest, kubernetes, rest-client, spring-boot-3",,,,2025-06-25T02:51:33
79678419,Automating FluxCD Image Update,"I am deploying a web with Flux, GitHub Actions, DockerHub, and Kubernetes. [Here is the flowchart.](https://i.sstatic.net/xFztyaLi.png) The method I am using currently to update the web is to automate deleting the old pod with GitHub Actions so Flux can create the new and updated website pod. It just feels so primitive, so I tried finding ways for Flux to automate updating my image/website. Here's the directory containing Flux Configuration:

```
├── flux-system
│   ├── gotk-components.yaml
│   ├── gotk-sync.yaml
│   ├── kustomization.yaml
│   └── restaurant.yaml
└── restaurant-app
    ├── deployment.yaml
    ├── image-automation
    │   ├── imagepolicy.yaml
    │   ├── imagerepo.yaml
    │   ├── imageupdate.yaml
    │   └── kustomization.yaml
    ├── kustomization.yaml
    └── service.yaml
```

Here's the content of all script I did:

```
# cat imageupdate.yaml
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImageUpdateAutomation
metadata:
  name: restaurant-update
  namespace: flux-system
spec:
  interval: 1m
  sourceRef:
    kind: GitRepository
    name: flux-system
    namespace: flux-system
  git:
    checkout:
      ref:
        branch: main
    commit:
      author:
        name: FluxCD
        email: fluxcd@example.com
      messageTemplate: ""Update image to {{range .Updated.Images}}{{.}}{{end}}""
    push:
      branch: main
  update:
    strategy: Setters

# cat imagepolicy.yaml
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImagePolicy
metadata:
  name: restaurant-latest
  namespace: flux-system
spec:
  imageRepositoryRef:
    name: restaurant
  policy:
    semver:
      range: 1.0.x
# cat imagerepo.yaml
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImageRepository
metadata:
  name: restaurant
  namespace: flux-system
spec:
  image: tyvenchr/restaurant-landing-page
  interval: 1m
cat imagepolicy.yaml
apiVersion: image.toolkit.fluxcd.io/v1beta2
kind: ImagePolicy
metadata:
  name: restaurant-latest
  namespace: flux-system
spec:
  imageRepositoryRef:
    name: restaurant
  policy:
    semver:
      range: 1.0.x

# cat service.yaml
apiVersion: v1
kind: Service
metadata:
  name: restaurant-landing
  namespace: default
spec:
  selector:
    app: restaurant-landing
  ports:
    - protocol: TCP
      port: 80
      targetPort: 3000
  type: NodePort

# cat deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: restaurant-landing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: restaurant-landing
  template:
    metadata:
      labels:
        app: restaurant-landing
    spec:
      containers:
        - name: web
          image: tyvenchr/restaurant-landing-page
          imagePullPolicy: Always
          ports:
            - containerPort: 3000

# cat restaurant.yaml
apiVersion: kustomize.toolkit.fluxcd.io/v1
kind: Kustomization
metadata:
  name: restaurant
  namespace: flux-system
spec:
  interval: 1m
  path: ./clusters/my-cluster/restaurant-app
  prune: true
  sourceRef:
    kind: GitRepository
    name: flux-system

# cat flux-system/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
- gotk-components.yaml
- gotk-sync.yaml
- restaurant.yaml

# cat restaurant-app/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: default
resources:
  - deployment.yaml
  - service.yaml
  - image-automation/
images:
  - name: tyvenchr/restaurant-landing-page
    newTag: 1.0.6 # optional, Flux will update this
    # {""$imagepolicy"": ""flux-system:restaurant-latest"")

# root@kube-master-tyven-1:~/restaurant-kustomize/clusters/my-cluster# cat restaurant-app/image-automation/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - imagepolicy.yaml
  - imagerepo.yaml
  - imageupdate.yaml
```

I have no idea what to do next. Any help?","kubernetes, cicd, dockerhub, fluxcd",,,,2025-06-25T02:42:23
79672356,cp Command Fails to Copy JAR File to Target Folder,"I am trying to copy a file (jar file) so that I can run the jar (`java -jar`) in my pod. But the copy command just doesn't work. The pod logs don't throw any error also.

My `deployment.yaml` looks like (in brief):

```
- name: glowroot-jar-init-container
image: ""{{ .Values.images.repository }}/{{ .Values.config.aptm.image }}""
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  runAsNonRoot: true
  readOnlyRootFilesystem: true
imagePullPolicy: {{ .Values.images.pullPolicy }}
command: [""cp"",""/opt/tools/aptm/glowroot.jar"",""/aptm""]
volumeMounts:
  - name: aptm-data-glowroot
    mountPath: /aptm
.
.
.
.
.
.
containers:
- name: {{ template ""name"" . }}
  image: ""{{ .Values.images.repository }}/com.gtt.ecomp.vod.dev/vod:{{ .Values.images.vodTag }}""
  imagePullPolicy: {{ .Values.images.pullPolicy }}
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    readOnlyRootFilesystem: true
  command:
    - sh
    - -c
    - -x
    - >
      .
      .
      .
      .

      echo ""Copying aptm JAR.""
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;

      .
      .
      .

      bash /mounted-config/start_tomcat.sh;
  args:
  - ""30000""
.
.
.
.
- name: aptm-data
  mountPath: /opt/tools/aptm
- name: aptm-data-glowroot
  mountPath: /aptm
.
.
.
.
- name: aptm-data
  emptyDir: {}
- name: aptm-data-glowroot
  emptyDir: {}
 .
 .
 .
```

The file `hram-agent-0.13.jar` is present in the `WEB-INF/lib/` folder. But when I do a bash and get into the pod to check the if the jar file was copied or not I do not see it.

```
vodadmin@vod-58867c5dc6-lg8ch:/usr/local/tomcat/webapps/vod/WEB-INF/lib$ ls -lrt hr*
-rw-r--r-- 1 vodadmin vodadmin 13864793 Apr 25 12:56 hram-agent-0.13.jar
vodadmin@vod-58867c5dc6-lg8ch:/usr/local/tomcat/webapps/vod/WEB-INF/lib$
```

But when `cd` to the target folder:

```
vodadmin@vod-58867c5dc6-lg8ch:/opt/tools/aptm$ ls -lrt
total 0
```

All the trouble started when I changed everything to read only root file system in my pod.","kubernetes, kubernetes-helm",79672475.0,"Just a quick guess -

When you use `>` in a yaml, it stacks the lines of its data together into one line.

```
   - >
      echo ""Copying aptm JAR.""
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

becomes

```
echo ""Copying aptm JAR."" cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

which I bet outputs

```
Copying aptm JAR. cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm
```

So add a semicolon after the `echo` statement. Then

```
   - >
      echo ""Copying aptm JAR."";
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

becomes

```
echo ""Copying aptm JAR.""; cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

and might work.

Or use a `|` instead, which preserves the internal newlines.

I'm still suspicious of how the syntax gets delivered to the parser, though.
Maybe write a script that wraps all that in a simpler call, and rebuild it into your image?",2025-06-19T18:05:50,2025-06-19T16:15:04
79671963,Multiple GPUs are visible in a container despite setting limits in Kubernetes manifest,"## Summary

Multiple GPUs are visible in a container despite setting limits in Kubernetes manifest

Each GPU should be exclusively allocated to each container that requests a GPU and it should run on that GPU only.

## Setup

Here's the setup in which I am getting this problem.
I am using microk8s 1.32.3. I have 2 nodes in my cluster. The first is my local system which is the master node and the other is a server with 2 GPUs (NVIDIA RTX 3080) that is a worker. I am deploying DeepStream instances in a Pod using the manifest shown below. For multiple deployments I am changing the names and necessary labels as `app-deepstream-1`, `app-deepstream-2` and so on.

```
apiVersion: v1
kind: Pod
metadata:
  name: app-deepstream-1                                # modify
  labels:
    name: app-deepstream-1                              # modify
    family: app-deepstream
spec:
  restartPolicy: Always
  runtimeClassName: nvidia
  nodeSelector:
    nvidia.com/gpu.present: ""true""
  containers:
    - name: app-ai
      image: 192.168.65.106:32000/nvcr.io/nvidia/deepstream
      securityContext:
        privileged: true
      imagePullPolicy: IfNotPresent
      tty: true
      resources:
        limits:
          nvidia.com/gpu: 1
      workingDir: /opt/app/ai-app-prod/
      command: [""bash"", ""run.sh""]
      volumeMounts:
        - name: app-volume
          mountPath: /opt/app/
  volumes:
    - name: app-volume
      persistentVolumeClaim:
        claimName: app-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: app-deepstream-svc-1                           # modify
  labels:
    name: app-deepstream-svc-1                         # modify
    family: app-deepstream
spec:
  type: NodePort
  selector:
    name: app-deepstream-1                             # modify
  ports:
    - port: 9000           # ClusterIP port
      targetPort: 9000     # Container port
      protocol: TCP
```

I have enabled gpu and registry add-ons in microk8s. The node with GPU is correctly labelled and I have checked that mig capability is marked as false (this will become important later).

I needed a custom auto-scaler that scales the number of Pods up or down based on the number of streams running in an instance. I have used `Python 3.10` with the `kubernetes` package for this. The upscale script and the downscale script just modify the manifest template and deploy the Pod and the service in the cluster. I face no issues when I run these scripts at all. This is just a wrapper of the `v1.create_namespaced_pod()` and `v1.delete_namespaced_pod()` provided by the Kubernetes library in a try-except block.

**Note**: The DeepStream app config (present in the mount) specifies the GPU index which is set to 0 with the expectation that a single GPU will be assigned and hence visible to the container.

**The Pods and respective services are deployed without any problems even when I deploy multiple Pods. Once multiple pods are deployed and in the `Running` phase I checked `nvidia-smi` in my GPU node. I found out that both DeepStream apps in the 2 pods that are deployed are running on the same GPU (0).**

The interesting thing is when I check `microk8s kubectl describe <node>` I can see that 2 GPUs have been allocated as shown below.

```
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                365m (2%)   0 (0%)
  memory             320Mi (0%)  5632Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
  nvidia.com/gpu     2           2
```

Now when I go inside a container and check with `nvidia-smi` I see that 2 GPUs are visible as shown below.

```
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA GeForce RTX 3080        Off | 00000000:01:00.0 Off |                  N/A |
|  0%   34C    P8              16W / 340W |    564MiB / 10240MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
|   1  NVIDIA GeForce RTX 3080        Off | 00000000:03:00.0 Off |                  N/A |
|  0%   30C    P8              18W / 340W |     12MiB / 10240MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
+---------------------------------------------------------------------------------------+
```

Each GPU should be exclusively allocated to each container that requests a GPU and it should run on that GPU only. I thought GPUs are not shared by default unless we enable MIGs or time-slicing explicitly.

### There seems to be a communication gap between the nvidia-device-plugin and the nvidia-container runtime.

## Some further details

1. I checked whether nvidia-device-plugin was working correctly or not. To do this I checked the value of the environment variable `NVIDIA_VISIBLE_DEVICES` inside each container. **The values are unique in each container as it should be**.

```
nvidia-smi -L          # executed in GPU host
GPU 0: NVIDIA GeForce RTX 3080 (UUID: GPU-e2a7d5f1-f4e4-9585-61ee-b64cce744228)
GPU 1: NVIDIA GeForce RTX 3080 (UUID: GPU-95f9f092-00da-bbb0-7d4e-180cd0f6d7ff)

# NVIDIA_VISIBLE_DEVICES in each container
app-deepstream-1: GPU-e2a7d5f1-f4e4-9585-61ee-b64cce744228
app-deepstream-2: GPU-95f9f092-00da-bbb0-7d4e-180cd0f6d7ff
```

Despite this, when I do `nvidia-smi` I can see both the GPUs in both containers which I don't believe should be happening.

1. I was using the nvidia-runtime-container set up by the add-on. Since, I have nvidia-container-runtime in my host I thought maybe there is some interference. Hence, I switched to the host runtime using the steps mentioned [here](https://microk8s.io/docs/addon-gpu#p-20002-use-host-nvidia-container-runtime). This did not solve the issue. I have attached the `containerd.toml` and `containerd-template.toml` for reference (using .txt extension as .toml extension is not allowed)

[containerd.txt](https://github.com/user-attachments/files/20896484/containerd.txt)
[containerd-template.txt](https://github.com/user-attachments/files/20896483/containerd-template.txt)

1. Finally, I came across [these settings](https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#microk8s) recommended by the official NVIDIA documentation of the gpu operator. I injected the options during install. The validator got stuck in init:3/4 state and hence, I could not verify whether this solved the issue.
2. I also added a RuntimeClass to the manifest to select the nvidia runtime. This too did not resolve the issue.

```
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: nvidia  # <-- This name MUST match what you put in runtimeClassName
handler: nvidia  # <-- This name MUST match a runtime defined in your containerd-template.toml
...
...
...
```","kubernetes, microk8s, kubernetes-python-client",,,,2025-06-19T11:07:36
79670788,Setting environment variables in deployment from Vault secret,"I have configured delivery of secrets from Valt to kubernetes cluster. configured via CSI with creation of SecretProviderClass. secret put in volume:

```
...
    volumeMounts:
    - name: secrets-store-inline
      mountPath: ""/mnt/secrets-store""
      readOnly: true
  volumes:
    - name: secrets-store-inline
      csi:
        driver: secrets-store.csi.k8s.io
        readOnly: true
        volumeAttributes:
          secretProviderClass: ""vault-database""
```

use:
cat /mnt/secrets-store/test_secret
i can read this secret

I need to set this secret in the deployment env DB_PASSWORD, for example.
how can I best do this?","kubernetes, devops, hashicorp-vault, vault",,,,2025-06-18T14:13:52
79670263,How to call a env variable in React VITE without the need of a .env,"I'm currently trying to deploy a frontend app in a container in a kubernetes cluster, I have made a env variable named `VITE_SERVER_ADDRESSS` following the vite naming convention for environment variables. I have also checked if the pod containing this container has indeed the env variable. This environment variable is loaded from a config map.

Nonetheless, I am still not able to call this env variable in my frontend code which works locally but makes use of a .env file.

Due to the fact that this variables changes according to the environment, I need to use a configmap. Which with helm, allows me to change the `VITE_SERVER_ADDRESS` according to the environment.

This is the code for the frontend that calls the env variable

```
const serverAddress: string = import.meta.env.VITE_SERVER_ADDRESS;
console.log(import.meta.env.VITE_SERVER_ADDRESS);
const url: string = `http://${serverAddress}:8000/run-main`; //in the cluster I now have a undefined
```","reactjs, kubernetes, vite",79670605.0,"Ok, so I found a fix to the problem,

The issue was with the fact that I needed to get the env variables *before *`npm run build` i.e build time. Me using a config map meant that I was getting the env variable in runtime and not build time. Instead with my original Dockerfile I was running `npm run build` which didn't have the environment variables due to the configmap.

Instead in my helm template manifest files, I overrode the CMD in the Dockerfile with CMD and ARGS, and also utilized helms variables (e.g. `{{.Values.env}}`)that changes according to the values.yml file with helm as shown bellow:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: project-ui-deployment
  namespace: {{.Values.namespace}}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: project-ui-app
  template:
    metadata:
      labels:
        app: project-ui-app
    spec:
      containers:
        - name: project-ui
          image: {{.Values.ui_image}}
          ports:
            - containerPort: 3000
              protocol: TCP
          env:
            - name: VITE_SERVER_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: project-configmap
                  key: VITE_SERVER_ADDRESS

          command: [""/bin/sh""]
          args:
            - ""-c""
            - |
              echo $VITE_SERVER_ADDRESS
              export VITE_SERVER_ADDRESS=project-server-service{{.Values.env}}.project{{.Values.env}}.svc.cluster.local
              npm run build
              node server.js
```",2025-06-18T12:06:14,2025-06-18T08:23:42
79670107,Grafana 404 Error After Re-Importing EKS Cluster — URL Points to Old Cluster ID,"Rancher version: v2.11.1
Installation option: v1.27.11+k3s1 on docker
Kubernetes version: EKS 1.32
Cluster Type (Local/Downstream): Import AWS EKS 1.32

I upgraded Rancher from v2.9.2 to v2.11.1. After the upgrade, due to certain reasons, I deleted the previously imported EKS cluster in Rancher and re-imported it. When I accessed the Grafana component, the page returned a 404 error, and the URL contained the new cluster ID. After clicking the ""Home"" button in the top-left corner, it redirected successfully, but the cluster ID in the URL was still the old one. I’m unable to click the ""Sign in"" button to access the correct login page, which prevents me from configuring Grafana settings. I’ve tried upgrading both Rancher and the monitoring component, but the issue still persists.

To Reproduce:
After installing the monitoring component, the cluster was deleted and re-imported, then open Grafana.

Result:
The page returned a 404 error, and the cluster ID in the URL was the new cluster ID; changing it to the old cluster ID allows the page to redirect correctly. However, the sign-in page with the new cluster ID fails to redirect properly.

Expected Result:
Grafana is accessible, and the sign-in page redirects correctly.

Screenshots:
[![enter image description here](https://i.sstatic.net/DdWUiEw4.png)](https://i.sstatic.net/DdWUiEw4.png)
[![enter image description here](https://i.sstatic.net/GPtPhNTQ.png)](https://i.sstatic.net/GPtPhNTQ.png)
[![enter image description here](https://i.sstatic.net/xF6O9YIi.png)](https://i.sstatic.net/xF6O9YIi.png)

Additional context:
I hope to update the cluster ID in the URL, or alternatively, is there a way to grant my current account Grafana admin permissions? That would also work as a temporary solution.","kubernetes, grafana, amazon-eks, dashboard, rancher",,,,2025-06-18T06:20:35
79668213,Podman pod and hostPath mount volume permission issue,"With the kubernetes manifest below and the command `podman kube play vaultwarden.pod.yaml --replace --userns=auto`, I am able to run a rootless readonly container within a podman pod (with user vaultwarden).

```
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: vaultwarden
  name: vaultwarden
spec:
  containers:
  - image: docker.io/vaultwarden/server:latest
    name: vaultwarden
    ports:
    - containerPort: 80
      hostPort: 8080
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
    volumeMounts:
    - mountPath: /data
      name: vaultwarden-data
  hostUsers: false
  volumes:
  - name: vaultwarden-data
    emptyDir: {}
```

In order to preserve and ease the backup of data, I would like to mount the volume to a specific location.
This is normaly done with the following manifest file (the difference being the volumes section and the mountPath ending with :Z).

```
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: vaultwarden
  name: vaultwarden
spec:
  containers:
  - image: docker.io/vaultwarden/server:latest
    name: vaultwarden
    ports:
    - containerPort: 80
      hostPort: 8080
    securityContext:
      allowPrivilegeEscalation: false
      readOnlyRootFilesystem: true
    volumeMounts:
    - mountPath: /data:Z
      name: vaultwarden-data
  hostUsers: false
  volumes:
  - hostPath:
      path: /home/vaultwarden/data
      type: DirectoryOrCreate
    name: vaultwarden-data
```

but when running the commands:

- `podman kube play vaultwarden.pod.yaml --replace`, the container is not able to write to the folder.

```
[vaultwarden][ERROR] Error creating private key 'data/rsa_key.pem'
Io.
[CAUSE] Os {
    code: 2,
    kind: NotFound,
    message: ""No such file or directory"",
}
Exiting Vaultwarden!
```

- `podman kube play vaultwarden.pod.yaml --replace --userns=auto`, the container is not able to write to the folder.

```
[vaultwarden][ERROR] Error creating private key 'data/rsa_key.pem'
Io.
[CAUSE] Os {
    code: 2,
    kind: NotFound,
    message: ""No such file or directory"",
}
Exiting Vaultwarden!
```

- `podman kube play vaultwarden.pod.yaml --replace --userns=keep-id`, the container can crate date into the container but somehow there are still some permission denied error.

```
Error: Rocket.
[CAUSE] Bind(
    Os {
        code: 13,
        kind: PermissionDenied,
        message: ""Permission denied"",
    },
)
```

I used vaultwarden container as example but I have the same issue with other containers.

What is wrong with my config/command?

Many thanks","kubernetes, podman, selinux",,,,2025-06-16T20:38:05
79667409,How to list users who has API access?,"How can I list the apiusers in cluster? RKE2, self-hosted.

Not the users in local kubeconfig, but who has API access. Not Ingress TLS certificates!

I have applied CSR-s to cluster, approved it, so they can access.

But I need a list of all user certificates (the approved, revoked, denied, etc. certificates.)

Thank you

Already tried:

- RTFM
- get csr show no resources (even a cert signed last week for 10 years, not shown)",kubernetes,79667416.0,"The Kubernetes documentation on [Authentication](https://kubernetes.io/docs/concepts/security/controlling-access/#authentication) says:

> While Kubernetes uses usernames for access control decisions and in request logging, it does not have a `User` object nor does it store usernames or other information about users in its API.

That is: there is no standard Kubernetes API path to list users or get user-specific properties like certificates, and you can't do this using standard Kubernetes APIs.",2025-06-16T10:17:13,2025-06-16T10:11:29
79667196,Kubernetes Leader Election with Lease,"I created a simple example for Leader Election on Kubernetes:

[syself/pykubeleader: Simpe Python Application which uses Kubernetes Leader Election](https://github.com/syself/pykubeleader)

But it uses the old configMap based approach:

```
from kubernetes.leaderelection.resourcelock.configmaplock import ConfigMapLock
```

How to use the new [Lease](https://kubernetes.io/docs/concepts/architecture/leases/) based way with Python?

Background: Up to now I wrote Kubernetes controllers in Go (which is the best language for that task). But for some special case, I think about writing a controller in Python.

I Go it is very easy. You create a Deployment with several replicas. Controller-Runtime code ensures that only one is the leader.

You just need to set [LeaderElection (bool), and LeaderElectionID](https://pkg.go.dev/sigs.k8s.io/controller-runtime/pkg/manager#Options.LeaderElection)","python, kubernetes, leader-election",79691526.0,"Sorry for not posting the direct answer. I was struggling with coding up lease-based-master-election strategy in python. But I think this strategy is used for High Availibility only. Like scenarios where if the master fails, then the incoming requests should be quickly forwarded to the slaves.

But the catch is even when the master fails until the time is reached for which the lease was taken the incoming requests will be forwarded to the same master. So it will not be available for that time.

What my proposed design is - Keeping two pods with two different statefulset, a master and a slave. They both will be kept consistent asynchronously. And in the application level logic we can backoff exponentially and say for every even retry we try the master and for odd retry we try the slave. We can make more slaves to get higher availibility and in this way the downtime will be decreased dramatically.

Please let me know your opinion on this design.",2025-07-06T05:58:53,2025-06-16T07:25:55
79665804,Kubernetes Jenkins as a pod with docker pipeline error,"I have a Ubuntu VM with an installed Docker engine, a K3S cluster including Jenkins 2.511, and a JDK 21 pod.

I have been struggling with this kind of error for several days:

`cp: can't stat '/var/jenkins_home/jobs/<job_name>/branches/develop/workspace@tmp/durable-1bc899cb/script.sh': No such file or directory sh: can't create /var/jenkins_home/jobs/<job_name>/branches/develop/workspace@tmp/durable-1bc899cb/jenkins-log.txt: nonexistent directory sh: can't create /var/jenkins_home/jobs/<job_name>/develop/workspace@tmp/durable-1bc899cb/jenkins-result.txt.tmp: nonexistent directory mv: can't rename'/var/jenkins_home/jobs/<job_name>/branches/develop/workspace@tmp/durable-1bc899cb/jenkins-result.txt.tmp': No such file or directory `

I also saw [this thread](https://github.com/jenkinsci/docker/issues/626), where it has been discussed, however, none of these proposals worked for me.

The volume where /var/jenkins_home resides has been added to the container running on the host (VM), but I still received the same error, despite that the /var/docker/docker.sock from the host worked correctly. What is the solution?","docker, kubernetes, jenkins, jenkins-pipeline",79665805.0,"Inspired by [the official documentation](https://docs.cloudbees.com/docs/cloudbees-ci/latest/pipelines/docker-workflow) of the Jenkins Docker pipeline plugin, I subsequently decided to create Jenkins agent residing on the VM. Specifically, the described part in the image helped me.

[![Docker pipeline plugin](https://i.sstatic.net/n3tosHPN.png)](https://i.sstatic.net/n3tosHPN.png)

**The Docker server and Jenkins agent must use the same filesystem.** I decided to create a Jenkins agent with SSH access to the VM (host) and used the label node in my Jenkinsfile:

```
pipeline {
  agent {
    label 'my-vm-node'
  }
  stages {
    stage('Build in Docker') {
      steps {
        script {
          docker.withRegistry('https://docker-registry.com/v2/', 'my-docker-registry-cred') {
            docker.image('my-image').inside('-u 0') {
               stage('Compile') {
                   sh 'mvn version'

               }
            }
          }
        }
      }
    }
  }
}
```

This the only solution I explored after 10 days.",2025-06-14T11:32:34,2025-06-14T11:32:34
79665558,Weird logging behavior for spring security logging on container environment,"We have an application using spring security, we need to enable debug logging due to a issue at customer site. The application is a war file deployed in tomcat, hosted in K8s on a container.
We set up the logger in logback.xml

```
<!-- OFF, TRACE, DEBUG, INFO, WARN, ERROR, ALL -->
<property name=""defaultLevel"" value=""DEBUG"" />
<root level=""${defaultLevel}"">
    <appender-ref ref=""STDOUT"" />
</root>
<logger name=""org.springframework.security"" level=""DEBUG"" />
```

as well as tag in spring-security.xml

```
<sec:debug />
```

When we run it on our local server(without containerization) it works we get so much of logs containing requests and spring classes.

But when we push the same changes into image and then deploy on kubernetes, none of the spring-security debug logs appear, though we see our application debug logs. We have verified manually that the configuration is there in the image on the pod.

I am just not able to understand what am I missing here. Is there some other environment variables/setting that may suppress debug logging or logging behavior?","docker, kubernetes, spring-security, logback",79665599.0,"The core issue seems to be that even the FILE appender isn’t writing logs inside the container, despite working fine on local Tomcat. Since sec:debug only affects stdout, the real problem lies with Logback’s configuration or file accessibility within the container.

Possible root causes:

- The logback.xml in the container might not be in the classpath or may be overridden by a differently named file (e.g., logback-spring.xml).
- The file path for the FILE appender may not exist or is not writable inside the container’s filesystem or volume.
- Logback’s config may not be scanned or reloaded correctly in the container environment (e.g. missing scan=""true""), so changes are ignored.
- Verify that the logback.xml is located at /WEB-INF/classes/ (or classpath root), and double-check the FILE appender’s path and permissions inside the container. That should uncover why logging works locally but not in Kubernetes.",2025-06-14T05:29:52,2025-06-14T04:02:51
79665550,Flink autoscaler not scaling down task manger pods,"We have deployed flink pipeline using flink kubernetes operator in application mode. We have enabled flink autoscaler with the following configuration. I see that adpative scheduler is able to adjust the parallelism based on the load metrics. But the Task manager pods are not being removed when autoscaler has decresed the parallelism. I want to know what configuration needs to be tweaked so that pods are also removed and we get the cost benefit.

```
  job.autoscaler.enabled: ""true""
  job.autoscaler.scaling.enabled: ""true""
  job.autoscaler.stabilization.interval: ""10m""
  job.autoscaler.metrics.window: ""30m""
  job.autoscaler.utilization.target: ""0.6""
  job.autoscaler.utilization.max: ""0.7""
  job.autoscaler.utilization.min: ""0.4""
  job.autoscaler.restart.time: ""5m""
  job.autoscaler.catch-up.duration: ""20m""
```

[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#autoscaler-configuration](https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#autoscaler-configuration)","kubernetes, apache-flink, flink-streaming",,,,2025-06-14T03:41:56
79665286,How to configure a kafka server with oauthbearer authentication in kubernetes,"I am trying to deploy a kafka server por a POC that uses oauthbearer authentication with a custom identidy provider that exposes the endpoints /token and /.well-known/jwks.json. For some reason that I just can't figure out it returns this error:
java.lang.IllegalArgumentException: Could not find a 'KafkaServer' or 'external.KafkaServer' entry in the JAAS configuration. System property 'java.security.auth.login.config' is not set

But I already set the variables

```
    - name: KAFKA_EXTERNAL_KAFKASERVER
      value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
    - name: KAFKA_JAAS_CONFIG
      value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
    - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_JAAS_CONFIG
      value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
```

According:
[https://docs.confluent.io/platform/7.4/kafka/authentication_sasl/authentication_sasl_oauth.html](https://docs.confluent.io/platform/7.4/kafka/authentication_sasl/authentication_sasl_oauth.html)

Here is the full manifest

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
      - name: kafka
        image: confluentinc/cp-kafka:latest
        ports:
        - containerPort: 29092
          name: internal
        - containerPort: 29093
          name: controler
        - containerPort: 9092
          name: external
        - containerPort: 9101
          name: jmx
        env:
        - name: KAFKA_NODE_ID
          value: ""1""
        - name: KAFKA_BROKER_ID
          value: ""1""
        - name: KAFKA_ADVERTISED_LISTENERS
          value: ""PLAINTEXT://kafka-internal.kafka.svc.cluster.local:29092,EXTERNAL://localhost:9092""
        - name: KAFKA_JMX_PORT
          value: ""9101""
        - name: KAFKA_JMX_HOSTNAME
          value: ""localhost""
        - name: KAFKA_PROCESS_ROLES
          value: ""broker,controller""
        - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
          value: ""1""
        - name: KAFKA_CONTROLLER_QUORUM_VOTERS
          value: ""1@kafka-internal:29093""
        - name: KAFKA_LISTENERS
          value: ""PLAINTEXT://0.0.0.0:29092,CONTROLLER://0.0.0.0:29093,EXTERNAL://0.0.0.0:9092""
        - name: KAFKA_INTER_BROKER_LISTENER_NAME
          value: ""PLAINTEXT""
        - name: KAFKA_CONTROLLER_LISTENER_NAMES
          value: ""CONTROLLER""
        - name: CLUSTER_ID
          value: ""MkU3OEVBNTcwNTJENDM2Qk""
        - name: KAFKA_LOG4J_LOGGERS
          value: kafka.authorizer.logger=DEBUG,kafka.network.RequestChannel$=DEBUG,kafka.network.Processor=DEBUG,log4j.logger.org.apache.kafka.common.security=DEBUG
        # --- OAUTHBEARER for EXTERNAL ---
        - name: KAFKA_SASL_ENABLED_MECHANISMS
          value: ""PLAIN,OAUTHBEARER""
        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
          value: ""CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:SASL_PLAINTEXT""

        # OAUTHBEARER for EXTERNAL listener
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_ENABLED_MECHANISMS
          value: ""OAUTHBEARER""

        - name: KAFKA_EXTERNAL_KAFKASERVER
          value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
        - name: KAFKA_JAAS_CONFIG
          value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_JAAS_CONFIG
          value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""

        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_JAAS_CONFIG
          value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required unsecuredLoginStringClaim_sub='admin';""
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_TOKEN_VALIDATION_CALLBACK_HANDLER_CLASS
          value: ""org.apache.kafka.common.security.oauthbearer.OAuthBearerValidatorCallbackHandler""
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_TOKEN_ENDPOINT_URL
          value: ""http://idp-internal-oauth.default.svc.cluster.local:4000/token""
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_JWKS_ENDPOINT_URL
          value: ""http://idp-internal-oauth.default.svc.cluster.local:4000/.well-known/jwks.json""
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_JWKS_ENDPOINT_REFRESH_MS
          value: ""3600000""  # 1 hour in milliseconds
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_JWKS_ENDPOINT_CACHE_MAX_AGE_MS
          value: ""3600000""  # 1 hour in milliseconds
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_SCOPE_CLAIM_NAME
          value: ""scope""  # This should match the scope defined in your OAuth provider
        - name: KAFKA_LISTENER_NAME_EXTERNAL_OAUTHBEARER_SASL_OAUTHBEARER_SUB_CLAIM_NAME
          value: ""sub""  # This should match the scope defined in your OAuth provider

        # --- PLAIN for inter-broker ---
        - name: KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL
          value: ""PLAIN""
        - name: KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND
          value: ""false""
        - name: POD_NAME
          valueFrom:
            fieldRef:
              apiVersion: v1
              fieldPath: metadata.name
        resources:
          limits:
            cpu: ""1""
            memory: 700Mi
          requests:
            cpu: 250m
            memory: 512Mi
        volumeMounts:
        - mountPath: /etc/kafka
          name: config
        - mountPath: /var/lib/kafka/data
          name: pvc
        - mountPath: /var/log
          name: logs
      volumes:
      - emptyDir: {}
        name: config
      - emptyDir: {}
        name: logs
      - name: pvc
        persistentVolumeClaim:
          claimName: kafka
```","kubernetes, apache-kafka, oauth",79665611.0,"Kubernetes isn't the problem. The error is from the broker itself.

Per the linked documentation, you're missing the kafka_server_jaas.conf file, which holds the missing KafkaServer JAAS directive. You can use a Configmap for this",2025-06-14T05:58:08,2025-06-13T19:18:36
79664432,AKS-extension Deployment failed because used version was not found,"I have a problem and it's driving me crazy. The company i work for wants to offer a Kubernetes Cluster in the Azure Marketplace. I have build the necessary CNAB-Bundle and created the offering/plan in the Partner Center. The Bundle was verified by microsoft and is live inside the marketplace.

The Problem is now, that when i want to deploy the app from the marketplace to test it, the deployment fails when it tries to create the Microsoft.KubernetesConfiguration extensions.

[Deployment failed overview](https://i.sstatic.net/ElPrjpZP.png)

The error message it spits out:

```
{
  ""code"": ""InvalidResourceType"",
  ""message"": ""Resource type 'extensions' of provider namespace 'Microsoft.KubernetesConfiguration' was not found in global location for api version '2024-11-01'.""
}
```

First i was confused because the Bundle and all the templates went through all the validation steps and there was never a Problem.
So i tried to validate it again with ARM-TTK and sure enough, a similar warning popped up:

```
apiVersions Should Be Recent [?] apiVersions Should Be Recent (756 ms) The apiVersion 2024-11-01 was not found for the resource type:               Microsoft.KubernetesConfiguration/extensions
```

so i tried to change the version until i found one (2023-05-01) that was validated by ARM-TTK:

```
apiVersions Should Be Recent [+] apiVersions Should Be Recent (340 ms)
```

I rebuilt the Bundle and published it again. It went through the Microsoft validation and it went live again.

But i still got the same Error:

```
{
  ""code"": ""InvalidResourceType"",
  ""message"": ""Resource type 'extensions' of provider namespace 'Microsoft.KubernetesConfiguration' was not found in global location for api version '2023-05-01'.""
}
```

This is the ARM-Template ""clusterARMTemplate.json"" i used:

```
  ""$schema"": ""https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""variables"": {
    ""plan-name"": ""myPlanName"",
    ""plan-publisher"": ""myCompanyName"",
    ""plan-offerID"": ""planOfferID"",
    ""releaseTrain"": ""stable"",
    ""clusterExtensionTypeName"": ""myClusterExtensionTypeName""
  },
  ""plan"": {
    ""name"": ""myPlanName"",
    ""publisher"": ""myCompanyName"",
    ""product"": ""productName""
  },
  ""parameters"": {
    ""location"": {
      ""type"": ""string"",
      ""metadata"": {
        ""description"": ""Bereitstellungsregion""
      }
    },
    ""adminUsername"": {
      ""type"": ""string"",
      ""defaultValue"": ""azureuser"",
      ""metadata"": {
        ""description"": ""Administrator-Username für den Linux-Knoten""
      }
    },
    ""clusterName"": {
      ""type"": ""string"",
      ""defaultValue"": ""myApp-aks"",
      ""metadata"": {
        ""description"": ""Name des AKS Clusters""
      }
    },
    ""clusterDnsPrefix"": {
      ""type"": ""string"",
      ""defaultValue"": ""myAppdns"",
      ""metadata"": {
        ""description"": ""DNS Prefix für den AKS Cluster""
      }
    },
    ""agentCount"": {
      ""type"": ""int"",
      ""defaultValue"": 3,
      ""metadata"": {
        ""description"": ""Anzahl der Knoten im AKS Cluster""
      }
    },
    ""agentVMSize"": {
      ""type"": ""string"",
      ""defaultValue"": ""Standard_DS2_v2"",
      ""metadata"": {
        ""description"": ""VM-Größe für die AKS Knoten""
      }
    },
    ""extensionName"": {
      ""type"": ""string"",
      ""defaultValue"": ""myClusterExtensionTypeName"",
      ""metadata"": {
        ""description"": ""Name der Kubernetes Erweiterung""
      }
    },
    ""configurationSettings"": {
      ""type"": ""object"",
      ""metadata"": {
        ""description"": ""Konfigurationseinstellungen aus der UI-Definition (Namespace, Replica Count, etc.)""
      }
    },
    ""sshPublicKey"": {
     ""type"": ""string"",
     ""metadata"": {
       ""description"": ""SSH Public Key für den Zugriff auf den AKS Linux-Knoten""
     }
   }
  },
  ""resources"": [
    {
      ""type"": ""Microsoft.ContainerService/managedClusters"",
      ""apiVersion"": ""2023-10-01"",
      ""name"": ""[parameters('clusterName')]"",
      ""location"": ""[parameters('location')]"",
      ""identity"": {
        ""type"": ""SystemAssigned""
      },
      ""properties"": {
        ""dnsPrefix"": ""[parameters('clusterDnsPrefix')]"",
        ""agentPoolProfiles"": [
          {
            ""name"": ""agentpool"",
            ""count"": ""[parameters('agentCount')]"",
            ""vmSize"": ""[parameters('agentVMSize')]"",
            ""osType"": ""Linux"",
            ""type"": ""VirtualMachineScaleSets"",
            ""mode"": ""System""
          }
        ],
        ""linuxProfile"": {
          ""adminUsername"": ""[parameters('adminUsername')]"",
          ""ssh"": {
            ""publicKeys"": [
              {
                ""keyData"": ""[parameters('sshPublicKey')]""
              }
            ]
          }
        }
      }
    },
    {
      ""type"": ""Microsoft.KubernetesConfiguration/extensions"",
      ""apiVersion"": ""2023-05-01"",
      ""name"": ""[parameters('extensionName')]"",
      ""dependsOn"": [
        ""[resourceId('Microsoft.ContainerService/managedClusters', parameters('clusterName'))]""
      ],
      ""identity"": {
        ""type"": ""SystemAssigned""
      },
      ""plan"": {
        ""name"": ""[variables('plan-name')]"",
        ""publisher"": ""[variables('plan-publisher')]"",
        ""product"": ""[variables('plan-offerID')]""
      },
      ""properties"": {
        ""extensionType"": ""[variables('clusterExtensionTypeName')]"",
        ""autoUpgradeMinorVersion"": true,
        ""configurationSettings"": ""[parameters('configurationSettings')]"",
        ""scope"": {
          ""cluster"": {
            ""releaseNamespace"": ""[parameters('clusterDnsPrefix')]""
          }
        }
      }
    }
  ],
  ""outputs"": {
    ""plan-name"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-name')]""
    },
    ""plan-publisher"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-publisher')]""
    },
    ""plan-offerID"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-offerID')]""
    },
    ""releaseTrain"": {
      ""type"": ""string"",
      ""value"": ""[variables('releaseTrain')]""
    },
    ""clusterExtensionTypeName"": {
      ""type"": ""string"",
      ""value"": ""[variables('clusterExtensionTypeName')]""
    }
  }
}
```

I searched the Internet for a solution but the ones i found didn't work.
I saw someone mentioning that i need to register the feature in my companies azure subscription with:

```
az feature registration create --namespace Microsoft.KubernetesConfiguration --name ExtensionTypes
```

But it already was registered.

This is the first time i'm doing something like this and i can't find out what the Problem is. I hope someone here can help me with this.","azure, kubernetes, azure-aks, azure-marketplace",79664649.0,"First when you develop ARM templates that you will use for Marketplace item it is best to first test the deployment on your own before publishing it. Additionally I would recommend to develop via [Azure Bicep](https://learn.microsoft.com/en-us/azure/azure-resource-manager/bicep/overview?tabs=bicep?WT.mc_id=AZ-MVP-5000120), compile the template to ARM template and after that incorporate the necessary changes for being able to publish that template to the Azure Marketplace. Azure Bicep can catch more things that are not correct before the deployment compared to developing it as ARM template. The issue you have is with the scope. Resource type Microsoft.KubernetesConfiguration/extensions is [extension resource type](https://learn.microsoft.com/en-us/azure/azure-resource-manager/management/extension-resource-types?WT.mc_id=AZ-MVP-5000120). These type of resources cannot exists on their own. They can only be deployed if they are scoped to another resource. In this case they have to be on top of Microsoft.ContainerService/managedClusters resource type. So the template needs to have that scope:

```
{
""$schema"": ""https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#"",
  ""contentVersion"": ""1.0.0.0"",
  ""variables"": {
    ""plan-name"": ""myPlanName"",
    ""plan-publisher"": ""myCompanyName"",
    ""plan-offerID"": ""planOfferID"",
    ""releaseTrain"": ""stable"",
    ""clusterExtensionTypeName"": ""myClusterExtensionTypeName""
  },
  ""plan"": {
    ""name"": ""myPlanName"",
    ""publisher"": ""myCompanyName"",
    ""product"": ""productName""
  },
  ""parameters"": {
    ""location"": {
      ""type"": ""string"",
      ""metadata"": {
        ""description"": ""Bereitstellungsregion""
      }
    },
    ""adminUsername"": {
      ""type"": ""string"",
      ""defaultValue"": ""azureuser"",
      ""metadata"": {
        ""description"": ""Administrator-Username für den Linux-Knoten""
      }
    },
    ""clusterName"": {
      ""type"": ""string"",
      ""defaultValue"": ""myApp-aks"",
      ""metadata"": {
        ""description"": ""Name des AKS Clusters""
      }
    },
    ""clusterDnsPrefix"": {
      ""type"": ""string"",
      ""defaultValue"": ""myAppdns"",
      ""metadata"": {
        ""description"": ""DNS Prefix für den AKS Cluster""
      }
    },
    ""agentCount"": {
      ""type"": ""int"",
      ""defaultValue"": 3,
      ""metadata"": {
        ""description"": ""Anzahl der Knoten im AKS Cluster""
      }
    },
    ""agentVMSize"": {
      ""type"": ""string"",
      ""defaultValue"": ""Standard_DS2_v2"",
      ""metadata"": {
        ""description"": ""VM-Größe für die AKS Knoten""
      }
    },
    ""extensionName"": {
      ""type"": ""string"",
      ""defaultValue"": ""myClusterExtensionTypeName"",
      ""metadata"": {
        ""description"": ""Name der Kubernetes Erweiterung""
      }
    },
    ""configurationSettings"": {
      ""type"": ""object"",
      ""metadata"": {
        ""description"": ""Konfigurationseinstellungen aus der UI-Definition (Namespace, Replica Count, etc.)""
      }
    },
    ""sshPublicKey"": {
     ""type"": ""string"",
     ""metadata"": {
       ""description"": ""SSH Public Key für den Zugriff auf den AKS Linux-Knoten""
     }
   }
  },
  ""resources"": [
    {
      ""type"": ""Microsoft.ContainerService/managedClusters"",
      ""apiVersion"": ""2023-10-01"",
      ""name"": ""[parameters('clusterName')]"",
      ""location"": ""[parameters('location')]"",
      ""identity"": {
        ""type"": ""SystemAssigned""
      },
      ""properties"": {
        ""dnsPrefix"": ""[parameters('clusterDnsPrefix')]"",
        ""agentPoolProfiles"": [
          {
            ""name"": ""agentpool"",
            ""count"": ""[parameters('agentCount')]"",
            ""vmSize"": ""[parameters('agentVMSize')]"",
            ""osType"": ""Linux"",
            ""type"": ""VirtualMachineScaleSets"",
            ""mode"": ""System""
          }
        ],
        ""linuxProfile"": {
          ""adminUsername"": ""[parameters('adminUsername')]"",
          ""ssh"": {
            ""publicKeys"": [
              {
                ""keyData"": ""[parameters('sshPublicKey')]""
              }
            ]
          }
        }
      }
    },
    {
      ""type"": ""Microsoft.KubernetesConfiguration/extensions"",
      ""apiVersion"": ""2023-05-01"",
      ""name"": ""[parameters('extensionName')]"",
      ""scope"": ""[format('Microsoft.ContainerService/managedClusters/{0}', parameters('clusterName'))]"",
      ""dependsOn"": [
        ""[resourceId('Microsoft.ContainerService/managedClusters', parameters('clusterName'))]""
      ],
      ""identity"": {
        ""type"": ""SystemAssigned""
      },
      ""plan"": {
        ""name"": ""[variables('plan-name')]"",
        ""publisher"": ""[variables('plan-publisher')]"",
        ""product"": ""[variables('plan-offerID')]""
      },
      ""properties"": {
        ""extensionType"": ""[variables('clusterExtensionTypeName')]"",
        ""autoUpgradeMinorVersion"": true,
        ""configurationSettings"": ""[parameters('configurationSettings')]"",
        ""scope"": {
          ""cluster"": {
            ""releaseNamespace"": ""[parameters('clusterDnsPrefix')]""
          }
        }
      }
    }
  ],
  ""outputs"": {
    ""plan-name"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-name')]""
    },
    ""plan-publisher"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-publisher')]""
    },
    ""plan-offerID"": {
      ""type"": ""string"",
      ""value"": ""[variables('plan-offerID')]""
    },
    ""releaseTrain"": {
      ""type"": ""string"",
      ""value"": ""[variables('releaseTrain')]""
    },
    ""clusterExtensionTypeName"": {
      ""type"": ""string"",
      ""value"": ""[variables('clusterExtensionTypeName')]""
    }
  }
}
```",2025-06-13T09:44:12,2025-06-13T06:24:47
79663394,Helm Templates dockerconfigjson secret - Cannot unmarshal string into Go struct field Secret.data,"I have defined a helm template like the one below to get a predefined set of private registries in values and create a dockerconfigjson type secret in the namespace if needed by copying the templates into the helm chart.

**value.yaml**

```
privateregistries:
  - registry: internal1.local
    username: ""sxs""
    token: ""sxs""
  - registry: internal2.local
    username: ""sxs""
    token: ""sxs""
```

**template file**

```
{{- $auths := dict }}
{{ range $key, $value := .Values.privateregistries }}
   {{- $auth := printf ""%s:%s"" $value.username $value.token | b64enc }}
   {{- $_ := set $auths $value.registry (dict ""auth"" $auth) }}
{{ end }}
{{- $json := dict ""auths"" $auths | toJson }}

apiVersion: v1
kind: Secret
metadata:
  name: regcred
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: {{- quote (b64enc $json) }}
```

But when trying to apply, I see the error below. What could be the error?

cannot unmarshal string into Go struct field Secret.data of type map[string][]uint8","kubernetes, kubernetes-helm, go-templates",79663419.0,"The hyphen in the final line makes the YAML structure invalid.

```
data:
  .dockerconfigjson: {{- quote (b64enc $json) }}
  #                    ^ this one
```

You can just remove it.  You don't specifically need to `quote` the value either.  (YAML doesn't require the quotes and a base64 string won't have punctuation that potentially confuses YAML; if you do need to quote something, `toJson` will be more robust.)

```
data:
  .dockerconfigjson: {{ b64enc $json }}
```

The hyphen inside the curly braces causes the Go templating engine to remove all of the whitespace outside the curly braces.  That puts the value directly up against the key, but the YAML syntax requires at least one space after the colon.

```
# original form, doesn't parse:
.dockerconfigjson:""e30=""

# final form (without `quote`), works:
.dockerconfigjson: e30=
```

Running `helm template --debug` will dump out the output of the template even if it's not valid YAML, which can occasionally help you to find problems like this.  It tends to be more obvious with extra or missing hyphens at the start or end of whole lines where you can get lines joined together or missing indentation.",2025-06-12T11:11:39,2025-06-12T10:51:30
79662513,Why Istio virtual services rules seems to not affect traffic flow?,"I have two different virtual services that uses the same host (a kubernetes service) as source to apply a traffic routing rule using a specific header, the virtual services configs seems to be ok as I've seen at **Kiali** and validate the configurations with `istioctl analyze`, but the rules seems to not being applied correctly.

The kubernetes services and its target pods are registered into the mesh.

This is my Virtual Service for the **v1** version of the application, the main goal with this virtual service is to have a ""catch-all"" rule to route all traffic to the **v1** environment of the application (which is served at the same k8s service).

```
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  labels:
  name: example-app-v1
  namespace: example-app-v1
spec:
  exportTo:
    - .
  gateways:
    - mesh
  hosts:
    - example-app-v1.example-app-v1.svc.cluster.local
  http:
    - route:
        - destination:
            host: example-app-v1.example-app-v1.svc.cluster.local
```

Now this is my Virtual Service for the **v2** version of the application, the main goal with this virtual service is to route all traffic that matches the header and value `environment: v2` to the **v2** environment of the application (which is served by another k8s service) using the same source host as the **default** environment (which is `example-app-v1.example-app-v1.svc.cluster.local`).

```
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: example-app-v2
  namespace: example-app-v2
spec:
  exportTo:
    - .
  gateways:
    - mesh
  hosts:
    - example-app-v1.example-app-v1.svc.cluster.local
  http:
    - match:
        - headers:
            environment:
              exact: v2
      route:
        - destination:
            host: example-app-v2.example-app-v2.svc.cluster.local
```

All the requests are always forwarded to the **v1** environment, even with the header `environment` defined as `v2`. Is there anything wrong in my configuration? Do I need a Istio Gateway to make it work? (currently I'm using Emissary Ingress as ingress to the k8s services)

It works when writing only 1 VirtualService with all these rules defined within, but for this use-case is that we can have many different environments being created every day, each one of them is a copy from the **v1** environment but with its own modifications and differences, and these environments will be created on-demand, that's why I'm needing to create multiple virtual services that uses the same source host (the k8s service from **v1** environment)","kubernetes, istio, kubernetes-service, istio-gateway, istio-sidecar",79665384.0,"Even though your setup technically works in terms of configuration validation (`istioctl analyze`), according to the [official Istio documentation](https://docs.tetrate.io/istio-subscription/tools/tca/analysis/TIS1105#:%7E:text=Explanation%3A%20Both%20VirtualService,the%20same%20host.): Istio does not properly merge Virtual Services across different namespaces for the same host. The [rules](https://istio.io/latest/docs/concepts/traffic-management/#routing-rule-precedence) within its `http` block are evaluated sequentially from top-to-bottom, so ensure to place your most specific rules first, followed by the general [""catch-all""](https://docs.tetrate.io/istio-subscription/tools/tca/analysis/TIS1105#recommendation:%7E:text=Explanation%3A%20Including%20a%20%22catch%2Dall%22%20route%20ensures%20that%20all%20requests%20that%20do%20not%20match%20more%20specific%20routes%20are%20properly%20handled%2C%20preventing%20traffic%20from%20being%20dropped.) rule at the end.

For your use case, you can try to dynamically manage a single and merged Virtual Service through [automation of CI/CD pipelines](https://overcast.blog/automating-ci-cd-pipelines-in-kubernetes-for-microservices-architecture-4880f744fdd6). This approach gives you the flexibility you need while adhering to Istio's fundamental design principles, ensuring your routing is always predictable and stable. In addition, no, you don’t need an [Istio Gateway](https://www.getambassador.io/docs/emissary/latest/howtos/istio) as this is Istio’s own ingress solution, and you are already using Emissary that handles the ingress part.

For further reference you can also check: [How to design a deployment strategy for Istio applications that use traffic routing](https://discuss.istio.io/t/how-to-design-a-deployment-strategy-for-istio-applications-that-use-traffic-routing/1759)",2025-06-13T21:00:33,2025-06-11T18:59:09
79661287,How to listen for Kubernetes DNS changes,"I have a controller using the `kube` crate that manages various worker resources (services/deployments). In those containers, I receive updates from the controller via a `ConfigMap` and make downstream requests to other Kubernetes services. As part of this, I do DNS queries, via the `hickory-resolver` crate, to resolve the IP of those down stream services. I'm concerned a bit about the DNS caching.

What event/resource do I need to subscribe to from the controller to initiate a dump the DNS cache in Hickory (it has a method off its resolver to do so)? Or am I over thinking this and the TTL will solve this for me?","kubernetes, rust, dns, kube-rs",79664202.0,"After some more research and chatting with ChatGPT, I learned about the [`EndpointSlice`](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/) resources. These resources hold pages of the IP addresses (both v4/v6) for a given `Service`. Since I have the service names I require elsewhere, I can watch for `EndpointSlice` events and collect what I need. Additional benefits include:

- They have zone and node metadata so I can make more local routing decisions.
- They have distilled readiness checks and termination statuses, so I can filter before sending them out of my controller.

Overall, this is much more robust than DNS!",2025-06-12T23:37:07,2025-06-11T01:40:57
79658864,How to persist ConfigMap values on deployment upgrade?,"I'm using ConfigMap to switch on/off some functionality of the application in the pod. I have mounted it in the deployment like that:

```
volumes:
  - name: {{ .Chart.Name }}-config-volume
    projected:
      sources:
      - configMap:
          name: {{ .Chart.Name }}-content-config
```

then I have some configuration data in ConfigMap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Chart.Name }}-content-config
data:
  content.properties: |
    {
      ""Enabled"": false,
      ""ApiEndpoint"": ""...""
    }
```

When the functionality is configured and ready to be enabled, I run`kubectl edit cm` and set ""Enabled"" to true. Application is reading file every 2 minutes and refreshing configuration respectively without restarting the pod. Ok, it's working, it's persisting through pod restarts.

But, if I'm doing `helm upgrade` to the next version - everything is reset and again has default values, e.g. ""Enabled: false"". Is there any way to make ConfigMap persistent no matter the upgrades?","kubernetes, kubernetes-helm, configmap",79658954.0,"Don't try to use two separate tools to manage your Kubernetes manifests.  You should be able to manage this entirely in Helm.

For example, you can put the API endpoint value in deploy-time configuration

```
# values.yaml
apiEndpoint: https://...
```

Then when your Helm chart produces the ConfigMap, it can insert the values from your Helm-level configuration.  Helm includes a [`toJson` extension function](https://docs.helm.sh/docs/chart_template_guide/function_list/#type-conversion-functions) that can encode an arbitrary value as JSON.

```
# templates/configmap.yaml
data:
  content.properties: |
    {
      ""Enabled"": {{ toJson .Values.contentEnabled }},
      ""ApiEndpoint"": {{ toJson .Values.apiEndpoint }}
    }
```

Then you can keep a reference set of override values (probably in source control, maybe managed in your CD system).  If you need to change these values then you can use `helm upgrade`, and it will consistently redeploy everything from the rendered templates.

```
# deploy/values-dev.yaml
apiEndpoint: https://internal.example.com/api/
```

```
helm upgrade --install -f deploy/values-dev.yaml -n dev my-app .
```

Once Helm has deployed it, don't try to `kubectl edit` any of the resources (except maybe in very-short-term debugging scenarios, but if you do, make sure you put things back the way you found them).

(Some values of Helm have included a ""3-way merge"" that attempts to do what you describe.  IME that has been more a source of confusion than anything helpful: if a deploy fails then Helm tries to do a merge between the previous version, the failed deploy, and the corrected version, and you inevitably wind up with something that's plainly right there in your template file not showing up in the cluster.  A previous deploy pipeline went out of its way to explicitly uninstall the previous version specifically to get around the problems that 3-way merged introduced.)",2025-06-09T13:27:46,2025-06-09T12:20:48
79656270,How to configure Camel K to push build images to a custom Docker repository path instead of creating its own?,"**Requirement:**

Deploy Apache Camel K on a local Kubernetes cluster using Helm. Camel K should push all generated build images into a specific Docker Hub repo that I own, such as:

```
docker.io/kshitijtripathi/camel-k/<image-name:tag>
```

However, Camel K is currently pushing builds like this:

```
docker.io/kshitijtripathi/camel-k-kit-d11f2l3nb70c73e8el10:68565
```

**What I have done so far:**

I'm using Helm to install Camel K with a values.yaml like:

- Created namespace camel-k and added helm repo via **helm repo add camel-k [https://apache.github.io/camel-k/charts](https://apache.github.io/camel-k/charts)**
- Created values.yaml for camel-k and run **helm install camel-k camel-k/camel-k   --version 2.6.0   --namespace camel-k   -f values.yaml**

```
 platform:
   build:
     registry:
       address: docker.io
       organization: kshitijtripathi
       insecure: false
       secret: camel-secret
     publishStrategy: ""Spectrum""
 traits:
   gc:
     configuration:
       enabled: true
       maxBuilds: 2
       maxAge: 10m
```
- I created a Secret (camel-secret) with Docker Hub credentials using:

```
kubectl create secret docker-registry camel-secret \
--docker-server=docker.io \
--docker-username=kshitijtripathi \
--docker-password='<MY_DOCKERHUB_TOKEN>' \
--docker-email='<EMAIL>' \
-n camel-k
```
- Camel K successfully builds and pushes images for the test Integration config.

**Problem:**

Camel K still pushes images into separate auto-generated repositories like:

```
docker.io/kshitijtripathi/camel-k-kit-<random>:<tag>
```

I want to control the repo path, like:

```
docker.io/kshitijtripathi/camel-k/<my-image-name>:<tag>
```

**Question:**

Is there a way to configure Camel K (via Helm, Operator, or trait) so that:

1. It does not create a new repo for each build.
2. It uses a predefined custom image name or format, e.g., camel-k/helloworld:tag.
3. Keeps the builds organized under a single repository on Docker Hub?","kubernetes, apache-camel, apache-camel-k",,,,2025-06-06T17:25:31
79655560,Adding downstream api rate limiting in Istio service mesh,"I'm working on configuring a service mesh using Istio v1.2 with Envoy Proxy.
Scenario:
I have two services within the mesh:

```
Service A → Service B
```

Service A makes HTTP calls to specific endpoints of Service B.

Requirement:
I want to enforce rate limiting at the outbound side of Service A — i.e., when Service A makes requests to certain URL paths of Service B (e.g., /api/v1/users/{id}/details), with path parameters included.

Rate limiting should be based on specific URL patterns.

Ideally applied via EnvoyFilter at the sidecar of Service A.

What I’ve Tried:
I've attempted to use EnvoyFilters similar to the one below (attaching snippet), where I inject a rate limit filter in SIDECAR_OUTBOUND and match the paths using descriptors. But it doesn't seem to apply correctly to outbound traffic or path-based matching.

```
#not working
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: productpage-ratelimit
  namespace: default  # Replace with your namespace if different
spec:
  workloadSelector:
    labels:
      app: productpage  # Targets the productpage pods
  configPatches:
    - applyTo: HTTP_FILTER
      match:
        context: SIDECAR_OUTBOUND  # Applies to outbound traffic from productpage
        listener:
          filterChain:
            filter:
              name: envoy.filters.network.http_connection_manager
              subFilter:
                name: envoy.filters.http.router
      patch:
        operation: INSERT_BEFORE
        value:
          name: envoy.filters.http.local_ratelimit
          typed_config:
            ""@type"": type.googleapis.com/udpa.type.v1.TypedStruct
            type_url: type.googleapis.com/envoy.extensions.filters.http.local_ratelimit.v3.LocalRateLimit
            value:
              stat_prefix: productpage_to_reviews
              descriptors:
                - entries:
                    - key: "":path""
                      value: ""/details""
                  token_bucket:
                    max_tokens: 50
                    tokens_per_fill: 50
                    fill_interval: 1s
```

PS: I have also tried existing solution suggested here but Igues that does not works too:

[Istio rate limiting for external services](https://stackoverflow.com/questions/64501256/istio-rate-limiting-for-external-services)","kubernetes, istio, envoyproxy",,,,2025-06-06T07:47:55
79652690,Apache Camel route terminated early during pod scale-down due to in-flight shutdown timeout (HPA + parallel split processing),"I'm working on an Apache Camel-based Spring Boot application deployed in Kubernetes, where we consume files from an SFTP location, process the contents.
The issue occurs when processing a large file (e.g., 20,000 records) using a .split().parallelProcessing() strategy. Here's a simplified version of the route:

```
    from(""sftp://my-server/path?options..."")
    .split()
        .tokenizeXML(""<record>"", ""</record>"")
        .streaming()
        .parallelProcessing()
        .process(myRecordProcessor)
    .end();
```

We are using **Horizontal Pod Autoscaling (HPA)** in Kubernetes. At times, while the route is still processing a large file, the pod is terminated due to scale-down. We have configured the shutdown strategy using the default settings provided by Apache Camel:

- **Timeout:** 300 seconds (5 minutes)
- **Shutdown Now On Timeout:** true
- **Log Inflight Exchanges On Timeout:** true
- **Shutdown Routes In Reverse Order:** true
These are the default values for DefaultShutdownStrategy in Apache Camel versions 3.x.

**Problem:**
When the pod is terminated (scaled down by HPA), Camel initiates shutdown. If the file is still being processed:

- Camel gives **only 300 seconds** to complete all in-flight exchanges.
- After 300s, it forcefully stops, resulting in **incomplete file processing** (only partial records are processed).
- This leads to **data loss**, as the rest of the records in the file are never processed.

**What I've tried:**

- I’ve looked into increasing the shutdown timeout (setTimeout(600)), but this only delays the issue.
- HPA cannot differentiate between idle pods and those still processing a long file.
- The route uses .streaming().parallelProcessing(), making it harder to track individual record progress or checkpoint.

**Expecting**

- Ensure graceful completion of in-flight exchanges
- Avoid data loss during pod shutdown
- Support long processing times for large files","java, spring-boot, kubernetes, apache-camel",,,,2025-06-04T11:46:13
79652287,ReportPortal analyzer service unable to connect to RabbitMQ,"We have ReportPortal deployed with k8s. It works well overall but the issue auto-analysis doesn't work due to analyzer service not being able to start. Here are logs from the API pod:

```
2025-06-03 15:32:47.705 2025-06-03 13:32:47,609 - [un8qThWMTjKjDSy8SF4-4Q] - INFO - analyzerApp.amqp - Trying to connect to amqp://reportportal-rabbitmq.reportportal.svc.cluster.local:5672/analyzer?heartbeat=30
2025-06-03 15:32:47.706 2025-06-03 13:32:47,609 - [xy61hSzYS9-D13MzhYNnNA] - INFO - analyzerApp.amqp - AMQP connection established.
2025-06-03 15:32:47.706 2025-06-03 13:32:47,604 - [ofVyUEV-QzapYgMIoePE7w] - ERROR - analyzerApp.amqp - Connection/channel lost. Reconnecting. Exchange: 'analyzer-default'. Queue: 'index_suggest_info'.
2025-06-03 15:32:47.706 Traceback (most recent call last):
2025-06-03 15:32:47.706   File ""/backend/app/amqp/amqp.py"", line 187, in receive
2025-06-03 15:32:47.706     self._bind_queue(channel, queue, self._config.amqpExchangeName)
2025-06-03 15:32:47.706   File ""/backend/app/amqp/amqp.py"", line 147, in _bind_queue
2025-06-03 15:32:47.706     channel.queue_bind(exchange=exchange_name, queue=name, routing_key=name)
2025-06-03 15:32:47.706   File ""/venv/lib64/python3.11/site-packages/pika/adapters/blocking_connection.py"", line 2570, in queue_bind
2025-06-03 15:32:47.706     self._flush_output(bind_ok_result.is_ready)
2025-06-03 15:32:47.706   File ""/venv/lib64/python3.11/site-packages/pika/adapters/blocking_connection.py"", line 1339, in _flush_output
2025-06-03 15:32:47.706     raise self._closing_reason  # pylint: disable=E0702
2025-06-03 15:32:47.706     ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-06-03 15:32:47.706 pika.exceptions.ChannelClosedByBroker: (404, ""NOT_FOUND - no exchange 'analyzer-default' in vhost 'analyzer'"")
```

RabbitMQ pod logs:

```
2025-06-03 16:00:36.600 2025-06-03 14:00:36.117476+00:00 [error] <0.8690128.0> Channel error on connection <0.8690983.0> (10.19.172.218:43084 -> 10.19.227.194:5672, vhost: 'analyzer', user: 'rabbitmq'), channel 1:
2025-06-03 16:00:36.600 2025-06-03 14:00:36.117476+00:00 [error] <0.8690128.0> operation queue.bind caused a channel exception not_found: no exchange 'analyzer-default' in vhost 'analyzer'
```

Apparently the `analyzer-default` exchange is not being created. I didn't change any default settings for the analyzer service.

Helm chart version: 25.5.30","kubernetes, reportportal",79652288.0,"The problem is solved by setting the RabbitMQ exchange name explicitly in `values.yaml`:

```
    msgbroker:
      analyzerExchangeName: analyzer
```",2025-06-04T07:17:40,2025-06-04T07:17:40
79651504,KafkaConnector fails to auto-create topics while KafkaConnect creates its own successfully (Strimzi + Debezium + Avro + Oracle),"I'm working in a Kubernetes environment where we deploy Kafka and related services using a custom Helm chart. Here's the stack:

- Kafka via Strimzi Operator: `0.46.0`, deployed in KRaft mode, version `4.0.0`
- KafkaConnect: `quay.io/strimzi/kafka:0.46.0-kafka-4.0.0`, extended with:
  - Debezium JARs
  - Avro converter JARs
  - `ojdbc17.jar` for Oracle
- Schema Registry: `docker.io/bitnami/schema-registry:7.9.0`
- Kafka auto-create topics is enabled (`auto.create.topics.enable: true`)

**The Issue**:

My `KafkaConnect` cluster creates its required internal topics just fine:

- `connect-cluster-configs`
- `connect-cluster-offsets`
- `connect-cluster-status`

However, when I deploy a `KafkaConnector` (Debezium Oracle source connector), it fails to auto-create any topics (e.g., the topic for `schema.history.internal.kafka.topic`, `database.history.kafka.topic`, or for the data tables themselves). There’s no clear exception, but we see timeouts in the logs:

```
```log
2025-06-03 14:56:28 INFO  [kafka-producer-network-thread | next-schemahistory] NetworkClient:411 - [Producer clientId=next-schemahistory] Cancelled in-flight METADATA request with correlation id 126 due to node -1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, throttle time: 0ms, request timeout: 30000ms)
2025-06-03 14:56:28 WARN  [kafka-producer-network-thread | next-schemahistory] NetworkClient:1255 - [Producer clientId=next-schemahistory] Bootstrap broker kafka-kafka-bootstrap:9092 (id: -1 rack: null isFenced: false) disconnected
2025-06-03 14:56:28 INFO  [SourceTaskOffsetCommitter-1] BaseSourceTask:503 - Couldn't commit processed log positions with the source database due to a concurrent connector shutdown or restart
2025-06-03 14:56:28 INFO  [task-thread-debezium-connector-ko-employee-0] ConsumerCoordinator:1056 - [Consumer clientId=next-schemahistory, groupId=next-schemahistory] Resetting generation and member id due to: consumer pro-actively leaving the group
2025-06-03 14:56:28 INFO  [task-thread-debezium-connector-ko-employee-0] ConsumerCoordinator:1103 - [Consumer clientId=next-schemahistory, groupId=next-schemahistory] Request joining group due to: consumer pro-actively leaving the group
2025-06-03 14:56:28 INFO  [task-thread-debezium-connector-ko-employee-0] AppInfoParser:89 - App info kafka.consumer for next-schemahistory unregistered
2025-06-03 14:56:28 ERROR [task-thread-debezium-connector-ko-employee-0] WorkerTask:234 - WorkerSourceTask{id=debezium-connector-ko-employee-0} Task threw an uncaught and unrecoverable exception. Task is being killed and will not recover until manually restarted
org.apache.kafka.common.errors.TimeoutException: Timeout expired while fetching topic metadata
2025-06-03 14:56:28 INFO  [task-thread-debezium-connector-ko-employee-0] BaseSourceTask:436 - Stopping down connector
2025-06-03 14:56:28 INFO  [pool-14-thread-1] JdbcConnection:983 - Connection gracefully closed
2025-06-03 14:56:28 INFO  [pool-15-thread-1] JdbcConnection:983 - Connection gracefully closed
2025-06-03 14:56:28 INFO  [task-thread-debezium-connector-ko-employee-0] KafkaProducer:1367 - [Producer clientId=next-schemahistory] Closing the Kafka producer with timeoutMillis = 30000 ms.
```
```

This results in no topics being created, and the connector shuts down.

**Configuration Details**:

All services use the same SASL user (named `admin`) and configuration for auth. Relevant parts:

```
```yaml
security.protocol: SASL_PLAINTEXT
sasl.mechanism: SCRAM-SHA-512
sasl.jaas.config: ${secrets:admin/sasl.jaas.config}
```
```

The `KafkaConnector` config:

```
```yaml
---
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: debezium-connector-ko-employee
  namespace: {{ .Release.Namespace }}
  labels:
    strimzi.io/cluster: {{ .Values.debezium.cluster.name }}
  annotations:
    strimzi.io/use-connector-resources: ""true""
    argocd.argoproj.io/sync-wave: ""6""
spec:
  class: io.debezium.connector.oracle.OracleConnector
  tasksMax: 1
  autoRestart:
    enabled: true
  config:
    oracle.connection.string: ""jdbc:oracle:thin:@//<WORKING_JDBC_STRING>""
    database.hostname: ""<HOST>""
    database.url: ""jdbc:oracle:thin:@//<WORKING_JDBC_STRING>""
    database.port: ""<PORT>""
    database.user: ""<USER>""
    database.password: ""<PASS>"" # ... Think about using a Secret
    database.dbname: ""<DB_NAME>""
    database.pdb.name: ""<PDB_NAME>""
    table.include.list: ""SFNKO.MITARBEITER_OUTBOX,SFNKO.MITARBEITER""
    schema.include.list: ""SFNKO""
    database.include.list: ""<DBs>""
    database.history.kafka.bootstrap.servers: {{ .Values.kafka.authentication.security_protocol }}://{{ .Release.Name }}-kafka-bootstrap:{{ .Values.kafka.ports.plain }}
    database.history.kafka.topic: ""schema-changes""
    schema.history.internal.kafka.bootstrap.servers: {{ .Values.kafka.authentication.security_protocol }}://{{ .Release.Name }}-kafka-bootstrap:{{ .Values.kafka.ports.plain }}
    schema.history.internal.kafka.topic: ""history-changes""
    database.history.store.only.captured.tables.ddl: ""true""
    include.schema.changes: ""false""
    topic.prefix: ""next""
    snapshot.mode: ""when_needed""
    log.mining.strategy: ""online_catalog""
    schema.history.internal.store.only.captured.tables.ddl: ""true""
    schema.history.internal.store.only.captured.databases.ddl: ""true""
    errors.log.include.messages: ""true""
    cdc.flattening.enabled: ""true""
    key.converter: ""io.confluent.connect.avro.AvroConverter""
    key.converter.schema.registry.url: ""http://{{ .Values.schema_registry.name }}:{{ .Values.schema_registry.port }}""
    key.converter.schema.registry.auto-register: ""true""
    key.converter.schema.registry.find-latest: ""true""
    value.converter: ""io.confluent.connect.avro.AvroConverter""
    value.converter.schema.registry.url: ""http://{{ .Values.schema_registry.name }}:{{ .Values.schema_registry.port }}""
    value.converter.schema.registry.auto-register: ""true""
    value.converter.schema.registry.find-latest: ""true""
    schema.name.adjustment.mode: ""avro""
    signal.data.collection: ""<PREFIX>.SFNLNK.DEBEZIUM_SIGNAL""
    transforms: ""changes,unwrap""
    transforms.changes.type: ""io.debezium.transforms.ExtractChangedRecordState""
    transforms.changes.header.changed.name: ""Changed""
    transforms.changes.header.unchanged.name: ""Unchanged""
    transforms.unwrap.type: ""io.debezium.transforms.ExtractNewRecordState""
    transforms.unwrap.drop.tombstones: ""true""
    transforms.unwrap.delete.handling.mode: ""rewrite""
    transforms.unwrap.add.fields: ""op""
    incremental.snapshot.chunk.size: ""262144""
    max.batch.size: ""16384""
    max.queue.size: ""65536""
    snapshot.max.threads: ""1""
    topic.creation.default.replication.factor: ""1""
    topic.creation.default.partitions: ""1""
    topic.creation.default.cleanup.policy: ""compact""
    topic.creation.default.compression.type: ""lz4""

    consumer.sasl.mechanism: {{ upper .Values.kafka.authentication.type }}
    consumer.security.protocol: {{ .Values.kafka.authentication.security_protocol }}
    producer.sasl.mechanism: {{ upper .Values.kafka.authentication.type }}
    producer.security.protocol: {{ .Values.kafka.authentication.security_protocol }}
```
```

**What I’ve Verified**:

- `KafkaConnect` internal topics are created using the same credentials
- `KafkaConnector` starts but fails with metadata timeout errors
- Kafka is accessible, and schema registry is reachable from within the cluster
- The Kafka bootstrap address is reachable from the connector pod (manually verified)

**Question**:
Why does the `KafkaConnector` fail to create its required topics (like `schema-changes`, `history-changes`, or the table change topics), while the `KafkaConnect` instance is able to create its internal topics using the same authentication and configuration?

Any idea what I might be missing? Could it be Avro converter config, network/SASL misconfig, or something with topic authorization?

Any help is greatly appreciated.","kubernetes, apache-kafka, apache-kafka-connect, debezium, strimzi",79652353.0,"Kafka Connect itself can create required topics even if auto creation is disabled. The part you need to compare the connector and the Connect is authentication. You may be missing some specific configs for debezium connectors.

I compared your config with our debezium connectors and give you these:

```
""schema.history.internal.consumer.sasl.mechanism"": ""PLAIN"",
""schema.history.internal.consumer.sasl.jaas.config"": ""${file:/kafka/vty/pass.properties:sasl}"",
""schema.history.internal.consumer.security.protocol"": ""SASL_PLAINTEXT"",
""schema.history.internal.producer.sasl.mechanism"": ""PLAIN"",
""schema.history.internal.producer.sasl.jaas.config"": ""${file:/kafka/vty/pass.properties:sasl}"",
""schema.history.internal.producer.security.protocol"": ""SASL_PLAINTEXT"",
""schema.history.internal.kafka.bootstrap.servers"": ""http://dbDstdkaf01.vakifbank.intra:9072""
```

You need to declare security protocol for history topic too. And since the connector is going to produce and consume data from history topic, it will need configurations as producer and consumer, both.

I give the jaas config from file because of security concerns, you may change that of course. I think this is your exact problem since I faced a similar issue before.",2025-06-04T08:08:19,2025-06-03T15:16:10
79651052,Argo CD not able to pull helm library charts in private OCI repo,"Pretty new to ArgoCD.

I am trying to use ApplicationSet

```
iVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: xxx-pull-preview-test
spec:
  generators:
    - pullRequest:
        github:
          owner: xxx
          repo: xxxxx
          tokenRef:
            key: token
            secretName: github-token
          labels:
            - preview
        requeueAfterSeconds: 60
  template:
    metadata:
      name: 'xxx-preview-{{branch}}-{{number}}'
    spec:
      destination:
        namespace: 'preview-{{branch}}'
        server: 'https://kubernetes.default.svc'
      project: default
      source:
        path: aks/jatin-intra-chart
        repoURL: 'https://github.com/xxxx/xxxx.git'
        targetRevision: '{{head_sha}}'
        helm:
          valueFiles:
          - ../xxx/env/debug.yaml
          - env/debug.yaml
          values: |
            fullnameOverride: ""xxx-pr-{{number}}""

            ingress:
            - host: ""pr-{{number}}.debug-app.xxx.com""
              path: ""/""

            # Environment variables for preview environment
            extraEnv:
              - name: SERVER_NAME
                value: ""pr-{{number}}.debug-app.xxx.com""
              - name: xxx_REDIRECT_URL
                value: ""https://pr-{{number}}.debug-app.xxx.com/login/redirect""

            # Disable cronjobs for preview environments
            appCronJobs: []
      syncPolicy:
        syncOptions:
        - CreateNamespace=true
        automated:
          selfHeal: true
          prune: true
```

aks/jatin-intra-chart/chart.yaml

```
apiVersion: v2
name: xxx-chart
description: A Helm chart for Kubernetes
type: application

version: 0.1.0

appVersion: 1.16.0

dependencies:
  - name: library-chart
    version: ""0.1.9""
    repository: ""oci://xxxx.azurecr.io/helm""
```

aks/jatin-intra-chart/templates

```
{{- include ""library-chart.ingress"" . }}
---
{{- include ""library-chart.service"" . }}
---
{{- include ""library-chart.deployment"" . }}
---
{{- include ""library-chart.cronjob"" . }}
---
{{- include ""library-chart.hpa"" . }}
---
{{- include ""library-chart.pdb"" . }}
---
{{- include ""library-chart.serviceaccount"" . }}
---
{{- include ""library-chart.secretproviderclass"" . }}
```

aks/jatin-intra-chart is application chart directory which depends on library chart stored in private azure cr repo. all the connections to GitHub and azurecr repo are successful.

Right now the problem is when I open the pull request argocd is unable to pull the library chart templates from private acr.

If I locally run helm dependency update and commit the templated to the repo then it works.

But I dont want to commit them as I want to implement this feature for 10+ repos and committing templates to each repo does not make sense. IF I make changes to library chart then same changes I need to do.

Is there a way to work around this?

Argocd version v3.0.5","kubernetes, kubernetes-helm, cicd, argocd, gitops",,,,2025-06-03T10:15:26
79650756,Spring Boot Application in Kubernetes: CrashLoopBackOff Due to SSL Health Check Failures after upgrade to springboot 3.4.4 from 2.7.11,"I'm running a Spring Boot application in Kubernetes that's stuck in a CrashLoopBackOff loop. The application appears to start successfully (logback configures properly), but Kubernetes health checks are failing with SSL handshake errors.

Error Symptoms

```
kubectl describe pod my-app-pod
```

Events section shows:

```
Warning  Unhealthy  23m (x88 over 64m)  kubelet  Startup probe failed: Get ""https://10.10.232.247:4444/health/liveness"": remote error: tls: handshake failure
Warning  BackOff    3m18s (x409 over 113m)  kubelet  Back-off restarting failed container
```

Pod Status:

```
State:          Waiting
Reason:       CrashLoopBackOff
Last State:     Terminated
Reason:       Error
Exit Code:    1
Ready:          False
Restart Count:  25
```

Application Configuration
application.yml:

```
server:
  port: 4443
  ssl:
    enabled: true
    certificate: /var/lib/sia/certs/service.cert.pem
    certificate-private-key: /var/lib/sia/keys/service.key.pem
    trust-certificate: /var/lib/sia/certs/ca.cert.pem

management:
  server:
    port: 4444
    ssl:
      enabled: true
      certificate: /var/lib/sia/certs/service.cert.pem
      certificate-private-key: /var/lib/sia/keys/service.key.pem
      trust-certificate: /var/lib/sia/certs/ca.cert.pem
```

Kubernetes Health Checks:

```
livenessProbe:
  httpGet:
    path: /health/liveness
    port: 4444
    scheme: HTTPS
readinessProbe:
  httpGet:
    path: /health/readiness
    port: 4444
    scheme: HTTPS
startupProbe:
  httpGet:
    path: /health/liveness
    port: 4444
    scheme: HTTPS
```

What I've Tried

Verified certificates exist in the pod
Checked application logs (only shows logback configuration, no Spring Boot startup)
Confirmed the application works locally without SSL
SSL handshake continues to fail

I also disabled ssl still issue is not fixed","spring-boot, kubernetes, ssl",,,,2025-06-03T06:58:46
79650494,How to stream a large file on Kubernetes pod,"I want to create a large file(>200GB) and store it in Min.IO store. I deploy my attempts in a web app on a Kubernetes pod.
One attempt was with a modified Redeable stream and csv-writter library using the putObject  method. Somethig like this:

```
const { faker } = require('@faker-js/faker');
    const { createObjectCsvStringifier: createCsvStringifier } = require('csv-writer');
    const Minio = require('minio');
    const { Readable } = require('stream');
    const minioClient = new Minio.Client({...});

    const csvStringifier = createCsvStringifier({
       header: [
           { id: 'userId', title: 'userId' },
           { id: 'username', title: 'username' },
           .... ]});
    const generateRandomRow = () => ({
    userId: faker.database.mongodbObjectId(),
    username: faker.person.firstName(),
    ...});
    class csvGenerator extends Readable {
    #count = 0;
    #headerPushed = false;
    #numRows;

    constructor(numRows, options) {
        super(options);
        this.#numRows = numRows;
    }

    _read(size) {
        if (!this.#headerPushed) {
            this.push(csvStringifier.getHeaderString());
            this.#headerPushed = true;
        }

        this.push(csvStringifier.stringifyRecords([generateRandomRow()]));
        if (++this.#count === this.#numRows) {
            this.push(null);
        }
    }
}
router.options('/BigFileCreation', cors());
router.post('/BigFileCreation', cors(), async (request, response) => {
    const NUM_ROWS = parseInt(request.body.numberOfRows, 10);
    const NAME_FILE = request.body.nameOfFile;
    const BUCKET = request.body.bucket;

    response.status(202).json({""Request status"": ""Reached""});
    try {
        const requestFile = await minioClient.putObject(BUCKET, NAME_FILE, new csvGenerator(NUM_ROWS, { highWaterMark: 1 }), null, metaData);
        console.log(requestFile);
    } catch (error) {
        console.error(error);
        response.status(500).json(error.toString());
    }
});
```

This handles files less than 1GB with no issue, it takes less than 5 min. to create and upload, but when I request a 2 GB file or more my pod just stop, I guess I just get a OOMKilled status on my pod and thats why I don't get any error msg on the logs.

I also test it with a temporary file on disk with the same csv-writter library and then stream it with fPutObject method on MinioSDK

```
const csvWriter = createCsvWriter({
  path: 'StellarDB.csv',
  header: [
    { id: 'userId', title: 'userId' },
    { id: 'username', title: 'username' },
    { id: 'lastName', title: 'lastName' },
    { id: 'email', title: 'Email' },
    { id: 'column', title: 'column' },
    { id: 'float', title: 'float' },
    { id: 'jobArea', title: 'jobArea' },
    { id: 'jobTitle', title: 'jobTitle' },
    { id: 'phone', title: 'phone' },
    { id: 'alpha', title: 'alpha' }
  ]
});
const writeLargeCsvFile = async (NUM_ROWS) => {
  let batchSize = 500;
  let batch = [];

  for (let i = 0; i < NUM_ROWS; i++) {
    batch.push(generateRandomRow());

    if (batch.length === batchSize || i === NUM_ROWS - 1) {
      await csvWriter.writeRecords(batch);
      batch = [];
    }
  }
};
```

After more research I notice that probably the issue was on the library I was using for csv, so I changed to fast-csv and my final attempt was something like this:

```
const { format } = require('fast-csv');
async function generateAndUploadCSV(name, NUM_ROWS, bucketName) {
  const pass = new PassThrough();

  const uploadPromise = minioClient.putObject(bucketName, name, pass)
    .catch(err => {
      console.error('Error subiendo objeto:', err);
      throw err;
    });

  const csvStream = format({ headers: [
    'userId', 'username', 'lastName', 'email', 'column', 'float', 'jobArea', 'jobTitle', 'phone', 'alpha'
  ]});
  csvStream.pipe(pass);

  let i = 0;

  function write() {
    let ok = true;
    while (i < NUM_ROWS && ok) {
      i++;
      const record = {
        userId: i,
        username: faker.person.firstName(),
        lastName: faker.person.lastName(),
        email: faker.internet.email(),
        column: faker.database.column(),
        float: faker.number.float(3),
        jobArea: faker.person.jobArea(),
        jobTitle: faker.person.jobTitle(),
        phone: faker.phone.imei(),
        alpha: faker.string.alpha({ length: { min: 5, max: 10 } }),
      };
      ok = csvStream.write(record);
    if (i < NUM_ROWS) {
      csvStream.once('drain', () => setImmediate(write));
    } else {
      csvStream.end();
    }
  }
  csvStream.on('error', err => {
    pass.destroy(err);
  });
  write();
  const objInfo = await uploadPromise;
}
```

I also assign more resources on my pod (8GB memory and 4 cores). But all of them behave the same, just one file of 1GB and no more.

Also I modified my Dockerfile on the entrypoint with `CMD [""node"", ""--max-old-space-size=6144"",""index.js""] `

I research more about it and found there is this option to upload the file on pieces and the merge it back together when I'm going to use it. This could be useful for csv files but what if I want to use JSON files also.
Creating and storing the file is just my first step in testing tools that should handle large files without overflow issues. All of them run on Kubernetes pods.
Just adding more info my service pod is handled by Knative with a yml file similar to this:

```
apiVersion: serving.knative.dev/v1
kind: Service
metadata:
  name: transformTesting
spec:
  template:
    spec:
      containers:
        - image: .../...:transform-testing-SNAPSHOT
          env:
            - name: FORCE_NEW_REVISION
              value: ""true""
```

Wish someone could point me towards a solution or a concept that I'm ignoring.","javascript, kubernetes, stream, large-files, minio",79654000.0,"You can follow the same pattern that is used in large file uploads in AWS S3.
Link for AWS S3 - [https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#apisupportformpu](https://docs.aws.amazon.com/AmazonS3/latest/userguide/mpuoverview.html#apisupportformpu)

1. Initiate
•   POST /upload/initiate
•   Response: { ""uploadId"": ""xyz"", ""key"": ""uploads/filename"" }
2. Upload Parts
•   GET /upload/url?uploadId=xyz&key=...&partNumber=1
•   PUT to presigned URL with part data
•   Save returned ETag from S3 response
3. Complete
•   POST /upload/complete with {uploadId, key, parts: [{PartNumber, ETag}]}",2025-06-05T07:43:22,2025-06-03T00:35:47
79648776,Spark memoryOverheadFactor is set to 0.1 instead of 0.4 for non-JVM workloads,"I submit a Pyspark application on Kubernetes (EKS on EC2) and when I check the container memory request and limit, I can see that the memoryOverheadFactor is set to 0.1 by default instead of 0.4 (memory request and limit = memory + memory*memoryOverheadFactor). I'm using Spark 3.3.4 and submitting my application in Client mode where i submit the application from the driver container like this :

```
/opt/spark/bin/spark-submit   --conf <all_the_conf>  test.py
```

Given the fact that the memoryOverheadFactor is set to 0.1 it means that the application is submitted as a JVM app where it shouldn't be the case. Not sure how to debug this more. Any help would be appreciated","apache-spark, kubernetes, pyspark, amazon-eks",79648838.0,"The [default](https://spark.apache.org/docs/latest/running-on-kubernetes.html) `memoryOverheadFactor` for JVM-based applications is 0.1, while for non-JVM applications (like PySpark and SparkR), it's typically 0.4.

You can set this config directly to your application using below configs

```
  --conf spark.kubernetes.memoryOverheadFactor=0.4 \
  --conf spark.kubernetes.driver.memoryOverheadFactor=0.4
```",2025-06-02T13:39:32,2025-06-02T13:01:39
79646589,How to use Airflow params to template operator fields that receive complex objects,"I am building an Airflow DAG with a `KubernetesPodOperator` that I would like to parametrise heavily.

I would like to to parametrise `cmds`, `image`, and `volume_mounts` mount path. The first two work as I intend from the get go, however, I find parametrising mount path very difficult to deal with. I have double checked that all of the fields are included in `template_fields` in this operator.

Here is the code:

```
@dag(
    dag_id=PIPELINE_NAME,
    schedule=None,
    dag_display_name=""Run arbitrary command with persistent data space"",
    max_active_runs=1,
)
def run_arbitary_command_pipeline(
    command: str = """",
    image: str = ""python:3.13-slim"",
    data_storage_size: str = ""10Gi"",
    shared_data_mount_path=""/mnt/data/"",
):
    pipeline_name_normalized = PIPELINE_NAME.replace(""_"", ""-"").lower()
    pvc_name = f""{pipeline_name_normalized}-data-space-pvc""

    run_command = KubernetesPodOperator(
        task_id=""run_arbitrary_command"",
        cmds=[""sh"", ""-c"", command],
        image=image,
        task_display_name=""Run arbitrary command"",
        volumes=[k8s.V1Volume(name=pvc_name, persistent_volume_claim=k8s.V1PersistentVolumeClaimVolumeSource(claim_name=pvc_name))],
        volume_mounts=[k8s.V1VolumeMount(name=pvc_name, mount_path=shared_data_mount_path)],
    )
```

Airflow automagically deals with `command` and `image` – I can see both of them correctly expanded in the ""Rendered Templates"" section in UI. However, `volume_mounts` is expanded to:

```
""[{'mount_path': <airflow.sdk.definitions.param.DagParam object at 0xfad05e250590>,\n 'mount_propagation': None,\n 'name': 'run-arbitrary-command-pipeline-data-space-pvc',\n 'read_only': None,\n 'recursive_read_only': None,\n 'sub_path': None,\n 'sub_path_expr': None}]""
```

And the pipeline fails because:

```
RepresenterError: ('cannot represent an object', <airflow.sdk.definitions.param.DagParam object at 0xfad05cb8ede0>)
```

Which is not surprising given how the rendered template looks like.

If I try to use templating in a more explicit way with:

```
volume_mounts=[k8s.V1VolumeMount(name=pvc_name, mount_path=""{{ params.shared_data_mount_path }}"")],
```

then the template is not expanded at all:

```
""[{'mount_path': '{{ params.shared_data_mount_path }}',\n 'mount_propagation': None,\n 'name': 'run-arbitrary-command-pipeline-data-space-pvc',\n 'read_only': None,\n 'recursive_read_only': None,\n 'sub_path': None,\n 'sub_path_expr': None}]""
```

and the pipeline doesn't fail, but the parameter effectively doesn't work as the template string is used verbatim as mount path.

Is there any way to fix this and mount the PVC under the path provided from params? I feel that this problem results both from Airflow limitations and my poor understanding of its intricacies. Would appreciate any good sources to understand how and when the templating works beyond what the docs explain.","python, kubernetes, airflow, airflow-taskflow",,,,2025-05-31T11:48:48
79645559,Kubernetes Java Client (OpenAPI) fails to deserialize JSON response in Spark context,"Spark Application Running in Kubernetes Cluster.
The Spark context is initialized by the Google Spark Operator.

Spark dependencies are:

```
""org.apache.spark""   %% ""spark-hadoop-cloud""        % ""3.3.0""
""org.apache.spark""   %% ""spark-sql""                 % ""3.3.0""
""org.apache.spark""   %% ""spark-sql-kafka-0-10""      % ""3.3.0""
```

Client Api:

```
""io.kubernetes"" % ""client-java"" % ""19.0.1""
""io.kubernetes"" % ""client-java-api"" % ""19.0.1""
```

During execution, the application calls:

```
  io.kubernetes.client.openapi.ApiClient.handleResponse(Response response, Type returnType)
```

which internally invokes:

```
   io.kubernetes.client.openapi.deserialize(String body, Type returnType)
```

and fails with the error:

```
  Exception in thread ""main"" com.google.gson.JsonSyntaxException: java.lang.IllegalStateException: Expected BEGIN_OBJECT but was STRING at line 1 column 11921
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:176)
at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:40)
at com.google.gson.internal.bind.MapTypeAdapterFactory$Adapter.read(MapTypeAdapterFactory.java:187)
at com.google.gson.internal.bind.MapTypeAdapterFactory$Adapter.read(MapTypeAdapterFactory.java:145)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172)
at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.read(TypeAdapterRuntimeTypeWrapper.java:40)
at com.google.gson.internal.bind.CollectionTypeAdapterFactory$Adapter.read(CollectionTypeAdapterFactory.java:81)
at com.google.gson.internal.bind.CollectionTypeAdapterFactory$Adapter.read(CollectionTypeAdapterFactory.java:60)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.read(ReflectiveTypeAdapterFactory.java:93)
at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.read(ReflectiveTypeAdapterFactory.java:172)
at com.google.gson.Gson.fromJson(Gson.java:803)
at com.google.gson.Gson.fromJson(Gson.java:768)
at com.google.gson.Gson.fromJson(Gson.java:717)
at io.kubernetes.client.openapi.JSON.deserialize(JSON.java:168)
at io.kubernetes.client.openapi.ApiClient.deserialize(ApiClient.java:775)
at io.kubernetes.client.openapi.ApiClient.handleResponse(ApiClient.java:978)
```

The error only occurs in the Spark context. The same Kubernetes client logic works fine in standalone (non-Spark) applications!

According to [https://github.com/kubernetes-client/java/issues/591](https://github.com/kubernetes-client/java/issues/591), a similar issue was resolved by upgrading GSON from 2.2 to 2.8.

Dependency analysis shows that Kubernetes libraries depend on GSON 2.10.1 (which should be sufficient).

First of all I looked for possible conflict in gson versions in Spark and Kubernetes, so Spark replaces working gson version with older version.

I excluded GSON from Spark dependencies to avoid conflicts, but the error persisted.

Moreover, dependency tree confirmed Spark also uses GSON 2.10.1 (no version mismatch).

Then I Isolated the Kubernetes client logic in the Spark project - I took outside part of Kubernetes library into my project, which send request to other pod, receive response and deserialize with gson. Inside my spark project code this logic working as expected. It successfully parsed json from Response body!

I'm really at a dead end, and I don't know where to ""dig"" anymore.","apache-spark, kubernetes, dependencies, gson",,,,2025-05-30T13:37:12
79645483,Can&#39;t see my virtuals ports in my service&#39;s endpoint,"I've created an agent in order to list virtuals and physics ports.create virtuals ports etc.

But when i call the agent in order to display virtuals ports, it shows nothing.

I'm using socat to create virtual ports on my Alpine.

Here's my Kubernetes deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: socat-api
  namespace: camera
  labels:
    app: socat-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: socat-api
  template:
    metadata:
      labels:
        app: socat-api
    spec:
      nodeSelector:
        kubernetes.io/hostname: cs12
      containers:
        - name: socat-agent
          image: xxx.xx.com:xxx/cam/rs232:0.0.4
          ports:
            - containerPort: 3001
              hostPort: 3001
          securityContext:
            privileged: true
            runAsUser: 0
            runAsGroup: 0
          volumeMounts:
            - name: dev-volume
              mountPath: /host-dev
      volumes:
        - name: dev-volume
          hostPath:
            path: /dev
            type: Directory
```

And here is the Node.js API endpoint I'm using to list the virtual ports:

```
app.get('/serial-ports', (req, res) => {
  console.log(""API pour les ports virtuels appelée..."");

  execFile('ls /dev/ttyV* 2>/dev/null', async (error, stdout, stderr) => {
    if (error || stderr) {
      return res.status(500).json({ error: 'Erreur lors de la détection des ports virtuels.' });
    }

    const ports = stdout.split('\n').filter(line => line.trim() !== '');

    const portInfos = await Promise.all(
      ports.map(portPath => new Promise(resolve => {
        execFile(`stty -a -F ${portPath}`, (err, sttyOutput) => {
          if (err) {
            return resolve({ port: portPath, error: 'Erreur récupération infos' });
          }

          const baud = sttyOutput.match(/speed (\d+) baud/)?.[1] || 'unknown';
          const dataBits = sttyOutput.match(/\bcs(5|6|7|8)\b/)?.[0] || 'unknown';
          const parity = sttyOutput.includes('parenb')
            ? (sttyOutput.includes('parodd') ? 'odd' : 'even')
            : 'none';
          const stopBits = sttyOutput.includes('cstopb') ? '2' : '1';
          const flowControl =
            sttyOutput.includes('crtscts') ? 'hardware' :
            (sttyOutput.includes('ixon') || sttyOutput.includes('ixoff')) ? 'software' :
            'none';
          const mode = sttyOutput.includes('icanon') ? 'canonical' : 'non-canonical';
          const echo = sttyOutput.includes('echo');

          resolve({
            port: portPath.replace('/dev/', ''),
            path: portPath,
            baudRate: baud,
            dataBits,
            parity,
            stopBits,
            flowControl,
            mode,
            echo
          });
        });
      }))
    );
    res.json(portInfos);
  });
});
```

When I call `/serial-ports`, I get an empty list even though `/dev/ttyV0` and `/dev/ttyV1` exist on the host. Why is that?","kubernetes, socat",79645621.0,"the reason is that even if the /dev/ttyV0 and /dev/ttyV1 exist on your host you are not mounting at that path in your container:

This line is the path where you are mounting the host path thus you are mounting it to `/host-dev` in the container section of your deployment and then looking at `/dev` .

```
volumeMounts:
- name: dev-volume
  mountPath: /host-dev
```

change those lines to:

```
volumeMounts:
- name: dev-volume
  mountPath: /dev
```

Also note that you are mounting a host-path thus you need to make sure that path exist in all the nodes of your cluster...",2025-05-30T14:23:30,2025-05-30T12:45:37
79644133,Eureka services trying to connect to localhost instead of passed defaultZone,"im trying to configure my kubernetes cluster so that my microservices can register via eureka. however, no matter what defaultZone url I use, it always tries to connect via localhost. ive ensured that the env variables are binding correctly, and ive even tried hardcoding it by putting the value itself, but it still throws the same error. im writing everything in a docker-compose, which i then convert to kubernetes manifests using kompose. can anyone give me any feedback on what i may be missing or i may have configured wrong? thanks!

**The docker-compose which I am converting:**

```
  api-gateway:
    image: ghcr.io/pibbletv/pibbletv-gateway:latest
    container_name: api-gateway
    ports:
      - ""8078:8078""
    environment:

      EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://${EUREKA_USERNAME}:${EUREKA_PASSWORD}@service-registry:8761/eureka/
      KEYCLOAK_AUTH_SERVER_URL: ${KEYCLOAK_URL}
      KEYCLOAK_REALM: ${KEYCLOAK_REALM}
      KEYCLOAK_RESOURCE: ${KEYCLOAK_CLIENT_ID}
      KEYCLOAK_CREDENTIALS_SECRET: ${KEYCLOAK_CLIENT_SECRET}
      SPRING_SECURITY_OAUTH2_RESOURCESERVER_JWT_ISSUER_URI: ${KEYCLOAK_GATEWAY_URL}
      KEYCLOAK_SSL_REQUIRED: external
      SERVER_PORT: ""8078""
      KEYCLOAK_PUBLIC_CLIENT: ""false""
      SPRING_CLOUD_GATEWAY_DISCOVERY_LOCATOR_ENABLED: ""true""
      SPRING_CLOUD_GATEWAY_DISCOVERY_LOCATOR_LOWERCASE_SERVICE_ID: ""true""
      LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_CLOUD_COMMONS_UTIL_INETUTILS: TRACE
      LOGGING_LEVEL_COM_NETFLIX_DISCOVERY: DEBUG
      LOGGING_LEVEL_COM_NETFLIX_EUREKA: DEBUG
      SERVER_SSL_ENABLED: ""true""
      SERVER_SSL_CERTIFICATE: file:/opt/keycloak/certs/pibbletv.crt
      SERVER_SSL_CERTIFICATE_PRIVATE_KEY: file:/opt/keycloak/certs/pibbletv.key
    depends_on:
      service-registry:
        condition: service_healthy
    volumes:
      - /home/tanzerdx/certs:/opt/keycloak/certs
    healthcheck:
      test: [ ""CMD"", ""curl"", ""-f"", ""-k"", ""https://localhost:8078/actuator/health"" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - backend

service-registry:
    image: ghcr.io/pibbletv/pibbletv-service-registry:latest
    container_name: service-registry
    ports:
      - ""8761:8761""
    environment:
      SERVER_PORT: ""8761""
      EUREKA_CLIENT_SERVICEURL_DEFAULTZONE: http://${EUREKA_USERNAME}:${EUREKA_PASSWORD}@service-registry:8761/eureka/
      EUREKA_INSTANCE_HOSTNAME: service-registry
      EUREKA_CLIENT_REGISTER_WITH_EUREKA: ""false""
      EUREKA_CLIENT_FETCH_REGISTRY: ""false""
      EUREKA_SERVER_ENABLE_SELF_PRESERVATION: ""true""
      LOGGING_LEVEL_ORG_SPRINGFRAMEWORK_CLOUD_COMMONS_UTIL_INETUTILS: TRACE
      LOGGING_LEVEL_COM_NETFLIX_DISCOVERY: DEBUG
      LOGGING_LEVEL_COM_NETFLIX_EUREKA: DEBUG
      SPRING_DATASOURCE_USERNAME: ${DB_USER}
      SPRING_DATASOURCE_PASSWORD: ${DB_PASSWORD}

    networks:
      - backend
    healthcheck:
      test: [ ""CMD"", ""curl"", ""-f"", ""http://localhost:8761/actuator/health"" ]
      interval: 30s
      timeout: 10s
      retries: 5
```

**The error that I am getting:**

```
2025-05-28T21:49:00.548Z  INFO 1 --- [foReplicator-%d] com.netflix.discovery.DiscoveryClient    : DiscoveryClient_UNKNOWN/api-gateway-76fd76b654-g77bk:8078: registering service...
2025-05-28T21:49:00.549Z DEBUG 1 --- [foReplicator-%d] c.n.d.s.t.d.RetryableEurekaHttpClient    : Clearing quarantined list of size 1
2025-05-28T21:49:00.564Z  INFO 1 --- [foReplicator-%d] c.n.d.s.t.d.RedirectingEurekaHttpClient  : Request execution error. endpoint=DefaultEndpoint{ serviceUrl='http://localhost:8761/eureka/}, exception=I/O error on POST request for ""http://localhost:8761/eureka/apps/UNKNOWN"":
Connect to http://localhost:8761 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused
stacktrace=org.springframework.web.client.ResourceAccessException: I/O error on POST request for ""http://localhost:8761/eureka/apps/UNKNOWN"": Connect to http://localhost:8761 [localhost/127.0.0.1, localhost/0:0:0:0:0:0:0:1] failed: Connection refused
```","docker, kubernetes, netflix-eureka, spring-cloud-eureka",,,,2025-05-29T14:53:33
79643292,We are getting below issue while creating kind cluster using command : kind create cluster,"```
]# kind create cluster
Creating cluster ""kind"" ...
 ✓ Ensuring node image (kindest/node:v1.33.1) 🖼
⠈⠑ Preparing nodes 📦
 ✓ Preparing nodes 📦
 ✓ Writing configuration 📜
 ✗ Starting control-plane 🕹️
Deleted nodes: [""kind-control-plane""]
ERROR: failed to create cluster: failed to init node with kubeadm: command ""docker exec --privileged kind-control-plane kubeadm init --config=/kind/kubeadm.conf --skip-token-print --v=6"" failed with error: exit status 1
Command Output: I0529 04:20:36.316072     240 initconfiguration.go:261] loading configuration from ""/kind/kubeadm.conf""
W0529 04:20:36.316609     240 common.go:101] your configuration file uses a deprecated API spec: ""kubeadm.k8s.io/v1beta3"" (kind: ""ClusterConfiguration""). Please use 'kubeadm config migrate --old-config old-config-file --new-config new-config-file', which will write the new, similar spec using a newer API version.
```

We have tried to setup k8 cluster using kind tool and integrate with spark

Could you please anyone assist on this issue","docker, kubernetes, kind",79645253.0,"The error indicates that the kubeadm configuration file used internally by `Kind` is referencing a deprecated API version. The `kubeadm.k8s.io/v1beta3` is [no longer supported](https://pkg.go.dev/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta3#:%7E:text=DEPRECATED%3A%20v1beta3%20is%20deprecated%20in%20favor%20of%20v1beta4%20and%20will%20be%20removed%20in%20a%20future%20release%2C%201.34%20or%20later.%20Please%20migrate.) in the current version.

- Install the [latest Kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation) CLI version.
- Use [compatible node image](https://github.com/kubernetes-sigs/kind/releases) or the stable image instead.
- If the above option doesn’t work. If you're using a custom Kind configuration file, make sure it doesn't reference `kubeadm.k8s.io/v1beta3`.

You can migrate it with:

```
kubeadm config migrate --old-config old.yaml --new-config new.yaml
```

For additional information see this [troubleshooting Kind](https://kind.sigs.k8s.io/docs/user/known-issues#troubleshooting-kind) documentation.",2025-05-30T10:01:03,2025-05-29T04:34:39
79642916,I can&#39;t figure out the command/args settings for acronjob in kubernetes,"I am trying to create a cronJob in kubernetes which generates a report about an Auth0 tenant.  The cronJob can run for any of our Auth0 tenants, so it needs to take arguments to specify which tenant and which database in that tenant to run for.

My deployment file contains the following:

```
              command: [""/bin/sh"", ""-c"", "". /home/cronrun/start.sh""]
              args: [""tenant1"", ""legacy-db""]
```

My Dockerfile only copies the scripts into the image, sets up a non-root user, and sets the WORKDIR properly.  It does not contain CMD or ENTRYPOINT directives.

The script start.sh contains a debug line at the top of the script:

```
               echo ""DEBUG: in start.sh  1:  $1   2:  $2""
```

When the cron runs, I can see this in the logs:

```
               DEBUG: in start.sh  1: legacy-db  2:
```

So obviously I am mishandling the arguments position variables.

What am I doing wrong here?",kubernetes,79643190.0,"You don't need the `sh -c` wrapper, and it's causing the positional-parameter issue you're having.

```
command: [""/home/cronrun/start.sh""]  # no `sh -c` or `.`
args: [""tenant1"", ""legacy-db""]
```

Your shell script needs to be executable and begin with a correct ""shebang"" line, usually `#!/bin/sh`; these should be correct in your source tree, and you shouldn't need to do any special Docker-level setup for these.

If you run [**sh**(1)](https://pubs.opengroup.org/onlinepubs/9799919799/utilities/sh.html) with a `-c` option, its syntax is

```
sh -c command_string command_name argument ...
```

where only the `command_string` parameter is required.  In your original form, combining the `command:` and `args:` yielded

```
sh -c "". /home/cronrun/start.sh"" tenant1      legacy-db
#     command_string............ command_name argument
```

and matching these up you see `tenant1` assigned to the `command_name` parameter.  In your debugging script you'd see this as the positional parameter `$0`, which is typically the script name in normal use.

If you really did want to use `sh -c` here then you could supply an artifical parameter to be the script name

```
command:
  - /bin/sh
  - -c
  - "". /home/cronrun/start.sh""  # command_string
  - start.sh                    # command_name, $0
args:
  - tenant1                     # $1
  - legacy-db                   # $2
```",2025-05-29T01:51:12,2025-05-28T19:51:51
79642205,How can I promote database inserts and schema changes through dev → test → prod in a Kubernetes CI/CD pipeline without skipping environments?,"I have a simple Git-based CI/CD setup deployed to Kubernetes:

- Dev environment: Automatically updated when I push to the dev branch.
- Test environment: Automatically updated when I merge from dev into main.
- Prod environment: Automatically updated when I create a release on main.

On the application side this flow works fine, but I also need to migrate database changes like data inserts through the environments. I was thinking of having a UI that allows me to ""move"" objects from one environment to another.

- Test should only ever receive the migrations and inserts that have already run in Dev.
- Prod should only ever receive the migrations and inserts that have already run in Test.
- I want to avoid any direct “dev → prod” promotions, accidental skips, or out-of-order applies.

What patterns, tools, or pipeline configurations can enforce this one-way promotion data migrations?","database, git, kubernetes, continuous-integration, migration",,,,2025-05-28T12:27:52
79640662,How can I use ASP.NET Core development certificate inside Docker Desktop Kubernetes?,"When Visual Studio creates the docker compose file, it adds the necessary bindings so that the container has access to the user secrets and the dev certificate:

```
volumes:
  - ${APPDATA}/Microsoft/UserSecrets:/home/app/.microsoft/usersecrets:ro
  - ${APPDATA}/Microsoft/UserSecrets:/root/.microsoft/usersecrets:ro
  - ${APPDATA}/ASP.NET/Https:/home/app/.aspnet/https:ro
  - ${APPDATA}/ASP.NET/Https:/root/.aspnet/https:ro
```

How can I achieve the same using Docker Desktop Kubernetes?","docker, asp.net-core, kubernetes, docker-desktop",79651910.0,"In case anyone needs this, there are several ways you can achieve this. The one I went with was the following:

You need a `hostPath` volume, in Docker Desktop Kubernetes you can access the host machine with this prefix: `/run/desktop/mnt/host/`

So the `${APPDATA}/ASP.NET/Https` folder becomes `/run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https`.

And then use environment variables to configure Kestrel's certificate (in VS this is done by the `launchSettings.json` file).

This is the full deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      pod: gateway-pod
  template:
    metadata:
      labels:
        pod: gateway-pod
    spec:
      containers:
        - name: gateway-container
          image: localhost:5500/gateway
          env:
            - name: Kestrel__Endpoints__Https__Url
              value: ""https://+:8081""
            - name: Kestrel__Endpoints__Https__Certificate__Path
              value: ""/home/app/.aspnet/https/Gateway.pfx""
            - name: Kestrel__Endpoints__Https__Certificate__Password
              value: ""...""
            - name: ASPNETCORE_ENVIRONMENT
              value: ""Development""
            - name: ASPNETCORE_HTTP_PORTS
              value: ""8080""
            - name: ASPNETCORE_HTTPS_PORTS
              value: ""8081""
          volumeMounts:
            - name: https-certs
              mountPath: /home/app/.aspnet/https
              readOnly: true
      volumes:
        - name: https-certs
          hostPath:
            path: /run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https
            type: Directory
```

This is quick and simple to setup a local dev Kubernetes, but needless to say, for production YAML files used with GitOps, you should use Secrets and ConfigMaps.",2025-06-03T21:27:25,2025-05-27T14:17:39
79639441,Connecting to cnpg postgresql database through ssh tunnel,"I have a postgres database that I'm hosting in kubernetes on my private server. The project I'm using is [CNPG](https://cloudnative-pg.io/).

The deployment is super simple:

```
apiVersion: v1
kind: Namespace
metadata:
  name: database
  labels:
     name: database
---
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: pgsql-cluster
  namespace: database
spec:
  instances: 3
  managed:
    services:
      disabledDefaultServices: [""r""]
  storage:
    size: 1Gi
```

So this creates a deployment with read and write replicas as well as services for these corresponding services.

Basically, what I am doing, is that I am port forwarding the service for the read replica and establishing a tunnel with ssh from my workstation, i.e.:

```
ssh -L -v 5432:localhost:5432 username@hostname
```

The error that the ssh connection is throwing, is as follows:

```
debug1: Connection to port 5432 forwarding to localhost port 5432 requested.
debug1: channel 3: new [direct-tcpip]
channel 3: open failed: connect failed: Connection refused
debug1: channel 3: free: direct-tcpip: listening port 5432 for localhost port 5432,
connect from 127.0.0.1 port 61982 to 127.0.0.1 port 5432, nchannels 4
```

And the port-forward says ""Lost connection to pod. Deleting""

Locally on the server, I can use the shell in the pod to query data, I can also use telnet to connect to the service, i.e.:

```
telnet localhost 5432
Trying ::1...
Connected to localhost.
```","postgresql, kubernetes, ssh",79780926.0,"in CNPG  check the entries in pg-config.yml, you have to allow IP address or best is create new pod and test everything from there",2025-10-02T12:47:25,2025-05-26T18:44:36
79639343,spring-cloud-deployer-kubernetes:2.9.5 and kubernetes-model-core:7.3.1,"I want to use above versions together, but failing with error:

```
java.lang.NoSuchMethodError: 'io.fabric8.kubernetes.api.model.HTTPGetActionFluent io.fabric8.kubernetes.api.model.HTTPGetActionBuilder.withNewPort(java.lang.Integer)'
```

What I see as issue is:

```
public abstract class HttpProbeCreator extends ProbeCreator {

    // skipped code

    protected abstract Integer getPort();

    protected Probe create() {
        HTTPGetActionBuilder httpGetActionBuilder = new HTTPGetActionBuilder()
                .withPath(getProbePath())
                .withNewPort(getPort())
                .withScheme(getScheme());
    //rest of the code is skipped
```

Where method `getPort()` returns `Integer`, but method `withNewPort(Object)` (inherited from class `HTTPGetActionFluent`) expects `Object`.

Please, could someone help here? Thanks","java, kubernetes, spring-cloud, fabric8-kubernetes-client",79643590.0,Because the spring-cloud-deployer-kubernetes:2.9.5 depends on kubernetes-model-core:5.12.4 the only solution is to fix deployer-kubernetes - there's more incompatibilities.,2025-05-29T08:45:21,2025-05-26T17:11:08
79638651,Is it possible to make pods run sequentially,"Let’s say I have a Pod A (backend) in namespace 1, and a Pod B (backend) in namespace 2. I want Pod B to run only when Pod A is running and healthy. Otherwise, Pod B should remain in the Init:0 state, for example.

I tried this config, but I didn't get a good result.

```
spec:
  initContainers:
  - name: wait-for-a
    image: curlimages/curl
    command:
    - sh
    - -c
    - |
      until nslookup <DNS> &&
           curl -fsS http://<DNS>:8081/health;
      do
        echo ""Waiting for a pod to be ready..."";
        sleep 5;
      done
```",kubernetes,,,,2025-05-26T09:22:53
79633671,Kafka client attempt to connect only one node from advertised listeners,"I am running kafka in kubernetes using this configuration:

```
  KAFKA_ADVERTISED_LISTENERS: ""INTERNAL://localhost:9090,INSIDE_PLAINTEXT://proxy:19097""
  KAFKA_LISTENERS: ""INTERNAL://0.0.0.0:9090,INTERNAL_FAILOVER://0.0.0.0:9092,INSIDE_PLAINTEXT://0.0.0.0:9094""
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ""INTERNAL:PLAINTEXT,INTERNAL_FAILOVER:PLAINTEXT,INSIDE_PLAINTEXT:PLAINTEXT""
  KAFKA_INTER_BROKER_LISTENER_NAME: ""INTERNAL""
  KAFKA_BOOTSTRAP_SERVERS: ""kafka-mock:9090, kafka-mock:9092""
```

I am attempting to connect to this kafka from my client-app service, running in the same namespace as my kafka.

However my app connects to boostrap server, which should return list of nodes defined in `KAFKA_ADVERTISED_LISTENERS`, connecting to `localhost` node should fail since its not running in same pod, so it should proceed and attempt to conncet to `proxy:19097`, however this does not happen. It attempts to connect to `localhost` and thats it.

IS my configuration wrong for kafka? Did i missplace listener names ? Why isnt it connecting?

If i add another node in `ADVERTISED_LISTENERS` for example `'INTERNAL_PLAINTEXT:kafka-mock:9095'` and also add node that listens on port 9095 to kafka_listeners ( and also mapped 9095:9095), it works. The localhost connection fails but it sends data trough this node, but it always ignores proxy node.

Thanks for help","docker, kubernetes, apache-kafka",79634108.0,"your problem is here
` ""INTERNAL://localhost:9090`

Why kafka-mock:9095 Works ? Kafka broker returns a resolvable DNS name (kafka-mock) to your client.

Use resolvable service names for internal communication.

a config such this will work :

```
KAFKA_ADVERTISED_LISTENERS: ""INTERNAL://kafka-mock:9090,INSIDE_PLAINTEXT://kafka-service:19097""
KAFKA_LISTENERS: ""INTERNAL://0.0.0.0:9090,INSIDE_PLAINTEXT://0.0.0.0:9094""
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ""INTERNAL:PLAINTEXT,INSIDE_PLAINTEXT:PLAINTEXT""
KAFKA_INTER_BROKER_LISTENER_NAME: ""INTERNAL""
```",2025-05-22T15:48:47,2025-05-22T11:44:17
79632821,spring-cloud-starter-kubernetes-client errors out compatible versions of KubernetesClientPodUtils,"We have a application running inside the kubernetes cluster, which uses the `spring-cloud-starter-kubernetes-client:3.2.1` version loaded by the spring boot 3.4.5 bom.

The applicaion gets the pod for the namespace, the code snippet looks like below

```
import io.kubernetes.client.openapi.apis.CoreV1Api;
import io.kubernetes.client.openapi.models.V1Pod;
//...
private final CoreV1Api api;
//...
Optional<V1Pod> solverPod = api.listNamespacedPod(""my-namespace"").execute()
                .getItems()
                .stream()
                .findFirst();
//...
```

In kubernetes when the application deployed the application throws below error don't see any version  conflicts due to dependency jars.

```
Description:

An attempt was made to call a method that does not exist. The attempt was made from the following location:
org.springframework.cloud.kubernetes.client.KubernetesClientPodUtils.internalGetPod(KubernetesClientPodUtils.java:107)

The following method did not exist:
    'io.kubernetes.client.openapi.models.V1Pod io.kubernetes.client.openapi.apis.CoreV1Api.readNamespacedPod(java.lang.String, java.lang.String, java.lang.String)'

The calling method's class, org.springframework.cloud.kubernetes.client.KubernetesClientPodUtils, was loaded from the following location:

    jar:nested:/app.jar/!BOOT-INF/lib/spring-cloud-kubernetes-client-autoconfig-3.2.1.jar!/org/springframework/cloud/kubernetes/client/KubernetesClientPodUtils.class

The called method's class, io.kubernetes.client.openapi.apis.CoreV1Api, is available from the following locations:

    jar:nested:<redected>/app.jar/!BOOT-INF/lib/client-java-api-23.0.0.jar!/io/kubernetes/client/openapi/apis/CoreV1Api.class

The called method's class hierarchy was loaded from the following locations:

    io.kubernetes.client.openapi.apis.CoreV1Api: jar:nested:/<redacted/local>/app-.jar/!BOOT-INF/lib/client-java-api-23.0.0.jar!/

Action:
Correct the classpath of your application so that it contains compatible versions of the classes org.springframework.cloud.kubernetes.client.KubernetesClientPodUtils and io.kubernetes.client.openapi.apis.CoreV1Api
```","spring-boot, kubernetes",79650504.0,"The issue is with the incorrect dependency, based on the Maven dependency of `spring-cloud-kubernetes-client-autoconfig`:`3.2.1` seems the `client-java` and `client-java-extended` version supported is `19.0.2`. In my case was using higher version of those dependency. which caused the issue during spring boot auto-configuration.

[https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-kubernetes-client-autoconfig/3.2.1](https://mvnrepository.com/artifact/org.springframework.cloud/spring-cloud-kubernetes-client-autoconfig/3.2.1)

The `@EnableDiscoveryClient` annotation is used from the `spring-cloud-kubernetes-client`

```
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    <version>3.2.1</version>
</dependency>
```",2025-06-03T00:59:55,2025-05-21T22:07:35
79632098,How to expose multiple TCP services and HTTPS via a single LB in EKS (with IP restriction support)?,"I'm running several services (like Redis, MongoDB, MySQL, Elasticsearch) inside an Amazon EKS cluster. Currently, each service is exposed via a separate AWS Network Load Balancer (NLB) — plus one NLB for NGINX Ingress that handles all my HTTPS traffic.

This is causing a few issues:

- Cost is increasing due to multiple NLBs.
- IPs keep changing (unless I manually assign Elastic IPs).
- IP of LB gets change.

What I'm trying to achieve:
Using a single LB to expose:

- Ports 80/443 → go to NGINX Ingress (for HTTP/S)
- Ports like 6379, 27017, 3306 → go to backend TCP services (Redis, Mongo, MySQL, etc.)
- Ideally support static IPs using Elastic IPs (so the client can whitelist them)
- Also want to implement IP restrictions per service (e.g., Redis accessible only from specific source IPs) and domain maps.  (e.g., myql.domain.com,mongo.domain.com, etc.)

**Questions:
Can I use a single NLB in EKS to expose both HTTPS and multiple TCP ports?**

Which is better for this use-case:

**Option 1:** Use a single Service of type LoadBalancer with multiple ports and let AWS LB controller handle the routing

**Option 2:** Route TCP traffic through NGINX Ingress Controller using stream module (tcp-services ConfigMap)?

Can I apply per-service IP whitelisting in either of these approaches?

Will domain mapping (e.g. redis.mydomain.com, mysql.mydomain.com) work for the TCP services? Or is that only possible with HTTPS?

Any help or guidance is appreciated. If you have example configs (YAMLs, annotations, etc.), that would be even better.

Thanks!","amazon-web-services, kubernetes, kubernetes-ingress, amazon-eks",79641956.0,"**Can I use a single NLB in EKS to expose both HTTPS and multiple TCP ports?**

No, you cannot use a single AWS Network Load Balancer (NLB) to route both HTTPS (Layer 7) and TCP (Layer 4) traffic. Here's why:

- **HTTP/HTTPS traffic (Layer 7)** can be routed using Ingress Controllers (like NGINX or AWS ALB Ingress) which support features like hostname and path-based routing.

- **TCP traffic (Layer 4)** such as Redis, MySQL, and MongoDB cannot be routed through an Ingress Controller. Layer 4 protocols do not include hostnames or paths, so routing decisions must be made solely on port and IP address.

**Recommended Setup**

- You should use a **single NLB or ALB** to expose your Ingress Controller, which handles all HTTP/HTTPS services using Ingress resources.

- For **TCP services**, you generally need to expose each one using a separate `service `of type `LoadBalancer`. While it is possible to expose multiple TCP ports through a single NLB, this requires careful configuration and has limitations (e.g., no domain-based routing).

**Best Practice for Databases and Internal TCP Services**

In most cases, you **should not expose** databases (Redis, MySQL, etc.) to the external world. These services should only be accessible internally within the Kubernetes cluster.

You can expose these internally using a Kubernetes `Service`. For example, if your Redis service is called `redis-service`, it will be accessible as:

- `http://redis-service` from within the same namespace

- `http://redis-service.<namespace>.svc.cluster.local` from a different namespace

This approach enhances security and keeps internal traffic isolated from public exposure, only the applications running inside the cluster will be able to access them.

[![enter image description here](https://i.sstatic.net/TyeQ8QJj.png)](https://i.sstatic.net/TyeQ8QJj.png)

In the image above you see the AWS loadblancer routing the incoming traffic to the ingress controller service (purple) the Ingress controller routes to the corresponding services (blue and green). The internal applications such as reddis should not be expose via a loadbalancer and only your internal app should reach them (orange) using the service.

**Note:** The triangles are k8s services. and the circles represent pods from applications",2025-05-28T09:51:56,2025-05-21T13:31:08
79631601,Kafka connector can&#39;t parse DB credentials from Kubernetes,"I keep struggling with deployment of Kafka Connectors. Whichever config provider I use, I can't make my Debezium Postgres source connector use dynamic values in its configs.
For example, when I use :

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
<...>
config:
    config.providers: secrets
    config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider
<...>
```

And deploying connector:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
<...>
spec:
  class: io.debezium.connector.postgresql.PostgresConnector
  tasksMax: 1
  config:
    database.password: ""${secrets:namespace-name/secret-name:password}""
    database.user: ""${secrets:namespace-name/secret-name:username}""
<...>
```

Gives me an error:

```
FATAL: password authentication failed for user ""${secrets:secret-name:username}""
```

When I use:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
<...>
  config:
    config.providers: env
    config.providers.env.class: org.apache.kafka.common.config.provider.EnvVarConfigProvider
<...>
  externalConfiguration:
    env:
      - name: PG_USERNAME
        valueFrom:
          secretKeyRef:
            name: secret-name
            key: username
      - name: PG_PASSWORD
        valueFrom:
          secretKeyRef:
            name: secret-name
            key: password
```

And change credentials from constant to Env-s:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
<...>
spec:
  class: io.debezium.connector.postgresql.PostgresConnector
  tasksMax: 1
  config:
    database.password: ""${env:PG_PASSWORD}""
    database.user: ""${env:PG_USERNAME}""
<...>
```

I get same error:

```
FATAL: password authentication failed for user ""${env:PG_USERNAME}""
```

Here are my database logs with authorization attempts:

```
connection authenticated: identity=""user123"" method=md5
connection authorized: user=user123 database=db application_name=Debezium Validate Connection
disconnection: session time: 0:00:00.030 user=user123 database=db host=00.000.000.00 port=12345
connection received: host=00.000.000.00 port=67890
authentication ${secrets:secret-name:username}@db/on FATAL: password authentication failed for user ""${secrets:secret-name:username}""
authentication ${secrets:secret-name:username}@db/on DETAIL: Role ""${secrets:secret-name:username}"" does not exist.
```

I deploy connectors with simple `helm upgrade <...>`, without anything else.
Kubernetes secret is also nothing special: simple Opaque with base64 encrypted ""username"" and ""password"".

Interestingly, both Kafka Connect and Postgres logs shows that when connection is initially tested - connector gets username correctly. But when it's actually trying to authorize - it parses credentials incorrectly. That fact make me think the problem lies in some Java Class, particularly in some method responsible for passing credentials. But I can't figure out which one it is and how could I fix this.

EDIT:
I found something funny. When I am manually sending this config:

```
  ""config"": {
    database.password: ""${env:PG_PASSWORD}""
    database.user: ""${env:PG_USERNAME}""
  }
```

I am getting an error ""password authentification failed for user ""${env:PG_USERNAME}""

But when I am manually sending this config:

```
  ""config"": {
    database.password: ""${env:PG_PASSWORD}""
    database.user: ""${env:PG_USERNAME}}""
  }
```

I am getting an error ""password authentification failed for user ""user123}""","postgresql, kubernetes, apache-kafka, strimzi",,,,2025-05-21T08:42:30
79630518,Ingress route for Streamlit app returns 404 for static resources (JS/CSS/Fonts),"I'm deploying a Streamlit app in Kubernetes using Minikube. Everything works perfectly when I expose the app using a NodePort service:

http://minikube IP:30001/

However, when I try to expose the app via an Ingress at a subpath like:

http://minikube IP/app

I get the main Streamlit interface partially loaded, but many static resources fail with 404 errors. Examples of the errors shown in the browser console:

[enter image description here](https://i.sstatic.net/4hoBYzXL.png)

I'll share below my Service, Deployment, Dockerfile, and Ingress configurations. I would appreciate any help identifying what might be causing the issue.

```
apiVersion: v1
kind: Service

metadata:
  name: kairos-app-service
spec:
  type: NodePort

  selector:
    app: kairos-app

  ports:
  - port: 8501
    targetPort: 8501
    nodePort: 30001
```

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kairos-app-deployment
spec:
  replicas: 1

  selector:
    matchLabels:
      app: kairos-app
  template:
    metadata:
      labels:
        app: kairos-app

    spec:
      containers:
      - name: kairos-app-container
        image: rael167/kairos-app:latest

        ports:
        - containerPort: 8501

        resources:
          requests:
            cpu: 500m
            memory: 512Mi
          limits:
            cpu: 1
            memory: 1Gi
```

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kairos-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  rules:
  - http:
      paths:
      - path: /app(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: kairos-app-service
            port:
              number: 8501
```

```
FROM python:3.11-slim

WORKDIR /app

COPY Kairos_app /app/Kairos_app

COPY Kairos_app_info /app/Kairos_app_info

COPY requirements.txt /app/requirements.txt

RUN pip install --no-cache-dir -r requirements.txt

EXPOSE 8501

CMD [""streamlit"", ""run"", ""Kairos_app/Kairos_app.py"", ""--server.port=8501"", ""--server.headless=true"", ""--server.enableCORS=false"", ""--server.baseUrlPath=app""]
```

Thanks in advance!","docker, kubernetes, nginx, minikube, streamlit",,,,2025-05-20T13:13:43
79628267,Error starting the driver Pod in EKS with spark submit,"I am trying the deploy the spark job in the EKS cluster in AWS. However I keep getting this error when creating the driver pod. The deploy mode is set to cluster.

""failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""driver"": executable file not found in $PATH: unknown""

The error is thrown with 'status StartError' for the driver pod and the job terminates.","apache-spark, kubernetes, amazon-eks, spark-submit",79666127.0,"The error message is telling you what's wrong:

```
""failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""driver"": executable file not found in $PATH: unknown""
```

- Failed to create the containerd task
- unable to start container process exec ""driver""
- executable file not found in $PATH unknown

The message is telling you that the driver pod's container is trying to run the command ""driver"" but it can't find the exec file in the container's path.

You mentioned that --deploy-mode cluster is being used. Spark is trying to launch the driver inside a K8s pod using the Docker image.

This error usually happens when the following occurs:

- The image has no valid ENTRYPOINT or CMD
- Spark is missing from the image

Double check the configuration files (i.e YAML files), the entrypoint is correctly set and the Dockerfile is correct with the CMD.

I have found another [StackOverflow](https://stackoverflow.com/questions/72695311/failure-starting-docker-container-failed-to-create-shim-task-oci-runtime-crea) that looks similar to help resolve the issue, if not, I'd recommend:

- review the [Docker logs](https://docs.docker.com/reference/cli/docker/container/logs/)
- Check the logs on the EKS pod for any information on K8's end:

`$ kubectl logs <pod name> -n <namespace>`

Also giving us more information helps us help you, providing any logs from Docker or kubectl will give us more context/root cause of the issue.",2025-06-14T20:08:52,2025-05-19T07:22:17
79627889,How to setup k8s Pod readiness probe only for the initial phase,"Configuring a Kubernetes pod container `readinessProbe` hot to achieve the probes to be produces only on the initiation phase and once the container is ready just to consider the container is ready all the further way. So that it doesn't spam the traffic as the ready condition is not of a matter any more.

P.S. Found in the [docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes)

```
Note:
Readiness probes runs on the container during its whole lifecycle.
```

So still is there maybe any work around to achieve the intention.

Or at least is it possible to configure different check intervals for the two phases separately?

Is it possible to share the state among the checks and once we hit `ready` just go within the check process for internal sleep?","kubernetes, readinessprobe",79627903.0,"It's called [startupProbe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes)

Example from docs:

```
startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30
  periodSeconds: 10
```

Then you can either omit or create completely separate readinessProbe",2025-05-18T22:36:30,2025-05-18T22:17:20
79626543,How to override the health check port and endpoint for Kubernetes Service using OCI cloud controller manager,"I have a self-managed cluster consisting of a control plane node, and two worker nodes, all of which are hosted as VMs on OCI. Additionally, I also have configured the [OCI Cloud Controller Manager](https://github.com/oracle/oci-cloud-controller-manager) properly, in order to use OCI load balancers.

I can confirm that the OCI Cloud Controller Manager is configured properly because I have tried deploying a K8s service of type LoadBalancer, and it successfully provisions the LoadBalancer, and the K8s service also gets an external IP. However, when I inspect the LoadBalancer from the dashboard, it says the health is critical because the backendset by default sends HTTP pings on port 10256 at the endpoint ""/healthz"". To make things worse, all my nodes and control plane are returning `healthy: false` because IPv6 is not configured properly on them, and I don't want to debug why. I figured, the easiest solution would be to point the health checks to port 80 at the endpoint ""/"".

Here's the curl response from my control plane:

```
ubuntu@kubemaster:~$ curl -I http://localhost:10256/healthz
HTTP/1.1 503 Service Unavailable
Content-Type: application/json
X-Content-Type-Options: nosniff
Date: Sat, 17 May 2025 12:44:19 GMT
Content-Length: 284

ubuntu@kubemaster:~$ curl -s http://localhost:10256/healthz | jq
{
  ""lastUpdated"": ""2025-05-17T12:40:10.153177379Z"",
  ""currentTime"": ""2025-05-17T12:44:26.894449614Z"",
  ""nodeEligible"": true,
  ""healthy"": false,
  ""status"": {
    ""IPv4"": {
      ""lastUpdated"": ""2025-05-17T12:40:10.153177379Z"",
      ""healthy"": true
    },
    ""IPv6"": {
      ""lastUpdated"": ""2025-05-15T17:18:25.70543555Z"",
      ""healthy"": false
    }
  }
}
```

Here's my `frontend-service.yaml` file:

```
# frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  annotations:
    service.beta.kubernetes.io/oci-load-balancer-shape: ""flexible""
    service.beta.kubernetes.io/oci-load-balancer-internal: ""false""
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: ""2""
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: ""8""
    # Add these health check annotations:
    service.beta.kubernetes.io/oci-load-balancer-health-check-protocol: ""HTTP""
    service.beta.kubernetes.io/oci-load-balancer-health-check-port: ""80""
    service.beta.kubernetes.io/oci-load-balancer-health-check-path: ""/""
  labels:
    app: checklister-frontend
spec:
  type: LoadBalancer
  selector:
    app: checklister-frontend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
```","kubernetes, cloud, oracle-cloud-infrastructure, kubernetes-service",79627314.0,"Try to specify loadbalancer like this

```
apiVersion: v1
kind: Service
metadata:
  name: frontend-lb
  namespace: yournm
spec:
  type: LoadBalancer
  selector:
    app: checklister-frontend  # Selects pods with this label
  ports:
  - port: 80             # Port exposed by the load balancer
    targetPort: 80       # Port the container is listening on
    protocol: TCP
    name: http
  - port: 443            # HTTPS port
    targetPort: 80
    protocol: TCP
    name: https
  sessionAffinity: None
```",2025-05-18T09:36:14,2025-05-17T12:46:41
79624987,Websockets on GKE with Nginx Ingress,"I am trying to get websockets to work on GKE. Seems very trivial, but I am failing to get this to work, I just continuously keep getting 400 at Nginx Ingress.

The manifest is like this:

```
apiVersion: v1
kind: Namespace
metadata:
  name: my-test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-ws-backend
  namespace: my-test
  labels:
    app: my-ws-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-ws-backend
  template:
    metadata:
      labels:
        app: my-ws-backend
    spec:
      containers:
        - name: backend
          image: ksdn117/web-socket-test
          imagePullPolicy: Always
          ports:
            - containerPort: 8010
          env:
            - name: NODE_ENV
              value: production
            - name: DEBUG
              value: socket*
---
apiVersion: v1
kind: Service
metadata:
  name: my-ws-backend
  namespace: my-test
spec:
  selector:
    app: my-ws-backend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8010
  sessionAffinity: ClientIP
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ws-ingress
  namespace: my-test
  annotations:
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/upgrade-insecure-requests: ""true""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection $connection_upgrade;
      proxy_set_header Host $host;
    nginx.ingress.kubernetes.io/server-snippet: |
      error_log /var/log/nginx/error.log debug;
    cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
spec:
  ingressClassName: nginx
  rules:
    - host: ws-my-test.myhost.com
      http:
        paths:
          - path: /socket.io
            pathType: Prefix
            backend:
              service:
                name: my-ws-backend
                port:
                  number: 80
  tls:
    - hosts:
        - ws-my-test.myhost.com
      secretName: ws-my-test-cert
```

I tried hitting the endpoint with wscat and a simplistic Node.js script shown below to test. What am I missing?

```
const { io } = require('socket.io-client');

const socket = io('wss://ws-my-test.myhost.com', {
  transports: ['websocket'],
  reconnection: false,
});

socket.on('connect', () => {
  console.log('Connected!');
  socket.disconnect();
});

socket.on('connect_error', (err) => {
  console.error('Connection error:', err);
});
```","kubernetes, websocket, google-kubernetes-engine, nginx-ingress",79628162.0,"Got this working in the end, these are the annotations in my Ingress

```
cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
nginx.ingress.kubernetes.io/proxy-buffering: ""off""
nginx.ingress.kubernetes.io/proxy-connect-timeout: ""10""
nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
```

I think the problem was including duplicates as my annotations below:

```
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection $connection_upgrade;
proxy_set_header Host $host;
```

which were not required, and caused the header to have duplicate values set in the header, that caused rejection of the request with status 400.

Ingress-NGINX controller already comes preconfigured with the required Upgrade/Connection headers set, so not needed to set them again.",2025-05-19T06:16:53,2025-05-16T10:30:02
79624732,Two ingress controller have issues and not able to split traffic in Kubernetes,"I have two ingress setup on my Kubernetes cluster one for internal traffic called nginx-ingress and it has default setting and with values of

```
root@k8s-master-1:~/Ingress# cat values.yaml
controller:
  name: ingress-nginx
  ingressClassResource:
    name: nginx
    enabled: true
    default: false
    controllerValue: ""k8s.io/ingress-nginx""
  ingressClass: nginx
  replicaCount: 1
  service:
    type: LoadBalancer
    loadBalancerIP: 10.111.111.74
    externalTrafficPolicy: Local
  resources:
    requests:
      cpu: 500m
      memory: 512Mi
    limits:
      cpu: 2
      memory: 2Gi
  metrics:
    enabled: true
    serviceMonitor:
      enabled: false

#  nodeSelector:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: ""kubernetes.io/hostname""
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - ingress-nginx
```

I used helm charts and help create a ingress class for it.   Then I create another ingress control with different name with following values.

```
root@k8s-master-1:~/Ingress# cat values-external.yaml
controller:
  name: ext-ingress
  ingressClassResource:
    name: external
    enabled: true
    default: false
    controllerValue: ""k8s.io/ext-ingress""
  ingressClass: external  #  Diffrent class name
  replicaCount: 1

  service:
    type: LoadBalancer
    loadBalancerIP: 10.111.111.75 # MetalLB external LB IP
    externalTrafficPolicy: Local

  metrics:
    enabled: true
    serviceMonitor:
      enabled: false
  resources:
    requests:
      cpu: ""500m""
      memory: ""512Mi""
    limits:
      cpu: ""2""
      memory: ""2Gi""
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 5
    targetCPUUtilizationPercentage: 60
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            topologyKey: ""kubernetes.io/hostname""
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - ext-ingress
```

I have to create a class for it as it was not default ingress and I create it as follow.

```
root@k8s-master-1:~/Ingress#  cat ingressclass-external.yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: external
  annotations:
    ingressclass.kubernetes.io/is-default-class: ""false""
spec:
  controller: k8s.io/ext-ingress
```

Problem:
I have 4 applications 3 of them should pass through internal ingress and one should pass through external ingress. One application is internal-portal and to login it with it I need to use Keycloak. When I add my username and password of KC it access and then my application keep surfing and I see `error 401` in web console blinking. Finally I see a message site cant be reach.

The very strange behaviour is when I add Keycloak to use external class it works but if I search `kubectl get ingress -A` I can see my Keycloak is using external class but its IP is still internal class

```
root@k8s-master-1:~/Ingress# kubectl get ingress -A
NAMESPACE             NAME                           CLASS      HOSTS                            ADDRESS        PORTS     AGE
api-uat               gateway-api-external-ingress   external   gateway-api-uat   10.111.111.75   80, 443   21h
api-uat               payment-api-ingress            nginx      payments-uat      10.111.111.74   80, 443   34d
ui-external-uat       rtd-ext-portal-ingress         nginx      portal-uat        10.111.111.74   80, 443   35d
ui-internal-uat       rtd-int-portal-ingress         external   int-portal-uat    10.111.111.75   80, 443   37d
utility-service-uat   keycloak-uat-ingress           external   identity-uat      10.111.111.74   80, 443   34d
```

It is very strange as now KC and my int-portal are in different ingress and they work. I can login to int-portal by using KC and verification complete and I can see my application and use it. How does it work thats a big Question.
But when I add KC into same ingress it stop working. I can login to KC directly but whrn I use internal-portal and it ask my KC user and password after that it keep surfing and then site cant be reached.

I look into logs, noting found. I try changing different class and test still the same issue. I am not sure how can it has different class and different IP. I changed the values to specify use controller name to pick IP as you can see in my value. Yaml. Non of my controller set as default so noting can be picked by default as well.

I want that we KC and other related app should be under one ingress and that should be work fine. I am not allow to change IP for KC/Internal ingress and KC should be under internal Ingress.","kubernetes, keycloak, kubernetes-helm, kubernetes-ingress, nginx-ingress",,,,2025-05-16T07:54:23
79623799,Is there a way to pass a JWT to Airflow&#39;s SparkKubernetesOperator,"I am using `SparkKubernetesOperator` to submit my SparkApplication (via the CRD manifest file) to our onprem Kubernetes cluster.

The cluster authenticates our request via the kube api.

We have defined a connection in the Airflow admin for type `Kubernetes Cluster Connection` and provided path of the Kube Config file under the field `Kube config path`.

However the token mentioned in the kube config file expires after a certain period and we need to manually regenerate a new config file and overwrite it on the kube config path.

The way we generate a new kube config file is firing `kubengine login --cluster mycluster --namespace mynamespace` cli command from our local desktop (which validates us against our company's OIDC mechanism through our userid and password) and it refreshes the kubeconfig file with a fresh token.

We then take the refreshed kubeconfig file and replace it with the old file at the path mentioned in `Kube Config Path` field of admin connection on airflow dashboard.

We have been told by our K8S cluster team that there is an alternative way to authenticate ourselves against cluster and that is using kerberos based JWT.

Through a keytab file we can invoke an API that will provide us a JWT and that can be used when communicating with Kube api.

**ISSUE-1:**
However after reading documentation and going through various posts, I still do not find a way where I can pass the JWT to the `SparkKubernetesOperator` instead of the `kubernetes_conn_id` (which internally uses the connection details defined on the Connection dashboard of Airflow UI).

Our idea is to invoke the API via a regular `PythonOperator` and get the JWT from it and do a `xcom push` to `SparkKubernetesOperator` which can do a `xcom pull` of it and use it for submitting the spark application.

Is there a workaround to this issue ? Do we need to create a custom operator by extending `SparkKubernetesOperator` ? Any pointers/sample code snippets here will help.

**ISSUE-2:**
On a different issue, we are able to successfully submit our SparkApplication to the k8s cluster (using the `kubernetes_conn_id` approach), but Airflow logs does not include the pod logs in the output.
I see that the `get_logs` attribute is defaulted to `True`, so I would expect pod logs to flow into the Airflow.

In fact, when I try to explicitly give that attribute to the operator, airflow is unable to parse my dag and spits out error:

```
airflow.exceptions.AirflowException: Invalid arguments were passed to SparkKubernetesOperator (task_id: n-spark-on-k8s-airflow). Invalid arguments were:
**kwargs: {'get_logs': True}
```

Is this anything to do with the version of airflow we are using ? We are using 2.x version.

Below are our code details (have changed values of attributes and provided some dummy values for confidentiality reasons):

```
import datetime
import os
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.spark_kubernetes import SparkKubernetesOperator

dag_id = 'test_onpremk8s'
queue='default'

schedule_interval = None
default_args = {
    ""owner"": ""Me"",
    ""depends_on_past"": False,
    ""start_date"": str(datetime.datetime.now()),
    ""email"": [""abc@xyz.com""],
    ""email_on_failure"": True,
    ""email_on_retry"": False,
    ""retries"": 0,
    ""weight_rule"":""upstream"",
    ""queue"":queue
}

dag = DAG(dag_id, schedule_interval=schedule_interval, default_args=default_args, catchup=False)

spark_k8s_task = SparkKubernetesOperator(
    task_id='n-spark-on-k8s-airflow',
    trigger_rule=""all_success"",
    depends_on_past=False,
    retries=0,
    application_file='trf-drivers-os.yaml',
    namespace=""mynamespace"",
    kubernetes_conn_id=""xyz_k8s"",
    do_xcom_push=True,
    dag=dag,
    get_logs=True #able to submit job only when this attribute is not present
)

spark_k8s_task
```

Also attached is a screenshot of the Connection made via Airflow UI:

[![enter image description here](https://i.sstatic.net/DdYwCjo4.png)](https://i.sstatic.net/DdYwCjo4.png)","kubernetes, logging, jwt, airflow, kerberos",,,,2025-05-15T16:07:28
79622501,Testcontainer for Kubernetes and Argo Workflows,"I want to do some integration testing + setup local dev environment for a Java/Spring Boot application that interacts with Kubernetes via Argo Workflows.  I don't see a Kubernetes test container (probably because K8s is not just one container) but I did notice the K3 container which is supposedly a lightweight distro for kubernetes.

I usually use a locally installed Minikube instance to achieve this but Testcontainers are very cool and I'm already using them for my DB and a Kafka queue, would be nice to have one solution for everything.

Is there a way to use Testcontainers to test an app that interacts with Kubernetes via Argo Wofkflows?","java, kubernetes, junit5, testcontainers, argo-workflows",79653758.0,"This worked eventually for me.. also see discussion here ([https://github.com/spring-projects/spring-boot/issues/45717](https://github.com/spring-projects/spring-boot/issues/45717))

```
   @Bean
    K3sContainer k3s() {
        K3sContainer k3sContainer = new K3sContainer(DockerImageName.parse(""rancher/k3s:v1.21.3-k3s1""))
                .withCommand(""server"", ""--disable"", ""metrics-server"") // we don't need metrics
                .withLogConsumer(new Slf4jLogConsumer(logger)
                );

        // don't normally need to start the container in these @Bean methods but can't get the config unless its started
        k3sContainer.start();

        String kubeConfigYaml = k3sContainer.getKubeConfigYaml();
        Config config = Config.fromKubeconfig(kubeConfigYaml);  // requires io.fabric8:kubernetes-client:5.11.0 or higher

        // in the absence of @ServiceConnection integration for this testcontainer, Jack the test container URL into properties so it's picked up when I create a client in main app
        System.setProperty(Config.KUBERNETES_MASTER_SYSTEM_PROPERTY, config.getMasterUrl());
        System.setProperty(Config.KUBERNETES_CA_CERTIFICATE_DATA_SYSTEM_PROPERTY, config.getCaCertData());
        System.setProperty(Config.KUBERNETES_CLIENT_CERTIFICATE_DATA_SYSTEM_PROPERTY, config.getClientCertData());
        System.setProperty(Config.KUBERNETES_CLIENT_KEY_DATA_SYSTEM_PROPERTY, config.getClientKeyData());
        System.setProperty(Config.KUBERNETES_TRUST_CERT_SYSTEM_PROPERTY, ""true"");

        return k3sContainer;
    }
```",2025-06-05T03:33:10,2025-05-15T01:36:08
79622501,Testcontainer for Kubernetes and Argo Workflows,"I want to do some integration testing + setup local dev environment for a Java/Spring Boot application that interacts with Kubernetes via Argo Workflows.  I don't see a Kubernetes test container (probably because K8s is not just one container) but I did notice the K3 container which is supposedly a lightweight distro for kubernetes.

I usually use a locally installed Minikube instance to achieve this but Testcontainers are very cool and I'm already using them for my DB and a Kafka queue, would be nice to have one solution for everything.

Is there a way to use Testcontainers to test an app that interacts with Kubernetes via Argo Wofkflows?","java, kubernetes, junit5, testcontainers, argo-workflows",79622717.0,"I am currently experimenting with this. Testcontainers provides a K3s module ([https://testcontainers.com/modules/k3s/](https://testcontainers.com/modules/k3s/)), and since K3s starts faster than Minikube, it seems promising.

With this fairly simple setup, I can get K3s with Argo Workflows running using the K3s built-in Helm controller. The only problem is that it is quite slow.

I am considering preloading the container images for Argo and the Helm controller, which take a while to pull and start, or using a local image registry.

I have yet to resolve how to get my own app images running. Preloading or using a local image registry seem to be the available options.

```
from asyncio import sleep
from typing import AsyncGenerator, Literal

import pytest
import yaml
from kubernetes_asyncio.client import (
    CoreV1Api,
    CustomObjectsApi,
    AppsV1Api,
    V1Deployment,
)
from kubernetes_asyncio.client import Configuration, ApiClient
from kubernetes_asyncio.config import load_kube_config_from_dict
from testcontainers.k3s import K3SContainer

@pytest.fixture
def anyio_backend() -> Literal[""asyncio""]:
    return ""asyncio""

@pytest.fixture
def k3s_container():
    with K3SContainer() as k3s:
        yield k3s

@pytest.fixture
async def k3s_client(k3s_container, anyio_backend) -> AsyncGenerator[ApiClient]:
    config = k3s_container.config_yaml()
    client_configuration = Configuration()
    await load_kube_config_from_dict(
        config_dict=yaml.load(config, Loader=yaml.SafeLoader),
        client_configuration=client_configuration,
    )
    async with ApiClient(configuration=client_configuration) as client:
        yield client

async def testwf(k3s_client, anyio_backend):

    namespace_body = {
        ""apiVersion"": ""v1"",
        ""kind"": ""Namespace"",
        ""metadata"": {""name"": ""argo-workflows""},
    }
    await CoreV1Api(k3s_client).create_namespace(body=namespace_body)

    helm_chart_manifest = {
        ""apiVersion"": ""helm.cattle.io/v1"",
        ""kind"": ""HelmChart"",
        ""metadata"": {
            ""name"": ""argo-workflows"",
        },
        ""spec"": {
            ""chart"": ""argo-workflows"",
            ""repo"": ""https://argoproj.github.io/argo-helm"",
            ""targetNamespace"": ""argo-workflows"",
            ""valuesContent"": yaml.dump(
                {
                    ""controller"": {""workflowNamespaces"": [""network-automation""]},
                    ""server"": {""enabled"": True},
                    ""singleNamespace"": True,
                }
            ),
        },
    }

    await CustomObjectsApi(k3s_client).create_namespaced_custom_object(
        group=""helm.cattle.io"",
        version=""v1"",
        namespace=""kube-system"",
        plural=""helmcharts"",
        body=helm_chart_manifest,
    )

    while True:
        deps = await AppsV1Api(k3s_client).list_namespaced_deployment(
            namespace=""argo-workflows""
        )
        for d in deps.items:
            assert isinstance(d, V1Deployment)
            print(f""{d.metadata.name=} {d.status.replicas=} {d.status.ready_replicas=}"")
        if not deps.items:
            print(""No deployments found"")
        await sleep(2)
```",2025-05-15T06:09:05,2025-05-15T01:36:08
79621389,Strimzi does not parse secret when deploying Kafka Connector with Helm,"I am struggling with io.strimzi.kafka.KubernetesSecretConfigProvider when deploying connector to my Kafka Connect service in k8s. Pod log returns error when trying to create connector:

```
ERROR Uncaught exception in REST call to /connectors/pg-source-connector/config (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper)
org.apache.kafka.common.config.ConfigException:
Invalid path . It has to be in format <namespace>/<secret> (or <secret> for default namespace).
at io.strimzi.kafka.KubernetesResourceIdentifier.fromConfigString(KubernetesResourceIdentifier.java:33)
```

Seems that for some reason my connector can't parse ""${...}"" values in it's template. I'm not sure if  the problem is with Strimzi or some Kubernetes settings. I also previously had problem with parsing when I tried to use io.strimzi.kafka.KubernetesEnvProvider

Here are my templates. If necessary, I'll provide Role and Role-binding, but I doubt the problem lies with them.

kafka-connect.yaml:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: kc-cluster
  annotations:
    strimzi.io/use-connector-resources: ""true""
spec:
  image: {{ .Values.repository.image }}:{{ .Values.repository.tag }}
  template:
    pod:
      securityContext:
        runAsUser: 1
        runAsNonRoot: true
    connectContainer:
      securityContext:
        capabilities:
          drop:
            - ALL
        privileged: false
        runAsUser: 1
        runAsGroup: 1
        runAsNonRoot: true
        readOnlyRootFilesystem: true
        allowPrivilegeEscalation: false
        seccompProfile:
          type: RuntimeDefault
  replicas: 1
  resources:
    limits:
      cpu: 1000m
      memory: 1000Mi
    requests:
      cpu: 500m
      memory: 500Mi
  bootstrapServers: {{ .Values.kafka.bootstrapServers }}
  tls:
    trustedCertificates:
      - secretName: kc-ca-cert
        certificate: ca.crt
  authentication:
    type: scram-sha-512
    username: {{ .Values.kafka.username }}
    passwordSecret:
      secretName: {{ .Values.kafka.username }}
      password: password
  config:
    config.providers: secrets
    config.providers.secrets.class: io.strimzi.kafka.KubernetesSecretConfigProvider
    ssl.cipher.suites: TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
    ssl.protocol: TLSv1.3
    group.id: kc
    offset.storage.topic: kc-offsets
    config.storage.topic: kc-configs
    status.storage.topic: kc-statuses
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: false
    value.converter.schemas.enable: false
    config.storage.replication.factor: 1
    offset.storage.replication.factor: 1
    status.storage.replication.factor: 1
    metrics.reporter: ""none""
    log4j.logger.org.apache.kafka.connect.runtime: DEBUG
    log4j.logger.org.apache.kafka.connect.connector: DEBUG
```

secret.yaml:

```
kind: Secret
apiVersion: v1
metadata:
  name: db-creds
  namespace: kc-namespace
  annotations:
    kubesphere.io/creator: k8suser
data:
  password: EnCrYpTeDpAsSwOrD123
  username: EnCrYpTeDuSeRnAmE456
type: Opaque
```

pg-source-connector.yaml:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: pg-source-connector
  namespace: kc-namespace
  labels:
    strimzi.io/cluster: kc-cluster
spec:
  class: io.debezium.connector.postgresql.PostgresConnector
  tasksMax: 1
  config:
    database.dbname: ""database""
    database.hostname: ""12.345.678.910""
    database.password: ""${secrets:kc-namespace/db-creds/password}""
    database.port: ""5432""
    database.user: ""${secrets:kc-namespace/db-creds/username}""
    key.converter: ""org.apache.kafka.connect.json.JsonConverter""
    key.converter.schemas.enable: ""false""
    plugin.name: ""pgoutput""
    primary.key: ""id""
    publication.autocreate.mode: ""disabled""
    publication.name: ""test_kc""
    schema.include.list: ""public""
    snapshot.include.collection.list: ""public.test_kc""
    snapshot.mode: ""initial""
    table.include.list: ""public.test_kc""
    time.precision.mode: ""connect""
    topic.creation.default.cleanup.policy: ""delete""
    topic.creation.default.partitions: ""3""
    topic.creation.default.replication.factor: ""2""
    topic.delimiter: ""_""
    topic.prefix: ""kc-pg""
    value.converter: ""org.apache.kafka.connect.json.JsonConverter""
    value.converter.schemas.enable: ""false""
```

When I check deployed Kafka Connect in Kubernetes - there are no /opt/kafka/secrets directory, where creds should be stored. I understand that the problem lies in ""Invalid path ."", but can't figure out the cause of it.","kubernetes, apache-kafka, kubernetes-helm, apache-kafka-connect, strimzi",79621838.0,"There are two things to ensure:

1. That the special characters such as `$`, `{`, or `}` are not removed by your Helm Chart
2. That you use the format for the config provider correctly

I cannot comment much on the first point as it is your Helm chart and I have no idea what it does. But based on the error it probably works fine as it seems to have failed only later.

For the second part, you seem to have it wrong because the structure is not `<namespace>/<secretName>/<key>` as you see, to use it. It is `<namespace>/<secretName>:<key>` . So you will need to fix it. E.g.:

```
database.password: ""${secrets:kc-namespace/db-creds:password}""
```

You can see that also in the configuration provider docs: [https://github.com/strimzi/kafka-kubernetes-config-provider?tab=readme-ov-file#using-the-configuration-provider](https://github.com/strimzi/kafka-kubernetes-config-provider?tab=readme-ov-file#using-the-configuration-provider)",2025-05-14T15:42:35,2025-05-14T11:38:47
79617217,clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410),"I am trying to deploy `penpot` on a local `minikube` cluster using the following `yaml` file:

```
apiVersion: v1
kind: Namespace
metadata:
  name: penpot
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: penpot
spec:
  ports:
    - port: 5432
  selector:
    app: postgres
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:latest
          env:
            - name: POSTGRES_DB
              value: penpot
            - name: POSTGRES_USER
              value: penpot
            - name: POSTGRES_PASSWORD
              value: penpot
          ports:
            - containerPort: 5432
          volumeMounts:
            - mountPath: /var/lib/postgresql/data
              name: postgres-storage
      volumes:
        - name: postgres-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: penpot
spec:
  ports:
    - port: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:7
          ports:
            - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: penpot-backend
  namespace: penpot
spec:
  ports:
    - port: 6060
  selector:
    app: penpot-backend
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-backend
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-backend
  template:
    metadata:
      labels:
        app: penpot-backend
    spec:
      containers:
        - name: penpot-backend
          image: penpotapp/backend:latest
          env:
            - name: PENPOT_PUBLIC_URI
              value: http://penpot-frontend
            - name: PENPOT_DATABASE_URI
              value: postgresql://penpot:penpot@postgres:5432/penpot
            - name: PENPOT_REDIS_URI
              value: redis://redis:6379
          ports:
            - containerPort: 6060

---
apiVersion: v1
kind: Service
metadata:
  name: penpot-frontend
  namespace: penpot
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30090
  selector:
    app: penpot-frontend

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-frontend
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-frontend
  template:
    metadata:
      labels:
        app: penpot-frontend
    spec:
      containers:
        - name: penpot-frontend
          image: penpotapp/frontend:latest
          env:
            - name: PENPOT_BACKEND_URI
              value: http://penpot-backend:6060
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: penpot-exporter
  namespace: penpot
spec:
  ports:
    - port: 6061
  selector:
    app: penpot-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-exporter
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-exporter
  template:
    metadata:
      labels:
        app: penpot-exporter
    spec:
      containers:
        - name: penpot-exporter
          image: penpotapp/exporter:latest
          env:
            - name: PENPOT_PUBLIC_URI
              value: http://penpot-frontend
          ports:
            - containerPort: 6061
```

But I do have following problem with `penpot-backend` pod:

```
kubectl logs penpot-backend-58ff898db9-p5bz2 -n penpot

+ exec /opt/jdk/bin/java -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Dlog4j2.configurationFile=log4j2.xml -XX:-OmitStackTraceInFastThrow --enable-preview -jar penpot.jar -m app.main
[2025-05-12 04:59:18.265] I app.metrics - action=""initialize metrics""
[2025-05-12 04:59:18.290] I app.db - hint=""initialize connection pool"", name=""main"", uri=""postgresql://penpot:penpot@postgres:5432/penpot"", read-only=false, credentials=true, min-size=0, max-size=60
[2025-05-12 04:59:18.320] I app.migrations - hint=""running migrations"", module=:app.migrations/migrations
SUMMARY:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10005ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
 →  java.net.UnknownHostException: penpot:penpot@postgres (NioSocketImpl.java:567)
DETAIL:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
    at: integrant.core$build_exception.invokeStatic(core.cljc:410)
        integrant.core$build_exception.invoke(core.cljc:409)
        integrant.core$try_build_action.invokeStatic(core.cljc:421)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)

 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10005ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
    at: com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:710)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:189)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:147)
        com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:99)
        next.jdbc.connection$make_connection.invokeStatic(connection.clj:455)
        next.jdbc.connection$make_connection.invoke(connection.clj:439)
        next.jdbc.connection$eval18499$fn__18500.invoke(connection.clj:484)
        next.jdbc.protocols$eval16853$fn__16854$G__16844__16861.invoke(protocols.clj:25)
        next.jdbc$get_connection.invokeStatic(jdbc.clj:169)
        next.jdbc$get_connection.invoke(jdbc.clj:148)
        app.db$open.invokeStatic(db.clj:230)
        app.db$open.invoke(db.clj:227)
        app.migrations$apply_migrations_BANG_.invokeStatic(migrations.clj:445)
        app.migrations$apply_migrations_BANG_.invoke(migrations.clj:443)
        app.migrations$eval33608$fn__33610.invoke(migrations.clj:457)
        clojure.lang.MultiFn.invoke(MultiFn.java:234)
        integrant.core$try_build_action.invokeStatic(core.cljc:419)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
```

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
java.lang.Thread.run(Thread.java:1583)

```
 →  java.net.UnknownHostException: penpot:penpot@postgres (NioSocketImpl.java:567)
    at: sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)
        java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        java.net.Socket.connect(Socket.java:751)
        org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        org.postgresql.core.PGStream.<init>(PGStream.java:121)
        org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)
```

I also tried to use the `Helm-Chart` but had the same issue.

```
kubectl describe pod penpot-backend-58ff898db9-p5bz2 -n penpot

Name:             penpot-backend-58ff898db9-p5bz2
Namespace:        penpot
Priority:         0
Service Account:  default
Node:             minikube-m03/192.168.49.4
Start Time:       Sun, 11 May 2025 21:52:23 -0700
Labels:           app=penpot-backend
                  pod-template-hash=58ff898db9
                  skaffold.dev/run-id=29a9d1cc-d97f-4e49-9fd7-9ed7a4e32b99
Annotations:      <none>
Status:           Running
IP:               10.244.2.54
IPs:
  IP:           10.244.2.54
Controlled By:  ReplicaSet/penpot-backend-58ff898db9
Containers:
  penpot-backend:
    Container ID:   docker://03bf7598b9510734458239b8bcd3b7a73168d91d985492915b3ec0b324c38914
    Image:          penpotapp/backend:latest
    Image ID:       docker-pullable://penpotapp/backend@sha256:e82c0a7ce65920e4b21fb20d644ec15dd245182a09982c2be23806ef65f1f00c
    Port:           6060/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Sun, 11 May 2025 21:58:48 -0700
      Finished:     Sun, 11 May 2025 21:59:29 -0700
    Ready:          False
    Restart Count:  5
    Environment:
      PENPOT_PUBLIC_URI:    http://penpot-frontend
      PENPOT_DATABASE_URI:  postgresql://penpot:penpot@postgres:5432/penpot
      PENPOT_REDIS_URI:     redis://redis:6379
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-66wrb (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-66wrb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  8m19s                 default-scheduler  Successfully assigned penpot/penpot-backend-58ff898db9-p5bz2 to minikube-m03
  Normal   Pulled     8m18s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 814ms (814ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     7m37s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 737ms (737ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     6m43s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 807ms (807ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     5m33s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 850ms (850ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     4m9s                  kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 1.654s (1.654s including waiting). Image size: 1210864079 bytes.
  Normal   Pulling    114s (x6 over 8m18s)  kubelet            Pulling image ""penpotapp/backend:latest""
  Normal   Created    114s (x6 over 8m17s)  kubelet            Created container: penpot-backend
  Normal   Pulled     114s                  kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 828ms (828ms including waiting). Image size: 1210864079 bytes.
  Normal   Started    113s (x6 over 8m17s)  kubelet            Started container penpot-backend
  Warning  BackOff    11s (x19 over 6m55s)  kubelet            Back-off restarting failed container penpot-backend in pod penpot-backend-58ff898db9-p5bz2_penpot(79338548-cb82-49e0-99e8-71b2a354dd14)
```

`EDIT`:
I tried to modify the following part:

```
# PENPOT_DATABASE_URI: ""postgresql://penpot-postgresql:5432/penpot""
PENPOT_DATABASE_URI: postgresql://postgres/penpot
PENPOT_DATABASE_USERNAME: ""penpot""
PENPOT_DATABASE_PASSWORD: ""penpot""
```

But still get this error:

```
+ exec /opt/jdk/bin/java -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Dlog4j2.configurationFile=log4j2.xml -XX:-OmitStackTraceInFastThrow --enable-preview -jar penpot.jar -m app.main
[2025-05-14 00:44:28.488] I app.metrics - action=""initialize metrics""
[2025-05-14 00:44:28.507] I app.db - hint=""initialize connection pool"", name=""main"", uri=""postgresql://penpot-postgresql:5432/penpot"", read-only=false, credentials=true, min-size=0, max-size=60
[2025-05-14 00:44:28.532] I app.migrations - hint=""running migrations"", module=:app.migrations/migrations
SUMMARY:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10001ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
 →  java.net.UnknownHostException: penpot-postgresql (NioSocketImpl.java:567)
DETAIL:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
    at: integrant.core$build_exception.invokeStatic(core.cljc:410)
        integrant.core$build_exception.invoke(core.cljc:409)
        integrant.core$try_build_action.invokeStatic(core.cljc:421)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
        clojure.core$alter_var_root.doInvoke(core.clj:5558)
        clojure.lang.RestFn.invoke(RestFn.java:428)
        app.main$start.invokeStatic(main.clj:544)
        app.main$start.invoke(main.clj:540)
        app.main$_main.invokeStatic(main.clj:610)
        app.main$_main.doInvoke(main.clj:602)
        clojure.lang.RestFn.invoke(RestFn.java:400)
        clojure.lang.AFn.applyToHelper(AFn.java:152)
        clojure.lang.RestFn.applyTo(RestFn.java:135)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.core$apply.invokeStatic(core.clj:667)
        clojure.main$main_opt.invokeStatic(main.clj:515)
        clojure.main$main_opt.invoke(main.clj:511)
        clojure.main$main.invokeStatic(main.clj:665)
        clojure.main$main.doInvoke(main.clj:617)
        clojure.lang.RestFn.applyTo(RestFn.java:140)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.main.main(main.java:40)
    dt: {:reason :integrant.core/build-threw-exception,
         :system
         {:app.auth.oidc.providers/github nil,
          :app.db/pool #object[com.zaxxer.hikari.HikariDataSource 0x1faa9581 ""HikariDataSource (main)""],
          :app.auth.oidc.providers/gitlab nil,
          :app.http.client/client
          #object[jdk.internal.net.http.HttpClientFacade 0x129b3801 ""jdk.internal.net.http.HttpClientImpl@127cfcd2(1)""],
          :app.email/blacklist nil,
          :app.auth.oidc.providers/generic nil,
          :app.email/whitelist nil,
          :app.auth.oidc.providers/google nil,
          ...},
         :function #multifn[init-key 0x598cddca],
         :key :app.migrations/migrations,
         :value #:app.db{:pool #object[com.zaxxer.hikari.HikariDataSource 0x1faa9581 ""HikariDataSource (main)""]}}

 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10001ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
    at: com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:710)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:189)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:147)
        com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:99)
        next.jdbc.connection$make_connection.invokeStatic(connection.clj:455)
        next.jdbc.connection$make_connection.invoke(connection.clj:439)
        next.jdbc.connection$eval18499$fn__18500.invoke(connection.clj:484)
        next.jdbc.protocols$eval16853$fn__16854$G__16844__16861.invoke(protocols.clj:25)
        next.jdbc$get_connection.invokeStatic(jdbc.clj:169)
        next.jdbc$get_connection.invoke(jdbc.clj:148)
        app.db$open.invokeStatic(db.clj:230)
        app.db$open.invoke(db.clj:227)
        app.migrations$apply_migrations_BANG_.invokeStatic(migrations.clj:445)
        app.migrations$apply_migrations_BANG_.invoke(migrations.clj:443)
        app.migrations$eval33608$fn__33610.invoke(migrations.clj:457)
        clojure.lang.MultiFn.invoke(MultiFn.java:234)
        integrant.core$try_build_action.invokeStatic(core.cljc:419)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
        clojure.core$alter_var_root.doInvoke(core.clj:5558)
        clojure.lang.RestFn.invoke(RestFn.java:428)
        app.main$start.invokeStatic(main.clj:544)
        app.main$start.invoke(main.clj:540)
        app.main$_main.invokeStatic(main.clj:610)
        app.main$_main.doInvoke(main.clj:602)
        clojure.lang.RestFn.invoke(RestFn.java:400)
        clojure.lang.AFn.applyToHelper(AFn.java:152)
        clojure.lang.RestFn.applyTo(RestFn.java:135)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.core$apply.invokeStatic(core.clj:667)
        clojure.main$main_opt.invokeStatic(main.clj:515)
        clojure.main$main_opt.invoke(main.clj:511)
        clojure.main$main.invokeStatic(main.clj:665)
        clojure.main$main.doInvoke(main.clj:617)
        clojure.lang.RestFn.applyTo(RestFn.java:140)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.main.main(main.java:40)

 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
    at: org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:364)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)

 →  java.net.UnknownHostException: penpot-postgresql (NioSocketImpl.java:567)
    at: sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)
        java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        java.net.Socket.connect(Socket.java:751)
        org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        org.postgresql.core.PGStream.<init>(PGStream.java:121)
        org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)
```","postgresql, kubernetes",79618040.0,"Nothing in penpot documentation says that you can use

`postgresql://penpot:penpot@postgres:5432/penpot`  syntax to set both db url and user credentials in a single variable.

As per [docs](https://help.penpot.app/technical-guide/configuration/#database), you should set:

```
PENPOT_DATABASE_USERNAME: penpot
PENPOT_DATABASE_PASSWORD: penpot
PENPOT_DATABASE_URI: postgresql://postgres/penpot
```",2025-05-12T14:19:57,2025-05-12T05:05:59
79614680,Problem with hyperledger fabric offline signing,"I'm having some trouble with the implementation of Hyperledger Fabric offline signing method with the fabric-gateway package. My code looks like this at the moment:

```
async function main() {
  const identityPath = path.join(__dirname, 'wallet/INMETROMSP/', 'te.id');
  const identityData = JSON.parse(await fs.readFileSync(identityPath, 'utf8'));
  const certificate = utf8Encoder.encode(identityData.credentials.certificate);

  const privateKeyPath = path.resolve(__dirname, 'private_key.pem');
  const privateKeyPem = fs.readFileSync(privateKeyPath, 'utf8');
  const privateKey = crypto.createPrivateKey(privateKeyPem);

  // const signer = signers.newPrivateKeySigner(privateKey);

  const configContent = fs.readFileSync('connection-org.yaml','utf-8');
  const data = yaml.load(configContent);
  const cert = data.peers['inmetro-peer0.default'].tlsCACerts.pem;
  const tlsRootCert = Buffer.from(cert);

  const client = new grpc.Client('peer0.inmetro.br:443', grpc.credentials.createSsl(tlsRootCert));
  const noOpHash = (message) => message;

  const gateway = connect({
    identity: {
      mspId: MSPID,
      credentials: certificate,
     },
    // hash: noOpHash,
    // signer,
    client,
  });

  try {
    const network = gateway.getNetwork(CHANNEL);
    const contract = network.getContract(CC_NAME);

    const vehiclePlate = ""ABC1234""
    const reportData = JSON.stringify({
      data: [4199, 4179, 2923, 2815, 3453, 3120, 1962, 1452, 3384],
    });

    const unsignedProposal = contract.newProposal('RegisterMeter', vehiclePlate, reportData);
    const proposalBytes = unsignedProposal.getBytes();
    const proposalDigest = unsignedProposal.getDigest();
    const proposalSignature = crypto.sign('sha256', proposalDigest, privateKey);
    const signedProposal = gateway.newSignedProposal(proposalBytes, proposalSignature);

    const unsignedTransaction = await signedProposal.endorse();
    const transactionBytes = unsignedTransaction.getBytes();
    const transactionDigest = unsignedTransaction.getDigest();
    const transactionSignature = signDigest(unsignedTransaction.getDigest());
    const signedTransaction = gateway.newSignedTransaction(transactionBytes, transactionSignature);

    const unsignedCommit = await signedTransaction.submit();
    const commitBytes = unsignedCommit.getBytes();
    const commitDigest = unsignedCommit.getDigest();
    const commitSignature = signDigest(unsignedCommit.getDigest());
    const signedCommit = gateway.newSignedCommit(commitBytes, commitSignature);

    const result = signedTransaction.getResult();
    const status = await signedCommit.getStatus();

    console.log(result,status);
```

However, it breaks on the first endorse of the transaction, returning an ""access denied"" message. This is the full error message:

```
EndorseError: 10 ABORTED: failed to endorse transaction, see attached details for more info
    at /home/edu/fabric-node-webserver/client/braketester/node_modules/@hyperledger/fabric-gateway/dist/client.js:37:253
    at Object.callback (/home/edu/fabric-node-webserver/client/braketester/node_modules/@hyperledger/fabric-gateway/dist/client.js:102:20)
    at Object.onReceiveStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client.js:192:36)
    at Object.onReceiveStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client-interceptors.js:360:141)
    ... 2 lines matching cause stack trace ...
    at process.processTicksAndRejections (node:internal/process/task_queues:77:11) {
  code: 10,
  details: [
    {
      address: 'peer0.inmetro.br:443',
      message: 'error validating proposal: access denied: channel [demo] creator org [INMETROMSP]',
      mspId: 'INMETROMSP'
    }
  ],
  cause: Error: 10 ABORTED: failed to endorse transaction, see attached details for more info
      at callErrorFromStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/call.js:31:19)
      at Object.onReceiveStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client.js:192:76)
      at Object.onReceiveStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client-interceptors.js:360:141)
      at Object.onReceiveStatus (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client-interceptors.js:323:181)
      at /home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/resolving-call.js:99:78
      at process.processTicksAndRejections (node:internal/process/task_queues:77:11)
  for call at
      at Client.makeUnaryRequest (/home/edu/fabric-node-webserver/client/braketester/node_modules/@grpc/grpc-js/build/src/client.js:160:32)
      at /home/edu/fabric-node-webserver/client/braketester/node_modules/@hyperledger/fabric-gateway/dist/client.js:37:62
      at new Promise (<anonymous>)
      at GatewayClientImpl.endorse (/home/edu/fabric-node-webserver/client/braketester/node_modules/@hyperledger/fabric-gateway/dist/client.js:37:16)
      at ProposalImpl.endorse (/home/edu/fabric-node-webserver/client/braketester/node_modules/@hyperledger/fabric-gateway/dist/proposal.js:43:52)
      at async main (/home/edu/fabric-node-webserver/client/braketester/teste.js:61:33) {
    code: 10,
    details: 'failed to endorse transaction, see attached details for more info',
    metadata: Metadata { internalRepr: [Map], options: {} }
  },
  transactionId: '9276e29c9c4e29e40eca631ad3c568415ec345cd2952c05aee5ce814d0d33a3d'
}
```

And this is the error shown on the peer log:

```
2025-05-09 18:14:17.377 UTC 02ca WARN [endorser] Validate -> access denied: creator's signature over the proposal is not valid channel=demo txID=9276e29c mspID=INMETROMSP error=""The signature is invalid"" errorVerbose=""The signature is invalid\ngithub.com/hyperledger/fabric/msp.(*identity).Verify\n\t/msp/identities.go:202\ngithub.com/hyperledger/fabric/core/endorser.(*UnpackedProposal).Validate\n\t/core/endorser/msgvalidation.go:189\ngithub.com/hyperledger/fabric/core/endorser.(*Endorser).preProcess\n\t/core/endorser/endorser.go:258\ngithub.com/hyperledger/fabric/core/endorser.(*Endorser).ProcessProposal\n\t/core/endorser/endorser.go:335\ngithub.com/hyperledger/fabric/core/handlers/auth/filter.(*expirationCheckFilter).ProcessProposal\n\t/core/handlers/auth/filter/expiration.go:61\ngithub.com/hyperledger/fabric/core/handlers/auth/filter.(*filter).ProcessProposal\n\t/core/handlers/auth/filter/filter.go:32\ngithub.com/hyperledger/fabric/internal/pkg/gateway.(*EndorserServerAdapter).ProcessProposal\n\t/internal/pkg/gateway/gateway.go:43\ngithub.com/hyperledger/fabric/internal/pkg/gateway.(*Server).planFromFirstEndorser.func1\n\t/internal/pkg/gateway/endorse.go:205\nruntime.goexit\n\t/usr/local/go/src/runtime/asm_amd64.s:1700"" identity=""(mspid=INMETROMSP subject=CN=te,OU=client,O=Internet Widgits Pty Ltd,ST=Some-State,C=AU issuer=CN=ca,OU=Tech,O=Kung Fu Software,STREET=Alicante,L=Alicante,C=ES serialnumber=723095516253312706461993611318417152021379977104)""
2025-05-09 18:14:17.377 UTC 02cb WARN [endorser] ProcessProposal -> Failed to preProcess proposal error=""error validating proposal: access denied: channel [demo] creator org [INMETROMSP]""
2025-05-09 18:14:17.377 UTC 02cc WARN [gateway] func1 -> Endorse call to endorser failed channel=demo chaincode=braketester-external txID=9276e29c9c4e29e40eca631ad3c568415ec345cd2952c05aee5ce814d0d33a3d endorserAddress=peer0.inmetro.br:443 endorserMspid=INMETROMSP error=""error validating proposal: access denied: channel [demo] creator org [INMETROMSP]""
```

I know that the certificate is correct, because using the Signer method works fine, but the offline signing method returns this error always. I saw some other posts about this error being caused by the certificate being sent to the client method as an string instead of an utf8 object, but mine seems correct. Is there anything I'm missing?","kubernetes, hyperledger-fabric, hyperledger, digital-signature, hyperledger-bevel",79615877.0,"If you don't specify a [hash](https://hyperledger.github.io/fabric-gateway/main/api/node/interfaces/ConnectOptions.html#hash) in the options passed to the `connect()` call, SHA-256 is used by default. This means that `unsignedProposal.getDigest()` will return a SHA-256 hash of the proposal message. You are then using the Node crypto [`sign()`](https://nodejs.org/docs/latest-v22.x/api/crypto.html#cryptosignalgorithm-data-key-callback), which first creates a SHA-256 hash of the supplied data, then encrypts that hash with your ECDSA private key. You have created a signature for a message hash, not the for the message itself.

If you want to use the Node crypto library to create the signature, you will need to specify the [none](https://hyperledger.github.io/fabric-gateway/main/api/node/variables/hash.none.html) hash implementation (or your own `noOpHash`) in the connect options so that `unsignedProposal.getDigest()` returns the full proposal message for you to pass to `crypto.sign()`. Alternatively, you could use the Signer obtained from [`newPrivateKeySigner()`](https://hyperledger.github.io/fabric-gateway/main/api/node/functions/signers.newPrivateKeySigner.html), which uses a JavaScript ECDSA signing implementation that operates on a pre-computed hash / digest.",2025-05-10T20:10:36,2025-05-09T18:22:10
79614460,ingress controller does not serve pages after update to 1.12.x,"I have bumped into problems after update of our [nginx ingress](https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx) from version `1.11.5` (helm chart version `4.11.5`) to `1.12.2` (helm chart version `4.12.2`).

Basically I have ingress that is working with nginx `1.11.5` and prior versions without any problems (see template bellow), but when I upgrade it, I am only getting 404s from ingress.

When I remove `configuration-snippet` annotation from ingress template entirely, the webpage is displayed, but it has wrong `ContentSecurityPolicy` header because this header is also specified globally via `controller.addHeaders`.

I know that there were signifficant changes in order to fix these CVEs: CVE-2025-1097 CVE-2025-1098 CVE-2025-1974 CVE-2025-24513 and CVE-2025-24514 ([https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1)) so this is probably related to that

In order to upgrade to `1.11.5` previously I had to enable `controller.allowSnippetAnnotations` so for `1.12.2` upgrade this value is also set up: `controller.allowSnippetAnnotations: true`

How can I enable the `*-snippet` annotations again? Is it even possible due to the security related changes? Or is there any better way how to specify headers?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers ContentSecurityPolicy ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
      more_set_headers ""Cache-Control: no-store"";
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""false""
    nginx.ingress.kubernetes.io/cors-allow-headers: Authorization, Content-Type
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, PUT, DELETE, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: https://<host>,http://localhost:3000
    nginx.ingress.kubernetes.io/cors-max-age: ""3600""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""8""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.org/proxy-pass-headers: IDAM_USER,IDAM-USER
    nginx.org/server-tokens: ""False""
  labels:
    app: <release-name>
    app.kubernetes.io/managed-by: Helm
  name: <release-name>
  namespace: <namespace>
spec:
  ingressClassName: <ingressclass-name>
  rules:
  - host: <host>
    http:
      paths:
      - backend:
          service:
            name: <release-name>
            port:
              number: 80
        path: /(.*)
        pathType: Prefix
  tls:
  - hosts:
    - ‎<host>
    secretName: <secret-name>
```","kubernetes, nginx, kubernetes-ingress",79618557.0,"There's a new feature in `ingress-nginx 1.12` that allows you to filter annotations by risk using annotations-risk-level. Use `annotations-risk-level: Critical` to allow `allow-snippet-annotations: true.`

For further reference you can check this [blog](https://blog.nuvotex.de/nginx-allow-snippet-annotations/) and [discussion](https://github.com/rook/rook/issues/13449#issuecomment-2834968219).",2025-05-12T19:57:23,2025-05-09T15:42:45
79614460,ingress controller does not serve pages after update to 1.12.x,"I have bumped into problems after update of our [nginx ingress](https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx) from version `1.11.5` (helm chart version `4.11.5`) to `1.12.2` (helm chart version `4.12.2`).

Basically I have ingress that is working with nginx `1.11.5` and prior versions without any problems (see template bellow), but when I upgrade it, I am only getting 404s from ingress.

When I remove `configuration-snippet` annotation from ingress template entirely, the webpage is displayed, but it has wrong `ContentSecurityPolicy` header because this header is also specified globally via `controller.addHeaders`.

I know that there were signifficant changes in order to fix these CVEs: CVE-2025-1097 CVE-2025-1098 CVE-2025-1974 CVE-2025-24513 and CVE-2025-24514 ([https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1)) so this is probably related to that

In order to upgrade to `1.11.5` previously I had to enable `controller.allowSnippetAnnotations` so for `1.12.2` upgrade this value is also set up: `controller.allowSnippetAnnotations: true`

How can I enable the `*-snippet` annotations again? Is it even possible due to the security related changes? Or is there any better way how to specify headers?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers ContentSecurityPolicy ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
      more_set_headers ""Cache-Control: no-store"";
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""false""
    nginx.ingress.kubernetes.io/cors-allow-headers: Authorization, Content-Type
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, PUT, DELETE, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: https://<host>,http://localhost:3000
    nginx.ingress.kubernetes.io/cors-max-age: ""3600""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""8""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.org/proxy-pass-headers: IDAM_USER,IDAM-USER
    nginx.org/server-tokens: ""False""
  labels:
    app: <release-name>
    app.kubernetes.io/managed-by: Helm
  name: <release-name>
  namespace: <namespace>
spec:
  ingressClassName: <ingressclass-name>
  rules:
  - host: <host>
    http:
      paths:
      - backend:
          service:
            name: <release-name>
            port:
              number: 80
        path: /(.*)
        pathType: Prefix
  tls:
  - hosts:
    - ‎<host>
    secretName: <secret-name>
```","kubernetes, nginx, kubernetes-ingress",79618503.0,"Here is what we have configured in our Helm Chart `ingress-nginx-4.12.1` to enable config snippets.

```
 proxySetHeaders:
    allow-snippet-annotations: ""true""
```

```
podAnnotations:
    ingressclass.kubernetes.io/is-default-class: ""true""
    allow-snippet-annotations: ""true""
```",2025-05-12T19:02:52,2025-05-09T15:42:45
79614460,ingress controller does not serve pages after update to 1.12.x,"I have bumped into problems after update of our [nginx ingress](https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx) from version `1.11.5` (helm chart version `4.11.5`) to `1.12.2` (helm chart version `4.12.2`).

Basically I have ingress that is working with nginx `1.11.5` and prior versions without any problems (see template bellow), but when I upgrade it, I am only getting 404s from ingress.

When I remove `configuration-snippet` annotation from ingress template entirely, the webpage is displayed, but it has wrong `ContentSecurityPolicy` header because this header is also specified globally via `controller.addHeaders`.

I know that there were signifficant changes in order to fix these CVEs: CVE-2025-1097 CVE-2025-1098 CVE-2025-1974 CVE-2025-24513 and CVE-2025-24514 ([https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1)) so this is probably related to that

In order to upgrade to `1.11.5` previously I had to enable `controller.allowSnippetAnnotations` so for `1.12.2` upgrade this value is also set up: `controller.allowSnippetAnnotations: true`

How can I enable the `*-snippet` annotations again? Is it even possible due to the security related changes? Or is there any better way how to specify headers?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers ContentSecurityPolicy ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
      more_set_headers ""Cache-Control: no-store"";
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""false""
    nginx.ingress.kubernetes.io/cors-allow-headers: Authorization, Content-Type
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, PUT, DELETE, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: https://<host>,http://localhost:3000
    nginx.ingress.kubernetes.io/cors-max-age: ""3600""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""8""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.org/proxy-pass-headers: IDAM_USER,IDAM-USER
    nginx.org/server-tokens: ""False""
  labels:
    app: <release-name>
    app.kubernetes.io/managed-by: Helm
  name: <release-name>
  namespace: <namespace>
spec:
  ingressClassName: <ingressclass-name>
  rules:
  - host: <host>
    http:
      paths:
      - backend:
          service:
            name: <release-name>
            port:
              number: 80
        path: /(.*)
        pathType: Prefix
  tls:
  - hosts:
    - ‎<host>
    secretName: <secret-name>
```","kubernetes, nginx, kubernetes-ingress",79618035.0,"A better way to inject response headers is by using `nginx.ingress.kubernetes.io/custom-headers` annotation, as documented at [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers)

For the headers you want, define the following configmap first:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-ingress-headers
  namespace: your-namespace
data:
  ContentSecurityPolicy: ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
  Cache-Control: no-store
```

And use it in your ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/custom-headers: your-namespace/custom-ingress-headers
```

Please do note that in the linked page it is specifically stated that: `This annotation uses more_set_headers nginx directive.`",2025-05-12T14:18:19,2025-05-09T15:42:45
79614412,How to configure ArgoCD in Minikube for Google OIDC using non-standard ports?,"# Tldr

I'm installing ArgoCD into Minikube using the Helm chart, and `redirectURI` seems to be ignored. Logging into ArgoCD with Google returns ...

```
Invalid redirect URL: the protocol and host (including port) must match and the path must be within allowed URLs if provided
```

Here is my `values.yaml` file.

```
...
configs:
  cm:
    dex.config: |
      connectors:
        - type: oidc
          id: google
          name: Google
          config:
            issuer: https://accounts.google.com
            clientID: ${GOOGLE_OIDC_CLIENT_ID}
            clientSecret: ${GOOGLE_OIDC_CLIENT_SECRET}
            redirectURI: https://argocd.${DOMAIN}:${HTTPS_PORT}/api/dex/callback
...
```

The port is likely the issue.

## Update

It seems like the OIDC flow isn't even making it to Google. This might be an issue with the interaction between ArgoCD and Dex.

# The Whole Story

I am using Terraform to make a local replica of my EKS environment using Minikube for local development. I want my devs to be able to run a single shell script that applies multiple layers of Terraform, and when it's done, ArgoCD is running and configured to pull our Kubernetes manifests from GitHub and sync them to the Minikube cluster. I have all this working, but it requires `minikube tunnel` to access anything running inside the cluster from the host.

I'm not crazy about the fact that my developers will have to run `minikube tunnel`. I can see them tripping over that often. And since Minikube can't (easily) bind to privileged ports like 80 and 443, I'm binding to it to host ports 30080 and 30443 instead. This works. I can access ArgoCD on 30443 without needing `minikube tunnel`.

The problem is that when you try to log in with Google, you get ...

```
Invalid redirect URL: the protocol and host (including port) must match and the path must be within allowed URLs if provided
```

In the `argo-dex-server` logs, I found ...

```
level=INFO msg=""config issuer"" issuer=https://argocd.REDACTED/api/dex
```

It looks like ArgoCD isn't using the port in OIDC interactions. This works in EKS where everything is running on standard ports. It also works locally using `minikube tunnel` since again everything is on standard ports. But it doesn't work locally without `minikube tunnel` since I have to use non-standard ports.

1. Am I doing this correctly?
2. Is there another approach I should look into to allow my devs to run ArgoCD in Minikube and be able to log in with Google?","kubernetes, openid-connect, minikube, argocd",,,,2025-05-09T15:12:30
79614366,Laravel on Kubernetes: storage/logs/laravel.log permission denied despite setting permissions in Dockerfile,"I'm running a Laravel 11 application in a Kubernetes cluster, using a custom Dockerfile and deploying via GitHub Actions to AWS EKS.

Despite setting proper permissions for the `storage` directory in the Dockerfile, I consistently get the following error when the pod starts:

```
The stream or file ""/var/www/storage/logs/laravel.log"" could not be opened in append mode: Failed to open stream: Permission denied
The exception occurred while attempting to log: The stream or file ""/var/www/storage/logs/laravel.log"" could not be opened in append mode: Failed to open stream: Permission denied
```

To temporarily fix this, I have to manually SSH into the pod and run:

```
chmod -R 755 /var/www/storage
```

Here’s what I already do in my Dockerfile:

```
# Set permissions
RUN chown -R www:www-data /var/www/storage
RUN chmod -R ug+w /var/www/storage
RUN chmod -R 755 /var/www/storage
```

And I also run this in my GitHub Actions workflow before building the Docker image:

```
- name: Set Permissions
  run: chmod -R 755 storage bootstrap/cache
```

Still, the error persists every time the container is deployed.

Why does the Laravel container keep throwing a permission denied error for storage/logs/laravel.log, even though I'm setting permissions in the Dockerfile and GitHub Actions?

Is there something I’m missing, like user permissions in the Kubernetes runtime, volume mounts overwriting permissions, or something else?

Any insight or a permanent fix would be greatly appreciated.","laravel, docker, kubernetes",,,,2025-05-09T14:46:53
79613903,K8s node.js pod setup env.js from .yaml deployment,"I have a container where i released a node.js frontend, some of my envs are stored in an env.js, how can i configure this envs from the yaml file of the deployment (using the env in deployment is not working).

In alternative i tried setupping a pvc to mount in the pod the env.js that i deposit in the pv but it is not working as the file is copied as a directory and idk why.

**env.js:**

```
window.env = { ""API_URL"": ""http://ip:port"" }
```","reactjs, node.js, kubernetes",79614136.0,"I ended up finding that what David said was on the right track, apparently the file env.js cant be in the same folder as the application, but if you set it in a subfolder for example env/env.js and configuring the ConfigMap to write the file actually works.

ConfigMap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cfg-map
data:
  env.js: |
    window.env = {
      ""API_URL"": ""http://ip:port""
    }
```

Deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment
spec:
  ...
  selector:
    spec:
      ...
      volumeMounts:
      - name: storage
        mountPath: /usr/share/nginx/html/env
    volumes:
    - name: storage
      configMap:
        name: cfg-map
        items:
        - key: ""env.js""
          path: ""env.js""
```",2025-05-09T12:42:44,2025-05-09T10:10:57
79612671,Error when trying to migrate a GKE Autopilot cluster to cgroupv2,"We're trying to migrate our GKE Autopilot cluster from `cgroupv1` to `cgroupv2` following the official documentation ([https://cloud.google.com/kubernetes-engine/docs/how-to/migrate-cgroupv2#autopilot_1](https://cloud.google.com/kubernetes-engine/docs/how-to/migrate-cgroupv2#autopilot_1)), but we're encountering an error.

Cluster Details:

- Region: `europe-west2`
- GKE Version: `1.31.6-gke.1064001`
- Current cgroup mode: `EFFECTIVE_CGROUP_MODE_V1`
- Current COS version: `cos-117-18613-164-38`

Steps Taken:

1. Verified the current `cgroup` mode using: `gcloud container clusters describe development --format='value(nodePools[0].config.effectiveCgroupMode)' --region=europe-west2`    which returned `EFFECTIVE_CGROUP_MODE_V1`
2. Followed the documentation to update the cluster using: `gcloud container clusters update <CLUSTER_NAME> --autoprovisioning-cgroup-mode=v2 --region=europe-west2`
3. Received error: `ERROR: (gcloud.container.clusters.update) ResponseError: code=400, message=INVALID_ARGUMENT: invalid node_pool_auto_config.linux_node_config. Allowed fields are: [""cgroup_mode""]`

We've tried to update `gcloud` CLI to the latest version, with the same results.

We couldn't find anything online related to this.

Any ideas?","kubernetes, google-cloud-platform, google-kubernetes-engine, gke-autopilot",79614938.0,"You are encountering the error `ERROR: (gcloud.container.clusters.update) ResponseError: code=400, message=INVALID\_ARGUMENT: invalid node\_pool\_auto\_config.linux\_node\_config. Allowed fields are: [""cgroup\_mode""]` as there is an existing bug in GKE that prevents upgrading Autopilot clusters to `cgroupv2`. A fix is already expected to be in production within approximately two weeks. You can check the [GKE Release Notes page](https://cloud.google.com/kubernetes-engine/docs/release-notes) from time to time for updates.

In the meantime, as a workaround, since your cluster was created with GKE version 1.31, you can also try [upgrading your GKE version](https://cloud.google.com/sdk/gcloud/reference/container/clusters/upgrade) to at least 1.33 to enable the migration to `cgroupv2` automatically. However, as per [documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/migrate-cgroupv2#check-cgroup-mode), manual migration of cgroup modes is not supported in Autopilot clusters. If you can't wait for the next release in two weeks, you have the option to create a new Autopilot cluster with an updated version and migrate your workloads to it.",2025-05-09T22:16:18,2025-05-08T14:56:08
79611585,How to update reclaimpolicy in k8s storageclass,"Because of the company's needs, I need to change the `reclaimpolicy` attribute value of the storage class in the online k8s cluster from `Delete` to `Retain`.

But when I update and save it through `kubectl edit`, it shows that updating this field is **prohibited**.

Some sample information is as follows

```
# kubectl get sc
NAME                   PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
local-path (default)   rancher.io/local-path   Delete          WaitForFirstConsumer   false                  1y

# kubectl get pvc -n localtest
NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
redis-data-drc-redis-cluster-0-0   Bound    pvc-ba672bea-f80b-4f3f-a536-2975e24bbd13   5Gi        RWO            local-path     30d
redis-data-drc-redis-cluster-0-1   Bound    pvc-8cbfc2d3-60e8-46d0-85c2-e8ba3de428fd   5Gi        RWO            local-path     30d
redis-data-drc-redis-cluster-1-0   Bound    pvc-72c9f23b-bf11-4b6e-87d3-f67e52a7ed51   5Gi        RWO            local-path     30d
redis-data-drc-redis-cluster-1-1   Bound    pvc-6c850fca-cd19-4e39-ba89-6a8f08ca0b24   5Gi        RWO            local-path     30d
redis-data-drc-redis-cluster-2-0   Bound    pvc-a2b4430d-74b2-494a-a849-2ae34b792fb7   5Gi        RWO            local-path     30d
redis-data-drc-redis-cluster-2-1   Bound    pvc-14e1021c-c3af-4870-a7d7-89b39b3ab691   5Gi        RWO            local-path     30d

# kubectl get pv  | grep redis
pvc-14e1021c-c3af-4870-a7d7-89b39b3ab691   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-2-1    local-path                   30d
pvc-53d9d22b-cd17-4e6a-b4fa-260524c1993e   1Gi        RWO            Delete           Bound      default/redis-standalone-redis-standalone-0   local-path                   657d
pvc-6c850fca-cd19-4e39-ba89-6a8f08ca0b24   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-1-1    local-path                   30d
pvc-72c9f23b-bf11-4b6e-87d3-f67e52a7ed51   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-1-0    local-path                   30d
pvc-8cbfc2d3-60e8-46d0-85c2-e8ba3de428fd   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-0-1    local-path                   30d
pvc-a2b4430d-74b2-494a-a849-2ae34b792fb7   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-2-0    local-path                   30d
pvc-ba672bea-f80b-4f3f-a536-2975e24bbd13   5Gi        RWO            Delete           Bound      localtest/redis-data-drc-redis-cluster-0-0    local-path                   30d
```

How can I update the `reclaimpolicy` attribute value without affecting the service.

I searched for resources and found that it was said to force replacement through the resource list file. The example is as follows

```
kubectl replace -f new-sc.yaml --force
```

Is this the best solution? If so, do I need to modify some of the configurations of both **pvc** and **pv**? If not, please provide your proposal. looking forward to your help.",kubernetes,79612845.0,"You cannot directly edit the [reclaim policy](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#retain) since changing the StorageClass's reclaimPolicy will only affect newly created PVs. It will not change the persistentVolumeReclaimPolicy of your existing PVs.

The recommended approach is to create a new StorageClass with the ""Retain"" policy then migrate your existing PVCs using this command:

1. List the PersistentVolumes in your cluster:

```
kubectl get pv
```
2. Choose one of your PersistentVolumes and change its reclaim policy:

```
 kubectl patch pv <your-pv-name> -p '{""spec"":{""persistentVolumeReclaimPolicy"":""Retain""}}'
```
3. Verify that your chosen PersistentVolume has the right policy:

```
kubectl get pv
```

For further reference you can refer to this [documentation.](https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/#changing-the-reclaim-policy-of-a-persistentvolume)",2025-05-08T16:53:54,2025-05-08T01:05:00
79611352,Cannont add dynamic volume with lua and fluentbit,"I’m encountering an issue with routing logs into multiple namespaces using Fluent Bit. I’ve written a Lua filter to generate dynamic file paths:

```
function cb_map_tag(tag, ts, record)
    local ns = (record.kubernetes and record.kubernetes.namespace_name) or ""default""
    local t  = ""firstlog""

    record[""log_dir""]  = ""/test1/"" .. ns .. ""/logs""
    record[""log_file""] = t .. "".log""
    return 2, ts, record, tag
end
```

And here is the relevant section of my Fluent Bit configuration:

```
[FILTER]
Name   lua
Match  *
Script /fluent-bit/scripts/dynamic_path.lua
Call   cb_map_tag

[OUTPUT]
Name     file
Match    *
Format   template
Template {{log}}
Path     {{log_dir}}
File     {{log_file}}
Mkdir    true
```

My goal is to write each record’s log field to a plain file under the dynamically generated path, but this setup isn’t producing any output. Could you help me identify what’s wrong with my file output configuration?

Thank you.","kubernetes, lua, fluent-bit",,,,2025-05-07T20:09:44
79611111,How to avoid GKE evicts job pod and results in Prefect flow run failure?,"We use Prefect (v3.0) in our data platform to orchestrate pipelines. We use GKE (Kubernetes) for Prefect workers and flow run deployment. Our GEK cluster is in auto scale (horizontal) mode.

In the situation GKE AutoScaler scale down the node where the prefect flow job pod is running, it will result in flow run failure with the following log

```
  ""event"": ""prefect.kubernetes.pod.evicted"",
```

How to mitigate that?","kubernetes, google-kubernetes-engine, prefect",79611125.0,"In general, this could happens to any K8s job pod when a k8s auto-scaler or GKE AutoPilot is scaling down the cluster.

In particular, it also happens to Prefect flow runs on k8s/GKE.

To mitigate it, we can tell AutoScaler/AutoPilot to avoid evicting those flow run pods.

[https://cloud.google.com/kubernetes-engine/docs/how-to/extended-duration-pods](https://cloud.google.com/kubernetes-engine/docs/how-to/extended-duration-pods)

In Prefect v 3.0, you could do it by creating the Prefect deployment with specific `job_variables`

[https://docs-3.prefect.io/v3/deploy/infrastructure-concepts/work-pools#base-job-template](https://docs-3.prefect.io/v3/deploy/infrastructure-concepts/work-pools#base-job-template)

```
  ""job_configuration"": {
    ...
    ""job_manifest"": {
      ...
      ""spec"": {
        ""template"": {
          ""metadata"": {
            ""annotations"": {
              ""cluster-autoscaler.kubernetes.io/safe-to-evict"": ""false""
            },
          },
```",2025-05-07T17:32:12,2025-05-07T17:25:43
79610919,How to run k8s lightweight when developing controllers on slow PC,"I am using an old laptop with Intel dual-core T2390 @1.86GHz and 3GB RAM to play with k8s like dev controllers and operators. I managed to run minikube but my PC gets very slow, especially when trying to share screen. I am trying to come up with a lighter way to run k8s like run only `api` and `etcd` but afraid I will lose options and add complexity to my development like `kubeconfig`. Is there any suggestion on how to run nodeless k8s development environment?","go, kubernetes, minikube, kubernetes-apiserver",79611240.0,"Ah, wrestling with resource-hungry Kubernetes on older hardware is a familiar challenge! You're right, a full-fledged Minikube can really bog things down on a dual-core with 3GB of RAM. Running *just* the API server and etcd is an interesting idea, but as you suspect, it comes with its own set of complexities, especially around `kubeconfig` and actually deploying workloads.

Fear not, there are definitely lighter-weight approaches to running a Kubernetes development environment that might just breathe some life back into your laptop. Here are a few suggestions, ranging from still-Kubernetes-ish to more lightweight alternatives:

**1. K3s: Lightweight Kubernetes**

This is probably the most direct and popular answer to your need for a lighter Kubernetes distribution.

- **Why it's lighter:** K3s is a fully compliant Kubernetes distribution but significantly reduces the resource footprint by:
  - Combining several control plane components into a single binary.
  - Replacing etcd with SQLite as the default datastore (though you can still configure it to use etcd).
  - Removing many in-tree storage drivers and cloud providers.
- **Benefits for you:**
  - **Much lower resource consumption:** It's designed to run on resource-constrained environments.
  - **Still Kubernetes API compliant:** You'll be working with the familiar Kubernetes API, `kubectl`, and your existing manifests should largely work.
  - **Simple to install:** Installation is typically a single command.
  - **`kubeconfig` works:** It generates a standard `kubeconfig` file, so your existing tooling will likely work without modification.
- **Considerations:** While it's Kubernetes, some advanced features or specific cloud provider integrations might not be available by default. However, for local development and testing controllers/operators, it's usually more than sufficient.

**2. MicroK8s: Another Lightweight Option**

MicroK8s, from Canonical (the makers of Ubuntu), is another excellent choice for a minimal Kubernetes.

- **Why it's lighter:** It focuses on a small, self-contained Kubernetes deployment using snap packages.
- **Benefits for you:**
  - **Small footprint:** Designed to be lightweight and easy to install.
  - **Includes useful add-ons:** It offers easy enablement of common features like DNS, dashboard, ingress, and storage via `microk8s enable <addon>`.
  - **Standard Kubernetes API:** Fully compliant with the Kubernetes API.
  - **`kubeconfig` support:** Generates a standard `kubeconfig`.
- **Considerations:** Being a snap package, it might have some specific behaviors related to snap updates and permissions, but these are generally well-managed.

**3. Kind (Kubernetes in Docker): For Local Clusters**

Kind uses Docker to run Kubernetes nodes as containers.

- **Why it's lighter (in some ways):** It avoids the overhead of a full virtual machine, as Minikube often uses.
- **Benefits for you:**
  - **Fast startup:** Clusters can be created and destroyed quickly.
  - **Relatively low resource usage:** While it still runs full Kubernetes components, the containerized approach can be more efficient than a VM for some workloads.
  - **Good for CI/CD and local testing:** Excellent for quickly spinning up and tearing down clusters.
  - **Standard `kubeconfig`:** Provides a standard `kubeconfig`.
- **Considerations:** Since it runs on Docker, you'll need Docker installed. Networking can sometimes have subtle differences compared to a VM-based setup.

**4. DevSpace or Garden: Development Tools with Embedded Clusters (Optional)**

These are more development-focused tools that can simplify the local Kubernetes development workflow.

- **How they help:** They often manage the underlying Kubernetes cluster (which could be Minikube, K3s, Kind, or even a remote cluster) and provide features like hot reloading, automated deployments, and log streaming.
- **Benefits for you:**
  - **Improved developer experience:** Streamlines the inner development loop.
  - **Can work with lighter Kubernetes distributions:** They can be configured to use K3s or Kind.
- **Considerations:** They add another layer of abstraction, so you'll need to learn their specific concepts. They don't inherently make Kubernetes lighter, but they can make working with it more efficient on limited resources.

**Regarding your idea of running *only* the API server and etcd:**

While technically possible, this would indeed introduce significant complexity for your development workflow:

- **No Nodes:** Without worker nodes, you can't actually run any of your controllers or operators as Pods.
- **Manual Resource Management:** You'd have to manually manage the lifecycle and dependencies of your development components outside of Kubernetes.
- **`kubeconfig` Challenges:** While you could potentially generate a `kubeconfig` to talk to the API server, it wouldn't reflect a standard Kubernetes cluster setup, potentially leading to inconsistencies when you deploy to a real cluster.
- **Loss of Kubernetes Abstractions:** You'd lose the benefits of Kubernetes scheduling, resource management, and self-healing capabilities for your development environment.

**Recommendation:**

For your use case of developing controllers and operators on an older laptop, **K3s or MicroK8s are likely your best bets.** They provide a significantly lighter Kubernetes experience while still being fully API-compatible and providing a standard `kubeconfig`.

I'd suggest trying out K3s first due to its simplicity and minimal dependencies. You can usually get a basic cluster up and running with a single command.

Give one of these lightweight distributions a try, and hopefully, you'll find a much smoother Kubernetes development experience on your trusty laptop! Let me know if you have any more questions as you explore these options.",2025-05-07T18:44:14,2025-05-07T15:33:13
79610759,Registering kafka_ui users using Keycloak -- feeling blocked,"This is my first post, I hope that somebody can help me...

Here's my problem :

I want to configure Kafka_ui, using Keycloak users as ""admin"" or ""readonly"" role, to centralize RBAC.

I tried a lot for configmap mounted as volume with my deployment.
The service.yaml is a classic clusterIP (port 9000 targetport 8080)

this is my configmal.yaml

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: kafka-ui-config
  namespace: monitoring
data:
  application.yml: |
    kafka:
      clusters:
        - name: clusters
          bootstrapServers: ip_kafka_vm:9092

    auth:
      type: OAUTH2
      oauth2:
        client:
          keycloak:
            clientId: kafka-ui
            clientSecret: secret_password
            scope:
              - openid
            issuer-uri: https://ip_kafka_vm/realms/master
            user-name-attribute: preferred_username
            client-name: keycloak
            provider: keycloak
            custom-params:
              roles-field: realm_access.roles

    rbac:
      enabled: true
      roles:
        - name: ""admin""
          clusters:
            - clusters
          subjects:
            - provider: oauth
              type: role
              value: ""admin""
            - provider: oauth
              type: user
              value: user1
          permissions:
            - resource: applicationconfig
              actions: [ ""all"" ]

            - resource: clusterconfig
              actions: [ ""all"" ]

            - resource: topic
              value: "".*""
              actions: [ ""all"" ]

            - resource: consumer
              value: "".*""
              actions: [ ""all"" ]

            - resource: schema
              value: "".*""
              actions: [ ""all"" ]

            - resource: connect
              value: "".*""
              actions: [ ""all"" ]

            - resource: ksql
              actions: [ ""all"" ]

        - name: ""readonly""
          clusters:
            - clusters
          subjects:
            - provider: oauth
              type: role
              value: ""readonly""
            - provider: oauth
              type: user
              value: user2
          permissions:
            - resource: clusterconfig
              actions: [ ""view"" ]

            - resource: topic
              value: "".*""
              actions: [ ""view"" ]

            - resource: consumer
              value: "".*""
              actions: [ ""view"" ]

            - resource: schema
              value: "".*""
              actions: [ ""view"" ]

            - resource: connect
              value: "".*""
              actions: [ ""view"" ]
```

now the deployment.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: monitoring
  labels:
    my_network/broker-network: ""true""
    my_service: my-kafka-ui
  name: my-kafka-ui
spec:
  replicas: 1
  selector:
    matchLabels:
      my_service: my-kafka-ui
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        my_network/broker-network: ""true""
        my_service: my-kafka-ui
    spec:
      containers:
        - name: my-kafka-ui
          image: my_kafka-ui:version
          ports:
            - containerPort: 8080
          envFrom:
            - configMapRef:
                name: my.configmap
          env:
            - name: LOGGING_LEVEL_ROOT
              value: ""debug""
            - name: SPRING_CONFIG_LOCATION
              value: ""file:/etc/kafkaui/application.yml""
          volumeMounts:
            - name: config-volume
              mountPath: /etc/kafkaui/application.yml
              subPath: application.yml
          resources: {}
      volumes:
        - name: config-volume
          configMap:
            name: kafka-ui-config
      restartPolicy: Always
```

What do I miss ?

Thanks for your time,

Psowl

I wish my connection on kafka_ui interface was possible with keycloak_users to centralize RBAC control.","kubernetes, apache-kafka, keycloak, rbac",,,,2025-05-07T14:19:45
79610675,Wait until Workqueue of controller-runtime is empty,"I write a controller in Go.

My test (with envTest) uses `EnqueueRequestsFromMapFunc`.

I want to be sure that my function triggered the reconcile, and not another event which was still in the queue.

Currently, I do `time.Sleep(1)`, which seems to work. But I would like to avoid that.

Is there a way to wait until all pending events have been processed?

Some pseudo code:

I have resources `Item` and `Container`.

Items get created, and some logic puts Items into one Container.

> Parent resource watches Child resources

```
    err = ctrl.NewControllerManagedBy(mgr).
        WithOptions(options).
        For(&foov1.Container{}).
        Watches(
            &clusterv1.Item{},
            handler.EnqueueRequestsFromMapFunc(handler.EnqueueRequestsFromMapFunc(r.ItemToContainer(ctx, log))),
        ).
```

ItemToContainer() checks which container the new item belongs to, and triggers the reconcile of the matching container.

I want to test that `ItemToContainer()` works.

```
Expect(testEnv.Create(ctx, container)).To(Succeed())
time.Sleep(1)
Expect(testEnv.Create(ctx, item)).To(Succeed())
```

The sleep is necessary, because creating the container triggers a reconcile. If that happens after creation of the child, then ChildToContainer() is not needed.

I want to ensure ChildToContainer() works for a container which is already created (and all Reconcile events have been processed).","go, kubernetes, controller-runtime, envtest",79610874.0,"I would guess that you can do it in `for` loop and inside the loop it will `wait` in the `workqueue.Get()` until an item added you break when shutdown check this implementation : [https://github.com/kubernetes/sample-controller/blob/e3aa2202834f61b1c8415b33dd801d4417957010/controller.go#L193](https://github.com/kubernetes/sample-controller/blob/e3aa2202834f61b1c8415b33dd801d4417957010/controller.go#L193)

Here is some simple try I was also playing with workqueue but it depends on your implementation here I am not running process in go routine but you can figure out how to do best reference would be `sample-controller` code

you can also post code to help understand your implementation.

```
1   package main
  1
  2 import (
  3         ""fmt""
  4         ""time""
  5
  6         ""k8s.io/client-go/util/workqueue""
  7 )
  8
  9 func FillQueue(q workqueue.Interface) {
 10         time.Sleep(1 * time.Second)
 11         q.Add(""A"")
 12         q.Add(""B"")
 13         q.Add(""C"")
 14
 15 }
 16
 17 func processingQueue(q workqueue.Interface) bool {
 18         item, _ := q.Get()
 19         fmt.Printf(""Processing %s .. \n"", item)
 20         q.Done(item)
 21         return true
 22 }
 23
 24 func main() {
 25         q := workqueue.New()
 26         go FillQueue(q)
 27         for processingQueue(q) {
 28         }
 29 }
~
```",2025-05-07T15:12:36,2025-05-07T13:42:38
79609973,Trigger knative jobSink from external source,"I want to use jobSinks that can be triggered from external sources. By default, jobSinks can only be triggered from inside the kubernetes cluster (svc.cluster.local address).

For example I want to trigger the jobSink with a CURL from outside the kubernetes cluster. But so far I'm not able to expose it.

My hope with jobSink was, that I wont need a 24/7 running container that only listens for incoming requests and then triggers a job. Is it even possible at all?","kubernetes, knative",79610415.0,"If running a container 24/7 that only listens for incoming requests then triggers a job and securing services for exposure are the concerns, it is possible. I suggest an [Event-Driven](https://knative.dev/docs/getting-started/first-trigger/#creating-your-first-trigger:%7E:text=Some%20people%20call%20this%20%22Event%2DDriven%20Architecture%22%20which%20can%20be%20used%20to%20create%20your%20own%20%22Functions%20as%20a%20Service%22%20on%20Kubernetes) approach such as [CloudEvents Player](https://github.com/ruromero/cloudevents-player?tab=readme-ov-file#running-on-kubernetes) which is perfect for triggering jobSink with a Curl by:

1. Create the broker
2. Create the Knative Service
3. Bind the service with the broker
4. Create the Knative trigger
5. Send events over HTTP in [CloudEvents](https://docs.triggermesh.io/1.24/targets/cloudevents/) format.",2025-05-07T11:24:45,2025-05-07T07:15:23
79609973,Trigger knative jobSink from external source,"I want to use jobSinks that can be triggered from external sources. By default, jobSinks can only be triggered from inside the kubernetes cluster (svc.cluster.local address).

For example I want to trigger the jobSink with a CURL from outside the kubernetes cluster. But so far I'm not able to expose it.

My hope with jobSink was, that I wont need a 24/7 running container that only listens for incoming requests and then triggers a job. Is it even possible at all?","kubernetes, knative",79610285.0,"[Complete example available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/knative-jobsink-79609973)

Let's remember that triggering a JobSink actually works not because you use `curl`, but because a CloudEvent is sent to a certain endpoint using http as the transport protocol.

The job-sink service is set up by knative-eventing and should look something like this (a bit cleaned for readability:

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: job-sink
    app.kubernetes.io/name: knative-eventing
    app.kubernetes.io/version: 1.18.1
    sinks.knative.dev/sink: job-sink
  name: job-sink
  namespace: knative-eventing
spec:
  clusterIP: 10.96.159.186
  clusterIPs:
  - 10.96.159.186
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  - name: http-metrics
    port: 9092
  selector:
    sinks.knative.dev/sink: job-sink
```

Let's now deploy a simple JobSink:

```
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: job-sink-logger
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: main
              image: docker.io/library/bash:5
              command: [ ""cat"" ]
              args:
                - ""/etc/jobsink-event/event""
```

## Use an ingress

With that out of the way, we can simply create an ingress.

Given an ingress like

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jobsink-demo
  namespace: knative-eventing
spec:
  rules:
  - host: ""jobsink-demo.192-168-1-6.sslip.io""
    http:
      paths:
      - pathType: Prefix
        path: ""/demo""
        backend:
          service:
            name: job-sink
            port:
              number: 80
```

and assuming that your cluster runs on `192.168.1.6`, with the ingress listening on port 9090, sure enough we can send our request:

```
$ curl -v \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: 123"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://jobsink-demo.192-168-1-6.sslip.io:9090/demo/job-sink-logger

* Host jobsink-demo.192-168-1-6.sslip.io:9090 was resolved.
* IPv6: (none)
* IPv4: 192.168.1.6
*   Trying 192.168.1.6:9090...
* Connected to jobsink-demo.192-168-1-6.sslip.io (192.168.1.6) port 9090
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: jobsink-demo.192-168-1-6.sslip.io:9090
> User-Agent: curl/8.12.1
> Accept: */*
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 123
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/123
< date: Wed, 07 May 2025 09:40:30 GMT
< content-length: 0
< x-envoy-upstream-service-time: 22
< server: envoy
<
* Connection #0 to host jobsink-demo.192-168-1-6.sslip.io left intact
```

and when we look at output of the created pod, we get what we expected:

```
$ kubectl get jobs.batch -n demo
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-61cceec46c111666dbac62910030fd6e   Complete   1/1           10s        29m
$ kubectl -n demo logs job-sink-logger-61cceec46c111666dbac62910030fd6e-tsw9c
{""specversion"":""1.0"",""id"":""123"",""source"":""my/curl/command"",""type"":""my.demo.event"",""datacontenttype"":""application/json"",""data"":{""details"":""JobSinkDemo""}}
```

> ***Note***
>
>
> With a plain ingress, you expose the job-sink to the outside world **without authentication**.
>
>
> *Please* ensure to secure the access!
> Allmost all ingress controllers allow to add at least BasicAuth or DigestAuth authentication.

# Alternative solution 1: use kubectl to run a one-shot pod

If the people who need to trigger the job sink can access the cluster using kubectl and are allowed to run pods, it becomes rather easy:

```
$ kubectl run -n demo submit-$(( RANDOM  )) -it --restart=Never \
  --image=alpine/curl -- \
  -iv \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://job-sink.knative-eventing.svc/demo/job-sink-logger
* Host job-sink.knative-eventing.svc:80 was resolved.
* IPv6: (none)
* IPv4: 10.96.159.186
*   Trying 10.96.159.186:80...
* Connected to job-sink.knative-eventing.svc (10.96.159.186) port 80
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: job-sink.knative-eventing.svc
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 3500
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
< Date: Wed, 07 May 2025 11:55:50 GMT
Date: Wed, 07 May 2025 11:55:50 GMT
< Content-Length: 0
Content-Length: 0
< Connection: close
Connection: close
<

* shutting down connection #0
$ kubectl -n demo get jobs
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-613109358aa56215a871ef6e4a5b06ed   Complete   1/1           4s         3m43s
```

## Alternative solution 2: use a job resource

If the users have kubectl access, they can also simply create a job triggering the job sink. Semantically, I think this is the most semantically correct:

```
apiVersion: batch/v1
kind: Job
metadata:
  name: trigger-jobsink-log
  namespace: demo
spec:
  ttlSecondsAfterFinished: 0
  template:
    spec:
      containers:
      - name: trigger
        image: alpine/curl
        command:
        - /bin/sh
        - -c
        - |
          curl -iv -H ""Connection: Close"" \
          --fail-with-body \
          -H ""content-type: application/json"" \
          -H ""ce-specversion: 1.0"" \
          -H ""ce-source: my/curl/command"" \
          -H ""ce-type: my.demo.event"" \
          -H ""ce-id:+$(( RANDOM ))"" \
          -d '{""details"":""JobSinkDemo""}' \
          http://job-sink.knative-eventing.svc/demo/job-sink-logger
      restartPolicy: Never
```

This creates a job that will vanish immediately after it was finished (`ttlSecondsAfterFinished: 0`). Note that the job will be reattempted on a curl failure or http return codes >= 400, even though `restartPolicy` is set to `Never`.

## Alternative Solution 3: use port-forwarding

```
$ kubectl port-forward -n knative-eventing svc/job-sink 8181:80 &> /dev/null &
[1] 82301
$ curl -v \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://localhost:8181/demo/job-sink-logger
* Host localhost:8181 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:8181...
* Connected to localhost (::1) port 8181
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: localhost:8181
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 28258
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/28258
< Date: Wed, 07 May 2025 12:38:37 GMT
< Content-Length: 0
< Connection: close
<
* shutting down connection #0
$ kill -TERM 82301
```",2025-05-07T10:08:30,2025-05-07T07:15:23
79609942,How common are non-graceful container shutdowns in Azure Container Apps (or Kubernetes) compared to graceful ones?,"I'm building a distributed system running on Azure Container Apps, where graceful shutdowns of containers (e.g., to flush state, finalize jobs, etc.) are important for ensuring data integrity and consistency.

In this setup:

- Scaling is handled via KEDA.
- Data Ingress is disabled, meaning the app has no HTTP endpoints or health probes configured.
- The container does not receive any Kubernetes liveness or readiness probes.

I understand that Azure Container Apps (backed by Kubernetes) typically try to gracefully shut down containers using SIGTERM and allow them to exit cleanly within a grace period (terminationGracePeriodSeconds or similar).

However, I'm trying to assess how reliable this behavior is in practice:

- Are non-graceful shutdowns (e.g., SIGKILL, container crashes, node failures) a common occurrence and something I should defensively design for?
- Or are they rare events, only expected in exceptional cases such as hardware faults, power outages, or severe node-level failures?
- Is it safe to assume graceful shutdowns are the default and reliable behavior, or should I treat non-graceful termination as the norm?

Any input or real-world experience with Azure Container Apps, KEDA, or similar Kubernetes-based environments (e.g., AKS, GKE, EKS) would be greatly appreciated.","kubernetes, containers, azure-container-apps, keda",79610059.0,"In production, it's always best to be prepared for the worst. Unexpected failures can happen at any time for many reasons. Azure Container Apps are generally stable, but things can still go wrong—especially during upgrades or system changes.

For example, I recently faced an issue with my cluster. I upgraded the master node and three worker nodes without any problems. But when I started working on the fourth node, something went wrong. The drain command didn’t work, and all my pods got stuck in a terminating state.

I followed the same steps as before, but for some reason, this time it failed. This showed me that even when everything seems fine, unexpected issues can happen.

That’s why it’s important to plan for failures. If we prepare for things like forced shutdowns, crashes, or node failures, we can save time and effort and keep our system running smoothly. Does not matter if this happen often or not but you should be ready for it.",2025-05-07T08:04:27,2025-05-07T06:58:50
79609335,&quot;OOMKilled&quot; and &quot;CrashLoopBackOff&quot; errors deploying Cassandra on Minikube,"Previously I had 32GB RAM on my laptop and could deploy Cassandra with the following `.yaml` file and no issues on a 3-nodes Minikube cluster, each node had an 8GB RAM:

```
apiVersion: v1
kind: Service
metadata:
  name: cassandra-srv
spec:
  type: NodePort
  ports:
    - port: 9042
      targetPort: 9042
      nodePort: 30042
  selector:
    app: cassandra

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  serviceName: cassandra-srv
  replicas: 2
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      containers:
        - name: cassandra
          image: cassandra:latest
          ports:
            - containerPort: 9042
          env:
            - name: CASSANDRA_AUTHENTICATOR
              value: PasswordAuthenticator
            - name: CASSANDRA_AUTHORIZATION
              value: CassandraAuthorizer
            - name: CASSANDRA_PASSWORD_SEEDER
              value: ""true""
            - name: CASSANDRA_SUPERUSER_PASSWORD
              value: ""11111""
            - name: CASSANDRA_SEEDS
              value: cassandra-0.cassandra-srv.default.svc.cluster.local
          volumeMounts:
            - name: cassandra-data
              mountPath: /var/lib/cassandra
  volumeClaimTemplates:
    - metadata:
        name: cassandra-data
      spec:
        accessModes: [""ReadWriteOnce""]
        resources:
          requests:
            storage: 1Gi
```

Then I decided to improve my laptop and replaced the RAM kit with a new 64GB RAM kit. After I did `minikube delete` and created a new cluster, this time with RAM capacity doubled using the following command (in fact I just replaced `-memory=8192` with `-memory=16384` ) :

```
  minikube start --driver=docker \
    --cpus=2 --memory=16384 --disk-size=16g \
    --nodes=3 --addons=registry \
    --insecure-registry=""10.0.0.0/24"" \
    --insecure-registry=""192.168.49.0/24""
```

When I execute `skaffold run` with all the previous manifest files and nothing changed at all, I do see ""OOMKilled"" and ""CrashLoopBackOff"" errors.

I did also tried to add resources as follows but wasn't helpful:

```
 resources:
    requests:
      memory: ""4Gi""
      cpu: ""1""
    limits:
      memory: ""8Gi""
      cpu: ""2""
```","kubernetes, cassandra, out-of-memory, minikube",79609799.0,How much heap size is given Xmx and Xms ? sometimes providing huge heap size also will cause OOM issues.,2025-05-07T04:38:11,2025-05-06T19:03:54
79608038,Failed to start Kraft controller in K8s,"We are manually trying out Zookeeper to Kafka KRaft migration in our K8s environment. For that the initial step is to deploy Kraft controller in migration mode. We used configmaps to provide the controller.properties configuration during server start.

Our configMap:

```
# {% raw %}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kraft-controller-config
  namespace: cgbu-ums-dev
data:
  controller.properties: |
    zookeeper.metadata.migration.enable=true
    zookeeper.connect=zookeeper:2181
    process.roles=controller
    node.id=0
    controller.quorum.voters=0@kraft-controller-0.kraft-controller:9092
    #controller.quorum.bootstrap.servers=kraft-controller:9092
    listeners=CONTROLLER://kraft-controller-0:9092
    advertised.listeners=CONTROLLER://kraft-controller-0:9092
    controller.listener.names=CONTROLLER
    num.network.threads=3
    num.io.threads=8
    socket.send.buffer.bytes=102400
    socket.receive.buffer.bytes=102400
    socket.request.max.bytes=104857600
    log.dirs=/var/lib/kafka/data/kraft-controller-0
    num.partitions=1
    num.recovery.threads.per.data.dir=1
    offsets.topic.replication.factor=1
    transaction.state.log.replication.factor=1
    transaction.state.log.min.isr=1
    share.coordinator.state.topic.replication.factor=1
    share.coordinator.state.topic.min.isr=1
    log.retention.hours=168
    log.segment.bytes=1073741824
    log.retention.check.interval.ms=300000
# {% endraw %}
```

On server start I am getting this following error:

```
[2025-05-06 05:25:13,768] ERROR Encountered fatal fault: exception while completing controller activation (org.apache.kafka.server.fault.ProcessTerminatingFaultHandler)
java.lang.RuntimeException: Should not have ZK migrations enabled on a cluster that was created in KRaft mode.
    at org.apache.kafka.controller.ActivationRecordsGenerator.recordsForNonEmptyLog(ActivationRecordsGenerator.java:168)
    at org.apache.kafka.controller.ActivationRecordsGenerator.generate(ActivationRecordsGenerator.java:234)
    at org.apache.kafka.controller.QuorumController$CompleteActivationEvent.generateRecordsAndResult(QuorumController.java:1243)
    at org.apache.kafka.controller.QuorumController$ControllerWriteEvent.run(QuorumController.java:791)
    at org.apache.kafka.queue.KafkaEventQueue$EventContext.run(KafkaEventQueue.java:132)
    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.handleEvents(KafkaEventQueue.java:215)
    at org.apache.kafka.queue.KafkaEventQueue$EventHandler.run(KafkaEventQueue.java:186)
    at java.base/java.lang.Thread.run(Thread.java:842)
```

Initially I was starting the Kraft controller first w/o migration -- it started successfully, then I changed the configmap and added migration properties and did `kubectl rollout restart`. On restart I got the same error which I believe happened because the pod started in pure Kraft mode but when later told to do migration, there might be some issue in the formatting of logs.dir property so it failed.

Now that I am directly starting with migration properties enabled, I am not sure why I am getting this error. Need help in figuring out why this happened and how do I fix it.

Update:
ZK Configuration variables:

```
env:
    - name: ZK_REPLICAS
        value: ""3""
    - name: ZK_CLIENT_PORT
        value: ""2181""
    - name: ZK_SERVER_PORT
        value: ""2888""
    - name: ZK_ELECTION_PORT
        value: ""3888""
    - name: ZK_TICK_TIME
        value: ""2000""
    - name: ZK_INIT_LIMIT
        value: ""10""
    - name: ZK_SYNC_LIMIT
        value: ""5""
    - name: ZK_MAX_CLIENT_CNXNS
        value: ""500""
    - name: ZK_SNAP_RETAIN_COUNT
        value: ""3""
    - name: ZK_PURGE_INTERVAL
        value: ""0""
    - name: LOG4J_FORMAT_MSG_NO_LOOKUPS
        value: ""true""
```","java, kubernetes, apache-kafka, migration, kraft",,,,2025-05-06T05:42:26
79607627,Spring Cloud Gateway routing to a service in Kubernetes environments without discovery service,"Summary:

> How can I use Spring Cloud Gateway to route requests to a given
> service, without a discovery server in the Kubernetes environment?

We have our gateway implementation based on **Zuul**, running in Kubernetes environment, using relatively old versions:

- spring-boot-starter-parent: 2.2.6.RELEASE
- spring-cloud-dependencies: Hoxton.SR4
- spring-cloud-kubernetes: 1.1.1.RELEASE

Now I try to upgrade this gateway to **Spring Cloud Gateway** and newer libraries:

- spring-boot-starter-parent :3.3.2
- spring-cloud-dependencies: 2023.0.3
- spring-cloud-kubernetes: 1.1.10.RELEASE

Most of the features of the original application is successfully refactored, and working but I am stuck, when I want to create a route to a specific Kubernetes service (using the serviceId), rather than using Kubernetes DNS resolution: `http://<serviceId>:<servicePort>`

With Zuul it was almost automatic. We created a ServiceAccount with appropriate privileges to get access to Kubernetes services. In the application I enabled Kubernetes discovery:

```
    spring:
      cloud:
        kubernetes:
          discovery:
            enabled: true
            service-labels:
              discovery: enabled
          enabled: true
        service-registry:
          auto-registration:
            enabled: false
```

We had the following dependencies:

```
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-kubernetes-discovery</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-kubernetes-ribbon</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
```

and I could simply create a new ZuulRoute, passing the `serviceId` as one of the parameter.

In this solution we did not use Eureka or any other Discovery Service. My guess is that Zuul using Kubernetes API collected the service info and forwarded the requests.

I would like to implement the same functionality with Spring Cloud Gateway. Reading the documentation, the only possible way seems to be using the Loadbalacer, i.e. set the route's uri to `lb://serviceId`

I found this post: [How to set up Spring Cloud Gateway application so it can use the Service Discovery of Spring Cloud Kubernetes?](https://stackoverflow.com/questions/56170511/how-to-set-up-spring-cloud-gateway-application-so-it-can-use-the-service-discove)
Following this post, I have the following dependencies:

```
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-config</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-ribbon</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-loadbalancer</artifactId>
      <version>1.1.10.RELEASE</version>
    </dependency>
```

This might be more than the minimum, but copied from the test application from the above link.

If I enable kubernetes discovery, like above:

```
    spring:
      cloud:
        kubernetes:
          discovery:
            enabled: true
            service-labels:
              discovery: enabled
          enabled: true
        service-registry:
          auto-registration:
            enabled: false
```

then the pod will not start, for it requires a discovery server's URL:

```
'spring.cloud.kubernetes.discovery.discovery-server-url' must be specified and be a valid URL
```

But I do not want/cannot install a discovery server in the Kubernetes environment, and cannot use Eureka any longer. This was not needed required for the Zuul-based solution.

I was hoping that Cloud Gateway can do service discovery/load balancing ""internally"" - client side - accessing Kubernets API, but if I disable kubernetes discovery, the service is not found:

```
Service unavailable: Unable to find instance for serviceId
```

I was hoping that Kubernetes' Ribbon will be used by Cloud Gateway to discover services.

Any idea, how can I achive this?

Note, that the above article discusses the possibility of auto-registering services, i.e:

```
spring:
  application.name: gateway
  cloud:
    gateway:
      discovery:
        locator:
          enabled: true
```

That is also a feature I am interested in, but I feel first I should be able to solve the explicit routing issue.","spring-boot, kubernetes, load-balancing, spring-cloud-gateway, service-discovery",79619578.0,"It seems that I have to answer my own question, maybe someone will learn from my experiences.

First of all, I am using the reactive version of Spring Cloud Gateway, i.e. it is based on WebFlux and uses Netty as the HTTP server. Mostly studying sources for:

- [Spring Cloud Kubertnetes](https://github.com/spring-cloud/spring-cloud-kubernetes)
- [Spring Cloud Gateway](https://github.com/spring-cloud/spring-cloud-gateway)

I found out, that Spring Cloud Gateway will use a *DiscoveryClient* for both load balancing and auto-registering services. And like almost everything in Spring Boot, it'll use a *DiscoveryClient* implementation bean, whichever it'll find on the classpath.

In Spring Cloud Kubernetes I found 2 implementation of *ReactiveDiscoveryClient*:

- org.springframework.cloud.kubernetes.discovery.KubernetesReactiveDiscoveryClient
- org.springframework.cloud.kubernetes.client.discovery.reactive.**KubernetesInformerReactiveDiscoveryClient**

The first implementation requires a *DiscoveryServer*, and it needs the server's URL as a configuration. The second one seems a ""native"" DiscoveryClient, i.e. it uses Kubernetes API to discover the services.

My problem was that my application's Cloud Gateway found the first on the classpath and instantiated. After realizing this, it was ""only"" trying various combination of dependencies - with some educated guesses, hints from the source - to ensure that only *KubernetesInformerReactiveDiscoveryClient* would be instatiated. For the particular case I had to have **only** the dependency:

```
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
```

For example when I've added any of the followings:

```
      <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes</artifactId>
        <version>${spring-cloud-kubernetes.version}</version>
      </dependency>

      <dependency>
          <groupId>org.springframework.cloud</groupId>
          <artifactId>spring-cloud-kubernetes-discovery</artifactId>
          <version>${spring-cloud-kubernetes.version}</version>
      </dependency>
```

although they look ""logical"" and "" innocent"", they resulted the other *DiscoveryClient* instantiated, which requires a *DiscoveryServer*.

From here it was a simple path to make my program work. I ""only""  had to figure out, what dependency should I have for load balancing - including caching-, and a couple of settings to enable/configure these implementation.

Here is the *relevant* part of my solution. Dependencies:

```
  <dependencies>
    ...
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-loadbalancer</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
    ...
  </dependencies>
```

and the settings:

```
spring:
  cloud:
    gateway:
      discovery:
        locator:
          enabled: true
    discovery:
      blocking:
        enabled: false
      reactive:
        enabled: true
      client:
        health-indicator:
          enabled: false
    kubernetes:
      discovery:
        enabled: true
        service-labels:
          discovery: enabled
      enabled: true
    service-registry:
      auto-registration:
        enabled: false
```

I don't claim that this is the best, optimal, minimal, etc. set of depenencies and settings, but my Spring Cloud Gateway based application is working, i.e:

- I can use in my route definitions service ID's, for example:

```
.uri(URI.create(""lb://"" + serviceId))
```

- routes created automatically for all of the discovered services i.e. which has a label: *discovery=enabled*. This feature can be disabled by spring.cloud.discovery.locator.enabled=false

See the [doc](https://docs.spring.io/spring-cloud-gateway/reference/spring-cloud-gateway/the-discoveryclient-route-definition-locator.html) for the Reactive Cloud Gateway for both features.",2025-05-13T11:47:52,2025-05-05T20:59:10
79607292,dealing with reading/writing large files inside a k8s pod with a memory limit,"I have several applications that run inside a k8s pod, where the pod has a specific memory limit.

I noticed that if i run a python script that deal with a large file, e.g: unzip a large file example.zip, the pod memory usage really spikes.
As i understand, even if i use an unzip function that uses streaming, the whole data of the large zip file will be written into the os system cache, and the memory usage of the pod includes the page cache, therefore can spike and cause OOM errors.

When i could have control over how i read the file, using python, i could use `libc.posix_fadvise(fd, 0, 0, POSIX_FADV_DONTNEED)` to drop the file from the cache and control the process so my cache wouldnt explode.
But if i use unzip, i have no such control.

Is there a better, more general way to deal with large files inside a pod?
or maybe only way is to try to clear the cache with every application i use? (posix_fadvise)?

edit:
i was wrong and even if i read the zip file chunk by chunk, i am able to call posix_fadvise on the zip output and input file descriptor, so this solution works.

on the other hand, i wonder if there isnt a more ""global"" system oriented solution and not an applicative one?","python, linux, kubernetes, out-of-memory",,,,2025-05-05T16:22:01
79607052,Istio sidecar not injected despite correct namespace label and autoInject settings,"I'm setting up a service mesh with Istio (v1.21.0) on Kubernetes, deployed via FluxCD using a HelmRelease. The istiod component is running correctly in the istio-system namespace and the installation reports success when checking:

```
kubectl describe helmrelease istiod -n istio-system
```

However, the pods still only have a single container (the main app container), and no istio-proxy sidecar is injected:

`ui%`

What could be preventing Istio from injecting the sidecar, even though:

the namespace is correctly labeled,

istiod is running properly,

autoInject is enabled via the HelmRelease,

and the pods have been restarted?

My workloads are running in the app namespace. I labeled this namespace with istio-injection=enabled as recommended:

```
kubectl label namespace app istio-injection=enabled --overwrite
```

The label is confirmed:

`labels: istio-injection: enabled`

After labeling, I restarted the deployment:

```
kubectl rollout restart deployment -n app
```

This is the configuration I use:

helmrelease.yaml:

```
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: istiod
  namespace: istio-system
spec:
  releaseName: istiod
  interval: 5m
  chart:
    spec:
      chart: istiod
      version: ""1.25.0""
      sourceRef:
        kind: HelmRepository
        name: istio
        namespace: istio-system
  values:
    meshConfig:
      enableAutoMtls: true
    pilot:
      autoscaleEnabled: true
    global:
      proxy:
        autoInject: enabled
      istioNamespace: istio-system
  targetNamespace: istio-system
```

helmrepo.yaml:

```
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
  name: istio
  namespace: istio-system
spec:
  url: https://istio-release.storage.googleapis.com/charts
  interval: 1m
```

I already tried with version 1.25.0 but it still doesn't works.","kubernetes, kubernetes-helm, istio, istio-sidecar",,,,2025-05-05T13:49:48
79604020,How (if possible) to create a k8s object in a helm chart only if not exists,"Problem I'm trying to solve:

I'm using 1password as a secret vault and can create secrets that track those vault items just fine. I create an object OnePasswordItem using flux, which creates a secret in the namespace.

However, the process of setting up secrets to be stored in the cluster is clunky. Create the object in the flux repo, update the pipeline in the pipeline repo after. Copy the object multiple times if it's needed in multiple namespaces.

What I would like to do is include something in my chart templates:

```
{{- range .Values.onepass.items }}
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: {{ .name }}
  annotations:
    operator.1password.io/auto-restart: {{ .autorestart | default true | quote }}
spec:
  itemPath: {{ .path}}
---
{{- end }}
```

Then I can simply add to my extra values file:

```
onepass:
  items:
    - name: name
      path: ""path""
```

This works great for a single service in the namespace. However, if I want two services to use the same secret item, I get a helm error that the OnePasswordItem exists already.

Is there a flag or something that I can use in the chart that will only install that if it doesn't exist so that it can skip it and not just fail?","kubernetes, kubernetes-helm",79604036.0,"Found it. Apparently there is a lookup function. This works perfectly in my templates:

```
{{- range .Values.onepass.items }}
{{- if not (lookup ""onepassword.com/v1"" ""OnePasswordItem"" .Release.Namespace .name ) -}}
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: {{ .name }}
  annotations:
    operator.1password.io/auto-restart: {{ .autorestart | default true | quote }}
spec:
  itemPath: {{ .path}}
---
{{- end }}
{{- end }}
```",2025-05-02T19:47:51,2025-05-02T19:34:43
79603143,Kubernetes Kafka SpringBoot connection sometime failed,"I build a [kafka](https://github.com/bitnami/charts/tree/main/bitnami/kafka) server with helm on my baremetal kubernetes cluster. I have a master, a worker and a developer machine. I'd like access kafka outside of the cluster, from the developer machine (kubectl proxy not a solution in this case). So I made the kafka service type to LoadBalancer.

```
kubectl get svc -n kafka

NAME                        TYPE           CLUSTER-IP     EXTERNAL-IP    PORT(S)                      AGE
kafka                       LoadBalancer   10.103.15.48   192.168.1.15   9092:31710/TCP               6d1h
kafka-controller-headless   ClusterIP      None           <none>         9094/TCP,9092/TCP,9093/TCP   6s
```

I have two Java SpringBoot projects for testing:

- [https://github.com/pzoli/kafkaconsumer](https://github.com/pzoli/kafkaconsumer)
- [https://github.com/pzoli/kafkaproducer](https://github.com/pzoli/kafkaproducer)

I set `KAFKA_BOOTSTRAP_SERVER = master.me.local:31710`

Where master.me.local is the controller node.

I get error when I start the consumer:

```
2025-05-02T11:19:51.997+02:00  WARN 8602 --- [kafkaconsumer] [     test-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group1-1, groupId=group1] Error connecting to node kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 1 rack: null)
```

So, I set the /etc/hosts to solve DNS requests

```
192.168.1.15 kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local kafka-controller-1.kafka-controller-headless.kafka.svc.cluster.local kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local
```

if I set KAFKA_BOOTSTRAP_SERVER = 192.168.1.15:9092 I get this error:

```
2025-05-02T11:34:15.283+02:00  INFO 8975 --- [kafkaconsumer] [     test-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-group1-1, groupId=group1] Discovered group coordinator kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 2147483645 rack: null)
2025-05-02T11:34:15.304+02:00  INFO 8975 --- [kafkaconsumer] [     test-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-group1-1, groupId=group1] Group coordinator kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 2147483645 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted.
2025-05-02T11:34:15.305+02:00  INFO 8975 --- [kafkaconsumer] [     test-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-group1-1, groupId=group1] Requesting disconnect from last known coordinator kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 2147483645 rack: null)
2025-05-02T11:34:15.407+02:00  INFO 8975 --- [kafkaconsumer] [     test-0-C-1] org.apache.kafka.clients.NetworkClient   : [Consumer clientId=consumer-group1-1, groupId=group1] Client requested disconnect from node 2147483645
```

So, I set it back to master.me.local:31710

Then I tested the producer with success, but somtimes with failure. I get this errors when failed:

```
2025-05-02T10:39:12.650+02:00  WARN 858179 --- [kafkaproducer] [ucer-producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=kafkaproducer-producer-1] Got error produce response with correlation id 5 on topic-partition test-0, retrying (2147483646 attempts left). Error: NOT_LEADER_OR_FOLLOWER
2025-05-02T10:39:12.652+02:00  WARN 858179 --- [kafkaproducer] [ucer-producer-1] o.a.k.clients.producer.internals.Sender  : [Producer clientId=kafkaproducer-producer-1] Received invalid metadata error in produce request on partition test-0 due to org.apache.kafka.common.errors.NotLeaderOrFollowerException: For requests intended only for the leader, this error indicates that the broker is not the current leader. For requests intended for any replica, this error indicates that the broker is not a replica of the topic partition. Going to request metadata update now
```

The log when success is this:

```
2025-05-02T17:02:36.838+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.k.c.t.i.KafkaMetricsCollector        : initializing Kafka metrics collector
2025-05-02T17:02:36.930+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.k.c.s.authenticator.AbstractLogin    : Successfully logged in.
2025-05-02T17:02:36.990+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version: 3.8.1
2025-05-02T17:02:36.992+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId: 70d6ff42debf7e17
2025-05-02T17:02:36.992+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka startTimeMs: 1746198156989
2025-05-02T17:02:36.997+02:00  INFO 22631 --- [kafkaconsumer] [           main] o.a.k.c.c.internals.LegacyKafkaConsumer  : [Consumer clientId=consumer-group1-1, groupId=group1] Assigned to partition(s): test-0
2025-05-02T17:02:37.021+02:00  INFO 22631 --- [kafkaconsumer] [           main] h.i.k.KafkaconsumerApplication           : Started KafkaconsumerApplication in 1.738 seconds (process running for 2.442)
2025-05-02T17:02:37.309+02:00  INFO 22631 --- [kafkaconsumer] [     test-0-C-1] org.apache.kafka.clients.Metadata        : [Consumer clientId=consumer-group1-1, groupId=group1] Cluster ID: snDkfLPZ76IiyNWY03GK7z
2025-05-02T17:02:37.310+02:00  INFO 22631 --- [kafkaconsumer] [     test-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-group1-1, groupId=group1] Discovered group coordinator kafka-controller-2.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 2147483645 rack: null)
2025-05-02T17:02:37.338+02:00  INFO 22631 --- [kafkaconsumer] [     test-0-C-1] o.a.k.c.c.internals.ConsumerUtils        : Setting offset for partition test-0 to the committed offset FetchPosition{offset=53, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka-controller-0.kafka-controller-headless.kafka.svc.cluster.local:9092 (id: 0 rack: null)], epoch=20}}
```

With in the cluster the producer and consumer works well when I set KAFKA_BOOTSTRAP_SERVER=kafka.kafka.svc.cluster.local:9092.

But outside of cluster this happend.
I don't understand what is the problem. Please help me find solution.

Thanks for all suggestions!","kubernetes, apache-kafka, bitnami-kafka",,,,2025-05-02T09:45:17
79601761,Kubernetes Pod to Pod communication in a single node where master node is the worker,"I have a web socket server where the URL will be like ws://nginx-vnc.example.com:30406/ws/172.29.126.22 this WebSocket server is running in a Node Port pod and it is configured in ingress once this web socket is hit the logic in the web socket is it will hit an API http://:5000/novnc/connect where this will start creating tunnel for the provided IP address which is in the webscoket URL it will create a dynamic port and tunnel the RFB data to the dynamic port.

After that again the websocket will establish a TCP connetion to the dynamic port but here i am getting TCP socket connection error. where it cannot connect to that port. I have attached the codes below.

[![enter image description here](https://i.sstatic.net/TMW6Pu8J.png)](https://i.sstatic.net/TMW6Pu8J.png)

Log of the websocket  pod
[![enter image description here](https://i.sstatic.net/e8Gbs08v.png)](https://i.sstatic.net/e8Gbs08v.png)

```
const WebSocket = require('ws');
const net = require('net');
const axios = require('axios');

const WEB_SOCKET_PORT = 9000; // WebSocket proxy server port
const FLASK_API_URL = 'http://10.103.5.105:5000/novnc/connect'; // Flask app endpoint
// Mapping WebSocket clients to TCP servers
const clientToTcpServerMap = {};

// Create WebSocket server
const wss = new WebSocket.Server({ port: WEB_SOCKET_PORT });

console.log(`WebSocket Proxy Server running on ws://websocket-proxy:${WEB_SOCKET_PORT}`);

wss.on(""connection"", async (ws, req) => {
    console.log(""WebSocket client connected."");

    // Extract the target IP from WebSocket URL (e.g., ws://localhost:9000/172.29.126.22)
    const targetIp = req.url.startsWith(""/ws/"") ? req.url.replace(""/ws/"", """").trim() : """";

    if (!targetIp || !/^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$/.test(targetIp)) {
    console.error(`Invalid IP address: ${targetIp}`);
    ws.send(""Invalid IP address."");
    ws.close();
    return;
}

    // Generate a unique client ID
    const clientId = targetIp;
    console.log(`Assigning client ID: ${clientId} for IP: ${targetIp}`);

    try {
        // Request SSH tunnel from Flask app
        console.log(`Requesting SSH tunnel for ${targetIp} from ${FLASK_API_URL}`);
        const response = await axios.post(FLASK_API_URL, {
            ip: targetIp,
            establish: true
        }, {
            timeout: 10000 // 10-second timeout for Flask request
        });
        console.log(response);
        if (response.data.message !== ""Tunnel established"" || !response.data.port) {
            console.error(`Failed to establish tunnel for ${targetIp}: ${JSON.stringify(response.data)}`);
            ws.send(""Failed to establish SSH tunnel."");
            ws.close();
            return;
        }

        const tunnelPort = response.data.port;
        console.log(`Received tunnel port ${tunnelPort} for IP ${targetIp}`);

        // Update clientToTcpServerMap
        clientToTcpServerMap[clientId] = { host: ""10.103.5.105"", port: tunnelPort };

        const tcpServer = clientToTcpServerMap[clientId];
        console.log(`Routing client '${clientId}' to TCP server at ${tcpServer.host}:${tcpServer.port}`);

        // Create a TCP connection for this specific client
        const tcpSocket = net.createConnection(tcpServer.port, tcpServer.host, () => {
            console.log(`Connected to TCP server at ${tcpServer.host}:${tcpServer.port} for ${clientId}`);
        });

        // Forward WebSocket messages to assigned TCP server
        ws.on(""message"", (message, isBinary) => {
            console.log(`Received from WebSocket client ${clientId}: ${isBinary ? '[binary]' : message}`);
            tcpSocket.write(isBinary ? message : message.toString());
        });

        // Forward TCP server responses to WebSocket client
        tcpSocket.on(""data"", (data) => {
            //console.log(`Received from TCP server: ${data}`);
            ws.send(data, { binary: true });
        });

        // Handle TCP server closure
        tcpSocket.on(""end"", () => {
            console.log(`Disconnected from TCP server at ${tcpServer.host}:${tcpServer.port}`);
            ws.close();
            delete clientToTcpServerMap[clientId]; // Clean up mapping
        });

        // Handle WebSocket client closure
        ws.on(""close"", () => {
            console.log(`WebSocket client ${clientId} disconnected.`);
            tcpSocket.end();
            delete clientToTcpServerMap[clientId]; // Clean up mapping
        });

        // Error handling
        ws.on(""error"", (error) => {
            console.error(`WebSocket error for client ${clientId}:`, error);
            tcpSocket.end();
            delete clientToTcpServerMap[clientId];
        });

        tcpSocket.on(""error"", (error) => {
            console.error(`TCP socket error for client ${clientId}:`, error);
            ws.send(""TCP connection error."");
            ws.close();
            delete clientToTcpServerMap[clientId];
        });

    } catch (error) {
        console.error(`Error establishing tunnel for ${targetIp}:`, error.message);
        ws.send(""Error connecting to Flask server or establishing tunnel."");
        ws.close();
    }
});
```

and

```
from flask import Flask, request, jsonify
import os
import sys
import socket
import time
import logging
import tempfile
import shutil
from subprocess import check_call, CalledProcessError, Popen, DEVNULL, TimeoutExpired
import requests
from requests.exceptions import RequestException

app = Flask(__name__)

# Configuration
TIMEOUT = 5.0  # Timeout for SSH status polling
CERT_DIR = ""./certificate""
X509_CERTFILE = os.path.join(CERT_DIR, ""configmaintainer.crt"")
X509_KEYFILE = os.path.join(CERT_DIR, ""configmaintainer.key"")

# Global dictionary to track active tunnel processes
active_tunnels = {}

# Logging setup
logging.basicConfig(
    level=logging.INFO,
    format=""%(asctime)s [%(levelname)s] %(message)s"",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Ensure certificate files exist
if not os.path.exists(X509_CERTFILE) or not os.path.exists(X509_KEYFILE):
    logger.error(""Certificate files missing: %s or %s"", X509_CERTFILE, X509_KEYFILE)
    sys.exit(1)

def generate_keys(temp_dir):
    """"""Generate RSA SSH key pair in a temporary directory.""""""
    ssh_keyname = os.path.join(temp_dir, f""SWDCIN-{next(tempfile._get_candidate_names())}"")
    username = os.getenv('USER') or os.getenv('USERNAME') or ""default""
    logger.info(""Generating SSH keys for user: %s"", username)

    try:
        cmd = [
            ""ssh-keygen"",
            ""-t"", ""rsa"",
            ""-b"", ""2048"",
            ""-C"", username,
            ""-N"", """",  # No passphrase
            ""-f"", ssh_keyname,
            ""-q""
        ]
        check_call(cmd, stdout=DEVNULL, stderr=DEVNULL)
        logger.info(""Generated SSH keys at: %s"", ssh_keyname)
    except CalledProcessError as e:
        logger.error(""ssh-keygen failed with exit code %d"", e.returncode)
        raise RuntimeError(""Failed to generate SSH keys"")
    except FileNotFoundError:
        logger.error(""ssh-keygen not found in PATH"")
        raise RuntimeError(""ssh-keygen not found"")

    if not os.path.exists(ssh_keyname) or not os.path.exists(f""{ssh_keyname}.pub""):
        logger.error(""Key files not created at %s"", ssh_keyname)
        raise RuntimeError(""SSH key files not created"")

    return ssh_keyname

def find_free_port():
    """"""Find and return an available port number.""""""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(('', 0))
        return s.getsockname()[1]

def do_request(status, hostname, ssh_keyfile, x509_certfile, x509_keyfile):
    """"""Perform an HTTP request to the PCS on a scale to enable or disable SSH.""""""
    session = requests.Session()
    session.cert = (x509_certfile, x509_keyfile)
    session.verify = False  # Use CA certificate for verification

    url = f""https://{hostname}:8443/pcs/services/upload/json""
    pub_key_path = f""{ssh_keyfile}.pub""

    if not os.path.exists(pub_key_path):
        logger.error(""Public key file not found: %s"", pub_key_path)
        raise RuntimeError(""Public key file missing"")

    with open(pub_key_path, ""r"") as file:
        key_pub = file.read().strip()

    data = {""ssh"": {""sshkeys"": [key_pub], ""status"": status}}
    headers = {""Content-Type"": ""application/json""}

    try:
        # POST request to start/stop SSH
        response = session.post(url, json=data, headers=headers, timeout=10)
        response.raise_for_status()
        content = response.json()

        if ""success"" not in content:
            logger.error(""POST request failed: %s"", content.get('error', 'Unknown error'))
            raise RuntimeError(""Failed to execute SSH request"")

        # Poll status
        status_url = f""https://{hostname}:8443/pcs/services/data/{content['success']}""
        timeout_counter = 0.0
        iteration_time = 0.5
        while timeout_counter < TIMEOUT:
            response = session.get(status_url, timeout=10)
            response.raise_for_status()
            content = response.json()

            if content and content[0][""statuscode""] == ""0"":
                logger.info(""SSH service %s successfully"", status)
                return True
            logger.info(""Waiting for SSH: %s"", content)
            time.sleep(iteration_time)
            timeout_counter += iteration_time

        logger.error(""Timeout waiting for SSH service"")
        raise RuntimeError(""Timeout waiting for SSH"")

    except RequestException as e:
        logger.error(""HTTP request failed: %s"", e)
        raise RuntimeError(f""Request failed: {e}"")

def open_ssh_tunnel(hostname, ssh_keyfile):
    """"""Open an SSH tunnel and return the local port.""""""
    local_port = find_free_port()
    cmd = [
        ""ssh"",
        ""-N"",  # No remote command
        f""-L 0.0.0.0:{local_port}:localhost:5900"",  # Bind to all interfaces
        ""-i"", ssh_keyfile,
        ""-o"", ""StrictHostKeyChecking=accept-new"",  # Safer than disabling
        ""-T"",  # Disable pseudo-tty
        f""scaleremote@{hostname}"",
    ]

    logger.info(""Starting SSH tunnel on local port %d: %s"", local_port, ' '.join(cmd))
    try:
        process = Popen(cmd, stdout=DEVNULL, stderr=DEVNULL)
        time.sleep(1)  # Brief pause to ensure tunnel starts
        if process.poll() is not None:
            logger.error(""SSH tunnel failed to start"")
            raise RuntimeError(""SSH tunnel failed"")
        return local_port, process
    except Exception as e:
        logger.error(""SSH tunnel failed: %s"", e)
        raise RuntimeError(f""SSH tunnel failed: {e}"")

@app.route('/novnc/connect', methods=['POST'])
def connect():
    """"""Handle SSH connection requests.""""""
    try:
        data = request.get_json()
        if not data or not isinstance(data, dict):
            return jsonify({""error"": ""Invalid JSON payload""}), 400

        ip = data.get(""ip"")
        establish = data.get(""establish"", False)

        if not ip:
            return jsonify({""error"": ""IP address is required""}), 400

        # Create temporary directory for SSH keys
        temp_dir = tempfile.mkdtemp()
        try:
            ssh_keyfile = generate_keys(temp_dir)
            do_request(""start"" if establish else ""stop"", ip, ssh_keyfile, X509_CERTFILE, X509_KEYFILE)

            if establish:
                tunnel_port, tunnel_process = open_ssh_tunnel(ip, ssh_keyfile)
                # Store the tunnel process for later termination
                active_tunnels[ip] = tunnel_process
                logger.info(""Stored tunnel process for IP %s: PID %d"", ip, tunnel_process.pid)
                return jsonify({""message"": ""Tunnel established"", ""port"": tunnel_port})
            else:
                # If stopping SSH, remove any existing tunnel
                if ip in active_tunnels:
                    active_tunnels[ip].terminate()
                    active_tunnels[ip].wait(timeout=5)
                    del active_tunnels[ip]
                    logger.info(""Terminated tunnel process for IP %s"", ip)
                return jsonify({""message"": ""SSH disabled successfully""})

        finally:
            # Clean up temporary directory and keys
            if os.path.exists(temp_dir):
                shutil.rmtree(temp_dir, ignore_errors=True)

    except RuntimeError as e:
        logger.error(""Operation failed: %s"", e)
        return jsonify({""error"": str(e)}), 500
    except Exception as e:
        logger.error(""Unexpected error: %s"", e)
        return jsonify({""error"": ""Internal server error""}), 500

@app.route('/novnc/disconnect', methods=['POST'])
def disconnect():
    """"""Terminate an active SSH tunnel for a given IP.""""""
    try:
        data = request.get_json()
        if not data or not isinstance(data, dict):
            return jsonify({""error"": ""Invalid JSON payload""}), 400

        ip = data.get(""ip"")
        if not ip:
            return jsonify({""error"": ""IP address is required""}), 400

        if ip not in active_tunnels:
            logger.warning(""No active tunnel found for IP %s"", ip)
            return jsonify({""error"": ""No active tunnel for IP""}), 404

        # Terminate the tunnel process
        process = active_tunnels[ip]
        process.terminate()
        try:
            process.wait(timeout=5)  # Wait for process to terminate
            logger.info(""Terminated tunnel process for IP %s: PID %d"", ip, process.pid)
        except TimeoutExpired:
            logger.warning(""Tunnel process for IP %s (PID %d) did not terminate within timeout"", ip, process.pid)
            process.kill()  # Force kill if it doesn't terminate

        # Remove from active tunnels
        del active_tunnels[ip]
        return jsonify({""message"": ""Tunnel terminated successfully""})

    except Exception as e:
        logger.error(""Disconnect error: %s"", e)
        return jsonify({""error"": ""Internal server error""}), 500

if __name__ == '__main__':
    # Use a production WSGI server like gunicorn in production
    app.run(host=""0.0.0.0"", port=5000, debug=False)
```

```
FlaskDeployment.yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: flask-ssh-tunnel
      namespace: default
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: flask-ssh-tunnel
      template:
        metadata:
          labels:
            app: flask-ssh-tunnel
        spec:
          containers:
            - name: flask-container
              image: nithish862/flask-ssh-tunnel:4.0
              ports:
                - containerPort: 5000
                - containerPort: 7070
              resources:
                limits:
                  cpu: ""500m""
                  memory: ""512Mi""
                requests:
                  cpu: ""200m""
                  memory: ""256Mi""
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: flask-service
      namespace: default
    spec:
      selector:
        app: flask-ssh-tunnel
      type: NodePort
      ports:
        - name: http
          protocol: TCP
          port: 5000
          targetPort: 5000
        - name: ssh-tunnel
          protocol: TCP
          port: 7070
          targetPort: 7070

Websocket-deployent.yaml

    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: websocket-proxy
      namespace: default
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: websocket-proxy
      template:
        metadata:
          labels:
            app: websocket-proxy
        spec:
          containers:
            - name: websocket-container
              image: nithish862/websocket-proxy:6.0
              imagePullPolicy: Always
              ports:
                - containerPort: 9000
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: websocket-service
      namespace: default
    spec:
      selector:
        app: websocket-proxy
      type: NodePort
      ports:
        - protocol: TCP
          port: 9000
          targetPort: 9000
```","kubernetes, network-programming, kubernetes-networking",,,,2025-05-01T11:14:47
79601222,"Cloud SQL Proxy SSL error: `certificate had CN &quot;&quot;, expected &quot;&lt;project&gt;:&lt;region&gt;:&lt;instance&gt;&quot;` when connecting to PostgreSQL 14","We're encountering an issue with **Cloud SQL Proxy (v1 and v2)** when connecting to a newly created **PostgreSQL 14 Cloud SQL instance** via a Kubernetes deployment on GKE.

The error we see in the logs is:

```
certificate had CN """", expected ""<project-id>:<region>:<instance-name>""
```

### Context:

- We're using the **Cloud SQL Auth Proxy sidecar container** in our GKE deployment.
- The credentials JSON for a service account with the **Cloud SQL Client** role is correctly mounted and used via `-credential_file`.
- The instance connection string is correctly formatted.
- We’ve verified that the secret mounts and paths are accurate.
- The same setup works fine for an older PostgreSQL 11 instance.

### Observations:

- Proxy starts and listens on `127.0.0.1:5432`, but immediately fails on SSL handshake.
- The error appears to be related to the certificate’s Common Name (CN) being empty or invalid.
- The **username** used to connect differs from the old instance (`proxyuser`) vs. new one (`postgres`), but it's unclear if that's related.

### What we've tried:

- Regenerating and rotating service account keys
- Verifying IAM permissions
- Ensuring secrets are properly mounted
- Running the proxy locally with the same credentials (same error)

### Question:

What could cause the Cloud SQL Auth Proxy to reject the connection due to a `CN """"` certificate mismatch?

Is there a misconfiguration at the SSL level or something specific to PostgreSQL 14 instances?

We'd appreciate guidance on resolving this safely without exposing internal project data. Thanks in advance!","django, postgresql, kubernetes, ssl, gcloud",79601764.0,"I have a similar issue with the error message : Cloud SQL connection failed. Please see [https://cloud.google.com/sql/docs/mysql/connect-run](https://cloud.google.com/sql/docs/mysql/connect-run) for additional details: certificate had CN """", expected ""<project_id>:<instance_id>""

I have a simple setup where I deploy my Ruby backend API on a Cloud RUN Service using an artifact, and I connect it to a PostgreSQL database on Cloud SQL. I try to connect both through a UNIX socket as it seems to be the correct way to do it (rather than TCP).

In my Cloud Run configuration, I specifically selected the database instance to automatically establish the socket in the background (according to Google Cloud documentation). According to the documentation, I'm not supposed to setup a Cloud Auth Proxy with this setup, however, I can't make it work, the connection always fails.",2025-05-01T11:16:05,2025-04-30T23:36:44
79601222,"Cloud SQL Proxy SSL error: `certificate had CN &quot;&quot;, expected &quot;&lt;project&gt;:&lt;region&gt;:&lt;instance&gt;&quot;` when connecting to PostgreSQL 14","We're encountering an issue with **Cloud SQL Proxy (v1 and v2)** when connecting to a newly created **PostgreSQL 14 Cloud SQL instance** via a Kubernetes deployment on GKE.

The error we see in the logs is:

```
certificate had CN """", expected ""<project-id>:<region>:<instance-name>""
```

### Context:

- We're using the **Cloud SQL Auth Proxy sidecar container** in our GKE deployment.
- The credentials JSON for a service account with the **Cloud SQL Client** role is correctly mounted and used via `-credential_file`.
- The instance connection string is correctly formatted.
- We’ve verified that the secret mounts and paths are accurate.
- The same setup works fine for an older PostgreSQL 11 instance.

### Observations:

- Proxy starts and listens on `127.0.0.1:5432`, but immediately fails on SSL handshake.
- The error appears to be related to the certificate’s Common Name (CN) being empty or invalid.
- The **username** used to connect differs from the old instance (`proxyuser`) vs. new one (`postgres`), but it's unclear if that's related.

### What we've tried:

- Regenerating and rotating service account keys
- Verifying IAM permissions
- Ensuring secrets are properly mounted
- Running the proxy locally with the same credentials (same error)

### Question:

What could cause the Cloud SQL Auth Proxy to reject the connection due to a `CN """"` certificate mismatch?

Is there a misconfiguration at the SSL level or something specific to PostgreSQL 14 instances?

We'd appreciate guidance on resolving this safely without exposing internal project data. Thanks in advance!","django, postgresql, kubernetes, ssl, gcloud",79601708.0,"It seems Cloud SQL Auth Proxy failed to set up proxy connections to the instance. It is likely because the language connectors/ auth proxy version is too old. Have you tried upgrading your Cloud SQL proxy version? If you are using Cloud SQL Auth Proxy, make sure you are using the most recent version, see [keeping the Cloud SQL Auth Proxy up to date](https://cloud.google.com/sql/docs/postgres/sql-proxy#keep-current).

You can also check this documentation about [Requirements for using the Cloud SQL Auth Proxy](https://cloud.google.com/sql/docs/postgres/sql-proxy#requirements), it mentions connections to Cloud SQL instances using (shared/ customer-managed) Certificate Authority (CA) with the recommended Cloud SQL Auth Proxy version.

If upgrading the proxy version doesn't work for you and if you have a support package, I would recommend you getting help through reaching out to [Google Cloud Support](https://cloud.google.com/support-hub?) for a more in-depth analysis of your issue.",2025-05-01T10:27:45,2025-04-30T23:36:44
79600919,How to hide secret values in Inputs and Outputs parameters shown on UI in Argo Workflows?,"My requirements is that I am taking some secrets as inputs from user, that can be access-token or api-key for an example. Now as soon as we take an input and pass it in next step as input, it will be shown in plain text in the UI. I wanted to hide the secrets from input and output parameters for each of the pods shown in the workflow. How can I do it?

I went through the docs and various tutorials but could not find any such reference for doing it. If someone having some idea how to solve and can help, then that would be great.

By hiding the secret, I mean not even showing in UI even through the secret parameters are getting passed as inputs Or other way could be to write SECURE instead. Anything is fine by mean, I just do not want to show to secret to user.

As an example, In below template, I want to hide the api-key parameter from input and output of each pods.

Argo Workflow Example:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: demo-hello-world-
spec:
  entrypoint: hello-world
  ttlStrategy:
    secondsAfterCompletion: 72000  # Pods will be deleted 20 hours after workflow completion
  templates:
  - name: hello-world
    steps:
      - - name: task-ask-whale
          template: whale-say
          arguments:
            parameters:
            - name: message
              value: ""hello world""
            - name: api-key
              value: ""{{workflow.parameters.api-key}}""

  - name: whale-say
    inputs:
      parameters:
        - name: message
        - name: api-key
    container:
      image: docker/whalesay:latest
      command: [""cowsay""]
      args: [""echo"", ""{{inputs.parameters.message}}""]
```","kubernetes, argocd, argo-workflows, argo",,,,2025-04-30T18:31:11
79599829,Exposing webconsole when using Artemis operator with size &gt; 1,"I am using Artemis operator with `deploymentPlan.size=2`.

I would like to expose the Artemis management webconsole for each broker instance; I was thinking of doing it through (a kubernetes service + an ingress rule) for each instance.

The problem is that as far as I know the structure of the webconsole app requires the ingress rule to be like:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
...
  annotations:
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$1

...
          - path: /(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: broker-webconsole-svc
                port:
                  number: 8080
```

That is, it does not tolerate an url rewriting with some prefix:

```
/instance0/(.*) --> svc0 #KO
/instance1/(.*) --> svc1 #KO
```

Am I missing something?","kubernetes, activemq-artemis, artemiscloud, arkmq",79601533.0,"The ArkMQ Operator creates an ingress for each console when the field `spec.console.expose` is `true`, i.e.

```
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  console:
    expose: true
  ingressDomain: my-domain.com
```",2025-05-01T07:28:47,2025-04-30T08:06:02
79598778,How to patch a custom resource in kubernetes using the javascript client,"I created a custom resource definition and also added the status subresource:

```
subresources:
        status: {}
```

I'm trying to update it with the following code but I'm getting all sorts of errors:

```
async function setLastSyncedImage(jobsetName, image) {
  let patch = {
    status: {
      lastSyncedImage: image
    }
  }

  await k8sApi.patchNamespacedCustomObjectStatus(
    {
      group: GROUP,
      version: VERSION,
      namespace: NAMESPACE,
      plural: PLURAL,
      name: jobsetName,
      body: patch
    },
    {
      headers: {
        'Content-Type': 'application/merge-patch+json'
      }
    }
  )
}
```

With that, I'm getting the following error:

```
'{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""error decoding patch: json: cannot unmarshal object into Go value of type []handlers.jsonPatchOp"",""reason"":""BadRequest"",""code"":400}\n'
```

it seems to ignore the `Content-Type` header I give it. so if instead I give it the patch like so:

```
let patch = [{
    ""op"": ""replace"",
    ""path"": ""/status/lastSyncedImage"",
    ""value"": image
  }]
```

That's the error message I'm getting:

```
{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""the server rejected our request due to an error in our request"",""reason"":""Invalid"",""details"":{},""code"":422}\n'
```

I can't find any example online that will allow me to patch the status of the resource","javascript, kubernetes, kubernetes-custom-resources",79620965.0,"A friend helped me find a solution:

```
async function patchResourceStatus(crd, name, namespace, update) {
  const resource = await k8sApi.getNamespacedCustomObject({
    group: GROUP,
    version: VERSION,
    namespace: namespace,
    plural: crd,
    name: name
  });

  const operations = [];

  // Add the 'add' operation only if status doesn't exist
  if (!resource.status) {
    operations.push({
      ""op"": ""add"",
      ""path"": ""/status"",
      ""value"": {}
    });
  }

  operations.push(
    ...Object.entries(update).map(([key, val]) => {
      return {
        ""op"": ""replace"",
        ""path"": `/status/${key}`,
        ""value"": val
      }
    })
  );

  return await k8sApi.patchNamespacedCustomObjectStatus({
    group: GROUP,
    version: VERSION,
    namespace,
    plural: crd,
    name,
    body: operations,
    fieldValidation: ""Strict""
  })
}

async function test() {
    await patchResourceStatus(""sidecarjobsets"", ""example-sidecarjobset"", ""default"", {
        ""state"": ""updating""
    })

    await patchResourceStatus(""sidecarjobsets"", ""example-sidecarjobset"", ""default"", {
        ""lastSyncedImage"": ""busybox:1.37.0""
    })
}

test()
```

It's not great since I need to make a get request before updating
the resource to determine if I need to add an operation to add the
status field.

I would like to have a better solution but looking at the [code](https://github.com/kubernetes-client/javascript/blob/0b6e7166b233f291fd550d2130f29233700c21f0/src/gen/apis/CustomObjectsApi.ts#L2122-L2134) and [documentation](https://github.com/kubernetes-client/javascript/blob/main/src/gen/CustomObjectsApi.md#patchNamespacedCustomObjectStatus)
it looks like the current version of the javascript kubernetes client
does not accept a `Content-Type` header but instead sets it by itself",2025-05-14T07:42:57,2025-04-29T16:01:05
79598229,Adding PEM file to Kubernetes Airflow,"I'm trying to add a PEM file to the config for the latest airflow HELM chart.  I've added the certificate as a configMap,  I have added the following code to the HELM chart for the webserver.  It has created a folder with the same name as the PEM file but there isn't a file in the folder.  Does anyone know how I can mount the configMap as a file?

```
  extraVolumeMounts:
    - name: ca-pemstore
      mountPath: /etc/ssl/certs/{name}.pem
      subPath: zscaler.pem
      readOnly: false

  extraVolumes:
    - name: ca-pemstore
      configMap:
        name: ca-pemstore
```","kubernetes, airflow, kubernetes-helm",79598595.0,The subPath needs to be the same as the file.  Also I used secrets rather than a configMap.,2025-04-29T14:17:21,2025-04-29T11:15:02
79597527,Vault Agent Injector: How to render secrets to a subpath without overwriting existing files in the mount path?,"I am using HashiCorp Vault's Agent Injector to inject secrets into my Kubernetes pods using the vault.hashicorp.com/secret-volume-path annotation. I am facing an issue where the rendered secrets are being output directly to the specified path, such as /app, and this causes any existing files in the /app directory to be overwritten.

Here is the part of my configuration where I define the secret path:

```
annotations:
  vault.hashicorp.com/secret-volume-path: ""/app""
```

However, I want to render the secrets into a subdirectory under /app, such as /app/conf, while keeping the existing files in /app intact. I have checked the official documentation, but I cannot find any reference to using subPath in this context.

My goal is to preserve the contents of the /app directory and store the rendered secrets in /app/conf (or another subpath), without overriding any existing files in /app.

Has anyone encountered this issue or found a solution to render Vault secrets into a subdirectory without overwriting the contents of the original directory? Is there any way to achieve this with Vault Agent Injector in Kubernetes?","kubernetes, hashicorp-vault",79601030.0,"You should be mounting the Vault Volume Directly to the Subpath where the secrets should reside (/app/conf), rather than just the parent directory (/app).

Instead of:

```
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app"" # This mounts the VOLUME at /app
```

You set it to the desired subpath:

```
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app/conf"" # This mounts the VOLUME at /app/conf
```",2025-04-30T20:09:29,2025-04-29T02:34:25
79595243,Ingress nginx routing issues for cloned repo of dockersamples/example-voting-app,"**Context**

I'm using a cloned repo from `https://github.com/dockersamples/example-voting-app.git`, and my instructor tasked me to expose the cluster using Ingress. However, if I switched the wildcard catch-all path or `/` and specified the service as `vote`, my `result` service won't work, and vice versa. I am displaying over HTTP, because some troubles arise when using a cert-manager and a hostname. In addition, I am using a new AKS cluster because I had some trouble when I experimented with the frontend IPs/public IPs of my AKS.

**My yaml file:**

```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: db
  name: db
  namespace: vote-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: db
  template:
    metadata:
      labels:
        app: db
    spec:
      containers:
      - image: postgres:15-alpine
        name: postgres
        env:
        - name: POSTGRES_USER
          value: postgres
        - name: POSTGRES_PASSWORD
          value: postgres
        ports:
        - containerPort: 5432
          name: postgres
        volumeMounts:
        - mountPath: /var/lib/postgresql/data
          name: db-data
      volumes:
      - name: db-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: db
  name: db
  namespace: vote-app
spec:
  type: ClusterIP
  ports:
  - name: postgres
    port: 5432
    targetPort: 5432
  selector:
    app: db
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: redis
  name: redis
  namespace: vote-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - image: redis:alpine
        name: redis
        ports:
        - containerPort: 6379
          name: redis
        volumeMounts:
        - mountPath: /data
          name: redis-data
      volumes:
      - name: redis-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: redis
  name: redis
  namespace: vote-app
spec:
  type: ClusterIP
  ports:
  - name: redis
    port: 6379
    targetPort: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: result
  name: result
  namespace: vote-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: result
  template:
    metadata:
      labels:
        app: result
    spec:
      containers:
      - image: dockersamples/examplevotingapp_result
        name: result
        ports:
        - containerPort: 80
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: result
  name: result
  namespace: vote-app
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: result
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: vote
  name: vote
  namespace: vote-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: vote
  template:
    metadata:
      labels:
        app: vote
    spec:
      containers:
      - image: dockersamples/examplevotingapp_vote
        name: vote
        ports:
        - containerPort: 80
          name: http
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: vote
  name: vote
  namespace: vote-app
spec:
  type: ClusterIP
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: vote
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: worker
  name: worker
  namespace: vote-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: worker
  template:
    metadata:
      labels:
        app: worker
    spec:
      containers:
      - image: dockersamples/examplevotingapp_worker
        name: worker
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ingress
  namespace: vote-app
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/use-regex: ""true""
spec:
  ingressClassName: nginx
  rules:
    - http:
        paths:
          - path: /vote
            pathType: Prefix
            backend:
              service:
                name: vote
                port:
                  number: 80
          - path: /result
            pathType: Prefix
            backend:
              service:
                name: result
                port:
                  number: 80
          - path: /
            pathType: Prefix
            backend:
              service:
                name: vote
                port:
                  number: 80
```

**Solutions that didn't work**

1. As stated above, I tried adding a letsencrypt cert manager and a hostname. However, because I am on HTTPS, I got this error code: `https://my-voting-app.com/' was loaded over HTTPS, but requested an insecure script 'http://code.jquery.com/jquery-latest.min.js'. This request has been blocked; the content must be served over HTTPS.`. So, I have decided not to use that for now.
2. I tweaked the rewrite-target to /$1 and /$2. However, the CSS and JavaScript files won't show. In addition, if I remove the annotations completely, it won't work.
3. I tried tweaking the paths into ImplementationSpecific. It didn't work
4. I tried making another Ingress file. Version A serves static files. It didn't work. I got the inspiration here `https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/load-bal-ingress-c/create-unmanaged-ingress-controller?tabs=azure-cli`. Version B puts the `result` and `vote` path separate. It didn't work, so I also separated the `/` path in the other Ingress file. But it didn't work.

Lastly, I made my `result` and `vote` service into LoadBalancers, which worked fine. However, my professor isn't satisfied with that.","kubernetes, kubernetes-ingress, azure-aks",79595262.0,"It literally tells you the problem: You use LetsEncrypt and thus presumably you are accessing the application via ""https"". However, [`vote/templates/index.html` loads `jquery.min.js`via  ""http""](https://github.com/dockersamples/example-voting-app/blob/63e9150ca17af4ed05880d4245e486481f73fcb4/vote/templates/index.html#L29) - hardcoded:

```
<script src=""http://code.jquery.com/jquery-latest.min.js"" type=""text/javascript""></script>
```

## The problem explained

Imho, that is a problem within the example voting app, since it should load jquery exactly as the next line loads `jquery.cookie.js`:

```
<script src=""//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.js""></script>
```

Note the absence of both the ""http"" and ""https"" prefixes you usually find in URLs. That construct is called a ""protocol-relative"" URL. If the `index.html` is called via https, the call to `jquery.cookie.js` is translated to

```
<script src=""https://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.js""></script>
```

However, if called via http, it is translated to

```
<script src=""http://cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.js""></script>
```

## How to solve

### Fork the voting app and fix the problem for all

You are dealing with an oversight in your upstream code. You could fork the repo, fix the oversight and create a pull request to the original repo. This - imho - would be the best option. The advantage would be that you could work with your fork for now, until the PR is accepted upstream. Hint: for a finals project, it is always good to document the problems you were facing and how you overcame those problems. Having a PR under your belt which you can add to your paper (!) surely would increase the quality of the latter.

### Access the ingress via http

The easiest way would be to access the ingress via http - which depending on what you want to achieve may or may not be acceptable.

### Clone the voting app and use a local fix

Of course, you could simply check out the voting app, fix the problem in your clone and just use it for yourself. Imho, that is the worst option: While it somehow solves the problem for you, fixing it for all people is only about 10 minutes more work and you'd be giving back to the community who helped you. Also, you'd miss out on the opportunity to show that you really understood how FOSS works.",2025-04-27T16:56:40,2025-04-27T16:29:20
79595039,OpenTelemetry .NET Automatic Instrumentation: FileLoadException for OpenTelemetry.Instrumentation.Runtime,"I’m encountering a issue with a .NET 8 application (ClaimSubmissionService) running in a Kubernetes pod, using OpenTelemetry .NET Automatic Instrumentation (v1.11.0). The application does not send traces as it emits System.IO.FileLoadException for OpenTelemetry.Instrumentation.Runtime, Version=1.11.1.410, even though OpenTelemetry.Instrumentation.Runtime.dll is present in /otel-dotnet-auto/net. I need help resolving the version mismatch and the permission error to stabilize the pod.

Environment
Base Image: mcr.microsoft.com/dotnet/sdk:8.0.302-jammy (single-stage build)

OpenTelemetry Version: v1.11.0 (installed via otel-dotnet-auto-install.sh)

Kubernetes: AKS (version not specified, assume recent)

OS: Ubuntu 22.04 (Jammy, from base image)

.NET Runtime: .NET 8.0

Error Details
The pod crashes with the following error in /var/log/opentelemetry/dotnet/otel-dotnet-auto-1-ClaimSubmissionService.Api-StartupHook-20250427.log:

```
[2025-04-27T11:32:32.5366626Z] [Error] Error in StartupHook initialization: LoaderFolderLocation: /otel-dotnet-auto/net
Exception: Exception has been thrown by the target of an invocation.
System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.
---> System.TypeInitializationException: The type initializer for 'OpenTelemetry.AutoInstrumentation.Loader.Loader' threw an exception.
---> System.Reflection.TargetInvocationException: Exception has been thrown by the target of an invocation.
---> System.IO.FileLoadException: Could not load file or assembly 'OpenTelemetry.Instrumentation.Runtime, Version=1.11.1.410, Culture=neutral, PublicKeyToken=7bd6737fe5b67e3c'. Could not find or load a specific file. (0x80131621)
File name: 'OpenTelemetry.Instrumentation.Runtime, Version=1.11.1.410, Culture=neutral, PublicKeyToken=7bd6737fe5b67e3c'
```

However, OpenTelemetry.Instrumentation.Runtime.dll exists in /otel-dotnet-auto/net:

ls /otel-dotnet-auto/net

```
Microsoft.Extensions.Diagnostics.Abstractions.dll OpenTelemetry.Instrumentation.Process.dll
OpenTelemetry.Api.ProviderBuilderExtensions.dll OpenTelemetry.Instrumentation.Quartz.dll
OpenTelemetry.Api.dll OpenTelemetry.Instrumentation.Runtime.dll
OpenTelemetry.AutoInstrumentation.AspNetCoreBootstrapper.dll OpenTelemetry.Instrumentation.SqlClient.dll
```",".net, kubernetes, open-telemetry, open-telemetry-collector",,,,2025-04-27T12:51:45
79594093,How to prevent a socket error connecting with RabbitMQ,"I am getting this error on my processor side:

```
2025-04-26 14:27:04,630 INFO  [io.sma.rea.mes.rabbitmq] (main) SRMSG17036: RabbitMQ broker configured to [localhost:5672] for channel reportsqueue
2025-04-26 14:27:04,631 INFO  [io.sma.rea.mes.rabbitmq] (main) SRMSG17036: RabbitMQ broker configured to [localhost:5672] for channel order-response
2025-04-26 14:27:04,632 INFO  [io.sma.rea.mes.rabbitmq] (main) SRMSG17007: Connection with RabbitMQ broker established for channel `reportsqueue`
2025-04-26 14:27:04,632 INFO  [io.ver.rab.imp.RabbitMQClientImpl] (main) Starting rabbitmq client
2025-04-26 14:27:04,632 ERROR [io.ver.rab.imp.RabbitMQClientImpl] (executor-thread-1) Could not connect to rabbitmq: java.net.ConnectException: Connection refused
    at java.base@21.0.6/sun.nio.ch.Net.pollConnect(Native Method)
    at java.base@21.0.6/sun.nio.ch.Net.pollConnectNow(Net.java:682)
    at java.base@21.0.6/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:542)
    at java.base@21.0.6/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:592)
    at java.base@21.0.6/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
    at java.base@21.0.6/java.net.Socket.connect(Socket.java:751)
    at com.rabbitmq.client.impl.SocketFrameHandlerFactory.create(SocketFrameHandlerFactory.java:61)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1249)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1198)
    at com.rabbitmq.client.ConnectionFactory.newConnection(ConnectionFactory.java:1096)
    at io.vertx.rabbitmq.impl.RabbitMQClientImpl.newConnection(RabbitMQClientImpl.java:160)
    at io.vertx.rabbitmq.impl.RabbitMQClientImpl.connect(RabbitMQClientImpl.java:843)
    at io.vertx.rabbitmq.impl.RabbitMQClientImpl.lambda$tryConnect$36(RabbitMQClientImpl.java:763)
    at io.vertx.core.impl.ContextImpl.lambda$executeBlocking$5(ContextImpl.java:205)
    at io.vertx.core.impl.ContextInternal.dispatch(ContextInternal.java:270)
    at io.vertx.core.impl.ContextImpl$1.execute(ContextImpl.java:221)
    at io.vertx.core.impl.WorkerTask.run(WorkerTask.java:56)
    at io.vertx.core.impl.TaskQueue.run(TaskQueue.java:81)
    at io.quarkus.vertx.core.runtime.VertxCoreRecorder$15.runWith(VertxCoreRecorder.java:643)
    at org.jboss.threads.EnhancedQueueExecutor$Task.doRunWith(EnhancedQueueExecutor.java:2675)
    at org.jboss.threads.EnhancedQueueExecutor$Task.run(EnhancedQueueExecutor.java:2654)
    at org.jboss.threads.EnhancedQueueExecutor.runThreadBody(EnhancedQueueExecutor.java:1627)
    at org.jboss.threads.EnhancedQueueExecutor$ThreadBody.run(EnhancedQueueExecutor.java:1594)
    at org.jboss.threads.DelegatingRunnable.run(DelegatingRunnable.java:11)
    at org.jboss.threads.ThreadLocalResettingRunnable.run(ThreadLocalResettingRunnable.java:11)
    at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base@21.0.6/java.lang.Thread.runWith(Thread.java:1596)
    at java.base@21.0.6/java.lang.Thread.run(Thread.java:1583)
    at org.graalvm.nativeimage.builder/com.oracle.svm.core.thread.PlatformThreads.threadStartRoutine(PlatformThreads.java:896)
    at org.graalvm.nativeimage.builder/com.oracle.svm.core.thread.PlatformThreads.threadStartRoutine(PlatformThreads.java:872)
```

I have a RabbitMQ manager server running on port localhost:8085:15672 for the UI and the amqp port at 5672. with my application.properties set as:

```
rabbitmq.host=rabbitmq.rabbitmq.svc.cluster.local
rabbitmq.port=5672
rabbitmq.username=team1
rabbitmq.password=team1
mp.messaging.outgoing.order-response.rabbitmq.host=rabbitmq.rabbitmq.svc.cluster.local
mp.messaging.outgoing.order-response.rabbitmq.port=5672
mp.messaging.outgoing.order-response.rabbitmq.username=team1
mp.messaging.outgoing.order-response.rabbitmq.password=team1
mp.messaging.incoming.reportsqueue.rabbitmq.host=rabbitmq.rabbitmq.svc.cluster.local
mp.messaging.incoming.reportsqueue.rabbitmq.port=5672
mp.messaging.incoming.reportsqueue.rabbitmq.username=team1
mp.messaging.incoming.reportsqueue.rabbitmq.password=team1
```

My producer is set up similarly on application.properties, but I'm not getting any errors as shown above:

```
2025-04-26 03:01:09,439 INFO  [io.sma.rea.mes.rabbitmq] (main) SRMSG17036: RabbitMQ broker configured to [localhost:5672] for channel order-response
2025-04-26 03:01:09,440 INFO  [io.sma.rea.mes.rabbitmq] (main) SRMSG17036: RabbitMQ broker configured to [localhost:5672] for channel order-requests
2025-04-26 03:01:09,442 INFO  [io.quarkus] (main) team1-purchase-api 1.0.3 native (powered by Quarkus 3.20.0) started in 0.025s. Listening on: http://0.0.0.0:8080
2025-04-26 03:01:09,442 INFO  [io.quarkus] (main) Profile prod activated.
2025-04-26 03:01:09,442 INFO  [io.quarkus] (main) Installed features: [cdi, grpc-client, hibernate-orm, hibernate-reactive, hibernate-reactive-panache, hibernate-validator, kubernetes, messaging, messaging-rabbitmq, reactive-pg-client, rest, rest-jackson, smallrye-context-propagation, smallrye-openapi, swagger-ui, vertx]
```

This is my code on the processor side:

```
package team1_report;

import java.time.LocalDateTime;

import org.eclipse.microprofile.reactive.messaging.Incoming;
import org.eclipse.microprofile.reactive.messaging.Outgoing;

import io.quarkus.hibernate.reactive.panache.common.WithTransaction;
import io.smallrye.mutiny.Uni;

import org.jboss.logging.Logger;

import io.vertx.core.json.JsonObject;

import jakarta.enterprise.context.ApplicationScoped;

@ApplicationScoped
public class ReportProcessor {

    private static final Logger LOG = Logger.getLogger( ReportProcessor.class );

    @Incoming(""reportsqueue"")
    @Outgoing(""order-response"")
    public String handleOrderRequest(String msg) {
        LOG.info(""Received order-request: "" + msg);
        try {
            JsonObject json = new JsonObject(msg);
            String orderId = json.getString(""order_id"");
            Long customerId = json.getLong(""customer_id"");
            Long productId = json.getLong(""product_id"");
            Long quantity = json.getLong(""quantity"");
            Double totalAmount = json.getDouble(""total_amount"");
            String timestamp = LocalDateTime.now().toString();

            Orders order = new Orders();
            order.id = orderId;
            order.customerId = customerId;
            order.productId = productId;
            order.quantity = quantity;
            order.totalAmount = totalAmount;
            order.timestamp = timestamp;

            return persistOrder(order)
                .onItem().transform(unused -> {
                    JsonObject response = new JsonObject();
                    response.put(""orderID"", orderId);
                    response.put(""customerID"", customerId);
                    response.put(""productID"", productId);
                    response.put(""quantity"", quantity);
                    response.put(""totalAmount"", totalAmount);

                    LOG.info(""Saved order and emitted order-response: "" + response.encodePrettily());

                    return response.encode();
                })
                .await().indefinitely();

        } catch (Exception e) {
            LOG.error(""Error processing order-request"", e);
            return ""NULL - ERROR"";
        }
    }

    @WithTransaction
    public Uni<Void> persistOrder(Orders order){
        return order.persist().replaceWithVoid();
    }
}
```

Here are my running pods:

```
shubh@Mac Report-Microservice-Processor % kubectl get pods -n rabbitmq
NAME         READY   STATUS    RESTARTS   AGE
rabbitmq-0   1/1     Running   0          13h
shubh@Mac Report-Microservice-Processor % kubectl get pods
NAME                                      READY   STATUS    RESTARTS       AGE
team1-customer-68dcb85d4b-rqrbv           1/1     Running   8 (13h ago)    30h
team1-customer-client-665b678cd7-d79l7    1/1     Running   10 (22h ago)   2d15h
team1-customer-database-postgresql-0      1/1     Running   14 (22h ago)   4d22h
team1-purchase-api-94f4676cf-7pw48        1/1     Running   0              12h
team1-report-database-postgresql-0        1/1     Running   3 (22h ago)    29h
team1-report-processor-8696f4c5b4-c6vws   1/1     Running   0              3m13s
shubh@Mac Report-Microservice-Processor %
```

Edits:

The rabbitmq server yaml file description.

```
shubh@Mac ~ % kubectl get svc team1-rabbitmq -o yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: ""2025-04-26T16:10:49Z""
  labels:
    app.kubernetes.io/component: rabbitmq
    app.kubernetes.io/name: team1-rabbitmq
    app.kubernetes.io/part-of: rabbitmq
  name: team1-rabbitmq
  namespace: default
  ownerReferences:
  - apiVersion: rabbitmq.com/v1beta1
    blockOwnerDeletion: true
    controller: true
    kind: RabbitmqCluster
    name: team1-rabbitmq
    uid: 68cd3769-1bde-4967-91ab-4ef548cf9f7f
  resourceVersion: ""88607""
  uid: 7a471fb6-8507-46fa-b40d-868aff3d0ccb
spec:
  clusterIP: 10.43.136.31
  clusterIPs:
  - 10.43.136.31
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - appProtocol: amqp
    name: amqp
    port: 5672
    protocol: TCP
    targetPort: 5672
  - appProtocol: http
    name: management
    port: 15672
    protocol: TCP
    targetPort: 15672
  - appProtocol: prometheus.io/metrics
    name: prometheus
    port: 15692
    protocol: TCP
    targetPort: 15692
  selector:
    app.kubernetes.io/name: team1-rabbitmq
  sessionAffinity: None
  type: ClusterIP
status:
  loadBalancer: {}
shubh@Mac ~ %
```

My updated pod info.

```
shubh@Mac ~ % kubectl get all
NAME                                          READY   STATUS    RESTARTS        AGE
pod/team1-customer-68dcb85d4b-rqrbv           1/1     Running   18 (118s ago)   2d23h
pod/team1-customer-client-665b678cd7-d79l7    1/1     Running   15 (12m ago)    4d8h
pod/team1-customer-database-postgresql-0      1/1     Running   19 (12m ago)    6d15h
pod/team1-purchase-api-94f4676cf-7pw48        1/1     Running   5 (12m ago)     2d5h
pod/team1-rabbitmq-server-0                   1/1     Running   4 (12m ago)     40h
pod/team1-report-database-postgresql-0        1/1     Running   8 (12m ago)     2d22h
pod/team1-report-processor-769d55cdb6-j2756   1/1     Running   4 (2m ago)      31h

NAME                                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE
service/kubernetes                              ClusterIP   10.43.0.1       <none>        443/TCP                        53d
service/team1-customer                          ClusterIP   10.43.46.195    <none>        80/TCP,9000/TCP                6d15h
service/team1-customer-client                   ClusterIP   10.43.181.133   <none>        8081/TCP                       7d4h
service/team1-customer-database-postgresql      ClusterIP   10.43.172.107   <none>        5432/TCP                       6d15h
service/team1-customer-database-postgresql-hl   ClusterIP   None            <none>        5432/TCP                       6d15h
service/team1-purchase-api                      ClusterIP   10.43.46.133    <none>        80/TCP                         3d
service/team1-rabbitmq                          ClusterIP   10.43.136.31    <none>        5672/TCP,15672/TCP,15692/TCP   40h
service/team1-rabbitmq-nodes                    ClusterIP   None            <none>        4369/TCP,25672/TCP             40h
service/team1-report-database-postgresql        ClusterIP   10.43.229.14    <none>        5432/TCP                       2d22h
service/team1-report-database-postgresql-hl     ClusterIP   None            <none>        5432/TCP                       2d22h
service/team1-report-processor                  ClusterIP   10.43.213.152   <none>        80/TCP                         2d22h

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/team1-customer           1/1     1            1           6d15h
deployment.apps/team1-customer-client    1/1     1            1           7d4h
deployment.apps/team1-purchase-api       1/1     1            1           3d
deployment.apps/team1-report-processor   1/1     1            1           2d22h

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/team1-customer-55645666f7           0         0         0       4d7h
replicaset.apps/team1-customer-5769fcfb96           0         0         0       4d7h
replicaset.apps/team1-customer-5cb7894dd4           0         0         0       4d10h
replicaset.apps/team1-customer-5d4ccd8dd7           0         0         0       6d15h
replicaset.apps/team1-customer-6497d7b6f5           0         0         0       2d23h
replicaset.apps/team1-customer-68dcb85d4b           1         1         1       2d23h
replicaset.apps/team1-customer-6d74989c86           0         0         0       2d23h
replicaset.apps/team1-customer-7d9c58c994           0         0         0       6d15h
replicaset.apps/team1-customer-b75fbd57f            0         0         0       4d8h
replicaset.apps/team1-customer-client-5cd49cd966    0         0         0       4d8h
replicaset.apps/team1-customer-client-65ccf985b7    0         0         0       7d4h
replicaset.apps/team1-customer-client-665b678cd7    1         1         1       4d8h
replicaset.apps/team1-customer-client-6cf54cfb4d    0         0         0       7d3h
replicaset.apps/team1-purchase-api-5649cdcdf6       0         0         0       2d5h
replicaset.apps/team1-purchase-api-5dff7cb498       0         0         0       2d5h
replicaset.apps/team1-purchase-api-65bcd55fb8       0         0         0       2d15h
replicaset.apps/team1-purchase-api-65d8d5d4bc       0         0         0       2d15h
replicaset.apps/team1-purchase-api-6977b8564        0         0         0       2d19h
replicaset.apps/team1-purchase-api-78d85db5f7       0         0         0       2d19h
replicaset.apps/team1-purchase-api-796dcdcd7b       0         0         0       2d19h
replicaset.apps/team1-purchase-api-94f4676cf        1         1         1       2d5h
replicaset.apps/team1-purchase-api-c5dbfdb55        0         0         0       2d19h
replicaset.apps/team1-purchase-api-f4d7658c7        0         0         0       2d15h
replicaset.apps/team1-purchase-api-fcf98cfb8        0         0         0       2d19h
replicaset.apps/team1-report-processor-59d98d58fd   0         0         0       41h
replicaset.apps/team1-report-processor-667c5df867   0         0         0       41h
replicaset.apps/team1-report-processor-699445b644   0         0         0       41h
replicaset.apps/team1-report-processor-6bb7d46856   0         0         0       40h
replicaset.apps/team1-report-processor-769d55cdb6   1         1         1       31h
replicaset.apps/team1-report-processor-78b8c986c5   0         0         0       41h
replicaset.apps/team1-report-processor-796f4cbb56   0         0         0       31h
replicaset.apps/team1-report-processor-79869d9986   0         0         0       41h
replicaset.apps/team1-report-processor-858d85d77d   0         0         0       31h
replicaset.apps/team1-report-processor-8696f4c5b4   0         0         0       40h
replicaset.apps/team1-report-processor-8d74849d     0         0         0       40h

NAME                                                  READY   AGE
statefulset.apps/team1-customer-database-postgresql   1/1     6d15h
statefulset.apps/team1-rabbitmq-server                1/1     40h
statefulset.apps/team1-report-database-postgresql     1/1     2d22h

NAME                                          ALLREPLICASREADY   RECONCILESUCCESS   AGE
rabbitmqcluster.rabbitmq.com/team1-rabbitmq   True               True               40h
```","java, kubernetes, quarkus, rancher",79597172.0,"Hi [Arihant Singh](https://stackoverflow.com/users/30370718/arihant-singh),

Your rabbit hostname must be something like : `rabbitmq.host=<service-name>.<namespace>.svc.cluster.local`

So ,in your properties file, replace `rabbitmq.host=rabbitmq.rabbitmq.svc.cluster.local `by  `rabbitmq.host=team1-rabbitmq.default.svc.cluster.local `as defined in your kube service config.

Don't forget to do the same for other hostnames or use variables references to avoid reapeating config :

```
rabbitmq.host=team1-rabbitmq.default.svc.cluster.local
rabbitmq.port=5672
rabbitmq.username=team1
rabbitmq.password=team1
mp.messaging.outgoing.order-response.rabbitmq.host=${rabbitmq.host}
mp.messaging.outgoing.order-response.rabbitmq.port=${rabbitmq.port}
mp.messaging.outgoing.order-response.rabbitmq.username=${rabbitmq.username}
mp.messaging.outgoing.order-response.rabbitmq.password=${rabbitmq.password}
mp.messaging.incoming.reportsqueue.rabbitmq.host=${rabbitmq.host}
mp.messaging.incoming.reportsqueue.rabbitmq.port=${rabbitmq.port}
mp.messaging.incoming.reportsqueue.rabbitmq.username=${rabbitmq.username}
mp.messaging.incoming.reportsqueue.rabbitmq.password=${rabbitmq.password}
```",2025-04-28T19:32:25,2025-04-26T15:28:07
79594030,Kubernetes bitnami/kafka connecting failed,"I tried to connect to my kafka server on my bearmetal kubernetes installation with default settings.
Install with helm

```
helm install kafka oci://registry-1.docker.io/bitnamicharts/kafka -n kafka
```

client.properties:

```
security.protocol=SASL_PLAINTEXT
sasl.mechanism=SCRAM-SHA-256
sasl.jaas.config=org.apache.kafka.common.security.scram.ScramLoginModule required \
    username=""user1"" \
    password=""$(kubectl get secret kafka-user-passwords --namespace kafka -o jsonpath='{.data.client-passwords}' | base64 -d | cut -d , -f 1);
```

Producer calls on kafka-client

```
kubectl exec --tty -i kafka-client --namespace kafka -- bash

kafka-console-producer.sh \
            --producer.config /tmp/client.properties \
            --bootstrap-server kafka.kafka.svc.cluster.local:9092 \
            --topic test
```

Error message

```
ERROR [Producer clientId=console-producer] Connection to node -1 (kafka.kafka.svc.cluster.local/10.103.15.48:9092) failed authentication due to: Authentication failed during authentication due to invalid credentials with SASL mechanism SCRAM-SHA-256 (org.apache.kafka.clients.NetworkClient)
```

How can I solve this problem? Is user1 valid username ín this kafka installation?

I found user1 client ín [https://github.com/bitnami/charts/blob/main/bitnami%2Fkafka%2Fvalues.yaml](https://github.com/bitnami/charts/blob/main/bitnami%2Fkafka%2Fvalues.yaml)

Thanks for all suggestion!","kubernetes, apache-kafka",79594173.0,"Ok, I use plain Text auth for success test. I copied the output of this command to the client.properties:

```
kubectl get secret kafka-user-passwords --namespace kafka -o jsonpath='{.data.client-passwords}' | base64 -d | cut -d , -f 1
```

And the client.properties file looks like this:

```
security.protocol=SASL_PLAINTEXT
#sasl.mechanism=SCRAM-SHA-256
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username=""user1"" password=""OUTPUT_OF_GET_SECRET"";
```",2025-04-26T16:52:12,2025-04-26T14:30:58
79591871,"Understanding when EMQX horizontal scaling reduces span duration (process_message, send_publish_message)","I am tracing an EMQX broker and have identified two primary span types: process_message and send_publish_message.
I would like to understand under what circumstances horizontally scaling EMQX (adding more replicas/pods) would lead to a decrease in the duration time of these specific spans.

I previously conducted an experiment using QoS 0 and sending 100,000 messages per second (with a 1KB payload size, as mentioned earlier). During this test, I observed that the emqx_vm_process_messages_in_queues metric remained at 0.

Consequently, horizontally scaling under these conditions did not yield a significant reduction in the duration time for the spans.
My original expectation was that horizontal scaling would be most effective when a single pod becomes overloaded, potentially indicated by message queue build-up (blockage). In such a scenario, I anticipated that adding more pods would distribute the load and result in a noticeable decrease in span duration times.

Therefore, I am asking if anyone has other experiences or knows of alternative scenarios where horizontally scaling EMQX provides a greater benefit than vertical scaling specifically for reducing the process_message and send_publish_message span durations.
Additionally, when a new broker pod is added through horizontal scaling, how can it be configured or prepared to quickly start receiving client messages?","kubernetes, autoscaling, emqx",,,,2025-04-25T05:41:37
79591402,How to set default password for a user in percona-postgresql-operator v.2.6.0,"I am trying to set up PostgreSQL in a Kubernetes cluster (in my local environment with kind), using the percona-postgresql-operator. I am trying to set a default password for a user, but it doesn't work

The following is how I have defined the users under spec

```
apiVersion: pgv2.percona.com/v2
kind: PerconaPGCluster
metadata:
  name: postgres-cluster
  #namespace: postgres
spec:
  image: perconalab/percona-postgresql-operator:main-ppg17-postgres
  postgresVersion: 17
  users:
    - name: postgres
      databases:
        - quotesdb
      options: ""SUPERUSER""
      password:
        type: ASCII
      secretName: postgres-user-secret

  proxy:
    pgBouncer:
      replicas: 1
      image: percona/percona-postgresql-operator:2.6.0-ppg16.8-pgbouncer1.24.0
      exposeSuperusers: true
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    postgres-operator.crunchydata.com/role: pgbouncer
                topologyKey: kubernetes.io/hostname
  instances:
    - name: instance1
      replicas: 1
      dataVolumeClaimSpec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 1Gi
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    postgres-operator.crunchydata.com/data: postgres
                topologyKey: kubernetes.io/hostname
  backups:
    pgbackrest:
      image: perconalab/percona-postgresql-operator:main-pgbackrest17
      repoHost:
        affinity:
          podAntiAffinity:
            preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 1
                podAffinityTerm:
                  labelSelector:
                    matchLabels:
                      postgres-operator.crunchydata.com/data: pgbackrest
                  topologyKey: kubernetes.io/hostname
      manual:
        repoName: repo1
        options:
          - --type=full
      repos:
        - name: repo1
          volume:
            volumeClaimSpec:
              accessModes:
              - ReadWriteOnce
              resources:
                requests:
                  storage: 1Gi
```

I tried creating a secret named postgres-user-secret with a key named password however this gets overwritten when the pods related to percona pg starts up. Is it possible to setup a default password or is it a normal process to find the password from the secrets?","postgresql, kubernetes, percona",,,,2025-04-24T20:21:07
79590713,Typical way to keep persistent builds using Kubernetes pods instead of VMs?,"We have a Jenkins job that is currently running on VMs. The initial build takes a long time to complete, but since the binaries are persistent the job only takes 5-10 minutes after the first build is ran on the VM.

However, we have a mandate to swap to Kubernetes pods now. Since every pod is constructed/de-constructed on a per-job basis, it will need to run a full build each time (which takes 2 hours). I'm wondering what is the ideal method of handling this?

I understand that a Pod Host can have a persistent volume that can be mounted onto the Pods created on it, but my understanding is that these should generally be read-only (*ie*. for something like a Git cache) not for building because you wouldn't want Pods #1 and Pods #2 both concurrently running the build on a shared volume","kubernetes, jenkins, continuous-integration, cicd",79591763.0,"I generally front-load the build into the CI pipeline and publish a container image that already contains the compiled binary. Then the pods simply pull that image and execute in seconds. If I still need incremental builds inside Kubernetes, I can mount a network-backed cache like EFS for read-only dependency caches.",2025-04-25T03:31:22,2025-04-24T13:27:15
79590682,Configure Prometheus checks in Datadog Agent,"I have a kubernetes cluster with around 50 Deployments. I have a Datadog Agent running in the cluster that publishes metrics to Datadog. I want it to collect Prometheus & OpenMetrics][1](https://docs.datadoghq.com/containers/kubernetes/prometheus/?tab=kubernetesadv2) metrics exposed by the pods, but not all metrics, only a subset.

From the docs, I can annotate my pods with the autodiscovery annotations:

```
# (...)
metadata:
  #(...)
  annotations:
    ad.datadoghq.com/<CONTAINER_NAME>.checks: |
      {
        ""openmetrics"": {
          ""init_config"": {},
          ""instances"": [
            {
              ""openmetrics_endpoint"": ""http://%%host%%:%%port%%/<PROMETHEUS_ENDPOINT> "",
              ""namespace"": ""<METRICS_NAMESPACE_PREFIX_FOR_DATADOG>"",
              ""metrics"": [{""<METRIC_TO_FETCH>"":""<NEW_METRIC_NAME>""}]
            }
          ]
        }
      }

spec:
  containers:
    - name: '<CONTAINER_NAME>'
```

This will allow me to configure what metrics I want to scrape.

My question is - is there a way to centrally configure this somehow and not annotate each pod individually?","kubernetes, prometheus, datadog",,,,2025-04-24T13:11:12
79590109,Keycloak routing to /auth with Ingress but not on portforwarding,"I'm running Keycloak in Kubernetes and I've encountered a routing issue.
When I port-forward the Keycloak service like this:

```
kubectl port-forward -n MY_NAMESPACE service/keycloak 8080:80
```

And change the `KC_HOSTNAME_URL` env variable to be `localhost:8080`

Keycloak correctly redirects me to the login page and I can login.

However, when I access Keycloak via the Ingress, like this:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/proxy-buffer-size: 256k
    nginx.ingress.kubernetes.io/rewrite-target: /
  labels:
    argocd.argoproj.io/instance: MY_NAMESPACE_ingress-configuration
  name: keycloak
  namespace: MY_NAMESPACE
spec:
  ingressClassName: private
  rules:
    - host: MY_HOSTNAME
      http:
        paths:
          - backend:
              service:
                name: keycloak
                port:
                  number: 80
            path: /
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - MY_HOSTNAME
      secretName: keycloak-crt
```

Keycloak redirects me to /auth and, I guess since it is not a match to the hostname env variable, I can't login - it shows always invalid credentials.

At this point i'm not sure what is the culprit - the Ingress or the keycloak configuration.

This is the `keycloak` service:

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: keycloak
    app.kubernetes.io/version: 24.0.5
    argocd.argoproj.io/instance: MYNAMESPACE_keycloak
    helm.sh/chart: keycloak-21.6.0
  name: keycloak
  namespace: MY_NAMESPACE
spec:
  ports:
    - name: http
      nodePort: null
      port: 80
      protocol: TCP
      targetPort: http
  selector:
    app.kubernetes.io/component: keycloak
    app.kubernetes.io/instance: keycloak
    app.kubernetes.io/name: keycloak
  sessionAffinity: None
  type: ClusterIP
```

And the generated pod manifest env variables:

```
- name: KEYCLOAK_HTTP_RELATIVE_PATH
  value: /
- name: KEYCLOAK_EXTRA_ARGS
  value: '--import-realm'
- name: KC_SPI_ADMIN_REALM
  value: master
- name: KC_HOSTNAME_URL
  value: 'MY_HOSTNAME/'
- name: KC_HOSTNAME_ADMIN_URL
  value: 'MY_HOSTNAME/'
- name: KEYCLOAK_IMPORT
  value: /opt/bitnami/keycloak/data/import/realm-config.json
```

The readiness probe with `/auth` works:
https://MY_HOSTNAME/auth/realms/master

Without `/auth` it throws a Resource Not Found.

I already tried changing keycloak configurations and added the above relative_path but nothing changed. Does anyone have any idea on the solution for this?

Thanks!","kubernetes, keycloak",,,,2025-04-24T08:22:46
79590070,How to pre-warm EMQX broker during vertical scaling to prevent dropped messages?,"I am using an EMQX broker and observing its performance by tracing two specific span types: process_message and send_publish_message.
I conducted an experiment where sending messages at a high rate per second with a payload size of 20KB caused a significant increase in the EMQX broker's CPU utilization. To address this, I performed vertical scaling by patching the Pod's CPU resource limit from 500m to 1 core. This successfully reduced the duration time for both process_message and send_publish_message spans.
However, I encountered a critical issue during the vertical scaling process: some messages were dropped. This seems to happen because the new Pod (with increased resources) is still in the process of starting up and isn't fully ready when the old Pod is terminated as part of the scaling operation.
I would like to know how I can properly ""pre-warm"" the broker during this vertical scaling process to prevent messages from being dropped.
Below is my current EMQX broker YAML configuration file

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: emqx
  namespace: exp
spec:
  serviceName: emqx-headless
  replicas: 1
  selector:
    matchLabels:
      app: emqx
  template:
    metadata:
      labels:
        app: emqx
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchLabels:
                  app: emqx
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: NotIn
                values:
                - worknode2
      serviceAccountName: emqx
      securityContext:
        fsGroup: 0
        runAsUser: 0
        runAsNonRoot: false
      containers:
        - name: emqx
          resizePolicy:
          - resourceName: cpu
            restartPolicy: NotRequired
          - resourceName: memory
            restartPolicy: RestartContainer
          resources:
            requests:
              cpu: 300m
            limits:
              cpu: 1
              memory: 400Mi
          image: emqx/emqx:5.7.0
          volumeMounts:
            - name: config
              mountPath: /opt/emqx/data/configs/cluster.hocon
              subPath: log.conf
            - name: config
              mountPath: /opt/emqx/etc/emqx.conf
              subPath: log3.conf
          env:
            - name: EMQX_DEFAULT_LOG_HANDLER
              value: ""file""
            - name: EMQX_NAME
              value: emqx
            - name: EMQX_CLUSTER_DISCOVERY
              value: k8s
            - name: EMQX_CLUSTER__K8S__SERVICE_NAME
              value: emqx-headless
            - name: EMQX_CLUSTER__K8S__APISERVER
              value: ""https://10.1.0.1:6443""
            - name: EMQX_CLUSTER__K8S__NAMESPACE
              value: exp
            - name: EMQX_NODE__COOKIE
              value: ""emqxsecretcookie""
          readinessProbe:
            exec:
              command:
              - sh
              - -c
              - |
                emqx_ctl status | grep ""5.7.0 is started"" >/dev/null 2>&1
                      initialDelaySeconds: 15
          lifecycle:
            preStop:
              exec:
                command:
                - sh
                - -c
                - |
                  echo "">> 停止接收新 MQTT 連線…""
                  emqx_ctl listener stop mqtt:1883
                  sleep 5
                  echo "">> 斷開現有連線並移除集群會員…""
                  emqx_ctl clients disconnect all
                  emqx_ctl cluster leave
                  sleep 30
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 5
      volumes:
        - name: data
          hostPath:
            path: /opt/data
        - name: config
          configMap:
            name: emqx-config
        - name: log
          hostPath:
            path: /var/log
        - name: api-key
          configMap:
            name: emqx-api-key
        - name: conf
          configMap:
            name: emqx-conf
      imagePullSecrets:
      - name: regcred
```","kubernetes, emqx",,,,2025-04-24T07:56:27
79589597,WebSocket (WSS) to EMQX via NGINX Ingress Fails,"I'm running into a frustrating issue trying to establish a WebSocket connection (wss://ui-dev.url.com/mqtt) to an EMQX MQTT broker behind an NGINX Ingress Controller in a Kubernetes dev environment.

🔍 Problem Summary:
Trying to connect via WebSocket (wss://) from a Vue.js SPA to EMQX (/mqtt).

🧪 Setup:
NGINX Ingress with TLS termination (via tls.secretName)

Cert is self-signed (I’m okay with browser showing “not secure”)

EMQX is running as a service in the same cluster.

Domain (ui-dev.url.com) is set up in /etc/hosts for local use — DNS is not mine.

No cert-manager or Let’s Encrypt involved (don't want to manage DNS records for dev domains).

✅ What Works:
EMQX is up and running internally.

If I skip TLS and use plain ws://, things work — but obviously that’s not ideal.

❌ What Fails:
Any wss:// request hangs forever, then fails silently with status 0 after 6-7 requests then 101 succeed but takes around 60 seconds.

No relevant errors in NGINX logs.

Browser shows no handshake or TLS failure — just stalled.

🧠 What I’ve Tried:
Verified EMQX can serve WebSocket connections.

Played with Ingress annotations like:

nginx.ingress.kubernetes.io/backend-protocol: HTTPS, HTTP (HTTPS works but 60 second 6-7 attempt.)

nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""

Switched between self-signed and mkcert-generated certs — same result.

Confirmed secret is mounted and tls: block references correct domain.

Has anyone dealt with WebSocket over TLS getting stuck like this in an NGINX Ingress on Kubernetes?

Any ideas where to dig deeper — is it TLS handshake silently failing, some config I missed on the EMQX side, or Ingress not proxying WebSocket properly?

Appreciate any insight — thank you! 🙏

[![enter image description here](https://i.sstatic.net/H3no18AO.png)](https://i.sstatic.net/H3no18AO.png)","kubernetes, nginx, ssl, websocket, mqtt",79592612.0,"As per [EMQX documentation](https://www.emqx.com/en/blog/connect-to-mqtt-broker-with-websocket#:%7E:text=If%20you%27re%20using%20a%20self%2Dsigned%20certificate%20for%20the%20broker%2C%20you%20must%20manually%20add%20it%20to%20the%20browser%27s%20trust%20store), you must manually [add the self-signed certificate](https://documentation.avaya.com/bundle/AdministeringApplicationEnablementServicesForAvayaContactCenterExtendedCapacity_r102/page/Importing_a_trusted_certificate_into_the_browser_trust_store.html) to the browser’s trust store. This is likely the reason why you are getting status code 0. Unlike HTTPS, where you can manually accept the warning, WebSocket connection (wss://) fails silently if the certificate is not trusted and stored in the browser’s trust store.

Also you need to make sure that your backend-protocol is set to “HTTP” only, as EMQX websocket uses HTTP. It is also recommended to turn off the proxy-buffering to prevent NGINX from buffering WebSocket traffic, which can cause some delays.

```
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
```",2025-04-25T13:19:40,2025-04-23T22:38:27
79589305,What is the behaviour of the ServerTelemetryChannel when multiple processes are using the same StorageFolder in the same disk?,"More specifically, I am talking about Kubernetes pods, running different instances of the same application. I haven't found official recommendations for that scenario.

Besides attaching a Persistent Volume, it feels like I need to also care about the race conditions.

I am sending Application Insights' Events and Metrics to be consumed by an orchestrator service, so I need them to be reliable. Should I expect that if I don't care about inter-processes race conditions, duplicates might appear?",".net, kubernetes, azure-application-insights",79589618.0,"ServerTelemetryChannel creates a [subfolder](https://github.com/Azure/azure-sdk-for-net/blob/de55956d2972ada39292bc5f606ed42d43a22030/sdk/monitor/Azure.Monitor.OpenTelemetry.Exporter/src/Internals/PersistentStorage/StorageHelper.cs#L22) in the storage folder for process isolation. It uses a hash of user identity, process name and application directory.

```
// get unique sub directory
var userName = platform.GetEnvironmentUserName();
var processName = platform.GetCurrentProcessName();
var applicationDirectory = platform.GetApplicationBaseDirectory();
string subDirectory = HashHelper.GetSHA256Hash($""{instrumentationKey};{userName};{processName};{applicationDirectory}"");
```

In a case of running multiple pods pointing to the same shared volume it depends on whether process name includes pod name or not. If yes then it should be unique and results in no conflict.

**Update (April 24th, 2025)**: Confirmed with the team that above scenario (when all app instances use shared folder) is supported as well. In this case telemetry might be uploaded not by an instance it was emitted by. But it still will be uploaded (and without duplicates) and all race conditions are handled by SDK itself.",2025-04-23T23:15:34,2025-04-23T18:44:13
79587626,Use Kyverno to add environment variables if configmap is present,"I am trying to use the Kyverno sample policy for injecting environment variables into a container using a configmap in the pod's namespace:

[https://kyverno.io/policies/other/add-env-vars-from-cm/add-env-vars-from-cm/](https://kyverno.io/policies/other/add-env-vars-from-cm/add-env-vars-from-cm/)

If I create a pod in a namespace without this configmap the pod will fail to create. I cannot understand how to make the Kyverno rule apply only when the configmap is present in the pod's namespace. Here's my latest attempt which fails with an error:

```
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-env-vars-from-cm
  annotations:
    policies.kyverno.io/title: Add Environment Variables from ConfigMap
    policies.kyverno.io/minversion: 1.6.0
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/category: Other
    policies.kyverno.io/description: ""Instead of defining a common set of environment variables multiple times either in manifests or separate policies, Pods can reference entire collections stored in a ConfigMap. This policy mutates all initContainers (if present) and containers in a Pod with environment variables defined in a ConfigMap named `nsenvvars` that must exist in the destination Namespace.""
spec:
  rules:
    - name: add-env-vars-from-cm
      match:
        any:
          - resources:
              kinds:
                - Pod
      context:
        - name: envVarsCmCount
          apiCall:
            urlPath: ""/api/v1/namespaces/{{ request.namespace }}/configmaps/nsenvvars""
            jmesPath: ""data | length(@)""
            default: 0
      preconditions:
        all:
          - key: envVarsCmCount
            operator: GreaterThan
            value: 0
      mutate:
        patchStrategicMerge:
          spec:
            initContainers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
            containers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
```","kubernetes, kyverno",79592666.0,"Fixed

The key was not to add a precondition to the Kyverno policy but instead make the configmap ref in the mutation optional:

```
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-env-vars-from-cm
  annotations:
    policies.kyverno.io/title: Add Environment Variables from ConfigMap
    policies.kyverno.io/minversion: 1.6.0
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/category: Other
    policies.kyverno.io/description: ""Instead of defining a common set of environment variables multiple times either in manifests or separate policies, Pods can reference entire collections stored in a ConfigMap. This policy mutates all initContainers (if present) and containers in a Pod with environment variables defined in a ConfigMap named `nsenvvars` that must exist in the destination Namespace.""
spec:
  rules:
    - name: add-env-vars-from-cm
      match:
        any:
          - resources:
              kinds:
                - Pod
      mutate:
        patchStrategicMerge:
          spec:
            initContainers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
            containers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
```",2025-04-25T13:55:35,2025-04-23T00:04:33
79581979,Persistence volume node affinity,"I am facing some difficulty in implementing node affinity in persistence volume.

While create persistence volume, I am getting below error

```
PersistentVolume in version ""v1"" cannot be handled as a PersistentVolume: strict decoding error: unknown field ""spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution""
```

Below is my storage class, persistence volume and persistence volume claim

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: blue-stc-cka
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: blue-pv-cka
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: blue-stc-cka
  local:
   path: /opt/blue-data-cka
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: blue-pvc-cka
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: blue-pv-cka
  resources:
    requests:
      storage: 50Mi
  storageClassName: blue-stc-cka
```","kubernetes, persistent-volume-claims",79582307.0,"The [API documentation for PersistentVolumes](https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/) shows that the syntax for `nodeAffinity:` is different from Pods'.  There is only one ""kind"" of affinity, `required:`, as opposed to the two-phase setup that Pods have.  You should be able to change the affinity definition to

```
nodeAffinity:
  required: # <-- change this
    nodeSelectorTerms:
      - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
```

For almost all practical uses, you should just delete the manually-created StorageClass and PersistentVolume, and delete the `storageClassName:` from the PersistentVolumeClaim (unless your cluster administrator has told you something different).  The cluster will automatically create the PersistentVolume for you using the default StorageClass.  Depending on how the application uses the storage, you often will want to use a StatefulSet, and move the PVC definitions into that object as well.",2025-04-19T11:06:56,2025-04-19T02:52:46
79580793,Kubernetes startupProbe fails even though app becomes healthy within allowed threshold,"I'm running into an issue with my (GKE) Kubernetes deployment's startupProbe. My container exposes a /v1/health endpoint that returns JSON with a ""status"" field. The probe is configured as follows:

```
startupProbe:
  exec:
    command:
      - sh
      - -c
      - >
          curl --silent --fail http://localhost:8080/v1/health |
          grep --quiet -e '\""status\"":\""healthy\""'
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 10
  failureThreshold: 18
```

This should allow up to 3 minutes for the app to become healthy. However, the probe keeps failing and the pod restarts, even though:

The health endpoint returns ""status"":""undetermined"" for a while, then switches to ""status"":""healthy"" (usually within the 3-minute window).

If I manually exec into the pod and run the probe command, it succeeds once the app is up.

```
 k exec -ti <> -- sh -c 'curl -s http://localhost:8080/v1/health'
{""build_info"":{""app_name"":""<>"",""app_version"":""<>"",""build_timestamp"":""2025-04-16T18:08:36Z"",""built_by"":""<>"",""commit_id"":""<>""},""status"":""healthy"",""uptime"":""13m51.712240388s""}
```

Both curl and grep are present in the image.

This is the ouput when I describe the pod.

```
Warning  Unhealthy                          10m (x4 over 11m)  kubelet                  Startup probe failed:
```","kubernetes, google-kubernetes-engine",79642654.0,"The health-check was eventually passing and the restart attempt stopped. The issue is that Kubernetes does not report when a health check stops failing and starts passing.

After observing and troubleshooting for a long time I finally realized this when the error counter did not increase after many minutes.",2025-05-28T16:32:32,2025-04-18T09:45:19
79580793,Kubernetes startupProbe fails even though app becomes healthy within allowed threshold,"I'm running into an issue with my (GKE) Kubernetes deployment's startupProbe. My container exposes a /v1/health endpoint that returns JSON with a ""status"" field. The probe is configured as follows:

```
startupProbe:
  exec:
    command:
      - sh
      - -c
      - >
          curl --silent --fail http://localhost:8080/v1/health |
          grep --quiet -e '\""status\"":\""healthy\""'
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 10
  failureThreshold: 18
```

This should allow up to 3 minutes for the app to become healthy. However, the probe keeps failing and the pod restarts, even though:

The health endpoint returns ""status"":""undetermined"" for a while, then switches to ""status"":""healthy"" (usually within the 3-minute window).

If I manually exec into the pod and run the probe command, it succeeds once the app is up.

```
 k exec -ti <> -- sh -c 'curl -s http://localhost:8080/v1/health'
{""build_info"":{""app_name"":""<>"",""app_version"":""<>"",""build_timestamp"":""2025-04-16T18:08:36Z"",""built_by"":""<>"",""commit_id"":""<>""},""status"":""healthy"",""uptime"":""13m51.712240388s""}
```

Both curl and grep are present in the image.

This is the ouput when I describe the pod.

```
Warning  Unhealthy                          10m (x4 over 11m)  kubelet                  Startup probe failed:
```","kubernetes, google-kubernetes-engine",79592621.0,"Based on what you've shared, I have 2 theories about what might be wrong.

1. *(Most likely)* Since you didn't provide a full command output from inside container (i.e. `curl` vs `curl ... | grep ...`) I can assume that the `grep` version inside conatiner is working different than expected. This is usually happens with more complex commands (e.g. when using -E), but it worth checking a full piped pair.
2. *(Less likely)* Weird idea, but maybe YAML itself is not resolved correctly? Try to make it as simple as possible to 2x check:

```
startupProbe:
  exec:
    command: [""sh"", ""-c"", ""curl -s -f http://localhost:8080/v1/health | grep -q -e '\""status\"":\""healthy\""'""]
```

If this doesn't work, try to make it verbose and check the Pod logs:

```
startupProbe:
  exec:
    command:
      - echo ""PROBE DEBUG""
      - curl -v http://localhost:8080/v1/health
      - sh
      - -c
      - >
          curl http://localhost:8080/v1/health |
          grep -e '\""status\"":\""healthy\""'
      - echo ""$?""
```",2025-04-25T13:25:20,2025-04-18T09:45:19
79580584,Argo Workflows authentication with dex static password faling due permissions error:,"im trying to add authentication to argo workflows using dex server and static password. im working on an on prem environment with rke2 cluster. i have install the dex and workflows using helm charts. the services are connected, workflows is directing to dex, dex authentication is workin and redirecting to workflows but when workflows pop up ther e is permissions error and i cant do anything. Ive tried adding service account with admina role and annotation to the user, and tried creating a cluster role with permission and role binding to the user and nothing worked. this is the error in the ui and the logs:
[this is the error in the ui:](https://i.sstatic.net/2JsWT8M6.png)

this are the logs of the workflows pod :

```
level=error msg=""failed to perform RBAC authorization"" error=""no service account rule matches""
level=warning msg=""finished unary call with code PermissionDenied"" error=""rpc error: code = PermissionDenied desc = not allowed"" grpc.code=PermissionDenied grpc.method=GetUserInfo grpc.service=info.InfoService grpc.start_time=""2025-04-18T06:52:24Z"" grpc.time_ms=6.338 span.kind=server system=grpc
level=info duration=7.099024ms method=GET path=/api/v1/userinfo size=34 status=403
```

this is the workflows values.yaml:

```
server:
  ingress:
    enabled: true
    ingressClassName: nginx
    hosts:
      - ${ARGO_WORKFLOWS_URL}
    tls:
      - secretName: argocd-server-tls
        hosts:
          - ${ARGO_WORKFLOWS_URL}

  authModes:
    - sso

  sso:
    enabled: true
    issuer: https://${DEX_URL}
    clientId:
      name: argo-sso-id
      key: argo-sso-id
    clientSecret:
      name: argo-sso-secret
      key: argo-sso-secret
    redirectUrl: https://${ARGO_WORKFLOWS_URL}/oauth2/callback
    cliClientID: argo-cli
    cliClientSecret: argo-cli-secret
    rbac:
      enabled: true
    scopes:
      - openid
      - email
      - profile
    insecureSkipVerify: true

  extraVolumeMounts:
    - name: argo-ca
      mountPath: /etc/ssl/certs/ca.pem
      subPath: ado-certificate
      readOnly: true

  extraVolumes:
    - name: argo-ca
      secret:
        secretName: ado-certificate

  extraEnv:
      - name: SSL_CERT_FILE
        value: /etc/ssl/certs/ca.pem

  extraArgs:
    - --loglevel=debug

controller:
  serviceAccount:
    create: true
    name: argo-workflows-sa
```

this is the dex values.yaml:

```
config:
  issuer: ${DEX_SERVER_URL}
  storage:
    type: kubernetes
    config:
      inCluster: true

  staticClients:
    - id: argo-workflows
      name: argo-workflows
      secret: argo-workflows-secret
      redirectURIs:
        - ${dex_redirect_url}
    - id: argo-cli
      redirectURIs:
        - https://${ARGO_WORKFLOWS_URL}/oauth2/callback
      name: Argo CLI
      secret: argo-cli-secret

  staticPasswords:
    - email: <UserEmail>
      hash:<HASH>
      username: ""admin""
      userID: <UserID>

  enablePasswordDB: true

logLevel: debug

ingress:
  enabled: true
  className: ""nginx""
  hosts:
    - host: ${DEX_URL}
      paths:
        - path: /
          pathType: Prefix
  tls:
    - hosts:
        - dex.qa.mot.gov.il
      secretName: dex-tls

extraVolumeMounts:
  - name: argo-ca
    mountPath: /etc/ssl/certs/ca.crt
    subPath: ca.crt
    readOnly: true

extraVolumes:
  - name: argo-ca
    secret:
      secretName: argo-ca-cert

envVars:
    - name: SSL_CERT_FILE
      value: /etc/ssl/certs/ca.crt
```

this is the role binding for admin role script:

```
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: argo
  name: argo-admin-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: argo-admin
subjects:
  - kind: User
    name: <UserEmail>
    apiGroup: rbac.authorization.k8s.io
```

ive also addded service account iw binding, this is the service account script:

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: argo
  annotations:
    # match on the OIDC subject == the staticPasswords.userID:
    workflows.argoproj.io/rbac-rule: ""sub == <UserId>""
    workflows.argoproj.io/rbac-rule-precedence: ""1""
```

also, both services are using self sign certificate, therefore Ive added the ca certificate of each service to the other service using volumes.","kubernetes, argo, rke2",,,,2025-04-18T07:13:41
79578422,in k8s can a pod be OOMKilled if wss exceeds memory limits,"The memory limit on the pod is 1gb, the WSS does cross 1gb and RSS is always below 512mb. We are seeing the pod getting OOM killed when WSS cross 1gb.
As per this article
[https://community.ibm.com/community/user/instana/blogs/leo-varghese/2024/06/04/kubernetes-memory-metrics#:~:text=In%20most%20cases%2C%20it's%20confusing,metric%20values%20reach%20the%20limits](https://community.ibm.com/community/user/instana/blogs/leo-varghese/2024/06/04/kubernetes-memory-metrics#:%7E:text=In%20most%20cases%2C%20it%27s%20confusing,metric%20values%20reach%20the%20limits).
OOM killed can happen when either RSS or WSS exceeds memory limits.
Based on my reading WSS incldues reclaimable memory. so does this mean that the OS was not able to reclaim memory when it hit limits and k8s killed the pod?","kubernetes, amazon-eks, cadvisor",,,,2025-04-17T03:22:57
79578349,APISIX ingress controller on Kind cluster not routing requests correctly,"I am trying to setup APISIX gateway ingress controller enabled application (spring-boot) on a local KIND kubernetes cluster.
Here are the steps that I followed,

I was able to succesfully install and configure the APISIX gateway (PORT: 8090) in my local kind kubernetes cluster.
I can confirm that an ingress manifest is translated to an APISIX route correctly.
The target application is configured with a service ""client-app"" and I can confirm that it does have the corresponding endpoints configured correctly.

## Setup

Ingress YAML:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: httpserver-ingress
  namespace: oidcapp
spec:
  # we use APISIX Ingress and it watches Ingress resources with ""apisix"" ingressClassName
  ingressClassName: apisix
  rules:
  - host: authclient.com
    http:
      paths:
      - backend:
          service:
            name: client-app
            port:
              number: 80
        path: /oidcapp
        pathType: Prefix
```

Target Application service:

```
$ kubectl describe svc -n oidcapp client-app
Name:                     client-app
Namespace:                oidcapp
Labels:                   app=client-app
Annotations:              <none>
Selector:                 app=client-app
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.136.11
IPs:                      10.96.136.11
Port:                     http  80/TCP
TargetPort:               8080/TCP
Endpoints:                10.244.1.23:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
```

APISIX route mapping:

```
{
    ""createdIndex"": 11332,
    ""key"": ""/apisix/routes/33d660f9"",
    ""modifiedIndex"": 11532,
    ""value"": {
        ""priority"": 0,
        ""status"": 1,
        ""uris"": [
            ""/oidcapp"",
            ""/oidcapp/*""
        ],
        ""name"": ""ing_oidcapp_httpserver-ingress_4cafc3f3"",
        ""id"": ""33d660f9"",
        ""upstream_id"": ""fdcb23fc"",
        ""host"": ""authclient.com"",
        ""create_time"": 1744820603,
        ""update_time"": 1744851162,
        ""desc"": ""Created by apisix-ingress-controller, DO NOT modify it manually"",
        ""labels"": {
            ""managed-by"": ""apisix-ingress-controller""
        }
    }
}
```

I then installed a Loadbalancer in Kind cluster, following the instructions below: [Kind Docs | LoadBalancer](https://kind.sigs.k8s.io/docs/user/loadbalancer/)

Based on these instructions, added the following Service manifest:

```
kind: Service
apiVersion: v1
metadata:
  name: apisix-gateway-service
  namespace: apisix
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: apisix
  ports:
  - port: 5678
    targetPort: 8090
```

In my etc/hosts, I have configured the following host-mapping:

```
172.18.0.2      authclient.com
```

Here are the relevant kubectl command output:

```
$ kubectl get svc -n apisix

NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
apisix-admin             ClusterIP      10.96.220.99    <none>        9180/TCP            7d18h
apisix-gateway-service   LoadBalancer   10.96.183.128   172.18.0.2    5678:31448/TCP      80m
etcd-headless            ClusterIP      None            <none>        2379/TCP,2380/TCP   7d18h
```

## Problem

When I execute the following curl command, I get a *connection reset* error:

```
$ curl -v http://authclient.com:5678/oidcapp
*   Trying 172.18.0.2:5678...
* TCP_NODELAY set
* Connected to authclient.com (172.18.0.2) port 5678 (#0)
> GET /oidcapp HTTP/1.1
> Host: authclient.com:5678
> User-Agent: curl/7.68.0
> Accept: */*
>
* Recv failure: Connection reset by peer
* Closing connection 0
curl: (56) Recv failure: Connection reset by peer
```

I expect the CURL request to be forwarded to an apisix-gateway pod and then routed to one of the service endpoints determined by the gateway's route mapping.

```
cURL ---> Kind LoadBalancer ---> APISIX ---> client-app
```

Unfortunately, the logs of load balancer and apisix-gateway-ingress-controller does not provide any further details.","kubernetes, kind, apache-apisix",79579244.0,"## TL;DR

The problem is that you are binding the Service to the wrong port: `8090`.

## Explaination

The reason why your CURL request fails, is that:

- in APISIX container, port `8090` is not mapped to anything;
- even if it were, there's nothing that is listening on that port.

In fact, by default, APISIX listens for HTTP traffic on port `9080`. Therefore, the YAML for your Service should look like this:

```
kind: Service
apiVersion: v1
metadata:
  name: apisix-gateway-service
  namespace: apisix
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: apisix
  ports:
  - port: 5678
    targetPort: 9080 # <-- APISIX proxy port
```

Some useful references:

- [APISIX Docs | FAQ: How do I configure Apache APISIX to listen on multiple ports when handling HTTP or HTTPS requests?](https://apisix.apache.org/docs/apisix/FAQ/#how-do-i-configure-apache-apisix-to-listen-on-multiple-ports-when-handling-http-or-https-requests)
- [GitHub apache/apisix | config.yaml.example](https://github.com/apache/apisix/blob/master/conf/config.yaml.example)",2025-04-17T12:26:59,2025-04-17T02:10:07
79576136,Should liveness and readiness endpoints be `.well-known`?,"I don't think there is definitive guides on this, but I've always felt that liveness and readiness endpoints -- in the context of kubernetes -- should be exposed under `/.well-known/`. I looked for similar questions and found:

- [https://en.wikipedia.org/wiki/Well-known_URI](https://en.wikipedia.org/wiki/Well-known_URI)
- [well-known routes Kubernetes](https://stackoverflow.com/questions/71599523/well-known-routes-kubernetes)
- [Where does the convention of using /healthz for application health checks come from?](https://stackoverflow.com/questions/43380939/where-does-the-convention-of-using-healthz-for-application-health-checks-come-f/43381061#43381061)

Wikipedia says

> A well-known URI is a Uniform Resource Identifier for URL path prefixes that start with `/.well-known/`. They are implemented in webservers so that requests to the servers for well-known services or information are available at URLs consistent well-known locations across servers.

And then goes on to list a bunch of well-known endpoints for various services. I don't think the list is prescriptive, we could serve `/.well-known/live` and `/.well-known/ready` for the microservices we're building at my company, but my question is really, *would we be breaking any rules about well-known URIs by doing this*, or should we just stick to `/livez` and `/readyz`?","kubernetes, devops",79576264.0,"[RFC 8615 §3](https://www.rfc-editor.org/rfc/rfc8615.html#section-3) says

> Applications that wish to mint new well-known URIs MUST register them, following the procedures in [Section 5.1](https://www.rfc-editor.org/rfc/rfc8615.html#section-5.1)....

[The registry](https://www.iana.org/assignments/well-known-uris/) (pointed to by [§3.1](https://www.rfc-editor.org/rfc/rfc8615.html#section-3.1)) doesn't obviously list anything health-check oriented.  The text above suggests to me that you're really not supposed to put anything under `/.well-known` that's not in the registry.

That having been said, the various well-known paths are just machine-readable content being published at canonical per-host URL paths; there's nothing technically special about `/.well-known` URLs beyond the RFC conventions.  Particularly if your individual services will never be directly exposed to the public Internet, nothing is stopping you from using those URL paths if you really want to.",2025-04-16T01:38:24,2025-04-15T22:43:48
79575389,OAUTH2 Proxy: &quot;id token issued by a different provider&quot; OIDC provider,"I have deployed OAUTH2 proxy in AKS and using OIDC as the provider. Below are the parameters that I pass to the OAUTH2 proxy deployment:

```
- --http-address=0.0.0.0:4180
- --metrics-address=0.0.0.0:44180
- --azure-tenant=xxx
- --cookie-refresh=5m
- --email-domain=*
- --oidc-issuer-url=https://login.microsoftonline.com/xxx/v2.0
- --pass-authorization-header=true
- --provider=oidc
- --skip-jwt-bearer-tokens=true
- --set-authorization-header=true
- --upstream=file:///dev/null
- --config=/etc/oauth2_proxy/oauth2_proxy.cfg
```

I got the token from Azure App Registration like this:

```
curl -X POST -H ""Content-Type: application/x-www-form-urlencoded"" -d 'client_id=xxxx&scope=xxxx/.default&client_secret=xxxx&grant_type=client_credentials' 'https://login.microsoftonline.com/xxx/oauth2/v2.0/token'
```

But I get this error in OAUTH2 proxy:

```
Error retrieving session from token in Authorization header: [unable to verify bearer token, oidc: id token issued by a different provider, expected ""https://login.microsoftonline.com/xxx/v2.0"" got ""https://sts.windows.net/xxxx/""]
```

Any idea why it shows this error?","kubernetes, oauth-2.0, oauth2-proxy",,,,2025-04-15T14:19:22
79575214,Why use Kubernetes EmptyDir volumes instead of container filesystem?,"My colleagues develop an application where its logs are generated inside the container in a directory created via Dockerfile (e.g. /applogs). This container image, after built, is deployed as a Kubernetes Pod.

I've tried to reason with them that this kind of storage would be better suited into an EmptyDir volume defined in the Pod (Deployment) manifest - since they don't care about it being ephemeral - however I did not know exactly why, it just looks a bit of a dirty solution to store dynamic data inside the container itself and not use a native Kubernetes facility like either a volume or PV.

I've read that using a container filesystem might implicate in a performance penalty, due to the need of relying on the container runtime overlay storage. Does that apply to Kubernetes?

What are your opinions on the matter? I'd appreciate it very much if you could provide some technical documentation on how Kubernetes deals with container storage overlays.","kubernetes, dockerfile, docker-overlay2",79575282.0,"**Send container logs to stdout, not a file.**  Accessing the container filesystem is tricky, regardless of whether there's a mounted volume or not.  A standard practice with any sort of container system is to send logs to stdout, at which point `kubectl logs` can display them.  Kubernetes has a standard [logging architecture](https://kubernetes.io/docs/concepts/cluster-administration/logging/) as well, and popular open-source log collectors can retrieve container logs *if* they're using the standard Kubernetes log system.

If you really must log to a file and skip the standard Kubernetes logging setup:

**An emptyDir volume can give you a writable directory in a read-only filesystem.**  Your Pod spec might set `securityContext: { readOnlyRootFilesystem: true }`, or your cluster administrator might force this setting on you.  This is often a good practice both to avoid accidents overwriting the image's content and to avoid drift where one replica has a local change but another doesn't.  In this case, you need an `emptyDir:` volume for your log directory, which can be mounted read-write, even if the rest of the container filesystem is read-only.

Persistent volumes feel wrong to me here.  They can force you into using a StatefulSet to manage replicas, when you don't actually need most of its properties.  Many of the volume types that are straightforward to acquire still aren't externally readable; your cluster log collector won't be able to read a stray AWS EBS volume, for example.  The one advantage is that you won't lose your container logs on exit.  Beware ReadWriteMany volumes: they can be difficult to acquire (do you need an external NFS server?) and there's a real risk of conflict if multiple replicas are naïvely writing to the same log file.

**If you don't need an emptyDir volume's properties, you may not need a volume at all.**  An `emptyDir:` volume has two important properties.  As noted above, it can be a writable location in a read-only filesystem; and its content can be shared between containers in the same Pod.  If you don't need either of these properties, then writing straight into the container filesystem is just fine.

**You're not going to notice any performance difference here** (or else something bigger is wrong).  There should be negligible performance difference between the container filesystem and an `emptyDir:` volume.  A PersistentVolume could be slower if it's over the network.  But logging shouldn't usually be your critical path.  If it is, you'll also notice performance issues from a very large quantity of disk writes in a non-container setup.  Turn down your application's logging level, or remove unnecessary logging calls from your code.  (Don't run with debug- or trace-level logging by default.)",2025-04-15T13:32:04,2025-04-15T12:58:47
79574724,Unable to send Kafka Message after Login,"I have a single node kafka cluster set up with helm and kubernetes on [rancher desktop](https://rancherdesktop.io/). Everytime my java/spring-boot application starts, it cann log into kafka, start the producer but then fails to  send a message.

The kafka cluster is deployeyd with the help of the [bitnami helm script](https://github.com/bitnami/charts/tree/main/bitnami/kafka).

This is my value.yaml

```
controller:
  replicaCount: 1
  persistence:
    size: 1Gi
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  resources:
    limits:
      memory: 2Gi
  automountServiceAccountToken: true

externalAccess:
  enabled: true
  service:
    type: LoadBalancer
    ports:
      external: 9094
  autoDiscovery:
    enabled: true

broker:
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false

sasl:
  client:
    users: [ ""myuser"" ]
    passwords: [ ""mypassword"" ]

rbac:
  create: true

# siehe https://github.com/bitnami/charts/issues/19522
extraConfig: |
  deleteTopicEnable=true
  auto.create.topics.enable=false
  offsets.topic.replication.factor=1
  transaction.state.log.replication.factor=1

provisioning:
  enabled: true
  topics:
    - name: heartBeat
```

Wen I start my java application I do get this log output:

```
[2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.clients.producer.ProducerConfig.logAll - traceid= - ProducerConfig values:
    acks = -1
    auto.include.jmx.reporter = true
    batch.size = 16384
    bootstrap.servers = [localhost:9094]
    ...

org.springframework.kafka.support.serializer.JsonSerializer
    [2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector.init - traceid= - initializing Kafka metrics collector
    [2025-04-15 10:37:05,198] [main] severity=INFO - org.apache.kafka.clients.producer.KafkaProducer.configureTransactionState - traceid= - [Producer clientId=producer-1] Instantiated an idempotent producer.
    [2025-04-15 10:37:05,207] [main] severity=INFO - org.apache.kafka.common.security.authenticator.AbstractLogin.login - traceid= - Successfully logged in.
    [2025-04-15 10:37:05,213] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka version: 3.8.1
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka commitId: 70d6ff42debf7e17
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka startTimeMs: 1744706225213
    [2025-04-15 10:37:05,451] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.Metadata.update - traceid= - [Producer clientId=producer-1] Cluster ID: jg0sCtb1jEoEwK8lBIlDGj
    [2025-04-15 10:37:26,496] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.NetworkClient.handleDisconnections - traceid= - [Producer clientId=producer-1] Node 0 disconnected.
    [2025-04-15 10:37:26,497] [kafka-producer-network-thread | producer-1] severity=WARN - org.apache.kafka.clients.NetworkClient.processDisconnection - traceid= - [Producer clientId=producer-1] Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

After login it somehow tries to connect

```
Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

This does of course not work, since kafka is only available via `localhost:9094`. How can it setup-up my Client or Kafka to use the correct address to send the message?","kubernetes, apache-kafka, rancher-desktop",79575445.0,"There are two problems.

First there are breaking changes in the externalAccess section, and second as pointed out by @poisened_monkey the advertised listeners have to be configured.

The whole externAccess section has to be replace by this

```
externalAccess:
  enabled: true
  autoDiscovery:
    enabled: true
  broker:
    service:
      type: LoadBalancer
      ports:
        external: 9094
  controller:
    service:
      type: LoadBalancer
    containerPorts:
      external: 9094

defaultInitContainers:
  autoDiscovery:
    enabled: true

serviceAccount:
  create: true

rbac:
  create: true

listeners:
  advertisedListeners: CLIENT://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9092,INTERNAL://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9094,EXTERNAL://localhost:9094
```

The advertisedListeners list is sent to the client after successfull login. For the external client the correct address is localhost:9094.",2025-04-15T14:39:45,2025-04-15T08:49:38
79574724,Unable to send Kafka Message after Login,"I have a single node kafka cluster set up with helm and kubernetes on [rancher desktop](https://rancherdesktop.io/). Everytime my java/spring-boot application starts, it cann log into kafka, start the producer but then fails to  send a message.

The kafka cluster is deployeyd with the help of the [bitnami helm script](https://github.com/bitnami/charts/tree/main/bitnami/kafka).

This is my value.yaml

```
controller:
  replicaCount: 1
  persistence:
    size: 1Gi
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  resources:
    limits:
      memory: 2Gi
  automountServiceAccountToken: true

externalAccess:
  enabled: true
  service:
    type: LoadBalancer
    ports:
      external: 9094
  autoDiscovery:
    enabled: true

broker:
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false

sasl:
  client:
    users: [ ""myuser"" ]
    passwords: [ ""mypassword"" ]

rbac:
  create: true

# siehe https://github.com/bitnami/charts/issues/19522
extraConfig: |
  deleteTopicEnable=true
  auto.create.topics.enable=false
  offsets.topic.replication.factor=1
  transaction.state.log.replication.factor=1

provisioning:
  enabled: true
  topics:
    - name: heartBeat
```

Wen I start my java application I do get this log output:

```
[2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.clients.producer.ProducerConfig.logAll - traceid= - ProducerConfig values:
    acks = -1
    auto.include.jmx.reporter = true
    batch.size = 16384
    bootstrap.servers = [localhost:9094]
    ...

org.springframework.kafka.support.serializer.JsonSerializer
    [2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector.init - traceid= - initializing Kafka metrics collector
    [2025-04-15 10:37:05,198] [main] severity=INFO - org.apache.kafka.clients.producer.KafkaProducer.configureTransactionState - traceid= - [Producer clientId=producer-1] Instantiated an idempotent producer.
    [2025-04-15 10:37:05,207] [main] severity=INFO - org.apache.kafka.common.security.authenticator.AbstractLogin.login - traceid= - Successfully logged in.
    [2025-04-15 10:37:05,213] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka version: 3.8.1
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka commitId: 70d6ff42debf7e17
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka startTimeMs: 1744706225213
    [2025-04-15 10:37:05,451] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.Metadata.update - traceid= - [Producer clientId=producer-1] Cluster ID: jg0sCtb1jEoEwK8lBIlDGj
    [2025-04-15 10:37:26,496] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.NetworkClient.handleDisconnections - traceid= - [Producer clientId=producer-1] Node 0 disconnected.
    [2025-04-15 10:37:26,497] [kafka-producer-network-thread | producer-1] severity=WARN - org.apache.kafka.clients.NetworkClient.processDisconnection - traceid= - [Producer clientId=producer-1] Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

After login it somehow tries to connect

```
Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

This does of course not work, since kafka is only available via `localhost:9094`. How can it setup-up my Client or Kafka to use the correct address to send the message?","kubernetes, apache-kafka, rancher-desktop",79574795.0,"Add to your `values.yaml`

```
controller:
  extraEnvVars:
    - name: KAFKA_CFG_ADVERTISED_LISTENERS
      value: PLAINTEXT://localhost:9094
    - name: KAFKA_CFG_LISTENERS
      value: PLAINTEXT://:9094
```",2025-04-15T09:23:34,2025-04-15T08:49:38
79574545,Spark executor memory limit higher than expected,"I'm running Spark on EKS with Fargate. I don't quite understand why the memory of the Spark executor is always different from my expectations. When I set:

```
spark.executor.memory = 6g
spark.executor.memoryOverhead = 0.10
spark.memory.offHeap.size = 0
```

the resource limit of the Spark executor pod is actually 8601Mi. I don't understand why this is the case. (6144*0.10)+6144=6,758.4.
What else is occupying 1,842.6M of memory?

If I change the `spark-submit` parameters to:

```
spark.executor.pyspark.memory = 1g
spark.executor.memory = 6g
spark.executor.memoryOverhead = 0.10
spark.memory.offHeap.size = 0
```

then the resource limit of the Spark executor pod is 9625Mi.
9625 - ((6144*0.10)+6144+1024) = 1,842.6.

The larger the memory value of the Spark executor is, the greater the difference will be.","apache-spark, kubernetes, pyspark, amazon-eks, aws-fargate",79574601.0,"This is because for Spark on Kubernetes non-JVM tasks, the `spark.executor.memoryOverhead` default value is 0.4.

From the [Spark on Kubernetes documentation, ""Configuration"" section](https://spark.apache.org/docs/3.5.4/running-on-kubernetes.html#configuration):

> This sets the Memory Overhead Factor that will allocate memory to non-JVM memory, which includes off-heap memory allocations, non-JVM tasks, various systems processes, and `tmpfs`-based local directories when `spark.kubernetes.local.dirs.tmpfs` is true. For JVM-based jobs this value will default to 0.10 and 0.40 for non-JVM jobs. This is done as non-JVM tasks need more non-JVM heap space and such tasks commonly fail with ""Memory Overhead Exceeded"" errors. This preempts this error with a higher default. This will be overridden by the value set by `spark.driver.memoryOverheadFactor` and `spark.executor.memoryOverheadFactor` explicitly.",2025-04-15T07:43:05,2025-04-15T07:04:40
79574416,Unable to delete pod from EC2 instance,"I have an EKS cluster running, and I use an EC2 machine to submit jobs that get scheduled on the EKS cluster. The EKS cluster and EC2 machine are attached with an IAM role, which has access to multiple AWS accounts for business requirements. The IAM role is configured following this [AWS documentation](https://aws.amazon.com/blogs/containers/enabling-cross-account-access-to-amazon-eks-cluster-resources/).

I'm able to run all the `kubectl` commands from the EC2 except `kubectl delete pod <pod_name>` which results in the following error:

What could be missing here?

> Error from server (Forbidden): pods ""test-cronjob-29077280-s4gbn"" is forbidden: node ""EKSGetTokenAuth"" can only delete pods with spec.nodeName set to itself","amazon-web-services, kubernetes, amazon-ec2, amazon-iam, amazon-eks",79574426.0,"You need to ensure your IAM role (used by EC2) is correctly mapped to a Kubernetes user or group that has the correct RBAC permissions.

1.Check your IAM role ARN:

`aws sts get-caller-identity`

You’ll get something like:

`arn:aws:sts::123456789012:assumed-role/MyEKSRole/i-xxxxxxxxxxxx`

2.Update `aws-auth` `ConfigMap`:

Map the IAM role to a Kubernetes user or group:

```
mapRoles: |
  - rolearn: arn:aws:iam::123456789012:role/MyEKSRole
    username: ec2-user
    groups:
      - system:masters
```

You can edit the `ConfigMap` using:

`kubectl edit configmap aws-auth -n kube-system`",2025-04-15T05:27:01,2025-04-15T05:13:46
79571796,Access Refused Rabbit MQ get message from queue,"I am using the azure provider in rabbitmq and when I use my federated user that has the administrator tag (role), I receive an unauthorized message (401).

Return from the authentication api:

`{""error"":""not_authorised"",""reason"":""Access refused.""}`

The JWT follows this format:

```
{
""aud"": ""xxxxxxxxxx-xxxxx-9d73-7e5a34de6cf1"",
""iss"": ""https://sts.windows.net/858c1424-15c5-4788-bc86-9ef4bdcee2f7/"",
""iat"": 1744559994,
""nbf"": 1744559994,
""exp"": 1744565265,
""acr"": ""1"",
""aio"": ""dsad12exxxxxxxxxxxxxxxxxxxQG5pgTLpNOeaIQCz8scW+5WiLnl77/sJsSz5GpunIaU9OngiY7DuRHccLLtGWMR"",
 ""amr"": [
 ""pwd"",
 ""mfa""
 ],
 ""appid"": ""xxxxxxxxxxxx-xxxxx-48f0-9d73-xxxxxxxxx"",
 ""appidacr"": ""1"",
 ""family_name"": ""xxxxx"",
 ""given_name"": ""Renato"",
 ""ipaddr"": ""xxxx:xxx:xxxxx:xx:xx:xxxx:xx:xx"",
 ""name"": ""Renato de Souza"",
 ""oid"": ""xxxxxxxxx-2d19-483c-85a1-xxxxxxx"",
 ""onprem_sid"": ""S-1-5-21-xxxxxxxxxxxx07146231-1581001300-27402"",
 ""rh"": ""1.ASDQWxxxxxxxxxxxxxxxx8hp70vc7i97dxR132DASPBInXN-WjTebPFRAZsGAA."",
 ""roles"": [
 ""xxxxxxxxx-dsds-xxxx-9d73-xaadsdas.tag:administrator""
 ],
 ""scp"": ""email profile User.Read"",
 ""sid"": ""xxxxxxxxxxxxxxxxx49-98499ed1a05f"",
 ""sub"": ""xxxxxxxxxxxxxgjDV2_wxxxxxHEbGgipM"",
 ""tid"": ""xxxxxxxxxx-15c5-4788-xxxxxx-9ef4bdcee2f7"",
 ""unique_name"": ""renato@xxxx.com.br"",
 ""upn"": ""renato@xxxx.com.br"",
 ""uti"": ""xxxxxxxxxxxxxxxxxxx"",
 ""see"": ""1.0""
}
```

Log error rabbit:
Example create queue in other vhost:

```
2025-04-13 16:50:06.101210+00:00 [warning] <0.2662.0> Declare queue error: configure access to queue 'teste123' in vhost 'meu-vhost' refused for user 'zmAcvRqHKuaxhQ6s3kCWgjDV2_wubp49XrEHEbGgipM'
```

Steps to reproduce:

Environment: Kubernetes
Rabbit version: docker.io/bitnami/rabbitmq:3.12.14-debian-12-r7

Role Azure Portal:

`xxx-38a1-xxxx-9d73-7e5a34de6cf1.tag:administrator`

rabbit.config:

```
## OIDC Configuration
    auth_backends.1 = rabbit_auth_backend_oauth2
   auth_backends.2 = rabbit_auth_backend_internal

   management.oauth_enabled = true
   management.oauth_client_id = CLIENT_ID
   management.oauth_client_secret = SECRET_ID
   management.oauth_provider_url = https://login.microsoftonline.com/TENANT_ID

   auth_oauth2.resource_server_id = CLIENT_ID
   auth_oauth2.additional_scopes_key = roles
   auth_oauth2.jwks_url = https://login.microsoftonline.com/TENANT_ID/discovery/keys?appid=CLIENT_ID
```

Expected behavior
Azure user must be able to retrieve messages in queues.","kubernetes, rabbitmq, message",,,,2025-04-13T16:53:20
79570247,How to Handle Requeue and Prevent Event Queue Overflow in a k8s Operator?,"I encountered an issue related to `requeue` while developing an operator for `CustomerCronJob`. Consider this scenario: after creating a `CustomerCronJob`, I create and start a pod, then return `requeueAfter(nextTime)`. However, there is an external program continuously updating the status of the job, causing reconcile to be triggered repeatedly. Assuming each reconcile returns `requeueAfter(nextTime)` and ignoring the deduplication mechanism of controller-runtime, does this mean that the events in the queue will keep increasing? If such a problem exists, how should it be avoided?

```
func (r *CustomerCronJobReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
    _ = log.FromContext(ctx)

    // ...

    // list job and update CustomerCronJob status

    // ....

    // determine whether to create a new task

    // ...

    // get next run time
    nextRunTime := ...

    // ...

    // If the CustomerCronJob status is continuously updated and there are no exceptions during each reconcile, will it eventually lead to RequeueAfter causing the queue to keep growing?
    return ctrl.Result{
        RequeueAfter: nextRunTime.Sub(time.Now()),
    }, nil
}
```","go, kubernetes, controller-runtime",79584848.0,"I am wondering if you have practically checked and found that the queue is increasing? Because as far as I understand, this doesn't happen. Here is what happens.

The `controller-runtime` workqueue is smart. When you return `RequeueAfter`, it schedules the item (namespace/name) to be added back later. However due to and external program, if  a reconcile for the same item before that timer expires triggers, the reconcile runs immediately. If that reconcile also returns `RequeueAfter`, it essentially updates the single scheduled requeue timer for that item. You won't get an infinite build-up of pending scheduled requeues for the same object.

But there is a bigger problem here!

**Why every status update triggers your entire reconcile loop, even if it's not time for a job run??**

In my opinion, you can use predicates to setup the controller and not trigger reconcile every time `.status` changes.",2025-04-21T15:04:16,2025-04-12T09:20:49
79569961,Kubernetes - ValidatingAdmissionPolicy - Not Honoring Expression,"I have recently begun working with Kubernetes ValidatingAdmissionPolicies. I would like to set a max replica policy for my Deployment, so that no matter who/what is modifying `spec.replicas`, I can block the request. Eventually I would like to apply this as a cluster-wide policy so broken staging deployments don't scale out to 1000s of replicas. This is what I've been working with:

```
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicy
metadata:
  name: ""max-replicas-policy""
spec:
  failurePolicy: Fail
  matchConstraints:
    resourceRules:
    - apiGroups:   [""apps""]
      apiVersions: [""v1""]
      operations:  [""CREATE"", ""UPDATE""], // Why is PATCH not supported?
      resources:   [""deployments""]
  validations:
    - expression: ""object.spec.replicas <= 5""
      reason: Invalid
      message: ""The number of replicas cannot exceed 5.""
---
apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingAdmissionPolicyBinding
metadata:
  name: ""max-replicas-policy-binding""
spec:
  policyName: ""max-replicas-policy""
  validationActions: [Deny]
  matchResources:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: wdatp-infra-system
```

When I attempt to update my resource using `kubectl apply`, the error message is recorded as expected.

```
$ kubectl apply -f ~/Downloads/test-deployment.yaml
The deployments ""test-deployment"" is invalid: : ValidatingAdmissionPolicy 'max-replicas-policy' with binding 'max-replicas-policy-binding' denied request: The number of replicas cannot exceed 5.
```

## So what's the problem?

What I am struggling to understand is why this *admission* policy doesn't work for various other use-cases. I suspect it may be related to `PATCH` operation not being supported, but would love for an expert to chime in.

Examples of weird behavior I've seen:

### kubectl edit

If I edit the spec.replicas manually using kubectl edit and save, it blocks, but doesn't output my error specific message like kubectl apply. Why doesn't this output the error message?

```
$ kubectl edit deployment test-deployment -n wdatp-infra-system
error: deployments.apps ""test-deployment"" is invalid
```

### kubectl scale deployment

If I scale the deployment manually, I can completely override the policy. Why?

```
$ kubectl scale deployment test-deployment --replicas=6 -n wdatp-infra-system
deployment.apps/test-deployment scaled // this should fail with error message

$ kubectl get deployment test-deployment -n wdatp-infra-system
NAME              READY   UP-TO-DATE   AVAILABLE   AGE
test-deployment   6/6     6            6           29h
```

### HorizontalPodAutoscaler/ScaledObject (Keda)

When `test-deployment` has a corresponding horizontalPodAutoscaler. The HPA can overwrite the `spec.replicas` without being blocked. Why?","kubernetes, azure-aks",79570327.0,"ValidatingAdmissionPolicies don't support PATCH operations. That's the root cause of such behaviour

`kubectl scale` and KEDA use PATCH operations so they don't invoke ValidatingAdmissionPolicies.",2025-04-12T10:38:04,2025-04-12T02:23:04
79568461,Grafana k8s pods overview table,"I'm trying to construct a grafana dashboard that simply shows me exactly what `kubectl get pods -A` would show me. I cant seem to find any existing dashboards on the marketplace mimicking what i'm trying to achieve.

Specifically i'd love a table that could look like this, maybe also with restarts and age:

```
NAMESPACE              NAME                                                    READY   STATUS             RESTARTS      AGE
authentik              authentik-postgresql-0                                  0/1     Running            1 (17s ago)   53d
authentik              authentik-redis-master-0                                0/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-b6fd485d9-vm569                            1/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-cainjector-dcc5966bc-5g2cn                 1/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-webhook-dfb76c7bd-f88md                    0/1     Running            1 (17s ago)   53d
```

For the data source i'm using prometheus with all the standard metrics such as:

- `kube_pod_...`
- `container_...`

Can anyone more experienced that me give me a query or a dashboard i could import? :)","kubernetes, prometheus, grafana",79568594.0,"You can use some kind of these queries:

**Pod status**

`kube_pod_status_phase`

**Container Ready Status**

`sum by (namespace, pod) (kube_pod_container_status_ready{condition=""true""})`

**Restart Count:**

`sum by (namespace, pod) (kube_pod_container_status_restarts_total)`

**Pod Creation Timestamp**

`kube_pod_created`",2025-04-11T10:18:29,2025-04-11T09:09:50
79568461,Grafana k8s pods overview table,"I'm trying to construct a grafana dashboard that simply shows me exactly what `kubectl get pods -A` would show me. I cant seem to find any existing dashboards on the marketplace mimicking what i'm trying to achieve.

Specifically i'd love a table that could look like this, maybe also with restarts and age:

```
NAMESPACE              NAME                                                    READY   STATUS             RESTARTS      AGE
authentik              authentik-postgresql-0                                  0/1     Running            1 (17s ago)   53d
authentik              authentik-redis-master-0                                0/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-b6fd485d9-vm569                            1/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-cainjector-dcc5966bc-5g2cn                 1/1     Running            1 (17s ago)   53d
cert-manager           cert-manager-webhook-dfb76c7bd-f88md                    0/1     Running            1 (17s ago)   53d
```

For the data source i'm using prometheus with all the standard metrics such as:

- `kube_pod_...`
- `container_...`

Can anyone more experienced that me give me a query or a dashboard i could import? :)","kubernetes, prometheus, grafana",79568577.0,"`kube_pod_info` → namespace, pod name

`kube_pod_status_phase` → status

`kube_pod_container_status_ready` → ready containers

`kube_pod_container_status_restarts_total` → restarts

`kube_pod_created` → age (use transformation: now() - created)

Use Table panel with outer joins in transformations to merge by pod and namespace. This will give you a table with:

`NAMESPACE | NAME | READY | STATUS | RESTARTS | AGE`

Also you can use json from readymade dashboard available at market place.",2025-04-11T10:11:52,2025-04-11T09:09:50
79568076,Aws s3 presigned url showing ExpiredToken before the actual expiration time when running in k8s pod,"I am using AWS SDK to generate a presigned S3 object URL and sending it as a mail notification. Even though I am setting the expiration time to 2 days, I am getting below error on clicking the link immediately when its generating.

```
<Error>
<Code>ExpiredToken</Code>
<Message>The provided token has expired.</Message>
</Error>
```

I observe this issue is happening when I am running the application in k8s pod and using k8s service account IAM role.

Also I have a question about how should I register the AWS SDK in the .NET core as transient, singleton or scoped.

```
builder.Services.AddTransient<IAwsS3Service, AwsS3Service>();
// Configure AWS credentials and region
try
{
    builder.Services.AddDefaultAWSOptions(new AWSOptions
    {
        Region = RegionEndpoint.EUWest1
    });

    // Register AWS S3 service
    builder.Services.AddAWSService<IAmazonS3>();
    Log.Information(""AWS S3 service registered successfully."");
}
catch (Exception ex)
{
    Log.Error(ex, ""Failed to register AWS S3 service."");
    throw;
}
```

Code for generating the URL:

```
public string GeneratePresignedUrl(string bucketName, string key, TimeSpan expiration)
        {
            var request = new GetPreSignedUrlRequest
            {
                BucketName = bucketName,
                Key = key,
                Expires = DateTime.UtcNow.Add(expiration),
                Protocol = Protocol.HTTPS // Use HTTPS for secure access
            };

            return _s3Client.GetPreSignedURL(request);
        }
```","amazon-web-services, kubernetes, amazon-s3, .net-core, aws-sdk-net",79568427.0,"The S3 presigned URL expires at the time the credentials used to create the URL expire, from [the documentation](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html):

> If you created a presigned URL using a temporary credential, the URL expires when the credential expires. In general, a presigned URL expires when the credential you used to create it is revoked, deleted, or deactivated. This is true even if the URL was created with a later expiration time.

This is why you see the URL has expired before 48 hours. It is also only possible to create presigned URLs with expiration times greater than 36 hours [by using IAM users](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-presigned-url.html#who-presigned-url) with an access and secret key.",2025-04-11T08:57:25,2025-04-11T05:43:59
79567817,Run docker containers in Airflow hosted in kubernetes,"I have been trying to deploy airflow using the helm chart (as a subchart) on a kubernetes cluster that is hosted in my homelab.

I am facing some issues when migrating the dags I have been testing locally (on docker-compose) to the kubernetes cluster.
when you use airflow on kubernetes, is it possible to use the CeleryExecutor? if so, how should I change the DockerOperators in my dags? When trying to find an answer, i dtumbled upon the following stackoverflow post suggesting that using the DockerOperator is not a good choice when running on kubernetes , is that correct?

- Does this mean I shouldn't use the CeleryExecutor ?
- What are the best practice when migrating dags from docker-compose to kubernetes ?
Thanks","docker, kubernetes, airflow, dockeroperator, kubernetesexecutor",79567968.0,"1. Can you use CeleryExecutor on Kubernetes?

Yes, you can. It’s supported and works fine in k8s. You’ll just need:

- A Redis/RabbitMQ broker
- A result backend (like Postgres)
- Workers running as separate pods (as managed by the Helm chart)

But most people go with the KubernetesExecutor with KubernetesPodOperator instead, because they:

- Scale tasks as isolated pods
- Fit Kubernetes better
- Avoid the complexity of managing celery queues, concurrency, etc.

1. Should you avoid DockerOperator in Kubernetes?

Yes, generally.

Why?

- DockerOperator runs Docker containers inside a pod.
- That means the pod must be able to run Docker-in-Docker, or the host must expose the Docker socket (e.g., /var/run/docker.sock), which is:
  - Messy
  - A security risk
  - Not cloud-native

Alternatives:
Use KubernetesPodOperator instead. It:

- Spins up new pods to run containerized tasks
- Works naturally with Kubernetes
- Doesn’t require Docker socket access

1. Does this mean you shouldn’t use CeleryExecutor?

Not necessarily. You can continue using it, but you might consider migrating to:

- KubernetesExecutor (simple, scalable, each task = pod)
- CeleryKubernetesExecutor (hybrid, runs most tasks on celery, dynamic ones in pods)

It depends on your workloads. If you’re already comfortable with Celery, and need specific control over parallelism, queues, or workers—keep it.

Read more about [KubernetesPodOperator](https://www.astronomer.io/docs/learn/kubepod-operator/).",2025-04-11T03:53:29,2025-04-11T01:07:32
79567460,Spring Cloud Kubernetes does not reload properties from ConfigMap (watch and polling both fail to refresh),"I'm using Spring Boot 3.1.4 and Spring Cloud 2022.0.5 with the dependency:

```
<dependency>
  <groupId>org.springframework.cloud</groupId>
  <artifactId>spring-cloud-starter-kubernetes-fabric8-config</artifactId>
  <version>3.0.5</version> <!-- explicitly overridden -->
</dependency>
```

My goal is to dynamically populate a list of values when the Kubernetes ConfigMap is edited on the fly, without restarting the application.
Here's what I have:

@ConfigurationProperties bean with @RefreshScope

ConfigMap is loaded correctly at startup
I've enabled both watch mode and polling mode, but neither seems to trigger a refresh

bootstrap.properties:

```
spring.application.name=app-config
spring.cloud.kubernetes.enabled=true
spring.cloud.kubernetes.config.enabled=true
spring.cloud.kubernetes.config.name=app-config
spring.cloud.kubernetes.config.namespace=default
spring.cloud.kubernetes.config.reload.enabled=true
spring.cloud.kubernetes.config.reload.mode=polling
spring.cloud.kubernetes.config.reload.strategy=refresh
spring.cloud.kubernetes.config.reload.period=5000
```

role-reader:

```
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: configmap-reader
  namespace: default
rules:
  - apiGroups: [""""]
    resources: [""configmaps""]
    verbs: [""get"", ""list"", ""watch""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: configmap-reader-binding
  namespace: default
subjects:
  - kind: ServiceAccount
    name: default
    namespace: default
roleRef:
  kind: Role
  name: configmap-reader
  apiGroup: rbac.authorization.k8s.io
```

I verified:

1. The app has RBAC permissions to get, list, watch configmaps
2. The ConfigMap gets mounted and read
3. The @RefreshScope bean is injected properly and used in a REST
controller

But when I edit the ConfigMap, nothing happens:

1. No logs appear (even with TRACE logging)
2. The updated config values never show up via REST
3. @RefreshScope bean is never re-initialized","java, spring, kubernetes, spring-cloud",,,,2025-04-10T19:43:38
79567334,MountVolume.SetUp failed for volume &quot;&lt;azure_file_name&gt;&quot; : kubernetes.io/csi: mounter.SetUpAt failed,"Suddenly, I am having an issue in Azure AKS where the pod's status hangs at `ContainerCreating`. When I run `kubectl describe pods` I see this

```
Type      Reason            Age             From       Message
----      ------            ----            ----       -------
Warning   FailedMount  34s (x8 over 98s)   kubelet   MountVolume.SetUp failed for volume ""<azure_file_name>"" : kubernetes.io/csi: mounter.SetUpAt failed to get service accoount token attributes: failed to fetch token: serviceaccounts ""default"" is forbidden: audience ""api://AzureADTokenExchange"" not found in pod spec volume
```

Has anyone come across this or have any ideas on how I can troubleshoot? I am not sure what might be the issue, as the cluster was working fine some days ago.","azure, kubernetes, azure-aks, persistent-volumes",79567365.0,There is an [issue](https://github.com/kubernetes/kubernetes/issues/129935?ysclid=m9bp6y1osm375157162) in Kubernetes GitHub. According to this issue it was solved in 1.32,2025-04-10T18:37:34,2025-04-10T18:20:24
79566342,The k8s pod can obtain more gpu memory than volcano specifies,"the specify of volcano queue

```
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: model-deploy
spec:
  weight: 1
  capability:
    cpu: 10
    memory: 20Gi
    volcano.sh/vgpu-number: 2
    volcano.sh/vgpu-memory: 8000
```

the specify of pod tempalte

```
        resources:
          limits:
            cpu: 4
            memory: 8Gi
            volcano.sh/vgpu-number: 1
            volcano.sh/vgpu-memory: 14000
          requests:
            cpu: 4
            memory: 8Gi
            volcano.sh/vgpu-memory: 14000
```

I only set 8000 gpu memory in the queue, but my pod applied for 14000. Logically, this pod should not be run, but in fact it ran successfully.
I want to know how to achieve resource limit","kubernetes, gpu",,,,2025-04-10T10:08:29
79566064,What if you use vdbench for multi-host testing on k8s systems?,"If I use k8s to create 3 containers on each of the 5 servers for a total of 15 containers, what if I use vdbench for multi-host testing?",kubernetes,,,,2025-04-10T08:06:52
79565773,Docker buildx kubernetes driver within ec2/eks,"I have ssh'ed into an ec2 thats run in an eks cluster. Has anybody built images using buildx kubernetes driver?

I can build images locally but when I run:

```
docker buildx create --name kube-driver --driver kubernetes --use --bootstrap --config etc/buildkit.toml
```

```
[+] Building 0.2s (1/1) FINISHED
 => ERROR [internal] booting buildkit                                                                                                                                                                                                                                                                                               0.2s
------
 > [internal] booting buildkit:
------
ERROR: error while calling deploymentClient.Create for ""kube-driver0"": Deployment.apps ""kube-driver0"" is invalid: spec.template.spec.containers[0].volumeMounts[0].name: Not found: ""config-3"".
```

Does anyone know how to fix this?

Were expecting the driver to initiliase.","docker, image, kubernetes, amazon-ec2, buildx",,,,2025-04-10T05:18:15
79564133,Installing packages on node from a pod,"**disclaimer**

I know this is not the way we are supposed to work on Kubernetes cluster and that what want to perform may pose a security risk

**background**

A colleague went on holidays and forgot to document the root password of the nodes of our Kubernetes cluster. I now have to install a simple package on the nodes (nfs-common) in order to be able to mount an NFS volume.

I'm trying to `chroot` on the mounted host filesystem in order to be able to install a package.

In order to do that, I created the following pod:

```
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-pod
spec:
  containers:
  - name: ubuntu-container
    image: ubuntu:24.04
    command: [""/bin/bash"", ""-c"", ""while true; do sleep 30; done;""]
    volumeMounts:
    - name: host-root
      mountPath: /hostfs
    securityContext:
      privileged: true
      capabilities:
        add:
          - SYS_ADMIN
          - SYS_RESOURCE
          - SYS_NICE
          - SYS_PTRACE
          - SYS_BOOT
          - SYS_MODULE
          - SYS_RAWIO
          - SYS_PACCT
          - SYS_NICE
          - SYS_TIME
          - SYS_TTY_CONFIG
          - SYSLOG
          - NET_ADMIN
  hostPID: true
  volumes:
  - name: host-root
    hostPath:
      path: /
  restartPolicy: Never
```

Once the pod is started, I can a shell into it and `chroot /hostfs /bin/bash`.

However, the `apt update && apt install -y nfs-common` fails with the following error:

```
Ign:1 http://archive.ubuntu.com/ubuntu jammy InRelease
Ign:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Ign:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Ign:4 http://archive.ubuntu.com/ubuntu jammy-security InRelease
Ign:1 http://archive.ubuntu.com/ubuntu jammy InRelease
Ign:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Ign:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Ign:4 http://archive.ubuntu.com/ubuntu jammy-security InRelease
Ign:1 http://archive.ubuntu.com/ubuntu jammy InRelease
Ign:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
Ign:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
Ign:4 http://archive.ubuntu.com/ubuntu jammy-security InRelease
Err:1 http://archive.ubuntu.com/ubuntu jammy InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu jammy-security InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
29 packages can be upgraded. Run 'apt list --upgradable' to see them.
```

I believe that I'm missing a capability, but I can't figure it out.",kubernetes,79564375.0,"It looks like a dns issue, not capabilities issue. After `chroot /hostfs /bin/bash` try this:

`cp /etc/resolv.conf /hostfs/etc/resolv.conf`

UPD:

The solution above will kill the name resolution on the host. Instead you can update `/etc/resolv.conf` from `nameserver 127.0.0.1` to a public DNS server.",2025-04-09T12:43:42,2025-04-09T10:45:05
79563268,How to inject secrets into kube-prometheus-stack values.yaml (SMTP),"I have a deployment of kube-prometheus-stack (prometheus-community) and I am trying to inject secrets into the grafana values.yml specifically for the smtp configuration password.

I have created a secret resource in the same namespace as the prometheus/grafana deployment called ""grafana-secrets"", which contains the SENDGRID_API_KEY. I need to ""inject"" this into my values.yml file. Here is what I have;

```
grafana:
  grafana.ini:
    smtp:
      enabled: true
      host: smtp.sendgrid.net:587
      user: apikey
      password: ${SENDGRID_API_KEY}
      from_address: ""my-from-address""
      from_name: Grafana
      skip_verify: false
```","kubernetes, prometheus, grafana, sendgrid",79564400.0,"Just add `envFromSecret`

```
grafana:
  envFromSecret: grafana-secrets
  grafana.ini:
    smtp:
      enabled: true
      host: smtp.sendgrid.net:587
      user: apikey
      password: ${SENDGRID_API_KEY}
      from_address: ""my-from-address""
      from_name: Grafana
      skip_verify: false
```",2025-04-09T12:58:05,2025-04-09T00:55:50
79563007,ActiveMQ Artemis does not display console when runs in K8S,"I deployed `apache/activemq-artemis:2.40.0-alpine` in k8s cluster. First run goes well, but when I open console I'm unable to view literally everything but white list:
[![enter image description here](https://i.sstatic.net/LR6hVtVd.png)](https://i.sstatic.net/LR6hVtVd.png)
[![enter image description here](https://i.sstatic.net/8HdEn9TK.png)](https://i.sstatic.net/8HdEn9TK.png)

Log:

```
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-amqp-protocol]. Adding protocol support for: AMQP                                                                       │
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-hornetq-protocol]. Adding protocol support for: HORNETQ                                                                 │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-mqtt-protocol]. Adding protocol support for: MQTT                                                                       │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-openwire-protocol]. Adding protocol support for: OPENWIRE                                                               │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-stomp-protocol]. Adding protocol support for: STOMP                                                                     │
│ 2025-04-08 18:01:11,298 INFO  [org.apache.activemq.artemis.core.server] AMQ221034: Waiting indefinitely to obtain primary lock                                                                                                             │
│ 2025-04-08 18:01:11,299 INFO  [org.apache.activemq.artemis.core.server] AMQ221035: Primary Server Obtained primary lock                                                                                                                    │
│ 2025-04-08 18:01:11,510 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address DLQ supporting [ANYCAST]                                                                                                              │
│ 2025-04-08 18:01:11,594 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue DLQ on address DLQ                                                                                                              │
│ 2025-04-08 18:01:11,804 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address ExpiryQueue supporting [ANYCAST]                                                                                                      │
│ 2025-04-08 18:01:11,805 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue ExpiryQueue on address ExpiryQueue                                                                                              │
│ 2025-04-08 18:01:12,699 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61616 for protocols [CORE,MQTT,AMQP,STOMP,HORNETQ,OPENWIRE]                                                           │
│ 2025-04-08 18:01:12,700 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5445 for protocols [HORNETQ,STOMP]                                                                                    │
│ 2025-04-08 18:01:12,702 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5672 for protocols [AMQP]                                                                                             │
│ 2025-04-08 18:01:12,703 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:1883 for protocols [MQTT]                                                                                             │
│ 2025-04-08 18:01:12,704 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61613 for protocols [STOMP]                                                                                           │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221007: Server is now active                                                                                                                                    │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221001: Apache ActiveMQ Artemis Message Broker version 2.40.0 [0.0.0.0, nodeID=7401e03f-14a3-11f0-ac6e-02d91668a613]                                            │
│ 2025-04-08 18:01:12,713 INFO  [org.apache.activemq.artemis] AMQ241003: Starting embedded web server                                                                                                                                        │
│ 2025-04-08 18:01:14,205 INFO  [io.hawt.HawtioContextListener] Initialising Hawtio services                                                                                                                                                 │
│ 2025-04-08 18:01:14,210 INFO  [io.hawt.jmx.JmxTreeWatcher] Welcome to Hawtio 4.2.0                                                                                                                                                         │
│ 2025-04-08 18:01:14,292 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Authentication throttling is enabled                                                                                                                          │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Starting Hawtio authentication filter, JAAS realm: ""activemq"" authorized role(s): ""amq"" role principal classes: ""org.apache.activemq.artemis.spi.core.securit │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Looking for OIDC configuration file in: /var/lib/artemis-instance/etc/hawtio-oidc.properties                                                                  │
│ 2025-04-08 18:01:14,505 INFO  [io.hawt.web.auth.ClientRouteRedirectFilter] Hawtio ClientRouteRedirectFilter is using 1800 sec. HttpSession timeout                                                                                         │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241001: HTTP Server started at http://0.0.0.0:8161                                                                                                                          │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241002: Artemis Jolokia REST API available at http://0.0.0.0:8161/console/jolokia                                                                                           │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241004: Artemis Console available at http://0.0.0.0:8161/console                                                                                                            │
│ 2025-04-08 18:01:28,287 INFO  [io.hawt.web.auth.keycloak.KeycloakServlet] Keycloak integration is disabled                                                                                                                                 │
│ 2025-04-08 18:01:34,108 INFO  [io.hawt.web.auth.LoginServlet] Hawtio login is using 1800 sec. HttpSession timeout                                                                                                                          │
│ 2025-04-08 18:01:34,401 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 18:01:47,631 INFO  [io.hawt.web.servlets.JolokiaConfiguredAgentServlet] Jolokia overridden property: [key=policyLocation, value=file:/var/lib/artemis-instance/./etc/jolokia-access.xml]                                        │
│ 2025-04-08 18:01:47,634 INFO  [io.hawt.web.proxy.ProxyServlet] Proxy servlet is disabled                                                                                                                                                   │
│ 2025-04-08 18:02:47,861 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 20:16:49,260 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis
```

moreover, in browser console i see the following error:
[![enter image description here](https://i.sstatic.net/3SckIxlD.png)](https://i.sstatic.net/3SckIxlD.png)

My current ingress configuration:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: Ingress
  name: amq-artemis
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hidden
    external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only
    cert-manager.io/cluster-issuer: hidden
    cert-manager.io/duration: 2160h
    cert-manager.io/renew-before: 720h
    nginx.ingress.kubernetes.io/keepalive_timeout: ""1200""
    nginx.ingress.kubernetes.io/proxy-body-size: ""250m""
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""4 256k""
    nginx.ingress.kubernetes.io/proxy-buffering: 'on'
    nginx.ingress.kubernetes.io/proxy-buffer-size: ""128k""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-name: ""amq-artemis""
    nginx.ingress.kubernetes.io/session-cookie-samesite: ""None""
    nginx.ingress.kubernetes.io/session-cookie-secure: ""true""
    nginx.ingress.kubernetes.io/session-cookie-path: ""/; Secure""
    nginx.ingress.kubernetes.io/app-root: /console/artemis
    nginx.ingress.kubernetes.io/cors-allow-methods: ""PUT, GET, POST, OPTIONS""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""true""
    nginx.ingress.kubernetes.io/cors-allow-origin: ""*""
    nginx.ingress.kubernetes.io/cors-allow-headers: ""DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_busy_buffers_size   256k;
      client_body_buffer_size   10m;
      send_timeout              300;
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - hidden
      secretName: artemis-fqdn-cert
  rules:
    - host: hidden
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /jolokia
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /hawtio
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /console
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
```

I run broker with the following parameters: `--relax-jolokia --name art --http-host 0.0.0.0 `

I have tried with various of ingress rules and annotations, but futile. Any idea what it could be?","java, kubernetes, activemq-artemis",79640383.0,"Example K8s configuration that worked for me:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: activemq
  name: activemq
  labels:
    app.kubernetes.io/name: activemq
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: activemq
  template:
    metadata:
      labels:
        app.kubernetes.io/name: activemq
    spec:
      containers:
      - name: activemq
        image: apache/activemq-artemis:2.41.0-alpine
        imagePullPolicy: Always
        ports:
          - containerPort: 61616
            name: messaging
          - containerPort: 8161
            name: http
        volumeMounts:
          - name: activemq-config
            mountPath:  /var/lib/artemis-instance/etc-override
      volumes:
        - name: activemq-config
          configMap:
            name: activemq-config
            items:
              - key: jolokia-access.xml
                path: jolokia-access.xml
```

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: activemq
  name: activemq-config
  labels:
    app.kubernetes.io/name: activemq
data:
  jolokia-access.xml: |
    <cors>
      <ignore-scheme/>
    </cors>
```",2025-05-27T11:27:45,2025-04-08T20:31:47
79563007,ActiveMQ Artemis does not display console when runs in K8S,"I deployed `apache/activemq-artemis:2.40.0-alpine` in k8s cluster. First run goes well, but when I open console I'm unable to view literally everything but white list:
[![enter image description here](https://i.sstatic.net/LR6hVtVd.png)](https://i.sstatic.net/LR6hVtVd.png)
[![enter image description here](https://i.sstatic.net/8HdEn9TK.png)](https://i.sstatic.net/8HdEn9TK.png)

Log:

```
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-amqp-protocol]. Adding protocol support for: AMQP                                                                       │
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-hornetq-protocol]. Adding protocol support for: HORNETQ                                                                 │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-mqtt-protocol]. Adding protocol support for: MQTT                                                                       │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-openwire-protocol]. Adding protocol support for: OPENWIRE                                                               │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-stomp-protocol]. Adding protocol support for: STOMP                                                                     │
│ 2025-04-08 18:01:11,298 INFO  [org.apache.activemq.artemis.core.server] AMQ221034: Waiting indefinitely to obtain primary lock                                                                                                             │
│ 2025-04-08 18:01:11,299 INFO  [org.apache.activemq.artemis.core.server] AMQ221035: Primary Server Obtained primary lock                                                                                                                    │
│ 2025-04-08 18:01:11,510 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address DLQ supporting [ANYCAST]                                                                                                              │
│ 2025-04-08 18:01:11,594 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue DLQ on address DLQ                                                                                                              │
│ 2025-04-08 18:01:11,804 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address ExpiryQueue supporting [ANYCAST]                                                                                                      │
│ 2025-04-08 18:01:11,805 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue ExpiryQueue on address ExpiryQueue                                                                                              │
│ 2025-04-08 18:01:12,699 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61616 for protocols [CORE,MQTT,AMQP,STOMP,HORNETQ,OPENWIRE]                                                           │
│ 2025-04-08 18:01:12,700 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5445 for protocols [HORNETQ,STOMP]                                                                                    │
│ 2025-04-08 18:01:12,702 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5672 for protocols [AMQP]                                                                                             │
│ 2025-04-08 18:01:12,703 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:1883 for protocols [MQTT]                                                                                             │
│ 2025-04-08 18:01:12,704 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61613 for protocols [STOMP]                                                                                           │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221007: Server is now active                                                                                                                                    │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221001: Apache ActiveMQ Artemis Message Broker version 2.40.0 [0.0.0.0, nodeID=7401e03f-14a3-11f0-ac6e-02d91668a613]                                            │
│ 2025-04-08 18:01:12,713 INFO  [org.apache.activemq.artemis] AMQ241003: Starting embedded web server                                                                                                                                        │
│ 2025-04-08 18:01:14,205 INFO  [io.hawt.HawtioContextListener] Initialising Hawtio services                                                                                                                                                 │
│ 2025-04-08 18:01:14,210 INFO  [io.hawt.jmx.JmxTreeWatcher] Welcome to Hawtio 4.2.0                                                                                                                                                         │
│ 2025-04-08 18:01:14,292 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Authentication throttling is enabled                                                                                                                          │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Starting Hawtio authentication filter, JAAS realm: ""activemq"" authorized role(s): ""amq"" role principal classes: ""org.apache.activemq.artemis.spi.core.securit │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Looking for OIDC configuration file in: /var/lib/artemis-instance/etc/hawtio-oidc.properties                                                                  │
│ 2025-04-08 18:01:14,505 INFO  [io.hawt.web.auth.ClientRouteRedirectFilter] Hawtio ClientRouteRedirectFilter is using 1800 sec. HttpSession timeout                                                                                         │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241001: HTTP Server started at http://0.0.0.0:8161                                                                                                                          │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241002: Artemis Jolokia REST API available at http://0.0.0.0:8161/console/jolokia                                                                                           │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241004: Artemis Console available at http://0.0.0.0:8161/console                                                                                                            │
│ 2025-04-08 18:01:28,287 INFO  [io.hawt.web.auth.keycloak.KeycloakServlet] Keycloak integration is disabled                                                                                                                                 │
│ 2025-04-08 18:01:34,108 INFO  [io.hawt.web.auth.LoginServlet] Hawtio login is using 1800 sec. HttpSession timeout                                                                                                                          │
│ 2025-04-08 18:01:34,401 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 18:01:47,631 INFO  [io.hawt.web.servlets.JolokiaConfiguredAgentServlet] Jolokia overridden property: [key=policyLocation, value=file:/var/lib/artemis-instance/./etc/jolokia-access.xml]                                        │
│ 2025-04-08 18:01:47,634 INFO  [io.hawt.web.proxy.ProxyServlet] Proxy servlet is disabled                                                                                                                                                   │
│ 2025-04-08 18:02:47,861 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 20:16:49,260 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis
```

moreover, in browser console i see the following error:
[![enter image description here](https://i.sstatic.net/3SckIxlD.png)](https://i.sstatic.net/3SckIxlD.png)

My current ingress configuration:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: Ingress
  name: amq-artemis
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hidden
    external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only
    cert-manager.io/cluster-issuer: hidden
    cert-manager.io/duration: 2160h
    cert-manager.io/renew-before: 720h
    nginx.ingress.kubernetes.io/keepalive_timeout: ""1200""
    nginx.ingress.kubernetes.io/proxy-body-size: ""250m""
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""4 256k""
    nginx.ingress.kubernetes.io/proxy-buffering: 'on'
    nginx.ingress.kubernetes.io/proxy-buffer-size: ""128k""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-name: ""amq-artemis""
    nginx.ingress.kubernetes.io/session-cookie-samesite: ""None""
    nginx.ingress.kubernetes.io/session-cookie-secure: ""true""
    nginx.ingress.kubernetes.io/session-cookie-path: ""/; Secure""
    nginx.ingress.kubernetes.io/app-root: /console/artemis
    nginx.ingress.kubernetes.io/cors-allow-methods: ""PUT, GET, POST, OPTIONS""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""true""
    nginx.ingress.kubernetes.io/cors-allow-origin: ""*""
    nginx.ingress.kubernetes.io/cors-allow-headers: ""DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_busy_buffers_size   256k;
      client_body_buffer_size   10m;
      send_timeout              300;
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - hidden
      secretName: artemis-fqdn-cert
  rules:
    - host: hidden
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /jolokia
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /hawtio
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /console
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
```

I run broker with the following parameters: `--relax-jolokia --name art --http-host 0.0.0.0 `

I have tried with various of ingress rules and annotations, but futile. Any idea what it could be?","java, kubernetes, activemq-artemis",79633264.0,Adding `<ignore-scheme/>` was the solution for us. Just make sure to add it within the `<cors>` section of the `jolokia-access.xml`. We are using the official artemis image and mount it from a configmap to /`var/lib/artemis-instance/etc-override/jolokia-access.xml` so it gets copied to the install directory on startup.,2025-05-22T07:22:52,2025-04-08T20:31:47
79563007,ActiveMQ Artemis does not display console when runs in K8S,"I deployed `apache/activemq-artemis:2.40.0-alpine` in k8s cluster. First run goes well, but when I open console I'm unable to view literally everything but white list:
[![enter image description here](https://i.sstatic.net/LR6hVtVd.png)](https://i.sstatic.net/LR6hVtVd.png)
[![enter image description here](https://i.sstatic.net/8HdEn9TK.png)](https://i.sstatic.net/8HdEn9TK.png)

Log:

```
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-amqp-protocol]. Adding protocol support for: AMQP                                                                       │
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-hornetq-protocol]. Adding protocol support for: HORNETQ                                                                 │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-mqtt-protocol]. Adding protocol support for: MQTT                                                                       │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-openwire-protocol]. Adding protocol support for: OPENWIRE                                                               │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-stomp-protocol]. Adding protocol support for: STOMP                                                                     │
│ 2025-04-08 18:01:11,298 INFO  [org.apache.activemq.artemis.core.server] AMQ221034: Waiting indefinitely to obtain primary lock                                                                                                             │
│ 2025-04-08 18:01:11,299 INFO  [org.apache.activemq.artemis.core.server] AMQ221035: Primary Server Obtained primary lock                                                                                                                    │
│ 2025-04-08 18:01:11,510 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address DLQ supporting [ANYCAST]                                                                                                              │
│ 2025-04-08 18:01:11,594 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue DLQ on address DLQ                                                                                                              │
│ 2025-04-08 18:01:11,804 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address ExpiryQueue supporting [ANYCAST]                                                                                                      │
│ 2025-04-08 18:01:11,805 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue ExpiryQueue on address ExpiryQueue                                                                                              │
│ 2025-04-08 18:01:12,699 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61616 for protocols [CORE,MQTT,AMQP,STOMP,HORNETQ,OPENWIRE]                                                           │
│ 2025-04-08 18:01:12,700 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5445 for protocols [HORNETQ,STOMP]                                                                                    │
│ 2025-04-08 18:01:12,702 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5672 for protocols [AMQP]                                                                                             │
│ 2025-04-08 18:01:12,703 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:1883 for protocols [MQTT]                                                                                             │
│ 2025-04-08 18:01:12,704 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61613 for protocols [STOMP]                                                                                           │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221007: Server is now active                                                                                                                                    │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221001: Apache ActiveMQ Artemis Message Broker version 2.40.0 [0.0.0.0, nodeID=7401e03f-14a3-11f0-ac6e-02d91668a613]                                            │
│ 2025-04-08 18:01:12,713 INFO  [org.apache.activemq.artemis] AMQ241003: Starting embedded web server                                                                                                                                        │
│ 2025-04-08 18:01:14,205 INFO  [io.hawt.HawtioContextListener] Initialising Hawtio services                                                                                                                                                 │
│ 2025-04-08 18:01:14,210 INFO  [io.hawt.jmx.JmxTreeWatcher] Welcome to Hawtio 4.2.0                                                                                                                                                         │
│ 2025-04-08 18:01:14,292 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Authentication throttling is enabled                                                                                                                          │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Starting Hawtio authentication filter, JAAS realm: ""activemq"" authorized role(s): ""amq"" role principal classes: ""org.apache.activemq.artemis.spi.core.securit │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Looking for OIDC configuration file in: /var/lib/artemis-instance/etc/hawtio-oidc.properties                                                                  │
│ 2025-04-08 18:01:14,505 INFO  [io.hawt.web.auth.ClientRouteRedirectFilter] Hawtio ClientRouteRedirectFilter is using 1800 sec. HttpSession timeout                                                                                         │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241001: HTTP Server started at http://0.0.0.0:8161                                                                                                                          │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241002: Artemis Jolokia REST API available at http://0.0.0.0:8161/console/jolokia                                                                                           │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241004: Artemis Console available at http://0.0.0.0:8161/console                                                                                                            │
│ 2025-04-08 18:01:28,287 INFO  [io.hawt.web.auth.keycloak.KeycloakServlet] Keycloak integration is disabled                                                                                                                                 │
│ 2025-04-08 18:01:34,108 INFO  [io.hawt.web.auth.LoginServlet] Hawtio login is using 1800 sec. HttpSession timeout                                                                                                                          │
│ 2025-04-08 18:01:34,401 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 18:01:47,631 INFO  [io.hawt.web.servlets.JolokiaConfiguredAgentServlet] Jolokia overridden property: [key=policyLocation, value=file:/var/lib/artemis-instance/./etc/jolokia-access.xml]                                        │
│ 2025-04-08 18:01:47,634 INFO  [io.hawt.web.proxy.ProxyServlet] Proxy servlet is disabled                                                                                                                                                   │
│ 2025-04-08 18:02:47,861 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 20:16:49,260 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis
```

moreover, in browser console i see the following error:
[![enter image description here](https://i.sstatic.net/3SckIxlD.png)](https://i.sstatic.net/3SckIxlD.png)

My current ingress configuration:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: Ingress
  name: amq-artemis
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hidden
    external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only
    cert-manager.io/cluster-issuer: hidden
    cert-manager.io/duration: 2160h
    cert-manager.io/renew-before: 720h
    nginx.ingress.kubernetes.io/keepalive_timeout: ""1200""
    nginx.ingress.kubernetes.io/proxy-body-size: ""250m""
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""4 256k""
    nginx.ingress.kubernetes.io/proxy-buffering: 'on'
    nginx.ingress.kubernetes.io/proxy-buffer-size: ""128k""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-name: ""amq-artemis""
    nginx.ingress.kubernetes.io/session-cookie-samesite: ""None""
    nginx.ingress.kubernetes.io/session-cookie-secure: ""true""
    nginx.ingress.kubernetes.io/session-cookie-path: ""/; Secure""
    nginx.ingress.kubernetes.io/app-root: /console/artemis
    nginx.ingress.kubernetes.io/cors-allow-methods: ""PUT, GET, POST, OPTIONS""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""true""
    nginx.ingress.kubernetes.io/cors-allow-origin: ""*""
    nginx.ingress.kubernetes.io/cors-allow-headers: ""DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_busy_buffers_size   256k;
      client_body_buffer_size   10m;
      send_timeout              300;
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - hidden
      secretName: artemis-fqdn-cert
  rules:
    - host: hidden
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /jolokia
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /hawtio
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /console
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
```

I run broker with the following parameters: `--relax-jolokia --name art --http-host 0.0.0.0 `

I have tried with various of ingress rules and annotations, but futile. Any idea what it could be?","java, kubernetes, activemq-artemis",79563023.0,ActiveMQ Artemis 2.40.0 introduced a new web console based on a new version of Hawtio and Jolokia. Due to this change any request with an origin header using the `https` scheme which is ultimately received by Jolokia via HTTP is now discarded by default since it is deemed insecure. If you use a TLS proxy that transforms secure requests to insecure requests (e.g. in a Kubernetes environment) then consider changing the proxy to preserve HTTPS and switching the [embedded web server](https://activemq.apache.org/components/artemis/documentation/latest/web-server.html#embedded-web-server) to HTTPS. If that isn’t feasible then you can accept the risk by adding `<ignore-scheme/>` to `etc/jolokia-access.xml`. See the [Jolokia documentation](https://jolokia.org/reference/html/manual/security.html) for more details.,2025-04-08T20:41:41,2025-04-08T20:31:47
79562957,cant create service in kubernetes. - kubectl apply -f nginx-service.yaml,"Get the error, so could you please help, I am very new in Kubernetes. and gets the errors:

```
C:\Windows\system32>kubectl apply -f nginx-service.yaml
Error from server (BadRequest): error when creating ""nginx-service.yaml"":
Service in version ""v1"" cannot be handled as a Service:
strict decoding error: unknown field ""spec.ports[0].protocols""
```

```
apiVersion: v2
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```",kubernetes,79563089.0,"Two small things to fix.

`apiVersion` should be `v1`, not `v2`

```
apiVersion: v1
```

According to the error, in the port list, you should use `protocol`, not `protocols`. But in the snippet provided it seems you already fix it.",2025-04-08T21:32:06,2025-04-08T20:02:37
79562629,Why is my nginx+php pod accepting traffic too soon?,"I have a multi-container pod that houses an nginx container and a php-fpm container. they communicate over a file sock.

Years ago I learned that each container could isolate its own readiness, and then when both were ready, the whole pod would come online and accept traffic. I'm really not sure why that isn't working anymore.

Now it seems like no matter what, php is succeeding its readiness check but it doesn't appear to actually be online:

`2025/04/08 04:29:25 [crit] 13#13: *293 connect() to unix:/sock/php.sock failed (2: No such file or directory) while connecting to upstream, `

These errors persist for a couple of minutes:

[![graph depicting a large grouping of 500 error messages](https://i.sstatic.net/EGfYEBZP.png)](https://i.sstatic.net/EGfYEBZP.png)

nginx:

```
      - name: nginx
        image: {{.Values.image.nginx.repository}}:{{.Values.image.tag}}
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command: [
                ""/bin/sh"", ""-c"",
                # Introduce a delay to the shutdown sequence to wait for the
                # pod eviction event to propagate. Then, gracefully shutdown
                # nginx.
                ""sleep 10;"",
                ""/usr/sbin/nginx -s quit;""
                ,
              ]

        readinessProbe:
          httpGet:
            path: /online-status
            port: 8080
          initialDelaySeconds: 35
          timeoutSeconds: 4
          periodSeconds: 5
          successThreshold: 15
          failureThreshold: 2
        volumeMounts:
        - name: php-socket
          mountPath: /sock
```

php:

```
      - name: php
        image: {{.Values.image.php.repository}}:{{.Values.image.tag}}
        imagePullPolicy: IfNotPresent
        securityContext:
          capabilities:
            add:
            - SYS_PTRACE
        lifecycle:
          preStop:
            exec:
              command: [
                ""sh"", ""-c"",
                # Introduce a delay to the shutdown sequence to wait for the
                # pod eviction event to propagate. Then, gracefully shutdown
                # php. The php-fpm children should handle open requests before full shutdown (see process_control_timeout in start.sh)
                ""sleep 30; kill -QUIT 1"",
              ]
        readinessProbe:
          exec:
            command:
            - php
            - /var/www/health_check.php
          initialDelaySeconds: 15
          periodSeconds: 5
          timeoutSeconds: 3
       volumeMounts:
       - name: php-socket
         mountPath: /sock
```

Volume:

```
      volumes:
      - name: php-socket
        emptyDir: {}
```

I've deliberately tried artificially raising the nginx container's initialDelaySeconds, but that doesn't seem to impact the 500s at all. Am I missing something dumb? We recently ""moved"" to a new gke cluster. But I doubt this would impact anything? The kubernetes version isn't even much higher. Otherwise, nothing major has been changed on our end, and I thiiink we would have noticed this problem over the years lol","php, kubernetes, nginx, readinessprobe",79576237.0,"I am using GKE, and because I have a VPC enabled cluster, the BackendConfigs were using NEG to route traffic directly to pods, completely bypassing the usual kubernetes service behavior I was used to.

I had to declare my own backend config instead of allowing the default version, and I also had to instruct my service to use it of course.

This change was absolutely brutal to unpack. I'm confused that I haven't been able to find documentation very easily, and not much help from AI.",2025-04-16T00:55:59,2025-04-08T16:47:03
79562322,Envoy proxy keeps rejecting client&#39;s requests with CANCELLED despite setting infinite values everywhere in config,"This issue has been happening for months now and the only possible way to ""mitigate"" it is to overpower the server (60 instances for 30 max concurrent client requests).

Here is the envoy configuration, I have tried everything and the client still reports CANCELLED requests. It's not the client's configuration because even with my custom client (where I set all timeouts to infinity) the issue happens:

```
admin:
  address:
    socket_address:
      protocol: TCP
      address: 0.0.0.0
      port_value: 9901

static_resources:
  listeners:
  - name: listener_0
    address:
      socket_address:
        protocol: TCP
        address: 0.0.0.0
        port_value: 50051
    listener_filters:
    - name: ""envoy.filters.listener.tls_inspector""
      typed_config:
        ""@type"": type.googleapis.com/envoy.extensions.filters.listener.tls_inspector.v3.TlsInspector
    filter_chains:
    - filters:
      - name: envoy.filters.network.rbac
        typed_config:
          ""@type"": type.googleapis.com/envoy.extensions.filters.network.rbac.v3.RBAC
          rules:
            policies:
              ""check-client-cn"":
                permissions:
                  - any: true
                principals:
                  - authenticated:
                      principal_name:
                        exact: ""XX-XX-XX""
                  - authenticated:
                      principal_name:
                        exact: ""CN=XX-XX""
                  - authenticated:
                      principal_name:
                        exact: ""CN=client""
          stat_prefix: rbac
      - name: envoy.filters.network.http_connection_manager
        typed_config:
          ""@type"": type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager
          access_log:
          - name: envoy.access_loggers.stdout
            typed_config:
              ""@type"": type.googleapis.com/envoy.extensions.access_loggers.stream.v3.StdoutAccessLog
          codec_type: AUTO
          stat_prefix: ingress_http
          route_config:
            name: local_route
            virtual_hosts:
            - name: local_service
              domains:
              - ""*""
              routes:
              - match:
                  prefix: ""/""
                route:
                  cluster: service-grpc-tls
                  timeout: 2700s
                  retry_policy:
                    retry_on: cancelled,deadline-exceeded,resource-exhausted
                    num_retries: 3
                    per_try_timeout: 900s
          http_filters:
          - name: envoy.filters.http.router
            typed_config:
              ""@type"": type.googleapis.com/envoy.extensions.filters.http.router.v3.Router
      transport_socket:
        name: envoy.transport_sockets.tls
        typed_config:
          ""@type"": type.googleapis.com/envoy.extensions.transport_sockets.tls.v3.DownstreamTlsContext
          require_client_certificate: true
          common_tls_context:
            alpn_protocols: h2
            validation_context:
              trusted_ca:
                filename: /etc/ssl/envoy/ca.crt
              allow_expired_certificate: true
            tls_certificates:
            - certificate_chain:
                filename: /etc/ssl/envoy/server.crt
              private_key:
                filename: /etc/ssl/envoy/server.key

  clusters:
  - name: service-grpc-tls
    connect_timeout: 10s
    type: STRICT_DNS
    dns_lookup_family: V4_ONLY
    lb_policy: ROUND_ROBIN
    http2_protocol_options:
      max_concurrent_streams: 10000
    upstream_connection_options:
      tcp_keepalive: {}
    circuit_breakers:
      thresholds:
      - priority: DEFAULT
        max_requests: 10000
        max_pending_requests: 10000
        max_retries: 3
    load_assignment:
      cluster_name: service-grpc-tls
      endpoints:
      - lb_endpoints:
        - endpoint:
            address:
              socket_address:
                address: XXX-XXX.XXX-XXX
                port_value: 8080
```

I've set retries, huge timeouts, everything, and I'm still getting CANCELLED.

The worst is that neither /stats nor the prometheus metrics show anything. I see some retries here and there but nothing on why the requests get cancelled.

I'm about to switch to linkerd or something else. FYI, my calls are asynchronous but take quite some time (1-10mn) to finish. There are 0 issues with the server pods.","kubernetes, grpc, envoyproxy",79563638.0,"We first need to know where the cancellation is being thrown from - try connecting to backend without proxy and see if that succeeds then it's certainly not a problem from gRPC.

Also, can you put some logs around rpc error message along with status which you are seeing in client.",2025-04-09T06:41:14,2025-04-08T14:23:55
79562269,How to cache image downloads for GitHub actions-runner-controller runners?,"When running containerized GitHub action jobs it obviously is extremely impractical to download the entire required container image from scratch every time. Yet it seems like GitHub's `actions-runner-controller` *forces* you to do precisely this when using ephemeral runners.

I am configuring my runners via the following Flux setup:

```
apiVersion: source.toolkit.fluxcd.io/v1
kind: HelmRepository
metadata:
  name: actions-runner-controller-charts
  namespace: flux-system
spec:
  type: oci
  url: oci://ghcr.io/actions/actions-runner-controller-charts
  interval: 1h

---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: arc
  namespace: arc-system
spec:
  chart:
    spec:
      chart: gha-runner-scale-set-controller
      sourceRef:
        kind: HelmRepository
        name: actions-runner-controller-charts
        namespace: flux-system
  interval: 1m

---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: arc-runner-set
  namespace: arc-runners
spec:
  chart:
    spec:
      chart: gha-runner-scale-set
      sourceRef:
        kind: HelmRepository
        name: actions-runner-controller-charts
        namespace: flux-system
  values:
    minRunners: 4
    maxRunners: 4
    controllerServiceAccount:
      name: arc-gha-rs-controller
    template:
      spec:
        initContainers:
          - name: init-dind-externals
            image: ghcr.io/actions/actions-runner:latest
            command: [""cp"", ""-r"", ""/home/runner/externals/."", ""/home/runner/tmpDir/""]
            volumeMounts:
              - name: dind-externals
                mountPath: /home/runner/tmpDir
        containers:
          - name: runner
            image: ghcr.io/actions/actions-runner:latest
            command: [""/home/runner/run.sh""]
            env:
              - name: DOCKER_HOST
                value: unix:///var/run/docker.sock
            volumeMounts:
              - name: work
                mountPath: /home/runner/_work
              - name: dind-sock
                mountPath: /var/run
          - name: dind
            image: docker:dind
            args:
              - dockerd
              - --host=unix:///var/run/docker.sock
              - --group=$(DOCKER_GROUP_GID)
            env:
              - name: DOCKER_GROUP_GID
                value: ""123""
            securityContext:
              privileged: true
            volumeMounts:
              - name: work
                mountPath: /home/runner/_work
              - name: dind-sock
                mountPath: /var/run
              - name: dind-externals
                mountPath: /home/runner/externals
              # Added by us.
              - name: dind-docker-registry-ca
                mountPath: /etc/docker/certs.d/docker-registry:5000/ca.crt
                subPath: ca.crt
                readOnly: true
        volumes:
          - name: work
            emptyDir: {}
          - name: dind-sock
            emptyDir: {}
          - name: dind-externals
            emptyDir: {}
          # Added by us.
          - name: dind-docker-registry-ca
            configMap:
              name: docker-registry-ca
  valuesFrom:
    - kind: Secret
      name: github-token
      valuesKey: github-token
      targetPath: githubConfigSecret.github_token
  interval: 1m

---
# DNS ""alias""
apiVersion: v1
kind: Service
metadata:
  name: docker-registry
  namespace: arc-runners
spec:
  type: ExternalName
  externalName: docker-registry.docker-registry.svc.cluster.local
```

What I have tried:

- Share `/var/lib/docker` between the `dind` sidecar containers via a persistent volume. That obviously breaks as soon as a runner pod crashes and leaves this in an invalid state which seems to happen as soon as someone cancels a workflow.
- Create a fixed number of runners so that no new ones without a Docker cache have to be created. This does not work for whatever reason, when I run workflows existing runners are frequently replaced by new ones.

What am I missing? Without this the entire thing is essentially unusable unless you don't containerize your jobs.","docker, kubernetes, github-actions, flux",,,,2025-04-08T14:01:31
79562249,PromQL for CPU pressure of a certain container &quot;class&quot;,"We deploy our gitlab runners in kubernetes, where we offer different classes that distinguish in cpu/memory requests.

My goal is now to find a promQL query, that indicates a high CPU pressure over a certain runner class. Using the query, I would like to see, if the upper percentage/quantile of jobs is above a certain threshold.
Thus, the query should indicate, if a certain class needs more cpu/memory requests by default.

One of my approaches was

```
quantile_over_time(0.75,
  avg by (
    gitlab_runner_runner_powerclass,
    gitlab_runner_runner_feature
  )(jobs:cpu:utilization)[12h:]
) > 85
```

where `jobs:cpu:percent` is from a recording rule, that is granular down to job id and containers within a runner job pod.

Trying out queries and thinking about their meanings I realized, that I am not able to tell which query really represents my needs.

One crucial aspect to me is, that the graph must not be flattened by longer periods without jobs outside typical office hours. So no average on a temporal scale but compared to other, actual datapoints/maxima.
I already have monitoring to see, if certain jobs should switch their class.

Edit:

I know that one way could be to calculate the inner quartile region/IQR from Q1 and Q2, which would represent the memory distribution of the majority of jobs.
Using

```
quantile(<0.25 or 0.75>,
  (
    sum by (pod)
    (jobs:cpu:utilization{gitlab_runner_runner_powerclass=""large""})
  /
    sum by (pod)
    (kube_pod_container_resource_requests{resource=""cpu""})
  )
)
```

I could monitor the `Q3=Q2-Q1` value and watch for spikes, anomalies. But that does not really represent whats interesting to me.","kubernetes, prometheus, gitlab-ci-runner, metrics, promql",79568417.0,"You're on track..

```
quantile_over_time(0.75, (
  avg by (job_id, gitlab_runner_runner_powerclass) (
    jobs:cpu:utilization
  )
)[12h:]
)
```

This gives you the *75th percentile* of job utilization within each runner class over the last 12h.

Then, you can filter to check if **the top 25% of running jobs in a class are >85% of cpu use.**

```
quantile_over_time(0.75, (
  avg by (job_id, gitlab_runner_runner_powerclass) (
    jobs:cpu:utilization
  )
)[12h:]
) > 85
```",2025-04-11T08:50:34,2025-04-08T13:53:08
79562077,fluxcd v2 analog to valuesFrom chartFileRef,"What is the analog of Flux v1 HelmRelease expression bellow for flux v2? Can same custom.yaml be used for both flux v1 and flux v2?

```
  valuesFrom:
  - chartFileRef:
      path: values/custom.yaml
```","kubernetes, kubernetes-helm, fluxcd",,,,2025-04-08T12:46:24
79561630,Handling Early TCP Data and Registration Races in Spring Boot Behind an NLB in Kubernetes,"I'm experiencing an issue with how my Spring Boot application (using Spring Integration) handles TCP connections in a Kubernetes environment. My setup includes an NLB (Network Load Balancer) in front of my Kubernetes pod. Here’s the scenario:

- The underlying Linux OS (enhanced by the NLB) accepts incoming TCP
connections automatically.
- Once a connection is accepted, my Spring Boot application performs a
registration (handshake) process that determines whether to
ultimately accept or reject the connection.
- However, messages start arriving immediately after the Linux-level
acceptance—even before the Spring Boot application has finished its
registration phase.

If the connection is eventually rejected by the application, the TCP stack may have already ACKed messages or processed parts of the stream, resulting in message loss.

Given that I don’t control the client behavior, I’m trying to solve one of the following:

- Prevent the TCP stack (managed through the NLB and Linux pod) from
sending an ACK prematurely when the connection is only in the
registration phase, or
- Ensure that all messages received during registration are buffered,
properly processed, and then the connection is closed gracefully
without losing data.

I have a few specific questions:

- Is it feasible (or possible) to delay or prevent ACKs at the OS/TCP
level until the Spring Boot application has completed registration?
- If not, what’s the recommended approach to buffer all incoming
messages during the registration phase when you cannot control the
client?
- Are there any established design patterns, such as a buffered state
machine or an aggregator with a release strategy, that work well with
Spring Integration in this scenario?

ConnectionHandler class is for load balancing implementation as NLB doesn't support balancing on connections.
Client can have total 12 connections with my server - in order to distribute the connections evenly, I am managing total connections on my spring boot application.

ConnectionHandler only registers the connection if the connectionLimit is not crossed. It is during this process is where the issue occurs, from OS side the connection is already setup but from the spring boot side, it is not ready to accept connections yet

The ConnectionHandler ensures each pod enforces a max number of concurrent TCP connections to prevent overload. If the pod is over capacity, we close the connection immediately in addNewConnection without registering it.

The issue is: since the TCP connection is already accepted at the Linux kernel level, the client may send data right away, and the kernel ACKs it even though we never register the connection in Spring Boot. This leads to a situation where messages are lost because they were ""delivered"" from the client's point of view, but never handled by Spring.

I’m trying to find a way to either:

Prevent messages from being ACKed by the TCP stack if we are going to close the connection right after,

Or read (and process) all the buffered data before closing the connection even if the pod is over capacity.

We’re running in Kubernetes behind an NLB, and the NLB’s connection routing is not aware of the application-level connection limits.

Once a connection is accepted at the Linux level, it can’t be deferred or bounced to another pod unless we explicitly reject it at the application level.

The ConnectionHandler helps with this early rejection strategy by closing the connection immediately if the pod is full, allowing the client (or retry logic via the NLB) to establish the connection with a different pod.

```
class ConnectionHandler(
    private val configProperties: TcpListenerProperties,
    private val meterRegistry: MeterRegistry,
    connectionFactory: AbstractConnectionFactory,
) : TcpSender {

    // Flag to indicate whether shutdown has begun.
    @Volatile
    private var isShuttingDown: Boolean = false

    init {
        // Register this handler with the connection factory.
        connectionFactory.registerSender(this)
    }

    // Tracks active connections by their connectionId.
    private val activeConnections = mutableMapOf<String, TcpConnection>()

    // Atomic integer for monitoring the current connection count.
    private val activeConnectionCount = AtomicInteger()

    // Metric tags for connection metrics.
    private val metricTags = listOf(
        Tag.of(""region"", configProperties.region),
        Tag.of(""channel"", configProperties.channel),
        Tag.of(""port"", configProperties.port.toString())
    )

    @Synchronized
    override fun addNewConnection(connection: TcpConnection) {
        if (isShuttingDown) {
            connection.close()
            return
        }
        if (activeConnections.size >= configProperties.maxAllowedConnections) {
            connection.close()
            return
        }
        activeConnections[connection.connectionId] = connection
        activeConnectionCount.set(activeConnections.size)
        meterRegistry.gauge(""fep-relay.tcp.listener.connections"", metricTags, activeConnectionCount)
    }
}```
```","spring-boot, kubernetes, tcp, spring-integration, aws-nlb",79562286.0,"We don't know what is that `ConnectionHandler` for. Please, clarify, or show more code. Some simple project to reproduce and play with on our side would be great.

However, if I were implementing such a requirement, I wouldn't start accepting client connection from my server implementation until Spring Boot is fully up and running. Usually a lifecycle management on the `org.springframework.integration.ip.tcp.connection.AbstractServerConnectionFactory` is enough since this component is not going to accept connections from clients until its `start()` is not called.

You may think about not having that connection factory started automatically, but rather manually somewhere later when you are sure that you server application is ready.

In this case doesn't matter how hard your clients try to connect and emit their messages. They just won't be able to do that until you have your server started.

Or clarify, please, more what is your expectation.",2025-04-08T14:09:41,2025-04-08T09:15:21
79560831,How to multiply container resource limits by seconds container was running,"I'm putting together a Grafana dashboard to track Kubernetes container resource limits. This will effectively be a rudimentary billing dashboard for client workloads in our cluster based on resources reserved for their use. I'm having a similar problem with most of the data I'm dealing with, so to simplify let's focus specifically on memory limits.

I'm leaning on `kube_pod_container_resource_limits{resource=""memory""}` here. This is a gauge of bytes. I would like the dashboard to show a single stat of byte-seconds used over `$__range` (which will generally be a month or so).

Conceptually this seems trivial: multiply the memory limit of the container by the number of seconds the container was running. But the rubber meeting the road is proving troublesome, for a few reasons:

1. How do we calculate the container runtime? cadvisor only gives me container_start_time_seconds-- there's no end time there. So it seems like I need to use timestamps from the metrics themselves, i.e. take the timestamp of the first datapoint for this container and subtract it from the timestamp of the last datapoint for this container and call that the runtime in seconds. I'm not entirely sure how to do that, or if that's the right approach.
2. How do I multiply the magical query from (1) by the resource limit for that particular container?

Any assistance would be appreciated!","kubernetes, prometheus, grafana",,,,2025-04-07T21:32:52
79560825,Apply filter to allow duplicate headers in Istio with Sidecar,"I have a basic Kubernetes setup with Istio/Sidecar to encrypt the communication with mtls. I found out that by default I cannot send network requests between the pods with duplicated headers. Duplicated header keys are filtered/deleted. I tried to apply this configuration in order to disable this functionality:

```
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: auth-header-filter
spec:
  workloadSelector:
    labels:
      app: received_pod_name
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
      listener:
        filterChain:
          filter:
            name: ""envoy.filters.network.http_connection_manager""
    patch:
      operation: MERGE
      value:
        name: ""envoy.filters.network.http_connection_manager""
        typed_config:
          ""@type"": ""type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager""
          http_protocol_options:
            allow_duplicate_authorization_header: true
```

But again the second request header is removed. Any idea how to fix this?","kubernetes, istio, istio-sidecar",79621418.0,I don't think allow_duplicate_authorization_header is a valid field in the code. This is not going to work.,2025-05-14T12:01:30,2025-04-07T21:30:13
79560367,JanusGraph Unable to Authenticate with Elasticsearch: 401 Unauthorized Error,"I am running JanusGraph and Elasticsearch as separate containers in a Kubernetes cluster. Elasticsearch is configured with authentication enabled (`xpack.security.enabled: true`), and I have provided the correct credentials in the JanusGraph configuration. However, I am encountering the following error in the JanusGraph logs:

```
15:15:42 WARN  org.janusgraph.diskstorage.es.rest.RestElasticSearchClient.getMajorVersion - Unable to determine Elasticsearch server version. Default to EIGHT.
org.elasticsearch.client.ResponseException: method [GET], host [http://elasticsearch-headless:9200], URI [/], status line [HTTP/1.1 401 Unauthorized]
{""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""missing authentication credentials for REST request [/]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}}],""type"":""security_exception"",""reason"":""missing authentication credentials for REST request [/]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}},""status"":401}
```

**What I Have Tried:**

1.Verified Elasticsearch Authentication:

I tested the credentials manually using curl from the JanusGraph pod,
and it works:

```
    kubectl exec -it <janusgraph-pod-name> -- curl -u <elasticsearch-username>:<elasticsearch-password> http://cassandra-dravoka-elasticsearch-headless:9200
```

This returns the expected response from Elasticsearch.

1. Configured JanusGraph Environment Variables:

I have set the following environment variables in the JanusGraph deployment:

```
- name: INDEX_SEARCH_BACKEND
  value: ""elasticsearch""
- name: INDEX_SEARCH_HOSTNAME
  value: ""elasticsearch-headless""
- name: INDEX_SEARCH_PORT
  value: ""9200""
- name: INDEX_SEARCH_ELASTICSEARCH_USERNAME
  value: ""<elasticsearch-username>""
- name: INDEX_SEARCH_ELASTICSEARCH_PASSWORD
  value: ""<elasticsearch-password>""
- name: INDEX_SEARCH_ELASTICSEARCH_VERSION
  value: ""7""
```

1. Checked Elasticsearch Logs:

I am not seeing any logs in Elasticsearch related to the incoming requests from JanusGraph, which suggests that the requests might not be reaching Elasticsearch or are being rejected silently.

1. Verified Network Connectivity:

I tested connectivity from the JanusGraph pod to Elasticsearch using:

```
kubectl exec -it <janusgraph-pod-name> -- curl http://elasticsearch-headless:9200
```

**Environment Details:**

- JanusGraph Version: 0.6.3
- Elasticsearch Version: 8.x
- Elasticsearch Configuration: `xpack.security.enabled: true`
- JanusGraph Configuration:
  - Backend: Cassandra
  - Index Backend: Elasticsearch

**Expected Behavior:**

JanusGraph should authenticate with Elasticsearch using the provided credentials and connect successfully.

**Actual Behavior:**

JanusGraph fails to authenticate with Elasticsearch and logs a 401 Unauthorized error.

**Questions**

1. Is any additional configuration required in JanusGraph to pass the Authorization header correctly?
2. How can I debug this issue further to identify the root cause?

**What I Need Help With:**

I need assistance in resolving the 401 Unauthorized error and ensuring that JanusGraph can successfully connect to Elasticsearch with authentication enabled.

Let me know if you need any more details. Thank you in advance for your help!","authentication, elasticsearch, kubernetes, http-status-code-401, janusgraph",79562478.0,"See:

- [https://docs.janusgraph.org/index-backend/elasticsearch/#rest-client-basic-http-authentication](https://docs.janusgraph.org/index-backend/elasticsearch/#rest-client-basic-http-authentication)
- [https://docs.janusgraph.org/operations/container/#docker-environment-variables](https://docs.janusgraph.org/operations/container/#docker-environment-variables)

Combine these two documentation sections and create environment variables like:

```
janusgraph.index.search.elasticsearch.http.auth.type=basic
```",2025-04-08T15:30:51,2025-04-07T16:21:25
79559858,How to upgrade sidecar image without disrupting other containers in Kubernetes pod,"According to [sidecar containers docs](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#differences-from-init-containers):

> Changing the image of a sidecar container will not cause the Pod to restart, but will trigger a container restart.

Using their own [example application](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#sidecar-example):

```
$ kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
      initContainers:
        - name: logshipper
          image: alpine:latest
          restartPolicy: Always
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
EOF
deployment.apps/myapp created
```

Replacing the sidecar's image causes the entire pod to be replaced:

```
$ kubectl set image deploy myapp logshipper=busybox:latest
deployment.apps/myapp image updated
$ kubectl get pod -l app=myapp
NAME                     READY   STATUS        RESTARTS   AGE
myapp-5cdcbc5cff-nv6gz   2/2     Running       0          6s
myapp-b5f9c8894-lkth4    2/2     Terminating   0          86s
```

So how does one upgrade a sidecar image without disrupting other containers running in the pod?

```
$ kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.30.9-gke.1046000
$ kubectl get --raw /metrics | grep SidecarContainers
kubernetes_feature_enabled{name=""SidecarContainers"",stage=""BETA""} 1
```","image, kubernetes, containers, sidecar",79572872.0,"While it is true that from v1.29 onwards there is a beta feature in which an init-Container will effectively become a sidecar container if it's `restartPolicy` is set to `always`, sidecars have been used as a second container within the same pod for quite a while:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
        - name: logshipper
          image: alpine:latest
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
```

Note the absence of the `initContainer`key. Let's check if that worked:

```
$ kubectl logs deployments/myapp -c logshipper
logging
logging
logging
[...]
```

## Deployment

Does it work with the original deployment?

```
$ kubectl logs deployments/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

With that being said, why does the deployment restart? Well, as David Maze correctly pointed out, you changed the template of the deployment, and hence kubernetes does what it is supposed to do: reconcile the differences.

## Pod

But how about we are creating not a deployment, but a pod?

```
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
    - name: myapp
      image: alpine:latest
      command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
      volumeMounts:
        - name: data
          mountPath: /opt
  initContainers:
    - name: logshipper
      image: alpine:3.18
      restartPolicy: Always
      command: ['sh', '-c', 'tail -F /opt/logs.txt']
      volumeMounts:
        - name: data
          mountPath: /opt
  volumes:
    - name: data
      emptyDir: {}
```

That gives us the expected log output:

```
$ kubectl logs pods/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

Now, if we change the image for the `logshipper` container to `alpine:3.20` and reapply the resource

```
$ kubectl apply -f pod.yaml
pod/myapp configured
```

the command `kubectl events --for pod/myapp` will show the expected behavior:

```
LAST SEEN               TYPE      REASON      OBJECT      MESSAGE
[...]
116s                    Normal    Pulled      Pod/myapp   Container image ""alpine:3.18"" already present on machine
116s                    Normal    Scheduled   Pod/myapp   Successfully assigned default/myapp to kind-cluster-control-plane
115s                    Normal    Pulling     Pod/myapp   Pulling image ""alpine:latest""
113s                    Normal    Created     Pod/myapp   Created container: myapp
113s                    Normal    Pulled      Pod/myapp   Successfully pulled image ""alpine:latest"" in 1.564s (1.565s including waiting). Image size: 3653068 bytes.
113s                    Normal    Started     Pod/myapp   Started container myapp
41s                     Normal    Killing     Pod/myapp   Init container logshipper definition changed
11s (x2 over 115s)      Normal    Created     Pod/myapp   Created container: logshipper
11s                     Normal    Pulled      Pod/myapp   Container image ""alpine:3.20"" already present on machine
10s (x2 over 115s)      Normal    Started     Pod/myapp   Started container logshipper
```

> Note that your output may vary.

## Conclusion

If you change the deployment template, you will trigger a reconciliation. That is very much expected behavior and your applications should account for that. Since hardly anyone will ever deploy pods manually, the feature that the main container does not restart if the sidecar is defined as an initContainer with restartPolicy set to always is plainly utterly useless for all practical purposes.",2025-04-14T10:06:14,2025-04-07T12:20:57
79559648,To understand how multithreading works in a Kubernetes pod,"I have a multithreaded Spring Boot microservice running in a Kubernetes pod with a CPU limit of 1 (1000m). Does this mean only one CPU core is used to run all my threads one by one, or can multiple cores run my threads concurrently as long as the total CPU usage doesn't go over the 1 CPU limit?","spring-boot, multithreading, kubernetes, cpu",79559726.0,"> multiple cores [can] run my threads concurrently as long as the total CPU usage doesn't go over the 1 CPU limit

This.  You can have as many threads as you'd like, and the kernel will schedule them on as many CPU cores as are available, but your process won't be allocated more than 1 CPU-second per second.

Say, for example, that your application is largely database-bound, and a thread can only practically use 25% of a core.  If you had 4 threads running, together they'd still fit inside a `limits: { cpu: 1 }` constraint, and they would all get to run at their respective full speeds (maybe on different physical cores).",2025-04-07T11:13:26,2025-04-07T10:40:53
79557794,How to fix kafka connection to zookeeper (kubernetes)?,"I want to start kafka + zookeeper with kubernetes using docker desktop but I have an error:

> Unable to canonicalize address zookeeper/:2181 because it's not resolvable

my yaml:

```
apiVersion: v1
kind: Namespace
metadata:
  name: kafka
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper-service
  namespace: kafka
spec:
  selector:
    app: zookeeper
  ports:
    - port: 2181
      targetPort: 2181
      name: client
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - name: zookeeper
          image: confluentinc/cp-zookeeper:latest
          ports:
            - containerPort: 2181
          env:
            - name: ZOOKEEPER_CLIENT_PORT
              value: ""2181""
---
apiVersion: v1
kind: Service
metadata:
  name: kafka-service
  namespace: kafka
spec:
  selector:
    app: kafka
  ports:
    - port: 9092
      targetPort: 9092
      name: broker
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kafka
  namespace: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:latest
          ports:
            - containerPort: 9092
          env:
            - name: KAFKA_BROKER_ID
              value: ""1""
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: ""zookeeper-service:2181""
            - name: KAFKA_ADVERTISED_LISTENERS
              value: ""PLAINTEXT://kafka-service:9092""
            - name: KAFKA_LISTENERS
              value: ""PLAINTEXT://0.0.0.0:9092""
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: ""1""
```

I tried to change/remove namespace, add namespace to KAFKA_ZOOKEEPER_CONNECT, to make services name same as deplyments, ensure zookeeper starts first.

but every try ends with this error in logs:

> ERROR Unable to resolve address: zookeeper-service/:2181
> (org.apache.zookeeper.client.StaticHostProvider) 2025-04-06 10:06:49
> java.net.UnknownHostException: zookeeper-service: Name or service not
> known 2025-04-06 10:06:49     at
> java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)
> 2025-04-06 10:06:49     at
> java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(Unknown
> Source) 2025-04-06 10:06:49     at
> java.base/java.net.InetAddress.getAddressesFromNameService(Unknown
> Source) 2025-04-06 10:06:49     at
> java.base/java.net.InetAddress$NameServiceAddresses.get(Unknown
> Source) 2025-04-06 10:06:49     at
> java.base/java.net.InetAddress.getAllByName0(Unknown Source)
> 2025-04-06 10:06:49     at
> java.base/java.net.InetAddress.getAllByName(Unknown Source) 2025-04-06
> 10:06:49     at java.base/java.net.InetAddress.getAllByName(Unknown
> Source) 2025-04-06 10:06:49     at
> org.apache.zookeeper.client.StaticHostProvider$1.getAllByName(StaticHostProvider.java:88)
> 2025-04-06 10:06:49     at
> org.apache.zookeeper.client.StaticHostProvider.resolve(StaticHostProvider.java:141)
> 2025-04-06 10:06:49     at
> org.apache.zookeeper.client.StaticHostProvider.next(StaticHostProvider.java:368)
> 2025-04-06 10:06:49     at
> org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1204)
> 2025-04-06 10:06:49 [2025-04-06 04:06:49,752] WARN Session 0x0 for
> server zookeeper-service/:2181, Closing socket connection.
> Attempting reconnect except it is a SessionExpiredException.
> (org.apache.zookeeper.ClientCnxn)","kubernetes, apache-kafka, apache-zookeeper",,,,2025-04-06T04:15:09
79557607,"Cloudwatch log insights, how to select a field called: user.extra.authentication.kubernetes.io/node-name.0","I have an existing query that gets me the logs I want to look at from the k8s kube-apiserver-audit logStream.  And I want to see what nodes they came from.  So I just copied and pasted that field into the fields line.  But that column comes up empty for all of the logs.  The color of the 0 is different, making me thing I need to escape it or something.  I have tried backslash, single quote, double quote, backtick...   I tried some ""parse"" commands that chatgpt gave me, but It would always give me a ""Ephemeral field is already defined"" error no matter what I aliased the field to.

So any ideas how I can get this field to should up as a column with the actual data in it?","amazon-web-services, kubernetes, aws-cloudwatch-log-insights",79557639.0,"The issue you're experiencing is likely because the node information is nested within the JSON structure of your logs. CloudWatch Logs Insights requires specific syntax to access nested JSON fields.

Try these approaches:

1. First, check the exact JSON path using the `fields @message` command to see the raw log structure.
2. Then, depending on your JSON structure, use one of these syntaxes:

```
fields @timestamp,
    @message,
    @logStream,
    parse(@message, '$.node') as node
    # OR
    @message.node
    # OR
    get(@message, '$.node') as node
```

If you're still getting ""Ephemeral field is already defined"", try:

```
fields @timestamp,
    @message,
    @logStream
| parse @message ""*\""node\"":\""*\""*"" as before, node, after
```

To debug, you can run:

```
fields @timestamp, @message
| limit 1
```

This will show you the raw message structure, making it easier to determine the correct path to the node field.

Remember that CloudWatch Logs Insights uses a specific query syntax that's different from standard SQL, and nested JSON fields often need special handling.",2025-04-05T23:30:10,2025-04-05T22:59:44
79556629,DigitalOcean kubeconfig save suddenly stopped working with error 403,"DigitalOcean API has been pretty erratic in history. They randomly keep adding new access controls to the API with zero backward compatibility.

This time what I am trying to do is these:

```
doctl auth remove --context default
doctl auth init
doctl kubernetes cluster kubeconfig save mycluster
```

But it is just failing with error:

> Error: GET [https://api.digitalocean.com/v2/kubernetes/clusters/****](https://api.digitalocean.com/v2/kubernetes/clusters/****)* 403 (request ""*****"" ) You are not authorized to perform this operation.

Our automation which was just working so far stopped working. I will need to figure out now what permission to assign to a new token and propagate that token to all the automations.","kubernetes, digital-ocean",79557821.0,"The error  **403 (request ""*****"" ) You are not authorized to perform this operation** due to the API token not having the required permissions. So you need to check if the API key has the necessary permissions.Make sure the token you are using has both read and write access permissions to kubernetes.

**As per this[Digital ocean blog](https://docs.digitalocean.com/reference/api/create-personal-access-token/) digital ocean recently added custom scopes for tokens. If your token was created long back it might not include the new scopes automatically.**

> Previously, DigitalOcean personal access tokens (PATs) had two scopes: read access to all team resources or full (read and write) access to all team resources.
>
>
> Custom scopes grant more specific permissions, like only creating Droplets or updating cloud firewalls, which lets you secure your workflows by granting only the permissions the token needs and restricting access to other resources and actions.

If you missed a required scope the API may reject with a 403 or 422 Unprocessable Entity errors

**Refer to this [update from Digital ocean](https://docs.digitalocean.com/release-notes/upcoming/breaking-api-changes-resource-authorization/) about Breaking Change to Fix DigitalOcean API Incomplete Resource Authorization Issue.**

> To resolve your issue You may need to recreate API tokens with additional scopes to retain the same functionality, depending on your use cases.",2025-04-06T05:25:02,2025-04-05T06:22:20
79555381,How to use Spring Cloud Config for Kafka bindings with Spring Cloud Dataflow?,"I'm using `SCDF 2.11` with `Kafka` on `Kubernetes` platform and I would like to use `Spring Cloud Config` for application configuration, especially Kafka configurations like topic names.

In our company we have 1 SCDF with 1 Kafka cluster shared for multiple environments. Each environment has its own SCC server. Apps use `Spring Boot 3`.

So our streams are prefixed with env code (like `dev-mystream`) and we pass the SCC server URL in deploy command.
Then I would like the apps to get env specific topic names (also prefixed with env code like `dev-mytopic`) in the env SCC conf file, with the appropriate Spring app name.

But it seems that SCDF/Skipper server passes topic names as arguments to the pod (`spring.cloud.stream.bindings.input.destination` and `spring.cloud.stream.bindings.output.destination`) with classic value `<stream name>.<app label>` and my SCC configuration is ignored.

Is there a way to control which arguments SCDF passes to pods ? Or maybe just the topic names ?","kubernetes, spring-kafka, spring-cloud-dataflow, spring-cloud-config",,,,2025-04-04T12:58:38
79554735,Flux bootstrap stuck on &quot;waiting for GitRepository ... to be reconciled&quot;,"I am trying to bootstrap flux via Ansible with the following task:

```
- name: Bootstrap Flux
  shell: |
    flux bootstrap github \
      --owner={{ github_owner }} \
      --repository={{ github_repo }} \
      --branch={{ github_repo_branch }} \
      --path={{ github_repo_cluster_path }} \
      2>&1 | tee /tmp/bootstrap-flux.log
  environment:
    GITHUB_TOKEN: ""{{ github_token }}""
  when: not flux_bootstrapped
```

It is forever stuck at this point:

```
✔ public key: ecdsa-sha2-nistp384 AAAAE2VjZHNhLXNoYTItbmlzdHAzODQAAAAIbmlzdHAzODQAAABhBMhJw0cZ+xiz1Dn+xZ2RygzTHjXpC8eaL5aHQEYWoPdlPgBkRwXYq0SzIa7rV53kW4Fx73RXPiEpPUeceRto9bzxhq67KwfTa10HvlexVWuVV1jEHzb5SXVadcU0I0SZsQ==
✔ configured deploy key ""flux-system-main-flux-system-./clusters/infra"" for ""https://github.com/my-org/my-repo""
► applying source secret ""flux-system/flux-system""
✔ reconciled source secret
► generating sync manifests
✔ generated sync manifests
✔ sync manifests are up to date
► applying sync manifests
✔ reconciled sync configuration
◎ waiting for GitRepository ""flux-system/flux-system"" to be reconciled
```

This is very frustrating because Flux gives no indication as to what is going on. What do I even do to progress here?","kubernetes, ansible, flux",79620940.0,"I was stuck with the same problem for a few months, rebuilt RKE2 cluster a few times, tried Talos lately - the same. The problem was DNS configuration caused by OPNsense firewall's DHCPv4 settings.

### Finding cause

You could check logs from flux-system containers but let's suppose for now it's TLS Handshake Error. The same error I had with Flux, ArgoCD and Fleet.

Then You could also troubleshoot DNS with [dnsutils container](https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/) and execute nslookup inside it.

```
nslookup github.com
```

In my case it was appending given address (github.com) with my local domain, so it was looking for github.com.*mydomain.com*, returning IPs of cloudflare I use. It was caused by cluster nodes DNS settings, specifically by search domain entry (search mydomain.com). To check that run:

```
cat /etc/resolv.conf
```

or if You're running Talos:

```
talosctl read /etc/resolv.conf
```

To fix the problem the easiest approach would be to remove search domain entry from /etc/resolv.conf. If running Talos - disable search domain:

```
talosctl patch machineconfig --patch '{""machine"": {""network"": {""disableSearchDomain"": true}}}'
```",2025-05-14T07:29:50,2025-04-04T07:37:31
79554625,How to make internal DNS for AKS website work in Azure or Self Hosted Agent to run automated tests?,"I want to run some automated UI tests from the Azure Hosted or Self Hosted agent against my website that's running within AKS. However, I don't seem to be able to DNS the hostname from either the Azure Hosted Agent or Self Hosted Agent. Am I missing something here?

My first conclusion was that it's impossible to do with Azure Hosted Agent since it works outside of the internal/azure network for my corporation. So my assumption is to use a Self Hosted Agent.

However, I'm not able to figure out how the DNS part works, I've seen that AKS has a DNS Server and also running a CoreDNS pod. Somehow it seems to resolve, but I'm not sure what server is doing it within the landscape.","azure, kubernetes, azure-devops, dns, azure-aks",,,,2025-04-04T06:42:50
79554569,fluentd config for a simple Python logging format,"My python flask/Quart app has the following log format:

```
## 2025-04-04 05:16:07 INFO     Running app...
```

How can I configure the `<source>` of fluentd config? I have tried:

```
    <source>
      @type tail
      #dummy [{""python_log"": ""2025-04-04 05:16:07 INFO     Running app...""}]
      path /var/log/pythonrestapi/log*
      pos_file /var/log/td-agent/pos/pythonrestapi_logs.pos
      read_from_head true
      tag pythonrestapi
      key_name message
      reserve_data true
      time_type string
      time_format %Y-%m-%d %H:%M:%S
      keep_time_key true
      # even if we failed to parse the time send the record over to splunk
      emit_invalid_record_to_error false
      reserve_time true
      <parse>
          @type regexp
          expression /^(?<time>.+?)\s+(?<level>\w+)\s+(?<message>.*)$/
      </parse>
    </source>
```

The application runs in local k8s cluster. There is no output at all from `kubectl logs -f <pod>`. Inside the pod, this is what I see in the log:

```
root@pythonrestapi-0:/app# tail /var/log/pythonrestapi/log
2025-04-04 05:44:55 INFO     Running app...
2025-04-04 05:44:55 INFO     Running on https://0.0.0.0:443 (CTRL + C to quit)
2025-04-04 05:44:55 INFO     Running on http://0.0.0.0:8080 (CTRL + C to quit)
2025-04-04 05:44:55 INFO     Running on https://0.0.0.0:443 (QUIC) (CTRL + C to quit)
2025-04-04 05:44:56 INFO     Running on https://[::]:443 (QUIC) (CTRL + C to quit)
2025-04-04 06:05:07 INFO     [99903abd9bdedae87ca987a682701e7b6b6c40d8] ALPN negotiated protocol h3
2025-04-04 06:05:07 INFO     [99903abd9bdedae87ca987a682701e7b6b6c40d8] Connection close received (code 0x0, reason )
2025-04-04 06:05:24 INFO     [227c58078cee0ae359d042ff2b05fe97e191eba4] ALPN negotiated protocol h3
2025-04-04 06:05:24 INFO     [227c58078cee0ae359d042ff2b05fe97e191eba4] Connection close received (code 0x0, reason )
```","docker, kubernetes, fluentd, python-logging, td-agent",79554579.0,"```
<source>
  @type tail
  path /var/log/pythonrestapi/log*
  pos_file /var/log/td-agent/pos/pythonrestapi_logs.pos
  read_from_head true
  tag pythonrestapi

  <parse>
    @type multi_format
    <pattern>
      # Assuming log format is like ""2025-04-04 05:44:55 INFO Running app...""
      format /^(?<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) (?<level>\w+) (?<message>.*)$/
      time_format %Y-%m-%d %H:%M:%S
      keep_time_key true
    </pattern>
  </parse>
</source>
Can we be friends btw?
```",2025-04-04T06:16:28,2025-04-04T06:08:19
79554394,Running Ollama as a k8s STS with external script as entrypoint to load models,"I manage to run Ollama as a k8s STS. I am using it for Python Langchain LLM/RAG application. However the following Dockerfile `ENTRYPOINT` script which tries to pull a list of images exported as `MODELS` ENV from k8s STS manifest runs into problem. Dockerfile has the following `ENTRYPOINT` and `CMD`:

```
ENTRYPOINT [""/usr/local/bin/run.sh""]
CMD [""bash""]
```

`run.sh`:

```
#!/bin/bash
set -x
ollama serve&
sleep 10
models=""${MODELS//,/ }""
for i in ""${models[@]}""; do \
      echo model: $i  \
      ollama pull $i \
    done
```

k8s logs:

```
+ models=llama3.2
/usr/local/bin/run.sh: line 10: syntax error: unexpected end of file
```

David Maze's solution:

```
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    for i in $(seq 10); do
                      ollama ps && break
                      sleep 1
                    done
                    for model in ${MODELS//,/ }; do
                      ollama pull ""$model""
                    done
```

```
ollama-0          1/2     CrashLoopBackOff     4 (3s ago)        115s
ollama-1          1/2     CrashLoopBackOff     4 (1s ago)        115s
```

```
  Warning  FailedPostStartHook  106s (x3 over 2m14s)  kubelet            PostStartHook failed
```

```
$ k logs -fp ollama-0
Defaulted container ""ollama"" out of: ollama, fluentd
Error: unknown command ""ollama"" for ""ollama""
```

Update `Dockerfile`:

```
ENTRYPOINT [""/bin/ollama""]
#CMD [""bash""]
CMD [""ollama"", ""serve""]
```

I need the customized `Dockerfile` so that I could install Nvidia Container Toolkit.","kubernetes, dockerfile, py-langchain, ollama, docker-entrypoint",79555369.0,"At a mechanical level, the backslashes inside the `for` loop are causing problems.  This causes the shell to combine the lines together, so you get a single command `echo model: $i ollama pull $i done`, but there's not a standalone `done` command to terminate the loop.

The next problem you'll run into is that this entrypoint script is the only thing the container runs, and when this script exits, the container will exit as well.  It doesn't matter that you've started the Ollama server in the background.  If you wanted to run the container this way, you need to `wait` for the server to exit.  That would look something like

```
#!/bin/bash
ollama serve &
pid=$!                       # ADD: save the process ID of the server
sleep 10
models=""${MODELS//,/ }""
for i in ""${models[@]}""; do  # FIX: remove backslashes
  echo model: ""$i""
  ollama pull ""$i""
done
wait ""$pid""                  # ADD: keep the script running as long as the server is too
```

However, this model of starting a background process and then `wait`ing for it often isn't the best approach.  If the Pod gets shut down, for example, the termination signal will go to the wrapper script and not the Ollama server, and you won'd be able to have a clean shutdown.

In a Kubernetes context (you say you're running this in a StatefulSet) a [PostStart hook](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/) fits here.  This will let you run an unmodified image, but add your own script that runs at about the same time as the container startup.  In a Kubernetes manifest this might look like:

```
spec:
  template:
    spec:
      containers:
        - name: ollama
          image: ollama/ollama  # the unmodified upstream image
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                      for i in $(seq 10); do
                        ollama ps && break
                        sleep 1
                      done
                      for model in llama3.2; do
                        ollama pull ""$model""
                      done
```

This setup writes a shell script inline in the Kubernetes manifest.  It wraps it in `/bin/sh -c` to it can be run this way.  This uses an ""exec"" mechanism, so the script runs as a secondary process in the same container.  The first fragment waits up to 10 seconds for the server to be running, and the second is the loop to load the models.",2025-04-04T12:53:53,2025-04-04T03:19:47
79554028,How to copy/use a local jar file to a kubernetes pod directory (minikube on mac),"I'm trying to have a kubernetes deployment to work, on a mac with minikube, and using a jar file located in a local host directory:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: master
spec:
  replicas: 1
  (...)
      containers:
        - name: master-c
          (...)
          volumeMounts:
            (...)
            - name: master-volume
              mountPath: /server/plugins/master.jar
              #subPath: master.jar
      volumes:
        - name: master-volume
          hostPath:
            path: /localdir/master.jar
            type: File
```

using File type I get this error:

```
     Warning  FailedMount  10s (x6 over 26s)  kubelet
MountVolume.SetUp failed for volume ""master-volume"" : hostPath type check failed: /localdir/master.jar is not a file
```

How to get the jar file available ? I know that kubectl cp can copy the jar file into the pod directory, but application is started and does not recognize it, so I need to have the jar file available before to start the application. Any idea ?

Another option I've tried is simply to mount the local directory, which works, but shows the jar file as an empty directory:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: master
spec:
  replicas: 1
  (...)
      containers:
        - name: master-c
          (...)
          volumeMounts:
            (...)
            - name: master-volume
              mountPath: /server/plugins
              #subPath: master.jar
      volumes:
        - name: master-volume
          hostPath:
            path: /localdir
            type: Directory
```","file, kubernetes, jar, local",79556317.0,"To make the JAR file available for mounting into a pod directory before starting your application, ensure you expose directories from your host system into the Minikube VM using [9P mounts](https://minikube.sigs.k8s.io/docs/handbook/mount/). Use the following command:

```
$ minikube mount <source directory>:<target directory>
```

You can also remove the “#” from your subPath configuration file to ensure the file is correctly located within the container. Then ensure that the master.jar file is existing on `/localdir`.",2025-04-04T22:16:23,2025-04-03T21:17:01
79553854,K8s: EFS volume persistent volume will not bind to a claim despite being available,"Basic Setup:

1. AWS EKS Cluster
2. AWS EFS for volumes
3. Jira Service Management
4. Confluence

I have a pod that needs two statically provisioned AWS EFS volumes attached to function.  This is an instance of confluence that uses a volume each for the local and shared home directories.

The shared home directory EFS will attach without issue, but the local home directory volume will not.  I get the provisioner error ""Waiting for a volume to be created either by the external provisioner 'efs.csi.aws.com'"" and the PVC will not attach.

Note that the shared home and local home spec are exactly the same, so I don't see why one will work and other wont.  The SGs, networking, and everything else I am aware of, are also identical.

Local Home PV

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: confluence-local-efs
  labels:
    type: efs
spec:
  storageClassName: efs-local-sc
  claimRef:
    name: confluence-local-home
    namespace: app-confluence
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-0ee787b1d3c2ce898
```

Shared Home PV

```
kind: PersistentVolume
apiVersion: v1
metadata:
  name: confluence-shared-efs
  labels:
    type: efs
spec:
  storageClassName: efs-shared-sc
  claimRef:
    name: confluence-shared-home
    namespace: app-confluence
  capacity:
    storage: 5Gi
  volumeMode: Filesystem
  persistentVolumeReclaimPolicy: Retain
  accessModes:
    - ReadWriteMany
  csi:
    driver: efs.csi.aws.com
    volumeHandle: fs-0904d99352b0a2515
```

Storage Class

```
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: efs-shared-sc
provisioner: efs.csi.aws.com
parameters:
  provisioningMode: efs-ap
  fileSystemId: fs-**::fsap-***
```

CSI Driver

```
apiVersion: storage.k8s.io/v1
kind: CSIDriver
metadata:
  name: efs.csi.aws.com
spec:
  attachRequired: false
```

The PVCs are being generated via helm, the values are:

```
volumes:
  localHome:
    persistentVolumeClaim:
      create: true
      storageClassName: efs-local-sc
  sharedHome:
    persistentVolumeClaim:
      create: true
      storageClassName: efs-shared-sc
```

Results of describe on PVC

```
Name:          local-home-confluence-0
Namespace:     app-confluence
StorageClass:  efs-local-sc
Status:        Pending
Volume:
Labels:        app.kubernetes.io/instance=confluence
               app.kubernetes.io/name=confluence
Annotations:   volume.beta.kubernetes.io/storage-provisioner: efs.csi.aws.com
               volume.kubernetes.io/storage-provisioner: efs.csi.aws.com
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Used By:       confluence-0
Events:
  Type    Reason                Age                From                         Message
  ----    ------                ----               ----                         -------
  Normal  ExternalProvisioning  0s (x63 over 15m)  persistentvolume-controller  Waiting for a volume to be created either by the external provisioner 'efs.csi.aws.com' or manually by the system administrator. If volume creation is delayed, please verify that the provisioner is running and correctly registered.
```

Again, a near identical volume is being bound to the same pod, so it is baffling that this is happening on only one.

Any ideas would be greatly appreciated.","kubernetes, amazon-efs",,,,2025-04-03T19:38:57
79553210,unable to connect to public internet when injecting istio proxy/istio envoy,"i have an EKS cluster version 1.30 running on which i have installed istio-base, istiod, istio-ingressgateway using terraform helm resource.I have not installed egress gateway. It is all fine untill i inject the istio envoy proxy to the application pods. At that time i am unable to make any ssl based requests.
I am attaching errors below which i encountered for resolving an s3 bucket, as you can see the domain name is pointing to `es.amazonaws.com` in output. [![enter image description here](https://i.sstatic.net/oTNmDdfA.png)](https://i.sstatic.net/oTNmDdfA.png)

i also have a basic alpine pod that installs certain packages in bootup(pod yaml below). this also crashes as the packages do not get instaled.

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: alpine
  name: alpine
spec:
  containers:
  - image: alpine
    name: alpine
    command: [""sh"",""-c"",""apk update && apk add aws-cli &&  apk add mysql-client && apk add openssh && sleep 365d"" ]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

how can i fix this issue, really appreciate the help.","kubernetes, istio, istio-gateway, istio-sidecar, istio-operator",79580462.0,"You can connect to an S3 bucket from your application using an Istio ServiceEntry and DestinationRule. In this example, I’m assuming the namespace is `default`. The S3 bucket endpoint format should look like:

```
<bucket-name>.s3.us-east-1.amazonaws.com
```

**Service Entry**

```
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: s3-access
spec:
  hosts:
  - s3.us-east-1.amazonaws.com
  - ""###bucket name###""
  location: MESH_EXTERNAL
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
```

**Destination rule**

```
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: s3-destination
spec:
  host: ###bucket name#
  trafficPolicy:
    tls:
      mode: SIMPLE
      insecureSkipVerify: true%
```

After creating the `ServiceEntry` and `DestinationRule`, your application pod should be able to connect to the S3 bucket.",2025-04-18T05:23:47,2025-04-03T14:21:45
79552914,Dockerfile and container gets ENV from k8s Statefulset,"I need to process the ENV from k8s Statefulset in the container before it starts up.
`Dockerfile`:

```
RUN echo CREDENTIALS: $CREDENTIALS
ARG user=""${CREDENTIALS%/*}""
ARG password=""${CREDENTIALS#*/}""
ENV USER $user
ENV PASSWORD $password
```

`Statefulset` environment:

```
          env:
            - name: CREDENTIALS
              valueFrom:
                secretKeyRef:
                  name: app-secret
                  key: CREDENTIALS
```

When I run with `d run -dt -e CREDENTIALS=""user/P@$$w0rd"" myimage:latest`, the `PASSWORD` is missing.","kubernetes, dockerfile, environment-variables, kubernetes-statefulset",79552988.0,"You misunderstand how Dockerfile ARG and ENV work. These values are all baked into the container image at build time.
You need to modify your container's entry point to split `$CREDENTIALS` into separate USER and PASSWORD variables, for example with a small script:

```
#!/bin/sh
export USER=""${CREDENTIALS%/*}""
export PASSWORD=""${CREDENTIALS#*/}""
exec /actual/command ""$@""
```",2025-04-03T12:54:25,2025-04-03T12:24:32
79552479,Polars out of core sorting and memory usage,"From what I understand this is a main use case for Polars: being able to process a dataset that is larger than RAM, using disk space if necessary.  Yet I am unable to achieve this in a Kubernetes environment.  To replicate locally I tried launching a docker container with a low memory limit:

```
docker run -it --memory=500m --rm  -v `pwd`:/app python:3.12  /bin/bash
# pip install polars==1.26.0
```

I checked that it set up the memory limit in cgroups for the current process.
Then I ran a script that loads a moderately large dataframe (23M parquet file, 158M uncompressed), using `scan_parquet`, performs a sort, and outputs the head:

```
source = ""parquet/central_west.df""
df = pl.scan_parquet(source, low_memory=True)
query = df.sort(""station_code"").head()
print(query.collect(engine=""streaming""))
```

This leads to the process getting killed.
It works with a smaller dataframe, or a larger limit.
Is polars not reading the limit correctly?  Or not able to work with that low of a limit?  I understand the ""new"" streaming engine is still in beta, so I tried the same script with version 1.22.0 of polars, but the result was the same.  This seems like a very simple and common use case so I hope I am just missing a configuration trick.

On a hunch and based on a similar question I tried setting POLARS_IDEAL_MORSEL_SIZE=100, but that made no difference, and I feel like I am grasping at straws here.","python, kubernetes, python-polars, cgroups, polars",,,,2025-04-03T09:11:11
79552339,Path-based Routing for External Domains in Kubernetes with Istio/Gateway API,"We want to achieve path-based routing for external domains not owned by our Kubernetes cluster. We managed to configure routing successfully, but now we encounter a side-effect: workloads inside the cluster communicate using plain HTTP on port 443 to the external domain, resulting in SSL errors.

## Desired Behavior

- Requests to `www.example.com/graphql` are routed from the cluster ingress gateway to the external domain.
- Workloads within the service mesh can successfully query `https://graphql-api.mesh-external.example.com` using HTTPS without SSL issues.

## Current Approach

Our configuration uses Istio's `ServiceEntry`, `DestinationRule`, and the Gateway API's `HTTPRoute`:

```
apiVersion: networking.istio.io/v1
kind: ServiceEntry
metadata:
  name: www-example-com
spec:
  hosts:
  - graphql-api.mesh-external.example.com
  location: MESH_EXTERNAL
  ports:
  - name: https
    number: 443
    protocol: HTTPS
  resolution: DNS
---
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: www-example-com
spec:
  host: graphql-api.mesh-external.example.com
  trafficPolicy:
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: SIMPLE
        sni: graphql-api.mesh-external.example.com
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: www-example-com
spec:
  hostnames:
  - www.example.com
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: ingress
    namespace: gateway
  rules:
  - backendRefs:
    - group: networking.istio.io
      kind: Hostname
      name: graphql-api.mesh-external.example.com
      port: 443
      weight: 1
    matches:
    - path:
        type: PathPrefix
        value: /graphql
```

## Problem

- External requests through ingress work correctly, but internal mesh communication defaults to plain HTTP on port 443, causing SSL/TLS errors.

## Question

How can we configure Istio and Gateway API so that internal mesh workloads correctly perform HTTPS communication with the external domain while retaining proper path-based routing for ingress traffic?","kubernetes, istio",79552791.0,"You need to **modify** the **DestinationRule** to enforce [TLS settings](https://istio.io/latest/docs/reference/config/networking/destination-rule/#ClientTLSSettings) to make internal mesh workloads correctly perform HTTPS communication with the external domain while retaining proper path-based routing for ingress traffic without causing any SSL/TLS errors.

So you may need to update the **DestinationRule** by removing portLevelSettings as follows :

```
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: www-example-com
spec:
  host: graphql-api.mesh-external.example.com
  trafficPolicy:
    tls:
      mode: SIMPLE
      sni: graphql-api.mesh-external.example.com
```

For more information check this Isito [document](https://istio.io/latest/docs/concepts/traffic-management/#service-entries) and also go through this Medium [blog](https://harsh05.medium.com/understanding-ingress-gateway-in-istio-a-detailed-guide-9ee300b9da65) by Harsh, which might be helpful to resolve your issue.",2025-04-03T11:25:05,2025-04-03T08:11:15
79548445,Ingress-nginx: Failed to load the resty.core module when using include with js_import in http-snippet,"I'm customizing the ingress-nginx controller by using a modified nginx-base image. Specifically, I want to add the NJS module to enable advanced CORS handling logic via js_import and js_header_filter directives.

I build my own nginx-base image based on registry.k8s.io/ingress-nginx/nginx:v1.2.1, only added the NJS module.

Source: [https://github.com/kubernetes/ingress-nginx/](https://github.com/kubernetes/ingress-nginx/)

Built from the helm-chart-4.12.1 branch
The issue

When I use the custom base image and enable my custom CORS logic via http-snippet in the HelmRelease config:

```
  config:
    http-snippet: |
      include /etc/nginx/CORS_http.conf;
```

```
cat /etc/nginx/CORS_http.conf
js_var $cors_config_file; # set or override in server or location contexts of nginx configuration to point to configuration file
js_var $cors_is_valid_preflight_request; # set by cors.header_filter to signal detection of a valid pre-flight request
js_import cors from /etc/nginx/njs/http/cors.js;
```

I get the following error:
failed to load the 'resty.core' module

```
2025/03/31 09:41:20 [alert] 18#18: failed to load the 'resty.core' module (https://github.com/openresty/lua-resty-core); ensure you are using an OpenResty release from https://openresty.org/en/download.html (reason: module 'resty.core' not found:
 no field package.preload['resty.core']
 no file '/etc/nginx/lua/resty/core.lua'
 no file '../lua-resty-core/lib/resty/core.lua'
 no file '../lua-resty-lrucache/lib/resty/core.lua'
 no file './resty/core.lua'
 no file '/usr/local/share/luajit-2.1/resty/core.lua'
 no file '/usr/local/share/lua/5.1/resty/core.lua'
 no file '/usr/local/share/lua/5.1/resty/core/init.lua'
```

Which seems unrelated to my CORS config or NJS module.

This error disappears when I comment out the http-snippet. But the snippet itself only references js_import (NJS), and does not use Lua at all.

It seems like enabling NJS somehow triggers Lua module loading even when Lua isn't used.

Also tried building from main, from branches starting with controller v1.12.0 down to v1.10.0, and from helm-chart-4.12.1 down to helm-chart-4.10.0 - same result every time.
Tried both building manually and using the current ci.yaml from a fork of the ingress-nginx repo.

Thanks.","kubernetes, nginx, lua, ingress-nginx, njs",,,,2025-04-01T11:03:51
79547914,How Can I Use # (Pound Sign) in Yaml For Istio Path,"I have a URL like this:

> `http://www.test.com/test1/#/test2`

When I used Istio AuthorizationPolicy to denied the path include #, it won't be work.

```
apiVersion: security.istio.io/v1
kind: AuthorizationPolicy
metadata:
  name: test
spec:
  action: DENY
  rules:
  - to:
    - operation:
        paths: [""/test1/#/test2""] # not work
```

When I removed #, istio can successfully block website. But other path will be blocked also.

```
apiVersion: security.istio.io/v1
kind: AuthorizationPolicy
metadata:
  name: test
spec:
  action: DENY
  rules:
  - to:
    - operation:
        paths: [""/test1/*""] # works, but others path be blocked also
```

**How can I block website when path include # ([http://www.test.com/test1/#/test2](http://www.test.com/test1/#/test2)) ?**","kubernetes, yaml, istio, argocd",,,,2025-04-01T06:56:37
79547873,Kubernetes engine error with apiVersion and kind,"I'm getting an error

> This apiVersion and/or kind does not reference a schema known by Cloud Code. Please ensure you are using a valid apiVersion and kind.

on both manifest.yaml and nordered-service.yaml.

manifest.yaml =

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nodered-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nodered
  template:
    metadata:
      labels:
        app: nodered
    spec:
      containers:
        - name: nodered
          image: nodered/node-red
          ports:
            - containerPort: 1880
```

nordered-service.yaml =

```
apiVersion: v1
kind: Service
metadata:
  name: nodered-service
spec:
  type: LoadBalancer
  selector:
    app: nodered
  ports:
    - name: http
      port: 80
      targetPort: 1880
```

I'm getting the same errors on both files with apiVersion and kind.","kubernetes, google-kubernetes-engine",79551177.0,"The error message you're seeing is related to the Cloud Code extension (such as VS Code, IntelliJ, or another IDE with Kubernetes integration), rather than an issue with the Kubernetes API server itself, as your apiVersion and kind appear to be standard and correct. This likely indicates that Cloud Code's internal validation system either doesn't recognize or isn't properly syncing with the latest Kubernetes API schema, or there could be a caching problem.

You can try the following workarounds:

- Test directly with kubectl by running: `kubectl apply -f manifest.yaml`. If this works and the error doesn't occur when applying the files directly, it suggests the issue is likely related to the Cloud Code plugin or your IDE configuration.
- You can also try the workarounds mentioned [here](https://github.com/GoogleCloudPlatform/cloud-code-vscode/issues/649) by adding a custom schema.
- Try restarting the IDE and reloading the Kubernetes context. Cloud Code caches Kubernetes API schemas for validation, and sometimes this cache can become outdated or corrupted, particularly after cluster upgrades or changes.
- Make sure you have the latest version of the Cloud Code extension installed and verify that your Kubernetes version is compatible with the API versions.

I also suggest filling a [bug or issue](https://github.com/GoogleCloudPlatform/cloud-code-vscode/issues) for more thorough resolution.",2025-04-02T17:14:57,2025-04-01T06:33:51
79544835,Kubernetes ingress with multiple backend services throws site insecure error for specific contexts,"I have deployed multiple backend services behind a single ingress controller. When I load the portal application (its a web portal through which other applications can be launched from menu), the portal site loads fine and the site shows secure. When I launch the other application by selecting one from the menu, the app that is selected launches in a new tab and when this is done, not only the new web page that shows insecure on the new tab but the portal web page on the previous tab also switches to insecure. This is occurring only for specific backend applications. Some of the containers have more than 1 application running on them, some applications do not complain insecure where as others do even though they run on same tomcat. For instance if I open multiple applications via main app (lets say about 4 backend apps on 4 tabs) and all of them show secure and now if I launch a 5th application that pulls up on the 5th tab and this shows insecure, now if I go back to those previous 4 tabs they too now complains insecure without even refreshing those 4 tabs. The ingress domain SSL certificates are managed by infra and are valid.

Screenshot:
Before:

[![enter image description here](https://i.sstatic.net/HazqpgOy.png)](https://i.sstatic.net/HazqpgOy.png)

After launching a web app on a new tab, the new and the original tab switches to insecure:

[![enter image description here](https://i.sstatic.net/H3P6x6hO.png)](https://i.sstatic.net/H3P6x6hO.png)

This has been puzzling me for sometime, is it that the browser detecting some insecure faulty config ?

My app run on tomcat on a container on a Rancher. I have tried below approaches which did not work,

1> Initially my backends were running on 8080, so I went ahead and generated a java keystore and enabled https on the tomcat and made it run on 8443. Made the services point to 8443 instead of 8080

2> Enable crossContext=""true"" on the tomcat/conf/context.xml thinking the sharing of contexts between sites may help but it did not.

Below is my ingress controller yaml. Much appreciated if anyone can provide pointers on what else I can try or what could be the problem area.

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: 'abcd'
  namespace: 'dev1'
  annotations:
    appgw.ingress.kubernetes.io/backend-path-prefix: ""/""
    nginx.ingress.kubernetes.io/backend-protocol: HTTPS
    nginx.ingress.kubernetes.io/ssl-redirect: 'true'
  labels:
    project: 'abcd'
spec:
  ingressClassName: nginx
  rules:
    - host: somedomain.corp.intranet
      http:
        paths:
          - backend:
              service:
                name: portal-dev1
                port:
                  number: 8443
            path: /
            pathType: ImplementationSpecific
          - backend:
              service:
                name: portal-dev1
                port:
                  number: 8443
            path: /portal
            pathType: ImplementationSpecific
          - backend:
              service:
                name: cfml-dev1
                port:
                  number: 8443
            path: /cfml
            pathType: ImplementationSpecific
          - backend:
              service:
                name: webapp1-dev1
                port:
                  number: 8443
            path: /webappA
            pathType: ImplementationSpecific
  tls:
    - hosts:
        - somedomain.corp.intranet
status:
  loadBalancer:
    ingress:
      - ip: someIpList
```","kubernetes, ssl, tomcat, kubernetes-ingress, rancher",79548706.0,"The error suggests (!?) that at least one reference in the apps that trigger the error is being retrieved over a non-TLS endpoint.

Identify each of the apps that trigger the error and scour them for `http://`.

Not just HTML but images, scripts, css, every file that loads over a URL needs to use TLS.",2025-04-01T12:59:26,2025-03-30T15:58:13
79542567,How to configure SpringBoot and Keycloak pod in kubernetes for successful authentication flow?,"I want to configure SpringBoot Thymeleaf app together with Keycloak in the kubernetes namespace that runs on my local computer. In the way that Springboot app runs on its own pod and Keycloak app on its own pod and they communicate with each other in order to authenticate users and etc... I do not know how to correctly configure SpringBoot with Keycloak in my local k8s cluster so they can successfully communicate with each other.

Here is what I have:

1. My `pom.xml`

```
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.4.1</version>
        <relativePath/> <!-- lookup parent from repository -->
    </parent>
    <groupId>pl.jacekhorabik</groupId>
    <artifactId>urlshortener</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>urlshortener</name>
    <description>urlshortener</description>

    <properties>
        <java.version>21</java.version>
        <commons-codec.commons-codec.version>1.17.1</commons-codec.commons-codec.version>
        <io.seruco.encoding.base62.version>0.1.3</io.seruco.encoding.base62.version>
        <org.jetbrains.annotations.version>13.0</org.jetbrains.annotations.version>
        <org.liquibase.liquibase-maven-plugin.version>4.27.0</org.liquibase.liquibase-maven-plugin.version>
        <org.keycloak.keycloak-spring-boot-starter.version>25.0.3</org.keycloak.keycloak-spring-boot-starter.version>
    </properties>

    <dependencies>
        <!--    spring-boot-starter    -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-data-jpa</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-security</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-thymeleaf</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-oauth2-client</artifactId>
        </dependency>
        <dependency>
            <groupId>org.keycloak</groupId>
            <artifactId>keycloak-spring-boot-starter</artifactId>
            <version>${org.keycloak.keycloak-spring-boot-starter.version}</version>
        </dependency>
        <!--    spring-boot-starter-test    -->
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.security</groupId>
            <artifactId>spring-security-test</artifactId>
            <scope>test</scope>
        </dependency>
        <!--    database    -->
        <dependency>
            <groupId>org.liquibase</groupId>
            <artifactId>liquibase-core</artifactId>
        </dependency>
        <dependency>
            <groupId>com.h2database</groupId>
            <artifactId>h2</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>com.mysql</groupId>
            <artifactId>mysql-connector-j</artifactId>
            <scope>runtime</scope>
        </dependency>
        <!--    frontend    -->
        <dependency>
            <groupId>org.thymeleaf.extras</groupId>
            <artifactId>thymeleaf-extras-springsecurity6</artifactId>
        </dependency>
        <!--    encoding    -->
        <dependency>
            <groupId>commons-codec</groupId>
            <artifactId>commons-codec</artifactId>
            <version>${commons-codec.commons-codec.version}</version>
        </dependency>
        <dependency>
            <groupId>io.seruco.encoding</groupId>
            <artifactId>base62</artifactId>
            <version>${io.seruco.encoding.base62.version}</version>
        </dependency>
        <!--    annotations    -->
        <dependency>
            <groupId>org.projectlombok</groupId>
            <artifactId>lombok</artifactId>
        </dependency>
        <dependency>
            <groupId>org.jetbrains</groupId>
            <artifactId>annotations</artifactId>
            <version>${org.jetbrains.annotations.version}</version>
            <scope>compile</scope>
        </dependency>
    </dependencies>

</project>
```

1. My spring security config in `application.yml`:

```
spring:
  security:
    oauth2:
      client:
        provider:
          urlshortener-keycloak-provider:
            issuer-uri: http://urlshortener-keycloak-service.urlshortener-dev:8080/realms/urlshortener-keycloak-realm
        registration:
          keycloak:
            provider: urlshortener-keycloak-provider
            authorization-grant-type: authorization_code
            client-id: urlshortener-keycloak-client
            client-secret: secret
            scope: openid
```

1. My `SecurityConfig.java` class:

```
import org.jetbrains.annotations.NotNull;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity;
import org.springframework.security.oauth2.client.oidc.web.logout.OidcClientInitiatedLogoutSuccessHandler;
import org.springframework.security.oauth2.client.registration.ClientRegistrationRepository;
import org.springframework.security.web.SecurityFilterChain;

@Configuration
@EnableWebSecurity
class SecurityConfig {

  @Bean
  SecurityFilterChain clientSecurityFilterChain(
      @NotNull HttpSecurity http, ClientRegistrationRepository clientRegistrationRepository)
      throws Exception {
    http.oauth2Login(
        login -> {
          login.defaultSuccessUrl(""/v1/"");
        });
    http.logout(
        logout -> {
          final var logoutSuccessHandler =
              new OidcClientInitiatedLogoutSuccessHandler(clientRegistrationRepository);
          logoutSuccessHandler.setPostLogoutRedirectUri(""{baseUrl}/v1/"");
          logout.logoutSuccessHandler(logoutSuccessHandler);
        });

    http.authorizeHttpRequests(
        requests -> {
          requests.requestMatchers(""/v1/**"", ""/actuator/health"", ""/favicon.ico"").permitAll();
          requests.requestMatchers(""/admin"").hasAuthority(""ADMIN"");
          requests.requestMatchers(""/user"").hasAuthority(""USER"");
          requests.anyRequest().denyAll();
        });
    return http.build();
  }
}
```

1. Keycloak Docker image I use, with my own realm import:

```
FROM quay.io/keycloak/keycloak:25.0.1

EXPOSE 8080/tcp

ENV KEYCLOAK_ADMIN=""admin""
ENV KEYCLOAK_ADMIN_PASSWORD=""admin""

ADD ./realm/urlshortener-keycloak-realm.json /opt/keycloak/data/import/

CMD [ ""start"", \
""--verbose"", \
""--features"", ""hostname:v2"", \
""--http-port"", ""8080"", \
""--hostname"" , ""localhost"", \
""--hostname-debug"", ""true"", \
""--http-relative-path"", ""/"", \
""--http-enabled"", ""true"", \
""--health-enabled"", ""true"", \
""--metrics-enabled"", ""true"", \
""--import-realm"", \
""--db"", ""mysql"", \
""--db-username"", ""keycloak-admin"", \
""--db-password"", ""password"", \
""--db-url-host"", ""urlshortener-db-service.urlshortener-dev"", \
""--db-schema"", ""keycloak"", \
""--db-url-port"", ""3306"" \
]
```

Now k8s resources:

1. K8s namespace, SpringBoot app deployment and service & Keycloak deployment and service:

```
apiVersion: v1
kind: Namespace
metadata:
  name: urlshortener-dev

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: urlshortener-backend-deployment
  labels:
    app: urlshortener
    layer: backend
  namespace: urlshortener-dev
spec:
  template:
    metadata:
      name: urlshortener-backend-pod
      labels:
        app: urlshortener
        layer: backend
    spec:
      containers:
        - image: urlshortener-backend:latest
          name: urlshortener-backend-container
          ports:
            - containerPort: 8080
          imagePullPolicy: IfNotPresent
  replicas: 1
  selector:
    matchLabels:
      app: urlshortener
      layer: backend
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

---

apiVersion: v1
kind: Service
metadata:
  name: urlshortener-backend-service
  namespace: urlshortener-dev
  labels:
    app: urlshortener
    layer: backend
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
      nodePort: 30008
  selector:
    app: urlshortener
    layer: backend
  type: NodePort

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: urlshortener-keycloak-deployment
  labels:
    app: urlshortener
    layer: keycloak
  namespace: urlshortener-dev
spec:
  template:
    metadata:
      name: urlshortener-backend-pod
      labels:
        app: urlshortener
        layer: keycloak
    spec:
      containers:
        - image: urlshortener-keycloak:latest
          name: urlshortener-keycloak-container
          ports:
            - containerPort: 8080
          imagePullPolicy: IfNotPresent
  replicas: 1
  selector:
    matchLabels:
      app: urlshortener
      layer: keycloak
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1

---

apiVersion: v1
kind: Service
metadata:
  name: urlshortener-keycloak-service
  namespace: urlshortener-dev
  labels:
    app: urlshortener
    layer: keycloak
spec:
  ports:
    - protocol: TCP
      port: 8080
      targetPort: 8080
      nodePort: 30009
  selector:
    app: urlshortener
    layer: keycloak
  type: NodePort

```

As you can see I also have MySQL db deployed in this k8s namespace, but it does not matter in the context of this problem.
You can browse the code here [https://github.com/HorabikJ/urlshortener/tree/stackoverflow-question](https://github.com/HorabikJ/urlshortener/tree/stackoverflow-question), it is my public repo for this project.
If you want to start the k8s setup please run `./k8s-run.sh` in the root of the repo.

When I start this setup, I get below error log from SpringBoot app pod (`urlshortener-backend-deployment`):

```
Caused by: org.springframework.beans.BeanInstantiationException:
 Failed to instantiate [org.springframework.security.oauth2.client.registration.InMemoryClientRegistrationRepository]:
 Factory method 'clientRegistrationRepository' threw exception with message:
 The Issuer ""http://localhost:8080/realms/urlshortener-keycloak-realm""
 provided in the configuration metadata did not match the requested issuer
 ""http://urlshortener-keycloak-service.urlshortener-dev:8080/realms/urlshortener-keycloak-realm""
```

How I can setup correct Keycloak url in Springboot app so it can can communicate with Keycloak pod and redirect users to login using Keycloak console and after succesfull login redirect back to SpringBoot?","java, spring-boot, kubernetes, keycloak",,,,2025-03-28T23:16:39
79542152,spark driver pod gets stuck at ContainerCreating state after be configured to use persisted volume for scratch space,"It was working well before adding the following change in the spark driver pod yaml specification to make driver pod use a PVC backed by externally provisioned storage for spilling files to a custom path:

```
spec:
  containers:
    env:
      - name: SPARK_LOCAL_DIRS
        value: /ephemeral
    volumeMounts:
      - name: spark-local-dir-1
        mountPath: /ephemeral
  volumes:
    - name: spark-local-dir-1
      persistentVolumeClaim:
        claimName: shared-persistence
```

Then the pod gets stuck at ContainerCreating state and there aren't any events or errors in `kubectl describe pod`. PVC `shared-persistence` is bound.
One potential reason I can think of is that we also have ephemeral-storage definitions in the resources section:

```
containers:
    - resources:
        limits:
          cpu: '1'
          ephemeral-storage: 4Gi
          memory: 2457Mi
        requests:
          cpu: '1'
          ephemeral-storage: 500Mi
          memory: 2457Mi
```

Are they conflicting? It is difficult to remove the ephemeral-storage definitions due to some issues in our codebase so I would like to confirm first. Thanks.","apache-spark, kubernetes, ephemeral-storage",,,,2025-03-28T18:15:44
79541591,How to add subpaths to url and delete subpaths to ingress,"I have a requirement to embed the dify app page into the web page.
Because of network and other reasons, I have to embed **additional sub-paths** in the original iframe src to route to different services.

I configure iframe and ingress as follows to access it normally.

```
<iframe
 src=""http://my.com/chatbot/abcdefg""
 style=""width: 100%; height: 100%; min-height: 700px""
 frameborder=""0""
 allow=""microphone"">
</iframe>
```

ingress

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /
            backend:
              serviceName: dify
              servicePort: 80
```

But when I try to add a subpath(proxy/dify) and configure the match rewrite in Ingress, an **error** occurs (default backend-404). From the developer tools, we can see an original request and a request without subpaths.

```
<iframe
 src=""http://my.com/proxy/dify/chatbot/abcdefg""
 style=""width: 100%; height: 100%; min-height: 700px""
 frameborder=""0""
 allow=""microphone"">
</iframe>
```

ingress

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /\$1
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/preserve-host: ""true""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /proxy/dify/(.*)
            backend:
              serviceName: dify
              servicePort: 80
```

I know there is a high probability of an error because I did not change the src etc inside the iframe. But I don't know any other way to achieve, looking forward to your help","kubernetes, iframe, kubernetes-ingress",79547153.0,"Try using regular expression `/proxy/dify(/|$)(.*)` with the rewrite target `/$2`. This will ensure that your every request will include the root path (/) after the ingress removes `/proxy/dify`. Consider adding `pathType` in your ingress paths as well.

Sample revised config:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/preserve-host: ""true""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /proxy/dify(/|$)(.*)      #update to (/|$)(.*)
            backend:
            pathType: ImplementationSpecific      #added pathType
              serviceName: dify
              servicePort: 80
```

You can also refer to this [Rewrite](https://kubernetes.github.io/ingress-nginx/examples/rewrite/) documentation.",2025-03-31T19:55:49,2025-03-28T13:40:54
79541565,how to enable cpu_rt_runtime in kubernetes,"I want the defalut container(contained) in the k8s pod to use the parameter cpu-rt-runtime, so I modified the following part of the k8s code to test it

1. `staging/src/k8s.io/cri-api/pkg/apis/runtime/v1/api.proto`
add  `int64 cpu_rt_runtime = 11` at the end of `LinuxContainerResources`
,then I rebuild api.pb.go
2. `pkg/kubelet/kuberuntime/kuberuntime_container_linux.go`
add `resources.CpuRtRuntime = 5000` before `return &resources` in `func (m *kubeGenericRuntimeManager) calculateLinuxResources`

then i create cluster using **kind** and build a debian pod with `kubectl run -it --rm debian-shell --image=debian:latest --restart=Never -- bash`, all success, but after i entered conatiner i found that
`cat /sys/fs/cgroup/cpu,cpuacct/cpu.rt_runtime_us` return **0**

May I ask if my changes are correct?

i am using kernel v5.8(CONFIG_RT_GROUP_SCHED=y), Ubuntu20.04, k8s v1.32.3, kind v0.27.0 docker 28.0.2

i followed [https://docs.docker.com/engine/containers/resource_constraints/#configure-the-real-time-scheduler](https://docs.docker.com/engine/containers/resource_constraints/#configure-the-real-time-scheduler)
and command

```
docker run -it \
    --cpu-rt-runtime=950000 \
    --ulimit rtprio=99 \
    --cap-add=sys_nice \
    debian:jessie
```

works well in my system","kubernetes, kind",,,,2025-03-28T13:34:03
79541529,Forwarding CF-IPCountry Header to Backend with NGINX Ingress and Cloudflare,"I’m currently running a GKE cluster with the NGINX Ingress Controller to manage incoming traffic. My domain is proxied through Cloudflare with IP Geolocation enabled, which adds the CF-IPCountry header to incoming requests to denote the client’s country code. However, I’m facing challenges in forwarding this header to my backend services.

Current Ingress Configuration:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-backend-ingress
  namespace: my-backend
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: 'true'
    nginx.ingress.kubernetes.io/body-size: '0'
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header CF-IPCountry $http_cf_ipcountry;
      real_ip_header CF-Connecting-IP;
      proxy_set_header Host $http_host;
      proxy_set_header Scheme $scheme;
      proxy_set_header SERVER_PORT $server_port;
      proxy_set_header REMOTE_ADDR $remote_addr;
      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
      proxy_set_header Upgrade $http_upgrade;
    nginx.ingress.kubernetes.io/proxy-body-size: '0'
    nginx.ingress.kubernetes.io/proxy-connect-timeout: '60'
    nginx.ingress.kubernetes.io/proxy-http-version: '1.1'
    nginx.ingress.kubernetes.io/proxy-read-timeout: '600'
    nginx.ingress.kubernetes.io/proxy-send-timeout: '600'
    nginx.ingress.kubernetes.io/rewrite-target: /$2
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - api.my-domain.com
      secretName: back-dev-tls
  rules:
    - host: api.my-domain.com
      http:
        paths:
          - path: /(/?|$)(.*)
            pathType: Prefix
            backend:
              service:
                name: my-backend-service
                port:
                  number: 80
```

Steps I’ve Taken:

1. Enabled IP Geolocation in Cloudflare: Verified that the CF-IPCountry header is added to incoming requests by enabling the IP Geolocation feature in the Cloudflare dashboard.
2. Configured NGINX Ingress Annotations: Added a configuration-snippet annotation to set the CF-IPCountry header for proxied requests.

Issue:

Despite these configurations, the CF-IPCountry header doesn’t appear to be reaching my backend services. I’m unable to retrieve the client’s location based on this header.","kubernetes, nginx, cloudflare, nginx-ingress, nginx-config",,,,2025-03-28T13:15:57
79540452,How to deploy a local image on K3D without pushing to a registry and upgrade Helm deployments locally?,"I have two questions regarding deploying a local Kubernetes cluster using K3D and Helm.

I have successfully built a local registry and cluster on K3D using the commands `k3d registry create registry.localhost -p 5000` and `k3d cluster create c1 --registry-use k3d-registry.localhost:5000`. I set the `imagePullPolicy` to `Always`, and it works.

However, I have to build the container, then push and pull it again (I use Helm) every time I want to test the service locally. To skip the push-pull process, I tried setting the `imagePullPolicy` to `Never` so that Helm would use the local container I just built. But I got failed `ErrImageNeverPull` like this:

```
NAME                                 READY   STATUS              RESTARTS   AGE
webtest-deployment-7fb8ccb485-7vpxz   0/1     ErrImageNeverPull   0          21s
```

So, how can I make the deployment successful without the push-pull process to the registry by setting imagePullPolicy to Never (just use the local image after it’s built)?

The second issue is that when I made changes or revisions to the project files, build a new Docker image and push it to the local registry. However, when I update the deployment using `helm upgrade <release> <chart>` or `helm upgrade <release> <chart> --force`, the changes do not take effect. Additionally, the pods are not replaced either before or after the upgrade. To apply the changes, I have to reinstall the package by running helm uninstall followed by helm install. Is this behavior common in Helm deployments, or am I missing a step to properly upgrade the service via Helm?

related question:
[Local Kubernetes Deployment using k3d - where should I push the docker images to?](https://stackoverflow.com/questions/73674229/local-kubernetes-deployment-using-k3d-where-should-i-push-the-docker-images-to)","docker, kubernetes, kubernetes-helm, docker-registry, k3d",79542100.0,"If I got you right you want to test custom images with existing helm charts without changing the helm chart or the hassle of setting up a registry and/or doing all the build/push/pull/imagePullSecrets stuff. This can be achieved using a clever combination of k3d and [tilt](https://tilt.dev)'s features and would go like this:

1. The image is built by tilt on every change in the context directory.
2. It is then automatically pushed by tilt into the registry created by k3d, which tilt auto-detects.
3. Then tilt ""injects"" the newly built image into the helm chart.

For the sake of this example, let's assume that you want to deploy an nginx image containing a custom web site for your company with the [bitnami helm chart for nginx](https://artifacthub.io/packages/helm/bitnami/nginx).

## Directory structure

```
.
├── Tiltfile
├── image
│   ├── Dockerfile
│   └── index.html
└── k3d.yaml
```

### `image/Dockerfile`

```
FROM bitnami/nginx:1.27.4-debian-12-r6
# Allow modifications to the image
USER 0
# Just an example for a custom image
ADD index.html /app/index.html
# Run nginx as a non-root user
USER 1001
```

Nothing much to see here. The HTML file is even more meaningless, so I leave it out.

### `k3d.yaml`

Also, not much of a surprise. However: tilt will automatically detect the registry created and be able to push images to it, so there is no need to adjust `insecure_registries` in your Docker settings. k3d in turn is able to pull images from said insecure registry, so we have the complete ""build->push->pull"" cycle.

```
apiVersion: k3d.io/v1alpha5
kind: Simple
metadata:
  name: demo # name that you want to give to your cluster (will still be prefixed with `k3d-`)
servers: 1 # same as `--servers 1`
agents: 1 # same as `--agents 1`
image: rancher/k3s:v1.29.15-k3s1 # same as `--image rancher/k3s:v1.29.15-k3s1`
ports:
  - port: 8080:80 # same as `--port '8080:80@loadbalancer'`
    nodeFilters:
      - loadbalancer
  - port: 8443:443 # same as `--port '8443:443@loadbalancer'`
    nodeFilters:
      - loadbalancer
registries: # define how registries should be created or used
  create: # creates a default registry to be used with the cluster; same as `--registry-create localregistry`
    name: localregistry
    host: ""0.0.0.0""
    hostPort: ""5000""
options:
  k3d: # k3d runtime settings
    wait: true # wait for cluster to be usable before returining; same as `--wait` (default: true)
    timeout: ""180s"" # wait timeout before aborting; same as `--timeout 60s`
  kubeconfig:
    updateDefaultKubeconfig: true # add new cluster to your default Kubeconfig; same as `--kubeconfig-update-default` (default: true)
    switchCurrentContext: true # also set current-context to the new cluster's context; same as `--kubeconfig-switch-context` (default: true)
```

### Tiltfile

```
# Build the custom image and push it to the local registry,
# which for k3d is autodetected by tilt.
# The image is built using the Dockerfile in the 'image' directory.
# Note that the docker image is rebuilt and the whole deployment starts over
# if the Dockerfile if any of the files in the `context` directory changes.
docker_build(""company/custom_nginx"",context=""image"")

# Load an extension to conveniently deal with the helm chart.
load(""ext://helm_resource"", ""helm_resource"", ""helm_repo"")

# We first need to load the helm repo
# and then we can load the helm resource.
helm_repo('bitnami',url=""https://charts.bitnami.com/bitnami"")

# Install the actual release.
# The image_deps are the images that are built before the helm resource is created.
# The image_keys are the keys that are used to inject the local image into the helm release.
helm_resource(
    'nginx-release',
    chart='bitnami/nginx',resource_deps=['bitnami'],
     flags=['--set=global.security.allowInsecureImages=true'],
    # THIS is where the magic happens:
    # 'helm_ressource'
    image_deps=['company/custom_nginx'],
    image_keys=[('image.registry', 'image.repository', 'image.tag')],
    )

```

## What is happening?

> Some parts of the following will be `[redacted]` for privacy reasons.

After [installing tilt](https://docs.tilt.dev/install.html), we can run `k3d cluster create --config k3d.yaml && tilt up` and watch the logs in tilt's UI.

1. Tiltfile is parsed

```
Loading Tiltfile at: [redacted]/Tiltfile
Successfully loaded Tiltfile (1.295717079s)
Auto-detected local registry from environment: &RegistryHosting{Host:localhost:5000,HostFromClusterNetwork:localregistry:5000,HostFromContainerRuntime:localregistry:5000,Help:https://k3d.io/stable/usage/registries/#using-a-local-registry,SingleName:,}
```

Note that tilt indeed detected the registry we just created.
2. The bitnami helm repo is added

```
Running cmd: helm repo add bitnami https://charts.bitnami.com/bitnami --force-update
""bitnami"" has been added to your repositories
```

Since we added the repo in the `resource_deps` of the `helm_resource`, the helm release will not be deployed before the helm repo was successfully added.
3. The helm release is deployed.

This is where it get's interesting. Since we declared the docker image `company/custom_nginx` in the `image_deps` of the helm resource and instructed the `helm_resource` where to use said image via `image_keys`, the image's values will be substituted:

```
STEP 1/3 — Building Dockerfile: [company/custom_nginx]
Building Dockerfile for platform linux/amd64:
[...]
STEP 2/3 — Pushing localhost:5000/company_custom_nginx:tilt-2de2a5b04212dc59
     Pushing with Docker client
     Authenticating to image repo: localhost:5000
     [...]
 STEP 3/3 — Deploying
      [...]
      Running cmd: ['helm', 'upgrade', '--install', '--set=global.security.allowInsecureImages=true', '--set', 'image.registry=localregistry:5000', '--set', 'image.repository=company_custom_nginx', '--set', 'image.tag=tilt-2de2a5b04212dc59', 'nginx-release', 'bitnami/nginx']
      Release ""nginx-release"" does not exist. Installing it now.
      NAME: nginx-release
      LAST DEPLOYED: Fri Mar 28 17:56:07 2025
      NAMESPACE: default
      STATUS: deployed
      REVISION: 1
      TEST SUITE: None
      NOTES:
      CHART NAME: nginx
      CHART VERSION: 19.0.3
      APP VERSION: 1.27.4
```

## Conclusion

Using tilt and some 4 lines of configuration, you not only can test your custom images easily, but can do so continuously, since the image will be rebuilt and redeployed each time there is a change in context directory of the image. And all this with two simple commands. Do not believe me? ""All"" the [code is available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/k3d-localregistry-tilt-79540452). Clone and try ;).",2025-03-28T17:46:15,2025-03-28T03:20:54
79540381,Spring boot application using maven build failing in Azure AKS- main &quot;ClassNotFoundException&quot;,"I have deployed a spring boot application in Azure kubernetes. The application uses maven for build and works normally on my local system. But when I deploy it in AKS using github action, the pod goes into error. The log shows following error:

`Exception in thread ""main"" java.lang.ClassNotFoundException: package com.LearnBS.JavaBS.JavaBsApplication at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592) at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.loadClass(JarUrlClassLoader.java:107 at org.springframework.boot.loader.launch.LaunchedClassLoader.loadClass(LaunchedClassLoader.java:91) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525) at java.base/java.lang.Class.forName0(Native Method) at java.base/java.lang.Class.forName(Class.java:467) at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:99) at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64) at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40) `

I looked at solutions provided for similar situation, but none of that is working.

Here is my POM.XML

4.0.0

org.springframework.boot
spring-boot-starter-parent
3.3.4

com.LearnBS
JavaBS
0.0.1-SNAPSHOT

```
<name>JavaBS</name>
<description>Demo project for Spring Boot</description>
<url/>
<licenses>
    <license/>
</licenses>
<developers>
    <developer/>
</developers>
<scm>
    <connection/>
    <developerConnection/>
    <tag/>
    <url/>
</scm>
<properties>
    <java.version>17</java.version>
</properties>
<dependencies>
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.mongodb/mongodb-driver-sync -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>mongodb-driver-sync</artifactId>
        <version>5.3.1</version>
    </dependency>
    <!-- https://mvnrepository.com/artifact/org.mongodb/mongodb-driver-core -->
    <dependency>
        <groupId>org.mongodb</groupId>
        <artifactId>mongodb-driver-core</artifactId>
        <version>5.3.1</version>
    </dependency>

    <!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-starter-data-mongodb -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-data-mongodb</artifactId>
        <version>3.4.2</version>
    </dependency>

    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-test</artifactId>
        <scope>test</scope>
    </dependency>

    <dependency>
        <groupId>com.google.code.gson</groupId>
        <artifactId>gson</artifactId>
        <version>2.8.2</version>
    </dependency>
</dependencies>

<build>
    <plugins>
        <plugin>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-maven-plugin</artifactId>
                  <configuration>

        <mainClass>package com.LearnBS.JavaBS.JavaBsApplication</mainClass>

  </configuration>
        <!--<configuration> -->
            <!-- <mainClass>package com.LearnBS.JavaBS.JavaBsApplication</mainClass>-->
            <!--</configuration>-->
        </plugin>

    </plugins>
</build>
```

And here is the workflow file:

name: Maven Package

on:
push:
branches: [ master ]
jobs:
build:

```
runs-on: ubuntu-latest
permissions:
  contents: read
  packages: write
  id-token: write
steps:
- uses: actions/checkout@v4
- name: Set up JDK 17
  uses: actions/setup-java@v4
  with:
    java-version: '17'
    distribution: 'adopt'

- name: Build with Maven
  run: mvn clean install spring-boot:repackage

- name: Docker Login
  uses: azure/docker-login@v1
  with:
    login-server: aksfirstregistry.azurecr.io
    username: xxxxxxxxxxxxx
    password: bxxxxxxxxxxxxxxxxx
- name: Set up Docker Buildx
  uses: docker/setup-buildx-action@v3

- name: Build and Push to ACR
  uses: docker/build-push-action@v6
  with:
    context: .
    push: true
    tags: aksfirstregistry.azurecr.io/g********:latest
    file: Dockerfile

  # Logs in with your Azure credentials
- name: Log in to Azure
  uses: azure/login@v1
  with:
    creds: |
      {
        ""clientId"": xxxxxxxxxxxxxx,
        ""clientSecret"": xxxxxxxxxxxx,
        ""tenantId"": xxxxxxxxxxxx,
        ""subscriptionId"": xxxxxxxxxxxx;
      }

- name: Set AKS context
  id: set-context
  uses: azure/aks-set-context@v3
  with:
    resource-group: xxxxxxxxx
    cluster-name: xxxxxxxxxxx

- name: Setup kubectl
  id: install-kubectl
  uses: azure/setup-kubectl@v3

- name: Deploy to AKS
  id: deploy-aks
  uses: Azure/k8s-deploy@v4
  with:
    namespace: 'default'
    manifests: |
      gitmongodb.yaml
    images: aksfirstregistry.azurecr.io/gxxxxx:latest
    pull-images: false
```

Any help is highly appreciated!","azure, maven, kubernetes",,,,2025-03-28T02:07:51
79539724,Deploy .NET Aspire project in a local environment with no hypervisor support,"I am developing a .NET 9 project using the .NET Aspire App Host model in Visual Studio, which contains 2 Azure Function apps.

The project will be deployed on a Windows Server 2022 with access to the local network but not the internet. Azure cloud services will then not be available.

Since this server is itself a virtual machine, the nested virtualization will not, ever, be available on it, I won't be able to use a hypervisor.

The Aspire dashboard that comes with this project model would be perfect for my use case since it's an effective orchestration tool that does not require a hypervisor and allows to manage local API easily with lots of nice features. Sadly, it can't run yet in a deployed environment, and there is not much news from Microsoft about implementing it. *[https://github.com/dotnet/aspire/discussions/931](https://github.com/dotnet/aspire/discussions/931)*

To deploy this project as is, I will then need to use a Kubernetes cluster, which I am pretty new at. After some research, I found that since I can't run a hypervisor, I need to do a ""Bare Metal"" installation. But, everything I found is either for Linux or Mac and not really newcomers friendly, to say the least.

Here then are my two questions:

1. Has anyone succeeded in using the Aspire dashboard in a deployed environment without hacking everything?
2. If it is even possible to do so, is there somewhere a simple script or tutorial to install and deploy a bare metal Kubernetes cluster on Windows, where I would be able to deploy this project?

Thanks.","kubernetes, windows-server, bare-metal, .net-9.0, dotnet-aspire",,,,2025-03-27T18:18:53
79537887,Is it possible to fix expect script breaking the terminal paste after spawning kubectl exec and exiting?,"Consider the following expect script:

### Bad.exp

```
#!/opt/homebrew/bin/expect

set timeout -1;

set pod [lindex $argv 0]
spawn kubectl exec $pod -c envoy-container -it -- bash
expect ""bash""
send ""exit\n\r""
```

When this script terminates, if I try to paste a string into the terminal, a garbled string appears instead. I suspect something is broken with the line discipline.
**I can fix this by using `reset` in bash, but I have so far discovered no lighter-weight tricks to prevent or fix this behavior.**

Now consider the following other expect script:

### Good.exp

```
#!/opt/homebrew/bin/expect

set timeout -1;

set pod [lindex $argv 0]
spawn kubectl exec $pod -it -- bash
expect ""bash""
send ""exit\n\r""
```

This script is nearly identical, except that it does not specify a container and goes into a different container instead.

### Good2.exp

In the same container where we saw the bad behavior, the bad behavior does not happen if we exec `cat` instead of `bash`, so the problem seems specific to bash.

```
#!/opt/homebrew/bin/expect

set timeout -1;

set pod [lindex $argv 0]
spawn kubectl exec $pod -c envoy-container -it -- cat
send ""hello world""
expect ""hello world""
send \004
```

## Additional observations

- The container that `Bad.exp` enters has `bash 5.2`.
- The container that `Good.exp` enters has `bash 4.4`.
- The only difference in the output of `stty -a` before and after running `Bad.exp` is the `-pendin` flag, and manually resetting that flag afterwards does not fix the clipboard.
- Opening `vim` and then closing it again seems to also fix the terminal's paste.

What causes this behavior, and is there anything I can do to prevent or fix it other than using bash `reset`?","bash, macos, kubernetes, expect",79538083.0,"After reasoning about this from first principles, I had a hypothesis:

- expect is literally just ferrying bytes back and forth from its `tty` and my `tty`.
- bash 5.2 is turning on some terminal setting that is breaking paste by writing something to the tty.
- Under normal circumstances, when bash exits, it would undo whatever it was doing by itself.
- However, expect is terminating immediately after it sends ""exit"" and never reads bash's final bytes.
- Thus, we need to ensure that bash's final bytes are read and reflected in the local terminal.

### Fix 1: Read bash's final output which includes the cleanup

```
#!/opt/homebrew/bin/expect

set timeout -1;

set pod [lindex $argv 0]
spawn kubectl exec $pod -c envoy-container -it -- bash
expect ""bash""
send ""exit\n\r""
expect eof
```

### Fix 2: Do not shuffle the bytes from expect's tty to my tty.

```
#!/opt/homebrew/bin/expect

set timeout -1
log_user 0

set pod [lindex $argv 0]
spawn kubectl exec $pod -c sq-cloud-envoy -it -- bash
expect ""bash""
send ""exit\n\r""
```",2025-03-27T07:11:23,2025-03-27T05:25:38
79536604,Java options within Kubernetes container,"I am working with Java application and I’m going to deploy it within container.
I have prepared Dockerfile with

`ENTRYPOINT [""java"", ""-jar"", ""java_j.jar""]`

in my Java application.
I have prepared some helm charts too.

Is it possible to use only one variable to specify all Java options interested by me in it to use it within container.args (Deployment.yaml)?

{root}/values.yaml:

```
TEST_JAVA_OPTS = ""-XX:+UseSerialGC""
TEST_JAVA_MEMORY_OPTS = ""-Xmx256m -XX:MetaspaceSize=64m""
{root}/templates/Deployment.yaml
```

{root}/templates/Deployment.yaml

```
...
spec:
   containers:
      - name: test-java-service
        command:
           - java
           - '{{ .Values.TEST_JAVA_MEMORY_OPTS }}'
           - '{{ .Values.TEST_JAVA_OPTS }}'
           - -jar
           - java_j.jar
...
```

For now it doesn’t work to me because each my application startup failes with `Improperly specified VM option`. I guess it tries to give java entire string as one java option. That is wrong of course.
My purpose is to avoid a lot of variables for each java option and to let change it in Deployment directly (I know that there is a possibility to set environment variables in Dockerfile at ENTRYPOINT part but let assume this option is disabled for us)

Kubernetes version: 1.28.12","java, kubernetes, kubernetes-helm",79536877.0,"In your Helm chart, you need to split out the different low-level JVM settings into individual items in the `command:` list.  The easiest way to do this is to make the Helm-level settings be a list of options, and then you can iterate over it.

```
# values.yaml
jvmOptions:
  - -XX:UseSerialGC
  - -Xmx256m
  - -XX:MetaspaceSize=64m
```

```
# templates/deployments.yaml
         command:
           - java
{{- range .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

Since `.Values.jvmOptions` is a list here, the template `range` construct loops through it, setting `.` to each item in turn.  In the example here, I use the `toJson` extension function to ensure each item is properly quoted as a string that fits on a single line.

Nothing would stop you from having multiple lists of option settings that you combined this way.

If you really want the JVM options as a space-separated string, then you need to split that string into words.  There is a [`splitList`](https://masterminds.github.io/sprig/string_slice.html) extension function (not mentioned in the Helm documentation but it's there) that can do this.

```
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```
# templates/deployments.yaml
         command:
           - java
{{- range splitList "" "" .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

The template part looks almost identical except for adding `splitList` in.  Note that this is a fairly naïve splitting; there's not going to be any support for quoting or embedding spaces inside a single option or any non-space whitespace.

Finally: note that the standard JVMs do support passing options in environment variables; see for example [What is the difference between JDK_JAVA_OPTIONS and JAVA_TOOL_OPTIONS when using Java 11?](https://stackoverflow.com/questions/52986487/what-is-the-difference-between-jdk-java-options-and-java-tool-options-when-using)  You could just set this environment variable without trying to reconstruct `command:`.  (IME if you have a choice, managing Kubernetes manifests tends to be easier if you can set environment variables as opposed to using command-line options.)

```
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```
# templates/deployments.yaml
         env:
{{- with .Values.jvmOptions }}
           - name: JDK_JAVA_OPTIONS
             value: {{ toJson . }}
{{- end }}
```",2025-03-26T17:57:47,2025-03-26T13:39:18
79536604,Java options within Kubernetes container,"I am working with Java application and I’m going to deploy it within container.
I have prepared Dockerfile with

`ENTRYPOINT [""java"", ""-jar"", ""java_j.jar""]`

in my Java application.
I have prepared some helm charts too.

Is it possible to use only one variable to specify all Java options interested by me in it to use it within container.args (Deployment.yaml)?

{root}/values.yaml:

```
TEST_JAVA_OPTS = ""-XX:+UseSerialGC""
TEST_JAVA_MEMORY_OPTS = ""-Xmx256m -XX:MetaspaceSize=64m""
{root}/templates/Deployment.yaml
```

{root}/templates/Deployment.yaml

```
...
spec:
   containers:
      - name: test-java-service
        command:
           - java
           - '{{ .Values.TEST_JAVA_MEMORY_OPTS }}'
           - '{{ .Values.TEST_JAVA_OPTS }}'
           - -jar
           - java_j.jar
...
```

For now it doesn’t work to me because each my application startup failes with `Improperly specified VM option`. I guess it tries to give java entire string as one java option. That is wrong of course.
My purpose is to avoid a lot of variables for each java option and to let change it in Deployment directly (I know that there is a possibility to set environment variables in Dockerfile at ENTRYPOINT part but let assume this option is disabled for us)

Kubernetes version: 1.28.12","java, kubernetes, kubernetes-helm",79536853.0,"According to the [Kubernetes docs](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/), split the command array and the arguments array into `command` and `args` sections.

> When you create a Pod, you can define a command and arguments for the containers that run in the Pod. To define a command, include the `command` field in the configuration file. To define arguments for the command, include the `args` field in the configuration file. The command and arguments that you define cannot be changed after the Pod is created.
>
>
> The command and arguments that you define in the configuration file override the default command and arguments provided by the container image. If you define args, but do not define a command, the default command is used with your new arguments.

```
spec:
  containers:
    - name: test-java-service
      image: <your_image_name_here>
      command:
        - java
      args:
        - {{ .Values.TEST_JAVA_MEMORY_OPTS | quote }}
        - {{ .Values.TEST_JAVA_OPTS | quote }}
        - ""-jar""
        - java_j.jar
```

When Helm populates values, don't specify the quotes yourself, or else the values replacement string will be interpreted literally as that string.  Instead, pipe the Helm value to `quote`.  Place quotes around any value that could be interpreted specially in YAML, such as values with `-` characters, like your Java options.",2025-03-26T17:48:28,2025-03-26T13:39:18
79536513,Istio: X-Forwarded-For and X-Real-IP Headers Show Internal IP Instead of Client&#39;s Real IP,"I'm encountering an issue where the X-Forwarded-For and X-Real-IP headers are showing internal IP addresses instead of the client's real IP when traffic is routed through Istio.

Problem:
In the logs, the X-Forwarded-For and X-Real-IP headers contain internal IP addresses (e.g., 10.244.8.14 or 10.116.0.105), while in a similar setup with NGINX ingress, these headers correctly show the client's real IP (e.g., 15.186.175.412).

Context:
I'm using Istio as my service mesh, with the default sidecar-based proxy mode.

The traffic goes through Istio Ingress Gateway before reaching the backend services.

The issue is observed when traffic passes through Istio, but the X-Forwarded-For and X-Real-IP headers show the internal IP addresses of the proxies in the mesh.

What I have tried:
Increased xff_num_trusted_hops in the Istio EnvoyFilter configuration to account for multiple hops, but the issue persists.

Explicitly set the X-Real-IP and X-Forwarded-For headers in the VirtualService configuration using %DOWNSTREAM_REMOTE_ADDRESS%, but this did not resolve the problem.

I have also ensured that the use_remote_address: true option is enabled in the EnvoyFilter configuration.

Verified that NGINX ingress works correctly and forwards the client IP in the headers, while Istio does not.

Configuration:
I have applied the following settings in the EnvoyFilter for Istio:

```
configPatches:
  - applyTo: NETWORK_FILTER
    match:
      context: GATEWAY
      listener:
        filterChain:
          filter:
            name: ""envoy.filters.network.http_connection_manager""
    patch:
      operation: MERGE
      value:
        typed_config:
          ""@type"": ""type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager""
          use_remote_address: true
          xff_num_trusted_hops: 2
          skip_xff_append: false
```

Additionally, I've tried to explicitly set the X-Forwarded-For and X-Real-IP headers in the VirtualService configuration like so:

yaml

```
http:
  headers:
    request:
      set:
        X-Real-IP: ""%DOWNSTREAM_REMOTE_ADDRESS%""
        X-Forwarded-For: ""%DOWNSTREAM_REMOTE_ADDRESS%""
```

How can I ensure that Istio forwards the real client IP in the X-Forwarded-For and X-Real-IP headers, similar to how NGINX handles this?

Are there any other Istio configurations I may have missed to properly preserve the client IP?

Is there any additional setup needed to propagate the real client IP from the ingress gateway to the application services?

Any help or insights would be greatly appreciated!","kubernetes, istio, envoyproxy, istio-sidecar",79537431.0,"Your X-Forwarded-For and X-Real-IP headers are showing internal IP addresses instead of the client's real IP since the Kubernetes Service for your Istio Ingress Gateway is not configured with **externalTrafficPolicy: Local**. Ensure to set this properly to [preserve the real client IP](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip:%7E:text=Preserving%20the%20client,type%3A%20LoadBalancer) in the X-Forwarded-For header. You can also check this [discussion](https://github.com/istio/istio/issues/7679) for more information.",2025-03-26T22:44:24,2025-03-26T13:06:51
79536307,"How to expose ports in Minikube (Docker driver, Windows) without port-forward or minikube service?","Here’s your Stack Overflow question in English:

### How to expose ports in Minikube (Docker driver, Windows) without `port-forward` or `minikube service`?

I'm using Minikube with the Docker driver on Windows and want to expose ports for my services (React, ASP.NET API, MSSQL) **without manual commands** like `port-forward`, `tunnel`, `minikube service`, or `minikube start --ports`.

#### What I have:

- **Minikube (Docker driver) on Windows**
- **Applications running in Kubernetes:**
  - React (`NodePort 4200:30002`)
  - ASP.NET API (`NodePort 8084:30001`)
  - MSSQL (`NodePort 1433:30003`)
- **Requirement**: I want to expose ports **only through Kubernetes manifests** (Ingress, LoadBalancer, etc.), without running manual Minikube commands.

#### The problem:

According to the Kubernetes documentation:

> *""The network is limited if using the Docker driver on Darwin, Windows, or WSL, and the Node IP is not reachable directly.""*

This means I **cannot** simply rely on `NodePort` as I would on Linux.

#### The question:

How can I **automatically** expose ports in Minikube (Windows, Docker driver) using **only Kubernetes manifests**, without relying on `port-forward` or `minikube service`?

Additionally, how can I make this solution portable so that it works for both **development (Windows)** and **deployment (Linux)** environments?","docker, kubernetes, windows-subsystem-for-linux, kubernetes-ingress, minikube",79536765.0,"You can try exposing the service using a Kubernetes Service of type** NodePort** or **LoadBalancer**.

As per this GeeksforGeeks [document](https://www.geeksforgeeks.org/kubernetes-nodeport-service/).

NodePort service will expose the pods of one node to the other and also it will expose the pods to the outside of the cluster from where the users can access from the internet by using the IP address of node and port.

```
 apiVersion: v1
        kind: Service
        metadata:
          name: my-app-service
        spec:
          selector:
            app: my-app
          ports:
            - protocol: TCP
              port: 80
              targetPort: 8080
          type: LoadBalancer # or NodePort or Ingress
```

Also refer to this Minikube [Accessing apps](https://minikube.sigs.k8s.io/docs/handbook/accessing/) and official kubernetes document on [Set up Ingress on Minikube with the NGINX Ingress Controller](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/) for more information.",2025-03-26T17:12:38,2025-03-26T11:48:44
79535236,Hangfire Multiples Jobs Simultaneously,"I have a payment application that uses Hangfire to process some payments.

However, in my production environment, I currently have only one pod running the worker that processes payments. I want to increase the capacity by running three pods simultaneously, each processing the same job.

My goal is to distribute the payments across three pods, making the process faster.

The job is recurring, and when it’s time for execution, it should start running on all three pods simultaneously.

The issue:

Even though I have three pods in my test environment, the job only runs on one pod at a time. The first pod that starts the execution is the only one processing the job, and the other pods remain idle. The job only runs again on the next scheduled execution, but still only on one pod.

Question:

How can I make Hangfire execute the same job on all three pods simultaneously when the scheduled time arrives?

Any ideas or suggestions would be greatly appreciated!","c#, .net, kubernetes, hangfire",,,,2025-03-26T01:50:11
79533978,From which pod are requests sent to admission webhooks?,"We want to create an Ingress network policy for a given port on pod that receives admission webhooks from kubernetes. We do not want to allow any other pod on the cluster to connect to this port.

Therefore, we have to know from which pod the webhooks originate? Is it kube-apiserver?","kubernetes, kubernetes-networkpolicy, kubernetes-security",79572092.0,"Yes, it is the kubeapi server. Admission Webhooks are called by the kubeapi server if a resource was sent to for which the admission web hook was registered.

From the [docs](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#configure-admission-webhooks-on-the-fly):

> The webhook handles the AdmissionReview request **sent by the API servers**, and sends back its decision as an AdmissionReview object in the same version it received.

Note however that this can be configured pretty granularly, it follows the same Verb and Ressource syntax as RBAC.",2025-04-13T21:45:01,2025-03-25T14:26:05
79533768,How to fetch pod endpoints from a headless service in Kubernetes,"I have a [headless service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) in my cluster:

```
apiVersion: v1
kind: Service
metadata:
  namespace: my-namespace
  name: my-headless-service
spec:
  clusterIP: None # <- Headless service
  selector:
    service: my-app
  ports:
  - port: 52323
    targetPort: 52323
```

However I don't quite get how I'm supposed to extract the endpoints the headless service defines. I can see them in OpenLens:
[![endpoints in openlens](https://i.sstatic.net/z1WOnA35.png)](https://i.sstatic.net/z1WOnA35.png)

How can I get these endpoints from my C# application that's running in the cluster?","c#, kubernetes",79533769.0,"To quote the [Kubernetes documentation](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services):

> headless Services report the endpoint IP addresses of the individual pods via internal DNS records, served through the cluster's DNS service.

So you can perform a DNS lookup on your headless service's host name to resolve those ip's:

```
var ips = Dns.GetHostEntry(""my-headless-service.my-namespace"").AddressList;
```

This won't return the ports however. Not quite sure how to get those 🤔",2025-03-25T13:05:07,2025-03-25T13:05:07
79533004,Issue in injecting environment variables during runtime in Vite + React,"I have created a react app using vite, where i have integrated azure sso, currently during local development I am utilizing the environment variables (client ID, tenantID, redirectURI) from .env.local file, which is working fine, but when I deploy it to Kubernetes the values are not getting burned into the variable during production, during local development in my main.jsx I am referencing the .env value using
`auth: { clientId: import.meta.env.VITE_APP_CLIENT_ID, authority: import.meta.env.VITE_APP_AUTHORITY, redirectUri: import.meta.env.VITE_APP_REDIRECT_URI, }`

for deployment I have a Kubernetes folder which contains dev folder (config-map.yml, micro-app.yml) containing the deployment code, in config-map.yml I have set the value of clientId,authority, redirectUri similar to .env file and the values are getting inserted into container, pods but not getting injected into the application's minified js, from R&D I understood that there is an issue with runtime injection.

any suggestions/method to solve this issue?","reactjs, kubernetes, deployment, vite",79534423.0,"Environment variables in files like .env.local (or injected via ConfigMaps) are only available at build time, not at runtime.

So even though your Kubernetes ConfigMap is correctly injecting the values into the container, your Vite app already had the environment variables ""burned in"" during the build step, which is why changes don’t reflect in the deployed JavaScript.

It happens because Vite replaces all import.meta.env.* values at build time.

To fix it:

Use a separate config.json (or similar) file that the app fetches at runtime, so you can inject values dynamically in Kubernetes.

1. Add a public/config.json file (empty placeholder for local dev):

`{ ""VITE_APP_CLIENT_ID"": ""local-client-id"", ""VITE_APP_AUTHORITY"": ""local-authority"",""VITE_APP_REDIRECT_URI"": ""http://localhost:3000""}`

1. In your app (e.g., main.jsx or a config loader):

const config = await fetch('/config.json').then(res => res.json());

```
const msalConfig = {
  auth: {
    clientId: config.VITE_APP_CLIENT_ID,
    authority: config.VITE_APP_AUTHORITY,
    redirectUri: config.VITE_APP_REDIRECT_URI,
  }
};
```

1. In your Kubernetes deployment:
Mount your dynamic config as a ConfigMap and inject it to /usr/share/nginx/html/config.json (or wherever your app is served).

```
volumeMounts:
  - name: config-volume
    mountPath: /usr/share/nginx/html/config.json
    subPath: config.json

volumes:
  - name: config-volume
    configMap:
      name: my-configmap
```",2025-03-25T17:23:19,2025-03-25T08:03:22
79532811,Issues with istio-proxy to apps communication while using STRICT peerauthentication with MUTUAL_TLS destinationrule,"I am using STRICT peerauthetication model for ALL Pods in my default namespace.
Also, ISTIO_MUTUAL destinationRule is ON for default namespace pods.

When peerauthentication is PERMISSIVE, curl from host VM to zookeeper is:

user@host:~$ curl -k -v -L 10.103.19.172:2181

- Trying 10.103.19.172:2181...
- TCP_NODELAY set
- Connected to 10.103.19.172 (10.103.19.172) port 2181 (#0)

> GET / HTTP/1.1
> Host: 10.103.19.172:2181
> User-Agent: curl/7.68.0
> Accept: */*

- Empty reply from server
- Connection #0 to host 10.103.19.172 left intact
curl: (52) Empty reply from server
user@host:~$

(Same happens fro kafka to zookeeper)- all good.

When it is STRICT:

user@host:~$ curl -k -v -L 10.103.19.172:2181

- Trying 10.103.19.172:2181...
- TCP_NODELAY set
- Connected to 10.103.19.172 (10.103.19.172) port 2181 (#0)

> GET / HTTP/1.1
> Host: 10.103.19.172:2181
> User-Agent: curl/7.68.0
> Accept: */*

- Recv failure: Connection reset by peer
- Closing connection 0
curl: (56) Recv failure: Connection reset by peer
user@host:~$","kubernetes, curl, istio, istio-sidecar, istio-kiali",79541769.0,"Fixed it. See the question I posted at [https://istio.slack.com/archives/C37A4KAAD/p1743157328271189](https://istio.slack.com/archives/C37A4KAAD/p1743157328271189)

--set values.pilot.env.ENABLE_NATIVE_SIDECARS=true  fixes it.

SO my istioctl command was:

```
istioctl install \
  --set meshConfig.defaultConfig.holdApplicationUntilProxyStarts=true \
  --set meshConfig.defaultConfig.proxyMetadata.REWRITE_PROBE_LEGACY_LOCALHOST_DESTINATION=true \
  --set meshConfig.defaultConfig.proxyMetadata.ISTIO_META_REWRITE_APP_HTTP_PROBERS=true \
  --set components.cni.enabled=true \
  --set values.pilot.env.ENABLE_NATIVE_SIDECARS=true \
  --set values.pilot.env.ENABLE_TLS_ON_SIDECAR_INGRESS=true
```",2025-03-28T14:57:35,2025-03-25T06:07:04
79532811,Issues with istio-proxy to apps communication while using STRICT peerauthentication with MUTUAL_TLS destinationrule,"I am using STRICT peerauthetication model for ALL Pods in my default namespace.
Also, ISTIO_MUTUAL destinationRule is ON for default namespace pods.

When peerauthentication is PERMISSIVE, curl from host VM to zookeeper is:

user@host:~$ curl -k -v -L 10.103.19.172:2181

- Trying 10.103.19.172:2181...
- TCP_NODELAY set
- Connected to 10.103.19.172 (10.103.19.172) port 2181 (#0)

> GET / HTTP/1.1
> Host: 10.103.19.172:2181
> User-Agent: curl/7.68.0
> Accept: */*

- Empty reply from server
- Connection #0 to host 10.103.19.172 left intact
curl: (52) Empty reply from server
user@host:~$

(Same happens fro kafka to zookeeper)- all good.

When it is STRICT:

user@host:~$ curl -k -v -L 10.103.19.172:2181

- Trying 10.103.19.172:2181...
- TCP_NODELAY set
- Connected to 10.103.19.172 (10.103.19.172) port 2181 (#0)

> GET / HTTP/1.1
> Host: 10.103.19.172:2181
> User-Agent: curl/7.68.0
> Accept: */*

- Recv failure: Connection reset by peer
- Closing connection 0
curl: (56) Recv failure: Connection reset by peer
user@host:~$","kubernetes, curl, istio, istio-sidecar, istio-kiali",79533211.0,"If you are seeing ""connection reset by peer"" errors, it could be due to the **client not having the correct mTLS certificates** or the server not expecting mTLS traffic.

If the zookeeper is the external service , you should configure **Istio** to allow the **non-mTLS traffic for that specific service**. And you can do this by creating an Istio PeerAuthentication and DestinationRule that disables mTLS for traffic between the host VM and zookeeper. Refer to this [documentation](https://istio.io/latest/docs/reference/config/security/peer_authentication/) for more information on this.

Also you will need to create a [client certificate](https://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/#generate-client-and-server-certificates-and-keys) for the host VM to authenticate itself with Istio’s mTLS. You can use Istio’s certificate authority to generate client certificates for your VM and configure curl to use that certificate.

You can verify your mTLS status between host VM and zookeeper as mentioned below,

```
Istiocl authn tls-check
<zookeeper-service-name>.<namespace>.svc.cluster.local
```

Refer to this [documentation](https://istio.io/latest/docs/tasks/security/authentication/authn-policy/#setup) to know more about how to configure and how to use Istio authentication policies.",2025-03-25T09:29:26,2025-03-25T06:07:04
79530566,MyBatis PersistenceException: Communication link failure with DB on VM intermittently while using services on Kubernetes,"*nested exception is org.apache.ibatis.exceptions.PersistenceException: \n### Error querying database.  Cause: org.springframework.jdbc.CannotGetJdbcConnectionException: Failed to obtain JDBC Connection; nested exception is com.mysql.cj.jdbc.exceptions.CommunicationsException: Communications link failure*

I am facing an intermittent issue where MyBatis throws a PersistenceException: Communication link failure while trying to communicate with a database running on a VM. My services are running on a Kubernetes cluster, and the database is hosted on a separate VM.
The issue does not occur consistently; most of the time the connection is established without any problems, while at other times, it fails with the above exception.

**Things I've tried:**

I have verified that the database is up and running on the VM.
also checked network configurations between the Kubernetes cluster and the VM, and I can ping the VM from within the cluster. Removed resource limits from my deployments to ensure that our services are not being throttled by the Kubernetes cluster.

Along with above I have Updated HikariCP configuration with the following properties:

```
spring.datasource.hikari.connection-timeout=30000
spring.datasource.hikari.maximum-pool-size=20
spring.datasource.hikari.idle-timeout=600000
```

Could the intermittent communication issue between my Kubernetes cluster and the database on the VM be related to the networking configuration, DNS resolution problems, or network timeouts? Are there any specific MyBatis or JDBC settings I should check to resolve this issue? How can I further troubleshoot the communication failure between the Kubernetes services and the VM-hosted database?

Any help or insights would be greatly appreciated!","mysql, spring-boot, kubernetes, jdbc, mybatis",,,,2025-03-24T08:38:52
79528439,How to start a container in a kubernetes pod only after its proxy is running?,"I have a kubernetes cluster and a PostgreSQL Database running on Google Cloud.

The pod that has the problem is a cronjob with the following configuration:

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: taxiq-cronjob-reminder
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
    cloud.google.com/backend-config: '{""default"": ""taxiq-healthconfig""}'
spec:
  schedule: ""31 4 * * *""
  timeZone: ""Europe/Berlin""
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: taxiq-stage-gke
          initContainers:
            - image:  gcr.io/google.com/cloudsdktool/cloud-sdk:326.0.0-alpine
              name: workload-identity-initcontainer
              command:
              - '/bin/bash'
              - '-c'
              - ""curl -s -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1""
          containers:
          - name: cloud-sql-proxy
            image: gcr.io/cloudsql-docker/gce-proxy:1.33.5
            command:
              - ""/cloud_sql_proxy""
              - ""-instances=taxiq-stage-app:europe-west3:taxiq-stage=tcp:5432""
            securityContext:
              runAsNonRoot: true
          - name: taxiq-cronjob-reminder
            image: europe-west3-docker.pkg.dev/brantpoint-artifacts/taxiq/cronjob-reminder:beta09.7
            env:
              - name: PGURL
                valueFrom:
                  secretKeyRef:
                    name: secrets
                    key: PGURL
          restartPolicy: Never
```

I expect the proxy to start as well as the cronjob to run without errors.

But I get the following error:

```
failed to connect to `host=127.0.0.1 user=masterchief database=tasks_buildingblock`: dial error (dial tcp 127.0.0.1:5432: connect: connection refused)
```

I know this error is related to the cloud-sql-proxy not running (yet). I had removed it with `restartPolicy: OnFailure`.

But I can not use this restartPolicy value for the following reason:

The cronjob is supposed to send mails -> if I get a Failure for any other reason after some mails have already been sent, the cronjob will run again and send the mails multiple times, which might make customers/users unhappy

How can I ensure the cloud-sql-proxy is listening before the cronjob starts?","kubernetes, google-kubernetes-engine, kubernetes-cronjob, cloud-sql-proxy",79536576.0,"A mentioned in one of the comments, deploying the Cloud SQL Proxy using `initContainers` is the solution to start the Proxy as a [native sidecar](https://kubernetes.io/blog/2023/08/25/native-sidecar-containers/).

Also worth pointing out that you are using the old v1 Cloud SQL Proxy. It is recommended to [migrate to the new v2 Cloud SQL Proxy](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/main/migration-guide.md) to leverage both performance and reliability benefits.

There are examples of using the v2 Cloud SQL Proxy as a sidecar [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/tree/main/examples/k8s-sidecar) and example health check usage [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/b6ca9c52ca41dfd0ceaf5dce104a533410e6dfe0/examples/k8s-health-check/proxy_with_http_health_check.yaml#L129).

Your sample updated would look like the following:

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: taxiq-cronjob-reminder
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
    cloud.google.com/backend-config: '{""default"": ""taxiq-healthconfig""}'
spec:
  schedule: ""31 4 * * *""
  timeZone: ""Europe/Berlin""
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: taxiq-stage-gke
          initContainers:
            - image:  gcr.io/google.com/cloudsdktool/cloud-sdk:326.0.0-alpine
              name: workload-identity-initcontainer
              command:
              - '/bin/bash'
              - '-c'
              - ""curl -s -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1""
            - name: cloud-sql-proxy
              # v2 Cloud SQL Proxy image
              image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.15.2
              restartPolicy: Always
              env:
                - name: CSQL_PROXY_HEALTH_CHECK
                  value: ""true""
                - name: CSQL_PROXY_HTTP_PORT
                  value: ""9801""
                - name: CSQL_PROXY_HTTP_ADDRESS
                  value: 0.0.0.0
              startupProbe:
                failureThreshold: 60
                httpGet:
                  path: /startup
                  port: 9801
                  scheme: HTTP
                periodSeconds: 1
                successThreshold: 1
                timeoutSeconds: 10
              args:
                - ""--port=5432""
                - ""taxiq-stage-app:europe-west3:taxiq-stage""
              securityContext:
                runAsNonRoot: true
          containers:
          - name: taxiq-cronjob-reminder
            image: europe-west3-docker.pkg.dev/brantpoint-artifacts/taxiq/cronjob-reminder:beta09.7
            env:
              - name: PGURL
                valueFrom:
                  secretKeyRef:
                    name: secrets
                    key: PGURL
          restartPolicy: Never
```",2025-03-26T13:30:43,2025-03-23T04:17:51
79526846,NullPointerException When Configuring Hadoop to Use JCEKS for Azure Data Lake Storage Authentication,"I'm integrating Azure Data Lake Storage (ADLS) as a destination in our on-premises application running on Kubernetes. To authenticate, I'm using a Java KeyStore (JCEKS) file containing our Azure Service Principal (SPN) credentials. Despite configuring everything as per the documentation, I'm encountering a `NullPointerException` related to the authentication endpoint.

**Current Setup:**

Proxy Configuration: Proxy chains are set up, and connectivity to Azure has been verified.​

JCEKS File: The JCEKS file includes the following entries:

- fs.azure.account.oauth2.client.endpoint
- fs.azure.account.oauth2.client.id
- fs.azure.account.oauth2.client.secret

Hadoop Configuration: The path to the JCEKS file is specified in the `hadoop.security.credential.provider.path` property.

**Issue Encountered:**

When attempting to connect to ADLS, the application throws a `NullPointerException`, indicating that it cannot resolve the `authEndpoint`. This suggests that the authentication endpoint value isn't being retrieved correctly from the JCEKS file.

**Troubleshooting Steps Taken:**

1. **Verified JCEKS Content:** Ensured that the JCEKS file contains the correct aliases and values for the SPN credentials.
2. **Checked File Accessibility:** Confirmed that the application has read access to the JCEKS file and that the file path is correct.
3. **Hadoop Configuration:** Ensured that the following properties are set:

- `fs.azure.account.auth.type` = `OAuth`
- `fs.azure.account.oauth.provider.type` = `org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider`

1. **Credential Retrieval:** Used the Hadoop Credential Shell to list and retrieve credentials from the JCEKS file, which worked as expected.
2. **Logging:** Enabled detailed logging for the Hadoop Azure module but didn't find additional insights.

Moreover, for more in-depth reference, I checked the Hadoop codebase where the exception is thrown in this line: [https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java#L1146C15-L1146C82](https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AbfsConfiguration.java#L1146C15-L1146C82) while fetching the `authEndpoint` in the below class:

```
  public AccessTokenProvider getTokenProvider() throws TokenAccessProviderException {
```

The `getMandatoryPasswordString` method is returning `null` for `FS_AZURE_ACCOUNT_OAUTH_CLIENT_ENDPOINT`, leading to the `NullPointerException`. This is because it could not derive the client endpoint from my application.

Has anyone encountered a similar issue where Hadoop fails to retrieve SPN credentials from a JCEKS file, resulting in a `NullPointerException`? Any insights into potential misconfigurations or overlooked steps would be greatly appreciated.

**NOTE:** `Hadoop version`: `3.3.2`","apache-spark, kubernetes, hadoop, azure-data-lake, jceks",,,,2025-03-22T02:02:17
79526694,Airflow on Kubernetes with KubernetesExecutor only running one pod at a time,"I am running airflow on kubernetes with a `Chart.yaml` file:

```
apiVersion: v2
name: airflow
description: Umbrella chart for Airflow
type: application
version: 0.0.1
appVersion: ""2.1.2""
dependencies:
  - name: airflow
    alias: airflow
    version: 8.9.0
    repository: https://airflow-helm.github.io/charts
```

and a `values.yaml` file:

```
airflow:
  airflow:
    legacyCommands: false
    image:
      repository: apache/airflow
      tag: 2.8.4-python3.9
    executor: KubernetesExecutor
    fernetKey: ""7T512UXSSmBOkpWimFHIVb8jK6lfmSAvx4mO6Arehnc""
    webserverSecretKey: ""THIS IS UNSAFE!""
    config:
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ""True""
      AIRFLOW__CORE__LOAD_EXAMPLES: ""True""
    users:
      - username: admin
        password: admin
        role: Admin
        email: tom.mclean@myemail.com
        firstName: admin
        lastName: admin
    connections: []
    variables: []
    pools: []
    extraPipPackages: []
    extraEnv: []
    extraVolumeMounts: []
    extraVolumes: []
    kubernetesPodTemplate:
      stringOverride: """"
      resources: {}
      extraPipPackages: []
      extraVolumeMounts: []
      extraVolumes: []
  scheduler:
    replicas: 1
    resources: {}
    logCleanup:
      enabled: true
      retentionMinutes: 21600
    livenessProbe:
      enabled: true
    taskCreationCheck:
      enabled: false
      thresholdSeconds: 300
      schedulerAgeBeforeCheck: 180
  web:
    replicas: 1
    resources: {}
    service:
      type: ClusterIP
      externalPort: 8080
    webserverConfig:
      stringOverride: |
        from airflow import configuration as conf
        from flask_appbuilder.security.manager import AUTH_DB

        # the SQLAlchemy connection string
        SQLALCHEMY_DATABASE_URI = conf.get(""core"", ""SQL_ALCHEMY_CONN"")

        # use embedded DB for auth
        AUTH_TYPE = AUTH_DB
      existingSecret: """"

  workers:
    enabled: false

  triggerer:
    enabled: true
    replicas: 1
    resources: {}
    capacity: 1000

  flower:
    enabled: false

  logs:
    path: /opt/airflow/logs
    persistence:
      enabled: false

  dags:
    path: /opt/airflow/dags
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: ""https://tom.mclean:mypassword@dev.azure.com/MyOrg/MyOrg/_git/Airflow""
      branch: ""main""
      revision: ""HEAD""
      syncWait: 60
      depth: 1
      repoSubPath: ""dags""
      cloneDepth: 1

      httpSecret: ""airflow-http-git-secret""
      httpSecretUsernameKey: username
      httpSecretPasswordKey: password

  ingress:
    enabled: true

    web:
      host: airflow.mydomain.com
      annotations:
        kubernetes.io/ingress.class: alb
        alb.ingress.kubernetes.io/group.name: grafana
        alb.ingress.kubernetes.io/listen-ports: '[{""HTTP"": 80}, {""HTTPS"":443}]'
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/ssl-redirect: '443'
        alb.ingress.kubernetes.io/target-type: ip

  serviceAccount:
    create: true
    name: """"
    annotations: {}

  extraManifests: []

  pgbouncer:
    enabled: true
    resources: {}
    authType: md5

  postgresql:
    enabled: true
    persistence:
      enabled: true
      storagClass: """"
      size: 8Gi

  externalDatabase:
    type: postgres

  redis:
    enabled: false

  externalRedis:
    host: localhost
```

I then tried to run a job which had parallel tasks:

```
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import random
import time

def heavy_computation(task_number):
    """"""Simulates a computationally heavy task.""""""
    sleep_time = random.uniform(0, 1)  # Simulate varying computation times
    time.sleep(sleep_time)
    print(f""Task {task_number} completed after {sleep_time:.2f} seconds"")

# Define default args
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 3, 21),
    'retries': 0,
}

# Define the DAG
with DAG(
    'parallel_computation_dag',
    default_args=default_args,
    schedule_interval=None,  # Manual trigger
    catchup=False,
    max_active_tasks=10,  # Allow multiple tasks to run in parallel
) as dag:

    tasks = [
        PythonOperator(
            task_id=f'heavy_task_{i}',
            python_callable=heavy_computation,
            op_kwargs={'task_number': i},
        ) for i in range(20)  # Creates 20 parallel tasks
    ]
```

However, only a single pod would run at a time, so the jobs would not run in parallel. Is there a way to change the config to allow multiple pods to run at the same time?

Thanks.","kubernetes, airflow, kubernetes-helm",79530847.0,"I suppose you use this [helm chart](https://airflow.apache.org/docs/helm-chart/stable/index.html)?
If so have a look at the [Parameter Reference](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html).
There are two specific values that set the replica count:

- [scheduler.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#scheduler): Airflow 2.0 allows users to run multiple schedulers. This feature is only recommended for MySQL 8+ and PostgreSQL
- [workers.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#workers): Number of Airflow Celery workers in StatefulSet.

As you want to run jobs in parallel, enabling workers and setting `replica` to >=2 deploys two workers so jobs can run in parallel.

Updated `values.yaml`:

```
airflow:
  workers:
    enabled: true
    replica: 2
```",2025-03-24T10:43:27,2025-03-21T22:48:05
79526292,What can cause Kubernetes tolerations to spontaneously change?,"I have a wavefront-proxy deployment running in the observability-system namespace. I run the following command to add tolerations:

```
kubectl patch deployment wavefront-proxy -n observability-system --type='json' -p='[{
  ""op"": ""add"",
  ""path"": ""/spec/template/spec/tolerations"",
  ""value"": [{
    ""key"": ""key1"",
    ""operator"": ""Equal"",
    ""value"": ""value1"",
    ""effect"": ""NoSchedule""
  }]
}]'
```

If I run

`kubectl edit deployment wavefront-proxy -n observability-system`

right away, I see in the yaml,

```
tolerations:
- effect: NoSchedule
  key: key1
  operator: Equal
  value: value1
```

As expected, but if I was several seconds and repeat the kubectl edit command, I see that my tolerations get overwritten with:

```
tolerations:
- effect: NoSchedule
  key: kubernetes.io/arch
  value: arm64
```

Is there anything that is part of the Kubernetes ecosystem that would spontaneously overwrite tolerations that I set?",kubernetes,,,,2025-03-21T18:04:51
79525781,How to change replica set in Helm release?,"GoCD default Helm creates on server and 0 agents.

```
helm show values gocd/gocd
```

shows

```
  # agent.replicaCount is the GoCD Agent replicas Count. Specify the number of GoCD agents to run
  replicaCount: 0
```

I created a new release

```
 helm upgrade --install gocd gocd/gocd -n gocd --set replicaCount=1
Release ""gocd"" has been upgraded. Happy Helming!
NAME: gocd
LAST DEPLOYED: Fri Mar 21 14:30:47 2025
NAMESPACE: gocd
STATUS: deployed
REVISION: 2
```

The Cluster does not have agent pods.

```
gocd-agent-65979fd7dc               0         0         0       9m48s
gocd-server-7b58d6ccd4              1         1         1       9m48s
```

Why did a new realese not resolve this issue?","kubernetes, kubernetes-helm",79526021.0,"I'm assuming you're using [this](https://github.com/gocd/helm-chart) helm chart to install GoCD.

From the [templates](http://github.com/gocd/helm-chart/blob/master/gocd/templates/gocd-agent-controller.yaml#L24) and [values file](https://github.com/gocd/helm-chart/blob/master/gocd/values.yaml#L298), we can see that the value to set the replica count of agent pods falls under `agent.replicaCount` variable.

Changing your helm command from

`helm upgrade --install gocd gocd/gocd -n gocd --set replicaCount=1`

to

`helm upgrade --install gocd gocd/gocd -n gocd --set agent.replicaCount=1`

should make the agent replica becomes 1.",2025-03-21T16:12:16,2025-03-21T14:41:06
79525423,Running the command - argocd cluster rotate-auth is throwing &quot;resource name may not be empty&quot; error,"```
$ argocd cluster list

SERVER                          NAME         VERSION  STATUS
https://kubernetes.default.svc  dev-k8s   v1.23.6  Successful
https://X.X.X.X:6443       gke-cluster  v1.22.8  Successful
```

Then I ran as below

```
$argocd cluster rotate-auth https://kubernetes.default.svc
or
$ argocd cluster rotate-auth dev-k8s

FATA[0001] rpc error: code = Unknown desc = resource name may not be empty
```

Can someone please shed some light here on what I am missing and share a possible solution?
Let me know if any further details are required.","kubernetes, continuous-deployment, argocd, gitops",,,,2025-03-21T12:25:06
79525377,Automatically Update MinIO Tenant Policies When ConfigMap Changes,"[**MinIO K8S Operator**](https://github.com/minio/operator)

I have defined a K8S manifest to deploy a MinIO tenant and a MinIOJob manifest that applies policies stored in a ConfigMap to the tenant. The policies (e.g. cdn-ro.json) are stored in a ConfigMap.

I want to automatically trigger an update of the tenant’s policies whenever the ConfigMap containing the policies is modified (e.g., via kubectl apply, tk apply or tools like ArgoCD).

**Current Setup:**

ConfigMap : Stores policy files (e.g., policies-configmap.yaml).

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: tenant-policies
data:
  cdn-ro.json: |
    { ""Version"": ""2012-10-17"", ... }
```

MinIOJob : Applies policies from the ConfigMap to the tenant (e.g., minio-policy-job.yaml).

```
apiVersion: job.min.io/v1alpha1
kind: MinIOJob
metadata:
  name: update-policies
spec:
  tenant:
    name: my-tenant
    namespace: my-namespace
  commands:
    - name: apply-cdn-ro
      command: [""mc"", ""admin"", ""policy"", ""set"", ""my-tenant"", ""cdn-ro"", ""/etc/policies/cdn-ro.json""]
      volumeMounts:
        - name: policies
          mountPath: /etc/policies
  volumes:
    - name: policies
      configMap:
        name: tenant-policies
```

**Question:**
What is the best way to automatically trigger the MinIOJob whenever the tenant-policies ConfigMap is updated?

**Constraints/Requirements:**

1. No manual intervention: The update should occur automatically after the ConfigMap changes (e.g., via kubectl apply).
2. Idempotency: The job should only run if the ConfigMap content has changed.
3. Compatibility: Works with standard Kubernetes tools (no custom controllers unless necessary).","kubernetes, minio",,,,2025-03-21T12:03:14
79525274,AWS EKS Auto Mode Cluster Gateway API,"I have deployed an EKS cluster with Auto Mode, all good. Now I want to use kubernetes Gateway API, which requires to install [`gateway-api-controller`](https://www.gateway-api-controller.eks.aws.dev/dev/guides/deploy/). I have follow the steps as in the documentation to found that the controller pod isn't able to pull instance data automatically from `IMDS`: it complains that is unable to detect the vpc id, cluster name and so on. After digging over the [internet](https://stackoverflow.com/questions/71884350/using-imds-v2-with-token-inside-docker-on-ec2-or-ecs) I have found that this is likely to a metadata configuration `http-put-response-hop-limit` being equal to 1 whereas we need it to be 2 in order to access `IMDS` from the pods.

Have anyone managed to make Gateway Api to work in an EKS cluster deployed with Auto Mode?","kubernetes, amazon-eks",,,,2025-03-21T11:08:58
79525027,CrashLoopBackOff error while running kubernet in starling,"In my project i have DockerFile and shiny-start.sh file. While deploying it in starling i am getting this error along with failed to fetch image getting pulled from regcred.

Please help me resolve this error

This is my DockerFile code

```
FROM ....

ENV .....
ENV .....
ENV no_proxy ""localhost, ....""
LABEL .....
LABEL .....
LABEL .....
COPY ./app
COPY renv.lock./app/renv.lock
WORKDIR /app
RUN apt update
RUN apt install gcc
RUN Rscript -e 'install.packages(""renv"")'
RUN Rscript -e 'renv::consent (provided = TRUE)'
RUN Rscript -e 'renv::restore()'
RUN chmod a+x ./shiny-start.sh
RUN chmod 777 /etc/ssl/private/apache.key
ENV APACHE_PID_FILE /var/run/apache2/apache2.pid
RUN echo ""export APACHE_PID_FILE=/var/run/apache2/apache2.pid"" >> /etc/apache2/envvars
ENTRYPOINT./shiny-start.sh
```

This is shiny-status.sh file code

```
#!/bin/bash
cd /app
# Start Apache Server
service apache2 restart
status=$?
if [ $status -ne 0 ]; then
  echo ""Failed to start Apache Server: $status""
  exit $status
fi

# Run Shiny Server
R-e ""shiny:: runApp('/app', host = '0 '0.0.0.0', port = 3838)""3
status=$?
if [ $status -ne 0 ]; then
  echo ""Failed to start Shiny Server: $status""
  exit $status
fi
```","docker, kubernetes, shiny, starling-framework",,,,2025-03-21T09:31:33
79523896,Apache Spark operator K8s ingress,"Using Apache Spark [operator](https://github.com/apache/spark-kubernetes-operator) I am attempting to add an nginx ingress to expose the following ports: 8080, 7077, 6066 and 15002

I am attempting to use the following ingress Yaml, which is not correct.
Anyone has a working nginx ingress yaml for Apache Spark master? Thx

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: spark-ingress
  namespace: operator
  annotations:
    kubernetes.io/ingress.class: ""nginx""
    nginx.ingress.kubernetes.io/add-base-url: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - host: localhost
    http:
      paths:
        - pathType: Prefix
          path: /web
          backend:
            service:
              name: qa-master-svc
              port:
                number: 8080
        - pathType: Prefix
          path: /master
          backend:
            service:
              name: qa-master-svc
              port:
                number: 7077
        - pathType: Prefix
          path: /api
          backend:
            service:
              name: qa-master-svc
              port:
                number: 6066
```","apache-spark, kubernetes",,,,2025-03-20T19:14:19
79523591,OPA Gatekeeper doesn&#39;t create CRD in Github Actions pipeline,"I'm trying to use Open Policy Agent gatekeeper in a github actions pipeline like so:

```
name: OPA Gatekeeper

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest-medium

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Kubernetes
        uses: engineerd/setup-kind@v0.6.2

      - name: Install kubectl
        uses: azure/setup-kubectl@v4

      - name: Install Gatekeeper with kubectl
        run: |
          kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user admin
          kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/v3.18.2/deploy/gatekeeper.yaml

      - name: Apply constraint template
        run: |
          sleep 30
          kubectl apply -f gatekeeper/constraint_template.yml

      - name: Apply Constraint
        run: |
          kubectl apply -f gatekeeper/constraint.yml

      - name: Run Ingress
        run: ./kubectl apply -f gatekeeper/ingress.yml
```

The constraint_template.yml file it uses is this:

```
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: hostvalidation
spec:
  crd:
    spec:
      names:
        kind: HostValidation
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
          package hostvalidation
           violation[{""msg"": msg}] {
             input.review.object.kind == ""Ingress""
             host := input.review.object.spec.rules[_].host
             not endswith(host, "".xp.com"")
             msg := sprintf(""Ingress host %s does not end with 'xp.com'"", [host])
           }
```

And the constraint.yml is this:

```
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: HostValidation
metadata:
  name:  hostvalidation
spec:
  match:
    kinds:
      - apiGroups: [""networking.k8s.io""]
        kinds: [""Ingress""]
```

All of this works locally.  However in the pipeline the `hostvalidation` crd never gets created.  We've tried numerous ways of creating it but nothing works.

Is there a known issue with using constraint templates in a pipeline?

Thanks","kubernetes, github-actions, opa, opa-gatekeeper",,,,2025-03-20T16:44:05
79523213,NGINX reload triggered due to a change in configuration,"System :

- Ubuntu-24.04-noble-amd64

K8s :

- Client Version: v1.31.2
- Kustomize Version: v5.4.2
- Server Version: v1.31.7

ingress-nginx

- install with ""registry.k8s.io/ingress-nginx/controller:v1.12.0""
- version 1.12.0

My Ingress-nginx-controller always reload :
""NGINX reload triggered due to a change in configuration""

The current deployment file :

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
  uid: 7af65198-0f4a-4b53-9d48-b2c6f37ccddb
  resourceVersion: '195468'
  generation: 33
  creationTimestamp: '2025-03-19T10:12:31Z'
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.0
    k8slens-edit-resource-version: v1
  annotations:
    deployment.kubernetes.io/revision: '33'
    kubectl.kubernetes.io/last-applied-configuration: >
      {""apiVersion"":""apps/v1"",""kind"":""Deployment"",""metadata"":{""annotations"":{},""labels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx"",""app.kubernetes.io/part-of"":""ingress-nginx"",""app.kubernetes.io/version"":""1.12.0""},""name"":""ingress-nginx-controller"",""namespace"":""ingress-nginx""},""spec"":{""minReadySeconds"":0,""revisionHistoryLimit"":10,""selector"":{""matchLabels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx""}},""strategy"":{""rollingUpdate"":{""maxUnavailable"":1},""type"":""RollingUpdate""},""template"":{""metadata"":{""labels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx"",""app.kubernetes.io/part-of"":""ingress-nginx"",""app.kubernetes.io/version"":""1.12.0""}},""spec"":{""containers"":[{""args"":[""/nginx-ingress-controller"",""--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller"",""--election-id=ingress-nginx-leader"",""--controller-class=k8s.io/ingress-nginx"",""--ingress-class=nginx"",""--configmap=$(POD_NAMESPACE)/ingress-nginx-controller"",""--validating-webhook=:8443"",""--validating-webhook-certificate=/usr/local/certificates/cert"",""--validating-webhook-key=/usr/local/certificates/key""],""env"":[{""name"":""POD_NAME"",""valueFrom"":{""fieldRef"":{""fieldPath"":""metadata.name""}}},{""name"":""POD_NAMESPACE"",""valueFrom"":{""fieldRef"":{""fieldPath"":""metadata.namespace""}}},{""name"":""LD_PRELOAD"",""value"":""/usr/local/lib/libmimalloc.so""}],""image"":""registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa"",""imagePullPolicy"":""IfNotPresent"",""lifecycle"":{""preStop"":{""exec"":{""command"":[""/wait-shutdown""]}}},""livenessProbe"":{""failureThreshold"":5,""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""periodSeconds"":10,""successThreshold"":1,""timeoutSeconds"":1},""name"":""controller"",""ports"":[{""containerPort"":80,""name"":""http"",""protocol"":""TCP""},{""containerPort"":443,""name"":""https"",""protocol"":""TCP""},{""containerPort"":8443,""name"":""webhook"",""protocol"":""TCP""}],""readinessProbe"":{""failureThreshold"":3,""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""periodSeconds"":10,""successThreshold"":1,""timeoutSeconds"":1},""resources"":{""requests"":{""cpu"":""100m"",""memory"":""90Mi""}},""securityContext"":{""allowPrivilegeEscalation"":false,""capabilities"":{""add"":[""NET_BIND_SERVICE""],""drop"":[""ALL""]},""readOnlyRootFilesystem"":false,""runAsGroup"":82,""runAsNonRoot"":true,""runAsUser"":101,""seccompProfile"":{""type"":""RuntimeDefault""}},""volumeMounts"":[{""mountPath"":""/usr/local/certificates/"",""name"":""webhook-cert"",""readOnly"":true}]}],""dnsPolicy"":""ClusterFirst"",""nodeSelector"":{""kubernetes.io/os"":""linux""},""serviceAccountName"":""ingress-nginx"",""terminationGracePeriodSeconds"":300,""volumes"":[{""name"":""webhook-cert"",""secret"":{""secretName"":""ingress-nginx-admission""}}]}}}}
  selfLink: /apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller
status:
  observedGeneration: 33
  replicas: 1
  updatedReplicas: 1
  unavailableReplicas: 1
  conditions:
    - type: Available
      status: 'True'
      lastUpdateTime: '2025-03-19T10:12:31Z'
      lastTransitionTime: '2025-03-19T10:12:31Z'
      reason: MinimumReplicasAvailable
      message: Deployment has minimum availability.
    - type: Progressing
      status: 'True'
      lastUpdateTime: '2025-03-20T10:46:08Z'
      lastTransitionTime: '2025-03-19T10:12:31Z'
      reason: NewReplicaSetAvailable
      message: >-
        ReplicaSet ""ingress-nginx-controller-8584ffb585"" has successfully
        progressed.
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.0
      annotations:
        kubectl.kubernetes.io/restartedAt: '2025-03-20T10:46:06Z'
    spec:
      volumes:
        - name: webhook-cert
          secret:
            secretName: ingress-nginx-admission
            defaultMode: 420
      containers:
        - name: controller
          image: >-
            registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
          args:
            - /nginx-ingress-controller
            - '--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller'
            - '--election-id=ingress-nginx-leader'
            - '--controller-class=k8s.io/ingress-nginx'
            - '--ingress-class=nginx'
            - '--configmap=$(POD_NAMESPACE)/ingress-nginx-controller'
            - '--validating-webhook=:8443'
            - '--validating-webhook-certificate=/usr/local/certificates/cert'
            - '--validating-webhook-key=/usr/local/certificates/key'
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
            - name: webhook
              containerPort: 8443
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: LD_PRELOAD
              value: /usr/local/lib/libmimalloc.so
          resources:
            requests:
              cpu: 500m
              memory: 450Mi
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            runAsUser: 101
            runAsGroup: 82
            runAsNonRoot: true
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
      restartPolicy: Always
      terminationGracePeriodSeconds: 300
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      serviceAccount: ingress-nginx
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
```

I can add file on demande.

I try to :

- change path to stock ssl keys
- restart auto certification
- restart deployment
- change allocation (cpu/memory)

I don't want helm install.","kubernetes, yaml, kubernetes-ingress, ubuntu-24.04",79526354.0,"By default, [reloading NGINX is necessary](https://kubernetes.github.io/ingress-nginx/how-it-works/#when-a-reload-is-required:%7E:text=The%20main%20implication%20of%20this%20requirement%20is%20the%20need%20to%20reload%20NGINX%20after%20any%20change%20in%20the%20configuration%20file.) after any configuration file changes. However, in certain situations, [reloads can be avoided](https://kubernetes.github.io/ingress-nginx/how-it-works/#avoiding-reloads), especially when there are changes to the endpoints, such as when a pod is started or replaced.

Since you’ve already tried a multiple of troubleshooting steps, and your ingress-nginx controller still constantly reloads, I suggest you try to identify the main cause of configuration changes that keeps triggering the reloads by carefully reviewing your logs:

First, increase your logging detail by editing the Ingress-Nginx Controller Deployment. Add the `-v `or `--v` flag to [increase verbosity](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#log-verbosity-level):

```
containers:

name: controller
image: registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
       args:
       /nginx-ingress-controller
       '--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--election-id=ingress-nginx-leader'
       '--controller-class=k8s.io/ingress-nginx'
       '--ingress-class=nginx'
       '--configmap=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--validating-webhook=:8443'
       '--validating-webhook-certificate=/usr/local/certificates/cert'
       '--validating-webhook-key=/usr/local/certificates/key'
       '-v=3' # Add this line
```

Second, check the controller logs by using:

```
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -f --all-containers
```

Look for messages before the “**NGINX reload triggered...**"" message. These messages will indicate which resource was modified.

Lastly, use the kubectl logs command with grep to filter the logs for relevant events:

```
kubectl logs -f ingress-nginx-controller-8584ffb585-abcd1 -n ingress-nginx | grep -E ""(Ingress|ConfigMap|Secret|update|change|reload)""
```

Once you are able to identify which resource changes are causing the reloads when you aren't explicitly making a change, it will help you take the appropriate action.

In addition, you might want to check this [thread](https://github.com/kubernetes/ingress-nginx/issues/10448#issuecomment-2350911143) for a possible workaround to your case.",2025-03-21T18:37:55,2025-03-20T14:13:00
79522002,Ingres to redirect request from one URL/HOST to another,"I have a ingres.jsonnet config in my openshift cluster that has two end points host1 and host2 pointing to same backend service/web app. I have added rewrite conI have an ingres.jsonnet configuration in my OpenShift cluster with two endpoints, host1 and host2, both pointing to the same backend service/web app. I added a rewrite configuration snippet to redirect all requests from host1 to host2. However, this code isn't working as expected, and requests from both hosts are going to the same backend without redirecting. What could be the issue?

```
local params = import 'params.libsonnet';

{
apiVersion: 'networking.k8s.io/v1',d
  kind: 'Ingress',
  metadata: {
    name: params.ingress,
    annotations: {
      'kubernetes.io/ingress.allow-http': 'false',
      'nginx.ingress.kubernetes.io/ssl-ciphers': 'TLS_AES_128_GCM_SHA256',
      'nginx.ingress.kubernetes.io/ssl-prefer-server-ciphers': 'on',
      'nginx.ingress.kubernetes.io/configuration-snippet': 'if ($host ==""' + params.host1 + '"") { rewrite ^(.*)$ https://' + params.host2 + '$1 permanent; }'
    },
  },
  spec: {
    ingressClassName: 'nginx',
    tls: [
      {
        hosts: [
          params.host1,  // The host that will be redirected
          params.host2,  // The actual backend service host
        ],
      },
    ],
    rules: [
      {
        host: params.host1,
        http: {
          paths: [
            {
              path: '/',
              pathType: 'Prefix',
              backend: {
                service: {
                  name: params.service_name,
                  port: { number: params.service_port },
                },
              },
            },
          ],
        },
      },
      {
        host: params.host2,  // Actual backend
        http: {
          paths: [
            {
              path: '/',
              pathType: 'Prefix',
              backend: {
                service: {
                  name: params.service_name,
                  port: { number: params.service_port },
                },
              },
            },
          ],
        },
      },
    ],
  },
}
```","kubernetes, http-redirect, jsonnet",79522575.0,"Configuration-snippet which is applicable for only specific location blocks in nginx configuration Where **Server-snippet** is applicable for server block level in nginx configurations. Sometimes instead of using **configuration-snippet** trying **server-snippet** may work fine. Like mentioned below :

```
'nginx.ingress.kubernetes.io/server-snippet':
 'if ($host == ""' + params.host1 + '"") { rewrite /preview $scheme://' + params.host2 + '$uri permanent; }'
```

Or in Configuration-snippet try like this

```
nginx.ingress.kubernetes.io/configuration-snippet: |
 rewrite /preview params.host2 $uri permanent;
```

For more information refer to this [github issue](https://github.com/kubernetes/ingress-nginx/issues/605#issuecomment-297022809)",2025-03-20T10:15:31,2025-03-20T05:47:14
79521874,Hazelcast IMap event listeners not working in Kubernetes,"**Issue**: Hazelcast (distributed object events) event listeners are not getting invoked in Kubernetes setup.

**Setup**

Spring Boot application with embedded Hazelcast deployed in Kubernetes as statefulset application with replica count 3. Hazelcast forms a cluster with k8s service dns. Following Distributed object events, registered `EntryAddedListeners` and `EntryUpdatedListeners` for a particular IMap ""users"".

When user is added, ""users"" cache updated with ""userid"" as the key and value. Listeners based on this ""users"" cache of each hazelcast instance in the cluster, are expected to receive this event and do its stuff.

```
// register listener
hazelcastInstance.getConfig().getMapConfig(""users"").addEntryListenerConfig(new UserListener(), false, false);
```

```
// k8s related configuration
hazelcast.network.join.k8s.enabled=true
hazelcast.kubernetes.service-dns=usermgmt.namespace.svc.cluster.local //headless service
```

Note: local flag is set to false to get events in all hazelcast instance of the cluster.

Same code base works as expected with TCP-IP based cluster in VMs

```
hazelcast.network.join.tcp-ip.enabled=true
hazelcast.network.join.tcp-ip.members=<<IP List>>
```

Also noted that when replicaCount set to 1 (one hazelcast instance), event listeners are receiving events but not with multiple instance.

Suggestions?","kubernetes, hazelcast",,,,2025-03-20T04:01:52
79521477,Elastic Agent &quot;To reactivate please reconfigure or enroll again.&quot; error on fresh Instance,"I setup a fresh Elasticsearch and Kibana instance. I added the fleet and kubernetes integration and created a fleet server. When adding the Kubernetes Agent, the Pod is starting up and crashing shortly after with the following error message:

```
{""log.level"":""warn"",""@timestamp"":""2025-03-19T21:37:33.278Z"",""log.origin"":{""function"":""github.com/elastic/elastic-agent/internal/pkg/agent/application.(*managedConfigManager).Run"",""file.name"":""application/managed_mode.go"",""file.line"":120},""message"":""Elastic Agent was previously unenrolled. To reactivate please reconfigure or enroll again."",""log"":{""source"":""elastic-agent""},""ecs.version"":""1.6.0""}
```

I created the Kubernetes Agent with the default Kubernetes Config file and applied it [Kubernetes Agent File](https://i.sstatic.net/f5OylvT6.png). I am confused however, because the elastic instance is fresh and the fleet server aswell. Therefore I don't know where I am supposed to enroll or re-configure the Elastic Agent.

Any help is appreciated.","elasticsearch, kubernetes, kibana, fleet",,,,2025-03-19T22:06:22
79519594,Helm - Install WordPress Plugins from a Local Directory,"I'm seeking your help with a Helm-related issue (Kubernetes package manager).

I wanted to use Helm to deploy an instance of:

- WordPress
- MariaDB

I find it to be a very useful tool!

Specifically, I’m interested in how to declare a custom local path where `.zip` files of certain WordPress plugins are stored. These plugins should be installed during the Helm installation process.

Currently, in the `values.yaml` file (which centralizes the configuration for WP/MariaDB), there is a parameter called `wordpressPlugins` that allows plugin installation in two ways:

- Declaring the plugin name, which Helm then downloads from WordPress.org (requires internet access).
- Providing the URL of a public repository on GitHub.

I would like to know how to reference a local path instead. Any guidance would be greatly appreciated!

The goal is to automatically download and install WordPress plugins without requiring an internet connection.

I attempted to achieve this using the following approach, but I encountered a failure:

- I downloaded WordPress from Bitnami.
- Inside the downloaded WordPress directory, I created a folder named `plugins`.
- I placed the `.zip` files of the plugins I had downloaded into this folder.
- In the `values.yaml` file, under `wordpressPlugins`, I specified the path to the folder containing the `.zip` plugin files.
- I verified the read and write permissions of the folder.

After deploying everything, the WordPress pod enters a **CrashLoopBackOff** state.

The issue is caused by this approach because, if I hadn't specified anything in `wordpressPlugins`, WordPress would have started without any problems. So, the **CrashLoopBackOff** is triggered by an incorrect instruction.

There isn't much information available online about this. If you have any knowledge or experience with this, please share it with me. I would really appreciate it. Thank you!","wordpress, kubernetes, kubernetes-helm",79519800.0,"There is 2 way you can add custom path:

**Using Helm Values File:**

Add the custom path for plugin

```
wordpress:
  plugins:
    customPluginsPath: /path/to/custom/plugins/
```

**Using Config File:**

Step1:  Create the K8S for your Plugin

If you have `.zip` files for WordPress plugins stored locally, you can create a ConfigMap to store them in Kubernetes:

`kubectl create configmap wordpress-plugins --from-file=/path/to/custom/plugins/`

Step2:   Mount the ConfigMap to the WordPress Pod :

```
 extraVolumes:
  - name: plugins-volume
    configMap:
      name: wordpress-plugins  # Name of the ConfigMap

extraVolumeMounts:
  - name: plugins-volume
    mountPath: /var/www/html/wp-content/plugins  # Mount the plugins at the correct directory inside WordPress container
```

You may also want to run a `post-install` script to unzip the plugin files after WordPress is deployed. This can be achieved using an `initContainer`:

```
initContainers:
  - name: unzip-plugins
    image: busybox
    command: [""sh"", ""-c"", ""unzip /plugins/*.zip -d /var/www/html/wp-content/plugins""]
    volumeMounts:
      - name: plugins-volume
        mountPath: /plugins
      - name: wordpress-volume
        mountPath: /var/www/html/wp-content/plugins
```

Install Wordpress using helm chart :

`helm upgrade --install my-wordpress bitnami/wordpress -f values-wordpress.yaml`",2025-03-19T09:52:14,2025-03-19T08:40:47
79519043,NodePort not accessible in Minikube on Windows 11 using Docker driver,"I am trying to expose my Worklenz frontend service using a NodePort in Minikube on Windows 11, but I am unable to access it using [http://192.168.49.2:30000](http://192.168.49.2:30000). I can access it using minikube service worklenz-frontend-service --url, but that's not a permanent solution for me.
this is my frontend deployment and services

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: worklenz-frontend-deployment
  labels:
    app: worklenz-frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: worklenz-frontend
  template:
    metadata:
      labels:
        app: worklenz-frontend
    spec:
      containers:
      - name: worklenz-frontend
        image: docker.io/kithceydigital/worklenz_frontend:latest
        ports:
        - containerPort: 5000

---

apiVersion: v1
kind: Service
metadata:
  name: worklenz-frontend-service
spec:
  selector:
    app: worklenz-frontend
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5000
      nodePort: 30000
  type: NodePort
  externalTrafficPolicy: Cluster
```

Kube-Proxy Mode: iptables
NodePort Addresses: null

This is output for `kubectl get pods`

```
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl get pods
NAME                                            READY   STATUS              RESTARTS   AGE
worklenz-backend-deployment-844ff59b45-q8rrv    1/1     Running             0          2m6s
worklenz-db-0                                   0/1     ContainerCreating   0          108s
worklenz-frontend-deployment-6ddf679f88-6cqkp   1/1     Running             0          2m13s
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl get svc
NAME                        TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes                  ClusterIP   10.96.0.1       <none>        443/TCP          40h
worklenz-backend-service    ClusterIP   10.111.33.122   <none>        3000/TCP         2m6s
worklenz-db                 ClusterIP   None            <none>        5432/TCP         108s
worklenz-frontend-service   NodePort    10.108.63.67    <none>        5000:30000/TCP  2m13s
```

i have 3 deployments and services in my application
i used `netstat -ano | Select-String ""30000""` and it gives me no output

i can only access it when i use this

```
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> minikube service worklenz-frontend-service --url
http://127.0.0.1:2670
❗  Because you are using a Docker driver on windows, the terminal needs to be open to run it.
```

When I try [http://192.168.49.2:30000](http://192.168.49.2:30000) in my browser, it does not respond.
Tried `curl http://192.168.49.2:30000` . it does not respond
Three pods are running without giving any problems in logs

here are logs of the three pods running

```
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl get pods
NAME                                            READY   STATUS    RESTARTS   AGE
worklenz-backend-deployment-7855d64798-swhpg    1/1     Running   0          25s
worklenz-db-0                                   1/1     Running   0          111s
worklenz-frontend-deployment-6ddf679f88-6cqkp   1/1     Running   0          23m
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl logs worklenz-backend-deployment-7855d64798-swhpg

> worklenz-backend@1.4.16 start
> node ./build/bin/www

notifications-cron-job: (cron) Email notifications job ready.
daily-digest-cron-job: (cron) Daily digest job ready.
project-digest-cron-job: (cron) Project digest job ready.
Listening on port 3000
DbTaskStatusChangeListener connected.
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl logs worklenz-backend-deployment-7855d64798-swhpg

> worklenz-backend@1.4.16 start
> node ./build/bin/www

notifications-cron-job: (cron) Email notifications job ready.
daily-digest-cron-job: (cron) Daily digest job ready.
project-digest-cron-job: (cron) Project digest job ready.
Listening on port 3000
DbTaskStatusChangeListener connected.
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl logs worklenz-frontend-deployment-6ddf679f88-6cqkp
 INFO  Accepting connections at http://localhost:5000
 HTTP  3/19/2025 2:58:26 AM 10.244.0.1 GET /
 HTTP  3/19/2025 2:58:26 AM 10.244.0.1 Returned 200 in 44 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/js/index-DQn5Id1F.js
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/js/antd-B1IHu5tK.js
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/js/i18n-MZoVdmad.js
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/css/index-D8BAkfl4.css
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 3 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 5 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 18 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 24 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /locales/en/translation.json
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 2 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /locales/en/auth/login.json
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/ico/favicon-B1xYTh5K.ico
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 4 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 200 in 4 ms
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 GET /assets/ico/favicon-B1xYTh5K.ico
 HTTP  3/19/2025 2:58:27 AM 10.244.0.1 Returned 304 in 1 ms
PS C:\Users\rkksg\Documents\js-projects\ceydigital\kubernetes\worklenz-k8> kubectl logs worklenz-db-0

PostgreSQL Database directory appears to contain a database; Skipping initialization

2025-03-19 03:08:51.264 UTC [1] LOG:  starting PostgreSQL 15.12 (Debian 15.12-1.pgdg120+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 12.2.0-14) 12.2.0, 64-bit
2025-03-19 03:08:51.265 UTC [1] LOG:  listening on IPv4 address ""0.0.0.0"", port 5432
2025-03-19 03:08:51.265 UTC [1] LOG:  listening on IPv6 address ""::"", port 5432
2025-03-19 03:08:51.269 UTC [1] LOG:  listening on Unix socket ""/var/run/postgresql/.s.PGSQL.5432""
2025-03-19 03:08:51.274 UTC [29] LOG:  database system was shut down at 2025-03-19 03:08:49 UTC
2025-03-19 03:08:51.278 UTC [1] LOG:  database system is ready to accept connections
```

Why is my NodePort (30000) not accessible via Minikube IP?
Is this related to Windows networking, Docker driver, or Minikube setup?
How can I make my NodePort accessible without using minikube service?
This is my first time using Kubernetes is there problem in my configuration?
this is my repo [link](https://github.com/kithmina1999/worklenz-k8)","docker, kubernetes, minikube, kube-proxy, kubernetes-nodeport",,,,2025-03-19T03:19:42
79518543,How to expose resources under /.well-known/ with K8s?,"I need to expose some resources under `https://app.my-domain.net/.well-known/` using Kubernetes (Android `assetlinks.json` and `apple-app-site-association`).

These resources are packaged in a Nginx container. I created a K8s deployment, a K8s service, and tried the following ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/app-root: /ui/
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - app.my-domain.net
      secretName: app-tls
  rules:
  - host: app.my-domain.net
    http:
      paths:
      - path: /.well-known
        pathType: Prefix
        backend:
          service:
            name: well-known-static-resources
            port:
              number: 80
```

But I got: `Warning: path /.well-known cannot be used with pathType Prefix`.

Reading the docs, [the dot in `/.well-known` seems incompatible with ingress path validation](https://kubernetes.github.io/ingress-nginx/faq/#validation-of-path).

But then, how should I route requests to the service for my `.well-known` resources? Or is there a better way to expose `.well-known` resources using K8s than ingress -> service -> pod -> Nginx container?","kubernetes, kubernetes-ingress, nginx-ingress",79518955.0,"I finally found a working solution with `pathType: ImplementationSpecific`.

Here is the modified yaml with usage of regexp and path rewrite I hadn't yet in the question, but now use on some other path:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/app-root: /ui/
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - app.my-domain.net
      secretName: app-tls
  rules:
  - host: app.my-domain.net
    http:
      paths:
      - path: /(\.well-known/.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: well-known-static-resources
            port:
              number: 80
```",2025-03-19T01:56:54,2025-03-18T19:59:13
79518515,otel-collector to scrap multiple pods,"I would like to use otel-collector to scrap multiple business pods.

This solution is already working for one (but just one) pod:

```
kubectl get pods

mycoolbusinesspod-7b4f8f4c4f-74sqq 1/1 Running 0
```

The concept is that there is one business pod which generates the metrics.

I expose a Kubernetes service svc for this business pod.

The collector is configured to scrap this only one endpoint and path.

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-conf
  labels:
    app: opentelemetry
    component: otel-collector-conf
data:
  otel-collector-config: |
    receivers:
      prometheus:
        config:
          scrape_configs:
            - job_name: ""jobname""
              scrape_interval: 5s
              metrics_path: '/actuator/prometheus'
              static_configs:
                - targets: [""mycoolbusinesspod-svc:8080""]

    processors:
      batch:

    exporters:
      prometheus:
        endpoint: ""localhost:8889""

    service:
      pipelines:
        metrics:
          receivers: [prometheus]
          processors: [batch]
          exporters: [prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-collector
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: otel-collector
    spec:
      containers:
        - command:
            - ""/otelcol""
            - ""--config=/conf/otel-collector-config.yaml""
          image: otel/opentelemetry-collector:latest
          name: otel-collector
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 400Mi
          ports:
            - containerPort: 55679 # Default endpoint for ZPages.
            - containerPort: 4317 # Default endpoint for OpenTelemetry receiver.
            - containerPort: 14250 # Default endpoint for Jaeger gRPC receiver.
            - containerPort: 14268 # Default endpoint for Jaeger HTTP receiver.
            - containerPort: 9411 # Default endpoint for Zipkin receiver.
            - containerPort: 8888  # Default endpoint for querying metrics.
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: GOMEMLIMIT
              value: 1600MiB
          volumeMounts:
            - name: otel-collector-config-vol
              mountPath: /conf
```

This is working, I can see the metrics.

However, now, I have multiple replicas.

```
kubectl get pods

mycoolbusinesspod-7b4f8f4c4f-lrphj 1/1 Running 0
mycoolbusinesspod-7b4f8f4c4f-n7v9h 1/1 Running 0
mycoolbusinesspod-7b4f8f4c4f-jght9 1/1 Running 0
```

Attempt 1:

I see the configuration accepts an array for ""target"". But I do not know what to input in the array.

Attempt 2:

I do some kind of load balancer as service to load balance between the multiple pods.

However, this will not get the metrics from the pods that were not load balanced.

How to configure Otel-collector to scrap metrics for all the business pods?","kubernetes, open-telemetry-collector, otel, otel-agent",79519046.0,"> I do some kind of load balancer as service to load balance between the multiple pods.
> However, this will not get the metrics from the pods that were not load balanced.

Since you're looking to scrape configuration on every pods, I think setting pod's annotation and use Prometheus [kubernetes service discovery](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) to scrape pods metrics directly should be the way to go in microservices deployment.

If you'd scrape through k8s ClusterIP service the scrape request will get randomly distributed to every pods you deploy, which is undesirable since we want to know metrics of every pods.

I believe otel collector support this feature out of the box. [ref](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver)

Sample configuration could be like this:

```
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
```

this configuration will make otel collector scrape metrics from any pods with annotations:

```
  prometheus.io/scrape: ""true""
  prometheus.io/path: ""/metrics""
  prometheus.io/port: ""8080""
```

while keeping `kubernetes_pod_name` and `kubernetes_namespace` value as metadata in labels.

You might need to set appropriate ClusterRole for your otel-collector ServiceAccount though.

something like:

```
rules:
- apiGroups: [""""]
  resources:
  - pods
  verbs: [""get"", ""list"", ""watch""]
- nonResourceURLs: [""/metrics""]
  verbs: [""get""]
```

should suffice.",2025-03-19T03:20:33,2025-03-18T19:38:47
79517053,Could not find a declaration file for module &#39;cookie-session&#39;,"I have a Typescript project and I get the following error.

[auth] [ERROR] 09:55:34 ⨯ Unable to compile TypeScript:

[auth] src/app.ts(3,27): error TS7016: Could not find a declaration file for module 'cookie-session'. '/app/node_modules/cookie-session/index.js' implicitly has an 'any' type.

[auth]   Try `npm i --save-dev @types/cookie-session` if it exists or add a new declaration (.d.ts) file containing `declare module 'cookie-session';`

my package.json

```
{
  ""name"": ""auth"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""start"": ""ts-node-dev src/index.ts"",
    ""test"": ""jest --watchAll --no-cache""
  },
  ""jest"": {
    ""preset"": ""ts-jest"",
    ""testEnvironment"": ""node"",
    ""setupFilesAfterEnv"": [
      ""./src/test/setup.ts""
    ]
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""@types/express"": ""^5.0.0"",
    ""cookie-session"": ""^2.1.0"",
    ""express"": ""^4.21.2"",
    ""express-async-errors"": ""^3.1.1"",
    ""express-validator"": ""^7.2.1"",
    ""jsonwebtoken"": ""^9.0.2"",
    ""mongoose"": ""^8.12.1"",
    ""ts-node-dev"": ""^2.0.0"",
    ""typescript"": ""^5.8.2""
  },
  ""devDependencies"": {
    ""@types/cookie-session"": ""^2.0.49"",
    ""@types/jest"": ""^29.5.14"",
    ""@types/jsonwebtoken"": ""^9.0.9"",
    ""@types/supertest"": ""^6.0.2"",
    ""jest"": ""^29.7.0"",
    ""mongodb-memory-server"": ""^10.1.4"",
    ""supertest"": ""^7.0.0"",
    ""ts-jest"": ""^29.2.6""
  }
}
```

I run this in a Kubernetes cluster using skaffold.

When I try to do this:

```
// @ts-ignore
import cookieSession from 'cookie-session'
```

I get

[auth] Compilation error in /app/src/middleware/current-user.ts

[auth] [ERROR] 10:06:31 ⨯ Unable to compile TypeScript:

[auth] src/middleware/current-user.ts(2,17): error TS7016: Could not find a declaration file for module 'jsonwebtoken'. '/app/node_modules/jsonwebtoken/index.js' implicitly has an 'any' type.

[auth]   Try `npm i --save-dev @types/jsonwebtoken` if it exists or add a new declaration (.d.ts) file containing `declare module 'jsonwebtoken';`

[auth] src/middleware/current-user.ts(18,12): error TS2339: Property 'session' does not exist on type 'Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>'.

[auth] src/middleware/current-user.ts(23,36): error TS2339: Property 'session' does not exist on type 'Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>'.","node.js, typescript, express, kubernetes, skaffold",79521788.0,"The issue is likely due to missing type declarations for `cookie-session` and `jsonwebtoken`, which TypeScript requires for proper type checking. Additionally, Express's Request object doesn't include session by default, so TypeScript doesn't recognize it unless explicitly extended. This can be fixed by installing the missing `@types` packages and extending Express's `Request` interface to include `session`.

1. **Install the missing type declarations:**

```
npm install --save-dev @types/cookie-session @types/jsonwebtoken
```

1. **Add the `cookie-session` types to your express request:**

Modify `current-user.ts`:

```
import { Request } from 'express';

declare module 'express' {
  interface Request {
    session?: any;
  }
}
```

1. **Reinstall packages**

```
 rm -rf node_modules package-lock.json && npm install
```

Hope this works",2025-03-20T02:56:42,2025-03-18T10:18:18
79517053,Could not find a declaration file for module &#39;cookie-session&#39;,"I have a Typescript project and I get the following error.

[auth] [ERROR] 09:55:34 ⨯ Unable to compile TypeScript:

[auth] src/app.ts(3,27): error TS7016: Could not find a declaration file for module 'cookie-session'. '/app/node_modules/cookie-session/index.js' implicitly has an 'any' type.

[auth]   Try `npm i --save-dev @types/cookie-session` if it exists or add a new declaration (.d.ts) file containing `declare module 'cookie-session';`

my package.json

```
{
  ""name"": ""auth"",
  ""version"": ""1.0.0"",
  ""description"": """",
  ""main"": ""index.js"",
  ""scripts"": {
    ""start"": ""ts-node-dev src/index.ts"",
    ""test"": ""jest --watchAll --no-cache""
  },
  ""jest"": {
    ""preset"": ""ts-jest"",
    ""testEnvironment"": ""node"",
    ""setupFilesAfterEnv"": [
      ""./src/test/setup.ts""
    ]
  },
  ""keywords"": [],
  ""author"": """",
  ""license"": ""ISC"",
  ""dependencies"": {
    ""@types/express"": ""^5.0.0"",
    ""cookie-session"": ""^2.1.0"",
    ""express"": ""^4.21.2"",
    ""express-async-errors"": ""^3.1.1"",
    ""express-validator"": ""^7.2.1"",
    ""jsonwebtoken"": ""^9.0.2"",
    ""mongoose"": ""^8.12.1"",
    ""ts-node-dev"": ""^2.0.0"",
    ""typescript"": ""^5.8.2""
  },
  ""devDependencies"": {
    ""@types/cookie-session"": ""^2.0.49"",
    ""@types/jest"": ""^29.5.14"",
    ""@types/jsonwebtoken"": ""^9.0.9"",
    ""@types/supertest"": ""^6.0.2"",
    ""jest"": ""^29.7.0"",
    ""mongodb-memory-server"": ""^10.1.4"",
    ""supertest"": ""^7.0.0"",
    ""ts-jest"": ""^29.2.6""
  }
}
```

I run this in a Kubernetes cluster using skaffold.

When I try to do this:

```
// @ts-ignore
import cookieSession from 'cookie-session'
```

I get

[auth] Compilation error in /app/src/middleware/current-user.ts

[auth] [ERROR] 10:06:31 ⨯ Unable to compile TypeScript:

[auth] src/middleware/current-user.ts(2,17): error TS7016: Could not find a declaration file for module 'jsonwebtoken'. '/app/node_modules/jsonwebtoken/index.js' implicitly has an 'any' type.

[auth]   Try `npm i --save-dev @types/jsonwebtoken` if it exists or add a new declaration (.d.ts) file containing `declare module 'jsonwebtoken';`

[auth] src/middleware/current-user.ts(18,12): error TS2339: Property 'session' does not exist on type 'Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>'.

[auth] src/middleware/current-user.ts(23,36): error TS2339: Property 'session' does not exist on type 'Request<ParamsDictionary, any, any, ParsedQs, Record<string, any>>'.","node.js, typescript, express, kubernetes, skaffold",79517515.0,"I figured out what was wrong with my project, I had this line in my Dockerfile.

```
RUN npm install --omit=dev
```",2025-03-18T13:01:28,2025-03-18T10:18:18
79516940,How to retrieve only the master node name using kubectl?,"I am trying to get the name of the master (control-plane) node in my Kubernetes cluster using `kubectl`. I want the command to return only the name of the node without any additional information.

I have tried the following command:

```
kubectl get nodes --selector=node-role.kubernetes.io/master
```

However, this returns a table with additional details. I would like to extract only the node name.

What is the correct `kubectl` command to achieve this?","kubernetes, kubectl",79516941.0,"With `kubernetes-v1.31.0`, this works:

```
kubectl get nodes --selector=node-role.kubernetes.io/master= --output=jsonpath='{.items[0].metadata.name}'
```",2025-03-18T09:41:10,2025-03-18T09:41:10
79516630,Using Promethus adapter as custom metrics server for HPA autoscaling,"I am trying to setup and use the Prometheus server and Prometheus adapter integration to replace the metrics-server in the local kubernetes cluster (built using kind) and use it to scale my HPA based on custom metrics.

I have 2 Promethus pod instances and 1 prometheus adapter deployed and running in the 'monitoring' namespace.

The Spring boot application deployment (to be scaled by HPA) is deployed and running in 'demo-config-app' namespace.

**Problem**: HPA (Horizontal Pod Autoscaler) is simply not able to fetch metrics from prometheus adapter which I intent to use as a replacement for K8S metrics-server.

Custom metrics query configured an Prometheus adapter ConfigMap is,

```
rules:
    - seriesQuery: 'http_server_requests_seconds_count{namespace!="""", service != """", uri = ""/""}'
      resources:
        overrides:
          namespace: {resource: ""namespace""}
          service: {resource: ""service""}
      name:
        matches: ""http_server_requests_seconds_count""
        as: ""http_server_requests_seconds_count""
      metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>,uri!~""/actuator/.*""}[15m]))
```

HPA Yaml manifest is as follows :

```
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2
metadata:
  name: demo-config-app
  namespace: dynamic-secrets-ns
spec:
  scaleTargetRef:
    # point the HPA at the sample application
    # you created above
    apiVersion: apps/v1
    kind: Deployment
    name: demo-config-watcher
  # autoscale between 1 and 10 replicas
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      metric:
        name: http_server_requests_seconds_count
      describedObject:
        apiVersion: v1
        kind: Service
        name: demo-config-watcher-svc-internal
      target:
        type: AverageValue
        averageValue: 10
```

Custom metrics, seems to have been correctly configured.
Executing the kubectl command,

```
    $ kubectl get --raw ""/apis/custom.metrics.k8s.io/v1beta2"" | jq

    OUTPUT:
        {
          ""kind"": ""APIResourceList"",
          ""apiVersion"": ""v1"",
          ""groupVersion"": ""custom.metrics.k8s.io/v1beta2"",
          ""resources"": [
            {
              ""name"": ""namespaces/http_server_requests_seconds_count"",
              ""singularName"": """",
              ""namespaced"": false,
              ""kind"": ""MetricValueList"",
              ""verbs"": [
                ""get""
              ]
            },
            {
              ""name"": ""services/http_server_requests_seconds_count"",
              ""singularName"": """",
              ""namespaced"": true,
              ""kind"": ""MetricValueList"",
              ""verbs"": [
                ""get""
              ]
            }
          ]
        }

```

Also When I execute the metrics query in prometheus console,

```
    sum(rate(http_server_requests_seconds_count{namespace=""dynamic-secrets-ns"",service=""demo-config-watcher-svc-internal"",uri!~""/actuator/.*""}[15m]))
```

I get an aggregated response value - 3.1471300541724765

***Following are the few points from my analysis of adapter logs :***

1. As soon as, Promethus adapter pod starts-up, it fires the following query,

```
http://prometheus-k8s.monitoring.svc:9090/api/v1/series?match%5B%5D=http_server_requests_seconds_count%7Bnamespace%21%3D%22%22%2C+service+%21%3D+%22%22%2C+uri+%3D+%22%2F%22%7D&start=1742277149.166
```

I tried executing the same query from an nginx pod in the same namespace as that of prometheus-adater (with the same ServiceAccount) and it gives me the following results:

```
{
   ""status"":""success"",
   ""data"":[
      {
         ""__name__"":""http_server_requests_seconds_count"",
         ""container"":""demo-config-watcher"",
         ""endpoint"":""http-internal"",
         ""error"":""none"",
         ""exception"":""none"",
         ""instance"":""10.244.2.104:8080"",
         ""job"":""demo-config-watcher-job"",
         ""method"":""GET"",
         ""namespace"":""dynamic-secrets-ns"",
         ""outcome"":""SUCCESS"",
         ""pod"":""demo-config-watcher-7dbb9b598b-k7cgj"",
         ""service"":""demo-config-watcher-svc-internal"",
         ""status"":""200"",
         ""uri"":""/""
      }
   ]
}
```

1. By increasing the verbosity of prometheus adapter logs, I can see following requests being repeatedly appearing in the log.
Not sure about the first GET request, where it is coming from.
The second request is clearly coming from HPA controller and it results in HTTP status 404. Not sure why ?

```
I0318 06:31:39.832124       1 round_trippers.go:553] POST https://10.96.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s 201 Created in 1 milliseconds
I0318 06:31:39.832343       1 handler.go:143] prometheus-metrics-adapter: GET ""/apis/custom.metrics.k8s.io/v1beta2/namespaces/dynamic-secrets-ns/services/demo-config-watcher-svc-internal/http_server_requests_seconds_count"" satisfied by gorestful with webservice /apis/custom.metrics.k8s.io
I0318 06:31:39.833331       1 api.go:88] GET http://prometheus-k8s.monitoring.svc:9090/api/v1/query?query=sum%28rate%28http_server_requests_seconds_count%7Bnamespace%3D%22dynamic-secrets-ns%22%2Cservice%3D%22demo-config-watcher-svc-internal%22%2Curi%21~%22%2Factuator%2F.%2A%22%7D%5B15m%5D%29%29&time=1742279499.832&timeout= 200 OK
E0318 06:31:39.833494       1 provider.go:186] None of the results returned by when fetching metric services/http_server_requests_seconds_count(namespaced) for ""dynamic-secrets-ns/demo-config-watcher-svc-internal"" matched the resource name
I0318 06:31:39.833600       1 httplog.go:132] ""HTTP"" verb=""GET"" URI=""/apis/custom.metrics.k8s.io/v1beta2/namespaces/dynamic-secrets-ns/services/demo-config-watcher-svc-internal/http_server_requests_seconds_count"" latency=""2.926569ms"" userAgent=""kube-controller-manager/v1.32.0 (linux/amd64) kubernetes/70d3cc9/system:serviceaccount:kube-system:horizontal-pod-autoscaler"" audit-ID=""8f71b62a-92bc-4f13-a409-01ec5b778429"" srcIP=""172.18.0.3:34574"" resp=404
```

HPA has following RBAC permissions configured,

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: ""2025-03-16T05:47:45Z""
  name: custom-metrics-getter
  resourceVersion: ""6381614""
  uid: 04106c39-be1f-4ee3-b2ab-cf863ef43aca
rules:
- apiGroups:
  - custom.metrics.k8s.io
  resources:
  - '*'
  verbs:
  - '*'

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""rbac.authorization.k8s.io/v1"",""kind"":""ClusterRoleBinding"",""metadata"":{""annotations"":{},""name"":""hpa-custom-metrics-getter""},""roleRef"":{""apiGroup"":""rbac.authorization.k8s.io"",""kind"":""ClusterRole"",""name"":""custom-metrics-getter""},""subjects"":[{""kind"":""ServiceAccount"",""name"":""horizontal-pod-autoscaler"",""namespace"":""kube-system""}]}
  creationTimestamp: ""2025-03-16T05:47:45Z""
  name: hpa-custom-metrics-getter
  resourceVersion: ""6381615""
  uid: c819798d-fdd0-47df-a8d1-55cff8101d84
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-metrics-getter
subjects:
- kind: ServiceAccount
  name: horizontal-pod-autoscaler
  namespace: kube-system
```

Appreciate any help on how to take this forward, thanks in advance.","kubernetes, prometheus, prometheus-adapter",79519131.0,"Finally, The problem was with the metricsQuery configured in the adapter config.

```
 rules:
    - seriesQuery: 'http_server_requests_seconds_count{namespace!="""", pod != """"}'
      resources:
        overrides:
          namespace: {resource: ""namespace""}
          pod: {resource: ""pod""}
      name:
        matches: ""^(.*)_seconds_count""
        as: ""${1}_per_second""
      metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>,uri!~""/actuator/.*""}[2m])) by (pod)'
```

HPA:

```
---
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2
metadata:
  name: demo-http
  namespace: dynamic-secrets-ns
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-config-watcher
  minReplicas: 1
  maxReplicas: 10
  metrics:
  # use a ""Pods"" metric, which takes the average of the
  # given metric across all pods controlled by the autoscaling target
  - type: Pods
    pods:
      metric:
        # use the metric that you used above: pods/http_requests
        name: http_server_requests_per_second
      target:
       # We configured the HPA to scale Pods if the average of requests is greater than 10 per seconds.
        type: AverageValue
        averageValue: 10000m
```

Huge shoutout for a youtube video - [Anton's guide on K8S-Prometheus integration](https://www.youtube.com/watch?v=iodq-4srXA8&t=1125s)",2025-03-19T04:39:31,2025-03-18T07:23:48
79516140,Istio retries on an API with circuit breaker is inconsistent,"We implemented circuit breaker on a application using Resilience4j. API from Service A calls API in Service B. SlidingWindowsize is 10. Application is deployed to K8s and managed by Istio.

To test circuit breaker implementation, we brought down Service B and called its API from Service A with the intention to catch 503 error of Service B. When the api is hit 10th time, we open the circuit with status 500. No api retries are configured. Istio retries are configured as 3.

We observed 9 retries happening in the background for 1 api hit.

The issue we noticed is that, when the api of Service A is invoked the first time from my application, we see 9 retries happening in the background and retries stop when we open the circuit with 500 status code. We notice that for each retry in the log having this ""x-envoy-attempt-count"" : ""1"", ""x-envoy-attempt-count"" : ""2"", ""x-envoy-attempt-count"" : ""3"". This is repeated 3 times. Same behaviour is observed when hitting from postman as well.

So we created a standalone circuit breaker api with same SlidingWindowsize 10, trying to call the api in service B that was brought down. But we noticed only 3 retries for 1 hit from postman and not 9.","spring-boot, kubernetes, istio, resilience4j, istio-sidecar",,,,2025-03-18T02:29:40
79516077,Cannot Connect my deployed kafka on Kubernetese with my spring boot application,"I created a Spring Boot application that uses Kafka, which I deployed on a Kubernetes cluster.

I am facing an error stating that the deployed Spring Boot application cannot resolve the bootstrap URLs inside the Kafka cluster.

I got this error when I tried to deploy my Spring Boot application:

```
rg.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:326) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:510) ~[spring-context-6.2.3.jar!/:6.2.3]
    at java.base/java.lang.Iterable.forEach(Iterable.java:75) ~[na:na]
    at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:295) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:240) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:1006) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:630) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1361) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1350) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at fr.formationacademy.scpiinvestpluspartner.ScpiInvestPlusPartnerApplication.main(ScpiInvestPlusPartnerApplication.java:10) ~[!/:0.0.1-SNAPSHOT]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:569) ~[na:na]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]

Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:265) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.ConsumerDelegateCreator.create(ConsumerDelegateCreator.java:65) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:600) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:595) ~[kafka-clients-3.8.1.jar!/:na]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer.<init>(DefaultKafkaConsumerFactory.java:498) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createRawConsumer(DefaultKafkaConsumerFactory.java:453) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:430) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumerWithAdjustedProperties(DefaultKafkaConsumerFactory.java:407) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:374) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumer(DefaultKafkaConsumerFactory.java:335) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.<init>(KafkaMessageListenerContainer.java:876) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer.doStart(KafkaMessageListenerContainer.java:387) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.ConcurrentMessageListenerContainer.doStart(ConcurrentMessageListenerContainer.java:264) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.startIfNecessary(KafkaListenerEndpointRegistry.java:436) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.start(KafkaListenerEndpointRegistry.java:382) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:323) ~[spring-context-6.2.3.jar!/:6.2.3]
    ... 20 common frames omitted

Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:103) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:62) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:58) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:184) ~[kafka-clients-3.8.1.jar!/:na]
    ... 37 common frames omitted
```

I configured Kafka inside Kubernetes using this configuration:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:6.1.1
          ports:
            - containerPort: 9092
            - containerPort: 9094
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

            - name: KAFKA_LISTENERS
              value: ""INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094""
            - name: KAFKA_ADVERTISED_LISTENERS
              value: ""INTERNAL://$(POD_NAME).kafka-headless:9092,OUTSIDE://$(POD_NAME).kafka-svc:9094""
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: ""INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT""
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: ""3""
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: ""INTERNAL""
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: ""zookeeper:2181""
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: ""false""

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
spec:
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094
  type: ClusterIP
```

And for zookeeper:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - name: zookeeper
          image: wurstmeister/zookeeper
          ports:
            - containerPort: 2181
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
spec:
  selector:
    app: zookeeper
  ports:
    - protocol: TCP
      port: 2181
      targetPort: 2181
```

My `application.yml`:

```
spring:
  application:
    version: 1.0.0
    name: scpi-invest-plus-partner
  kafka:
    bootstrap-servers: kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: ""*""
```","spring-boot, kubernetes, apache-kafka, spring-kafka",79518005.0,"After few modifications, I could resolve the error, but my topics are not recongnized by the application.

```
2025-03-18 15:37:45 [scpi-invest-plus-api] [int] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-scpi-partner-group-1, groupId=scpi-partner-group] Error while fetching metadata with correlation id 124 : {scpi-partner-response-topic=UNKNOWN_TOPIC_OR_PARTITION}
```

This is my configuration :

```
spring:
  application:
    name: scpi-invest-plus-api
    version: 1.0.0

  datasource:
    url: jdbc:postgresql://scpi-invest-db:5432/postgres
    username: postgres
    password: postgres
    driver-class-name: org.postgresql.Driver

  jpa:
    database: postgresql
    hibernate:
      ddl-auto: validate
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
  kafka:
    bootstrap-servers: kafka-headless.kafka:9092

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: ""*""

  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: https://keycloak.check-consulting.net/realms/master
          jwk-set-uri: https://keycloak.check-consulting.net/realms/master/protocol/openid-connect/certs

management:
  endpoints:
    web:
      exposure:
        include: health,prometheus
  endpoint:
    prometheus:
      enabled: true
  prometheus:
    metrics:
      export:
        enabled: true
```

The method where I send the message using Kafka:

```
public InvestmentDto saveInvestment(InvestmentDto investmentDto) throws GlobalException {
        log.info(""Début de la création d'un investissement."");

        if (investmentDto == null) {
            log.error(""L'objet InvestmentDto est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""InvestmentDto ne peut pas être null."");
        }

        if (investmentDto.getScpiId() == null) {
            log.error(""L'ID de la SCPI est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""L'ID de la SCPI ne peut pas être null."");
        }

        String email = userService.getEmail();
        log.info(""Récupération de l'email de l'utilisateur : {}"", email);

        ScpiDtoOut scpiDtoOut = scpiService.getScpiDetailsById(investmentDto.getScpiId());
        log.info(""Détails SCPI récupérés : {}"", scpiDtoOut);
        if (scpiDtoOut == null) {
            log.error(""SCPI non trouvée pour ID: {}"", investmentDto.getScpiId());
            throw new GlobalException(HttpStatus.NOT_FOUND, ""Aucune SCPI trouvée avec l'ID: "" + investmentDto.getScpiId());
        }
        log.info(""SCPI trouvée : {} - {}"", scpiDtoOut.getId(), scpiDtoOut.getName());

        Scpi scpiEntity = scpiRepository.findById(investmentDto.getScpiId())
                .orElseThrow(() -> new GlobalException(HttpStatus.NOT_FOUND, ""SCPI non trouvée""));

        Investment investment = investmentMapper.toEntity(investmentDto);
        investment.setInvestorId(email);
        investment.setInvestmentState(""En cours"");
        investment.setScpi(scpiEntity);

        Investment savedInvestment = investmentRepository.save(investment);
        log.info(""Investissement enregistré avec succès - ID: {}"", savedInvestment.getId());

        InvestmentKafkaDto kafkaDto = new InvestmentKafkaDto();
        InvestmentOutDto investmentOutDto = investmentMapper.toOutDto(savedInvestment);
        investmentOutDto.setId(savedInvestment.getId());
        kafkaDto.setInvestmentDto(investmentOutDto);
        kafkaDto.setInvestorEmail(email);
        kafkaDto.setScpi(scpiDtoOut);

        log.info(""Envoi la demande d'investissement au Bouchon pour Objet Traitement : {}"", kafkaDto);
        sendInvestment(kafkaDto);
        log.info(""Investissement envoyé avec succès à Kafka - ID: {}"", savedInvestment.getId());

        return investmentMapper.toDTO(savedInvestment);
    }
```

I also configured the topic :

```
import static fr.formationacademy.scpiinvestplusapi.utils.Constants.SCPI_REQUEST_TOPIC;

@Configuration
public class KafkaTopicConfig {
    @Bean
    public NewTopic getTopic() {
        return TopicBuilder.name(SCPI_REQUEST_TOPIC)
                .partitions(1)
                .replicas(1)
                .build();
    }
}
```",2025-03-18T15:43:42,2025-03-18T01:27:53
79516077,Cannot Connect my deployed kafka on Kubernetese with my spring boot application,"I created a Spring Boot application that uses Kafka, which I deployed on a Kubernetes cluster.

I am facing an error stating that the deployed Spring Boot application cannot resolve the bootstrap URLs inside the Kafka cluster.

I got this error when I tried to deploy my Spring Boot application:

```
rg.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:326) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:510) ~[spring-context-6.2.3.jar!/:6.2.3]
    at java.base/java.lang.Iterable.forEach(Iterable.java:75) ~[na:na]
    at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:295) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:240) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:1006) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:630) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1361) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1350) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at fr.formationacademy.scpiinvestpluspartner.ScpiInvestPlusPartnerApplication.main(ScpiInvestPlusPartnerApplication.java:10) ~[!/:0.0.1-SNAPSHOT]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:569) ~[na:na]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]

Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:265) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.ConsumerDelegateCreator.create(ConsumerDelegateCreator.java:65) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:600) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:595) ~[kafka-clients-3.8.1.jar!/:na]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer.<init>(DefaultKafkaConsumerFactory.java:498) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createRawConsumer(DefaultKafkaConsumerFactory.java:453) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:430) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumerWithAdjustedProperties(DefaultKafkaConsumerFactory.java:407) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:374) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumer(DefaultKafkaConsumerFactory.java:335) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.<init>(KafkaMessageListenerContainer.java:876) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer.doStart(KafkaMessageListenerContainer.java:387) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.ConcurrentMessageListenerContainer.doStart(ConcurrentMessageListenerContainer.java:264) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.startIfNecessary(KafkaListenerEndpointRegistry.java:436) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.start(KafkaListenerEndpointRegistry.java:382) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:323) ~[spring-context-6.2.3.jar!/:6.2.3]
    ... 20 common frames omitted

Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:103) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:62) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:58) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:184) ~[kafka-clients-3.8.1.jar!/:na]
    ... 37 common frames omitted
```

I configured Kafka inside Kubernetes using this configuration:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:6.1.1
          ports:
            - containerPort: 9092
            - containerPort: 9094
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

            - name: KAFKA_LISTENERS
              value: ""INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094""
            - name: KAFKA_ADVERTISED_LISTENERS
              value: ""INTERNAL://$(POD_NAME).kafka-headless:9092,OUTSIDE://$(POD_NAME).kafka-svc:9094""
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: ""INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT""
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: ""3""
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: ""INTERNAL""
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: ""zookeeper:2181""
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: ""false""

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
spec:
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094
  type: ClusterIP
```

And for zookeeper:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - name: zookeeper
          image: wurstmeister/zookeeper
          ports:
            - containerPort: 2181
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
spec:
  selector:
    app: zookeeper
  ports:
    - protocol: TCP
      port: 2181
      targetPort: 2181
```

My `application.yml`:

```
spring:
  application:
    version: 1.0.0
    name: scpi-invest-plus-partner
  kafka:
    bootstrap-servers: kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: ""*""
```","spring-boot, kubernetes, apache-kafka, spring-kafka",79516909.0,"i can see two problems in you application config.

1. beacuse you have headless svc you can just pass the service name and k8s dns will help you to resolveit
2. the client communication port as i see is 9094 not 9092 wich is internal brokers communication.

so the right config can look like:

```
spring:
  kafka:
    bootstrap-servers: kafka-svc:9094
```

also if the application and the kafka brokers are not in the same namespace use

```
spring:
  kafka:
    bootstrap-servers: kafka-svc.namespace-name.svc:9094
```",2025-03-18T09:30:35,2025-03-18T01:27:53
79515635,How the K8s service port mapping works?,"I have successully configured an external load balancer service with Kind (Kubernetes in Docker) using kind-cloud-provider. I can access the service using http://localhost:63238

Here is my yaml file (pods + service) :

```
kind: Pod
apiVersion: v1
metadata:
  name: foo-app
  labels:
    app: http-echo
spec:
  containers:
    - command:
        - /agnhost
        - serve-hostname
        - --http=true
        - --port=8080
      image: registry.k8s.io/e2e-test-images/agnhost:2.39
      name: foo-app
---
kind: Pod
apiVersion: v1
metadata:
  name: bar-app
  labels:
    app: http-echo
spec:
  containers:
    - command:
        - /agnhost
        - serve-hostname
        - --http=true
        - --port=8080
      image: registry.k8s.io/e2e-test-images/agnhost:2.39
      name: bar-app
---
kind: Service
apiVersion: v1
metadata:
  name: foo-service
spec:
  type: LoadBalancer
  selector:
    app: http-echo
  ports:
    - port: 5678
      targetPort: 8080
```

As described above, the port of the service is 5678 and is mapped with the port of the pods 8080. But when I get the service info using kubectl it shows that port 5678 is mapped with 30755. I have no clue what this port is refering to.

[![kubectl get service](https://i.sstatic.net/M6EPE9ap.png)](https://i.sstatic.net/M6EPE9ap.png)

Endpoints are using 8080 :

[![kubectl get endpoint](https://i.sstatic.net/JfpXjYz2.png)](https://i.sstatic.net/JfpXjYz2.png)

I have explored my cluster using kubectl and Lens. I can see that the port 30755 also shows up in the kind-cloud-provider config :

[![kind-cloud-provider conf screenshot](https://i.sstatic.net/z1l1m195.png)](https://i.sstatic.net/z1l1m195.png)

the only way to actually know which port to use is to scroll the kind-cloud-provider, looking for this line matching my service port (5678).

[![kind-cloud-provider conf screenshot](https://i.sstatic.net/pqCnaZfg.png)](https://i.sstatic.net/pqCnaZfg.png)

Can anyone enlighten me on this mysterious whole port mapping thing ?
Especially on the 30755 showing up in my service ?
thanks :)","kubernetes, port, kind",79515925.0,"When using a LoadBalancer as a Service type in kubernetes, it [starts off by creating a NodePort service](https://kubernetes.io/docs/concepts/services-networking/service/#:%7E:text=Kubernetes%20typically%20starts%20off%20by%20making%20the%20changes%20that%20are%20equivalent%20to%20you%20requesting%20a%20Service%20of%20type%3A%20NodePort) in the background to facilitate communication, the control plane will allocate the port from a default range [port: 30000-32767](https://kubernetes.io/docs/concepts/services-networking/service/#nodeport-custom-port). Then, configures the external load balancer to forward traffic to the assigned service port by cloud-controller-manager.

If you want to [toggle this type of allocation](https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation) you may set the field as:

> ```
> spec:
>   allocateLoadBalancerNodePorts: #true or false
> ```",2025-03-17T22:56:54,2025-03-17T19:57:42
79515159,View Kubernetes deployments,"I would like to view Kubernetes deployments in the following format. Do I need to run a Json query.

![enter image description here](https://i.sstatic.net/IYDekJuW.png)

kubectl get deployments -n admin2406 -o wide

I am quite new to Kubernetes. I couldnt find this format structure even on the Kubernetes.io documentation.",kubernetes,79521057.0,"You can use `jsonpath` but `custom-columns` will is much simpler. Because you don't need to deal with handling foreach over all elements. In `jsonpath` you need to handle all pods in `k get pods`, via a kind of loop structure. Also remembering jsonpath syntax is not that easy. In `custom-columns` you think only for single row. Also It handles multiple items in a cell (like `.containers[*].name`) and no item in a cell `<none>` for you.

It's syntax is like below:

```
COLUMN_NAME     :       path.from.single.item

Examples:

NAMESPACE:.metadata.namespace

Name:.metadata.name

ContainerNames:.containers[*].name

...
```

To find what you need to select start with investigating fields:`k get deployment deployment-name -o yaml` you can give multiple columns via adding commas.

For sorting give the jsonpath what you want to sort. There's only ascending sort in kubectl.

Below is the command you need.

```
k get deployment --sort-by=.metadata.name -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[*].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace
```",2025-03-19T18:13:57,2025-03-17T16:23:30
79514019,Docker image running in k8s which needs gcloud auth credentials,"I need run a python app in docker container on k8s. This app needs to use gcloud stuff like `gemini-2.0-flash`, `google_vertexai` and

```
vertexai.init(project=os.environ.get(""GOOGLE_CLOUD_PROJECT""), location=os.environ.get(""GOOGLE_CLOUD_LOCATION""))
```

I can install the `gcloud` CLI in Dockerfile. What's next after that? Any reference?","kubernetes, dockerfile, gcloud, google-cloud-vertex-ai, google-gemini",79516428.0,"As commented by @DazWilkin, your issue could be resolved if you leverage the instructions to use [ADC for local development](https://cloud.google.com/docs/authentication/set-up-adc-local-dev-environment). Using your user credentials (Google Account) or impersonating a Service Account will create a key (on Linux in `${HOME}/.config/gcloud/application_default_credentials.json`) that you can (volume) mount into the container, then reference using the environment variable `GOOGLE_APPLICATION_CREDENTIALS`. You need only have **gcloud** installed on the host not the container.

Posting the answer as community wiki for the benefit of the community that might encounter this use case in the future. Feel free to edit this answer for additional information.",2025-03-18T06:04:46,2025-03-17T08:38:21
79513547,Task processing failed with error .... WorkerType ActivityWorker Error context deadline exceeded,"I have some temporal workflows running on a kubernetes pod. One of the workflows starts an activity, which is doing a bunch of BatchWriteItem calls to DynamoDB(close to 40k requests by an activity execution). A snipper of the activity BatchWriteItem logic code is below:

```
        av, err := attributevalue.MarshalMap(msg)
        if err != nil {
            return err
        }

        writeRequests = append(writeRequests, types.WriteRequest{
            PutRequest: &types.PutRequest{
                Item: av,
            },
        })

        if len(writeRequests) == maxBatchSize {
            input := &dynamodb.BatchWriteItemInput{
        RequestItems: map[string][]types.WriteRequest{
            repo.tableName: writeRequests,
                },
             }
        out, err := repo.store.BatchWriteItem(ctx, input)
        if err != nil {
            return err
        }
    }
```

When executing a workflow, this activity runs for around 10 minutes without any issues, and suddenly I see this error log in the pod

```
2025/03/17 00:02:40 INFO Task processing failed with error Namespace <namespace> TaskQueue <queue-name> WorkerID 1@kubernetes-worker-pod-name@ WorkerType ActivityWorker Error context deadline exceeded
```

I set the Temporal Activity StartToClose timeout to 2 hours, so that is not the reason for error here. I checked the temporal worker pod CPU and memory usage and they are within limits. I also noticed there are no throttles write requests in DynamoDB dashboard. Not sure what else is causing this issue. Can I get some pointers on the cause of this error?

Update:

I see this error in the Temporal Activity's Call stack

```
coroutine root [blocked on chan-2.Receive]:
go.temporal.io/sdk/internal.(*decodeFutureImpl).Get(0xc05ecce318, {0x18f01f8, 0xc0007f0300}, {0x13f11c0, 0xc05ecce180})
/go/src/app/vendor/go.temporal.io/sdk/internal/internal_workflow.go:1588 +0x3e
github.com/twilio-internal/comms-api-broadcast-internal-api/internal/temporal/workflows.IngressWorkflow({0x18f01f8?, 0xc0007f0240?}, {{0xc0001b8150, 0x22}, {0xc0001b8180, 0x2a}, {0xc0007a0060, 0x11}, {0xc00035c557, 0x5}})
/go/src/app/internal/temporal/workflows/ingress.go:44 +0x452
reflect.Value.call({0x1460c60?, 0x1746018?, 0x4158c5?}, {0x16d5a9a, 0x4}, {0xc0007f0270, 0x2, 0xc0007f0270?})
/usr/local/go/src/reflect/value.go:581 +0xca6
reflect.Value.Call({0x1460c60?, 0x1746018?, 0x7fefb9f2c208?}, {0xc0007f0270?, 0x46f49d?, 0x7ff000dfb878?})
/usr/local/go/src/reflect/value.go:365 +0xb9
go.temporal.io/sdk/internal.executeFunction({0x1460c60, 0x1746018}, {0xc0006b0480, 0x2, 0x1466620?})
/go/src/app/vendor/go.temporal.io/sdk/internal/internal_worker.go:1940 +0x26b
go.temporal.io/sdk/internal.(*workflowEnvironmentInterceptor).ExecuteWorkflow(0xc0006c8190, {0x18f01f8, 0xc0007f0210}, 0xc0006a24f8)
/go/src/app/vendor/go.temporal.io/sdk/internal/workflow.go:619 +0x150
go.temporal.io/sdk/interceptor.(*tracingWorkflowInboundInterceptor).ExecuteWorkflow(0xc000517860, {0x18f03f0, 0xc00050a600}, 0xc0006a24f8)
/go/src/app/vendor/go.temporal.io/sdk/interceptor/tracing_interceptor.go:449 +0x2ca
go.temporal.io/sdk/internal.(*workflowExecutor).Execute(0xc0007e2180, {0x18f03f0, 0xc00050a600}, 0xc000802440)
/go/src/app/vendor/go.temporal.io/sdk/internal/internal_worker.go:835 +0x28b
go.temporal.io/sdk/internal.(*syncWorkflowDefinition).Execute.func1({0x18f01f8, 0xc000517920})
/go/src/app/vendor/go.temporal.io/sdk/internal/internal_workflow.go:556 +0xc6
```

Not really sure what this error means in my context","go, kubernetes, amazon-dynamodb, temporal, temporal-workflow",,,,2025-03-17T02:30:52
79512788,Django Static Files Not Loading in Production (DEBUG=False) with Gunicorn on Kubernetes,"**Problem**

I'm running a Django application in a Kubernetes pod with Gunicorn, and my static files (admin panel CSS/JS) are not loading when DEBUG=False.

In local development, I use runserver, and everything works fine with DEBUG=True. However, when I set DEBUG=False, my static files return 404.

Error Logs (Browser Console / Django Logs)

```
GET /static/admin/css/base.efb520c4bb7c.css HTTP/1.1"" 404 179
GET /static/admin/js/nav_sidebar.7605597ddf52.js HTTP/1.1"" 404 179
```

Relevant Django Settings (settings.py)

```
DEBUG = False

INSTALLED_APPS = [
    ""django.contrib.staticfiles"",
    # Other apps...
]

MIDDLEWARE = [
    'django.middleware.security.SecurityMiddleware',
    'whitenoise.middleware.WhiteNoiseMiddleware',
    # Other middleware...
]

BASE_DIR = Path(__file__).resolve().parent.parent
STATIC_URL = '/static/'
STATICFILES_STORAGE = ""whitenoise.storage.CompressedManifestStaticFilesStorage""
STATICFILES_DIRS = [
    BASE_DIR / 'static',
]
STATIC_ROOT = BASE_DIR / 'staticfiles'
```

How I Run My App (Docker Entrypoint)

```
#!/bin/sh

APP_PORT=${PORT:-8000}

echo ""Migrating database...""
/opt/venv/bin/python manage.py migrate --noinput

echo ""Collecting static files...""
/opt/venv/bin/python manage.py collectstatic --noinput
echo ""Static files collected""

echo ""Starting server...""
/opt/venv/bin/gunicorn secureuri.wsgi:application --bind ""0.0.0.0:${APP_PORT}"" --workers 1 --access-logfile - --error-logfile - --log-level debug
```

**What I Tried**

- Checked that collectstatic is running.
- Running ls -la staticfiles/admin/css/ shows that files exist inside the container.
Tried running Django’s built-in dev server with DEBUG=False
- Running python manage.py runserver --insecure does not work
- Checked Gunicorn Logs
- Gunicorn is running fine, and there are no errors related to static files.
- Tried adding whitenoise ❌ (Didn’t work)

```
INSTALLED_APPS = [
    ""whitenoise.runserver_nostatic"",
    ""django.contrib.staticfiles"",
]

MIDDLEWARE = [
    ""whitenoise.middleware.WhiteNoiseMiddleware"",
]
```

- Restarted the pod, but static files still return 404.

**Question**

1. How can I serve static files correctly when DEBUG=False without using an external service like Nginx or S3?
2. Is there something I’m missing in my Gunicorn/Kubernetes setup?","python, django, kubernetes, deployment, django-admin",,,,2025-03-16T15:30:10
79512596,"Microsoft Teams App Not Loading After Deployment in Kubernetes, but Logs Show in Console","I have built a custom Microsoft Teams app using TypeScript React and deployed it inside a Kubernetes (K8s) pod. When I upload the manifest to Microsoft Teams, I don't see the app in the UI. However, when I inspect the console logs (F12 in Teams), I can see logs coming from the app, meaning it is loading in the background.

When i run the app locally everything works fine.

**Manifest.json**

```
{
  ""$schema"": ""https://developer.microsoft.com/en-us/json-schemas/teams/v1.19/MicrosoftTeams.schema.json"",
  ""manifestVersion"": ""1.19"",
  ""version"": ""1.2.0"",
  ""id"": ""32e8e5ff-cfee-4782-a059-7e0ed215a3a4"",
  ""developer"": {
    ""name"": ""ExampleB.V"",
    ""websiteUrl"": ""https://example.com/"",
    ""privacyUrl"": ""https://example.com/privacy-statement/"",
    ""termsOfUseUrl"": ""https://example.com/privacy-statement/""
  },
  ""staticTabs"": [
    {
      ""entityId"": ""index"",
      ""name"": ""Home"",
      ""contentUrl"": ""https://example.com/index.html#/tab"",
      ""websiteUrl"": ""https://example.com/index.html#/tab"",
      ""scopes"": [""personal"", ""groupChat"", ""team""]
    }
  ],
  ""validDomains"": [
    ""*.example.com"",

  ],
  ""webApplicationInfo"": {
    ""id"": ""32e8e5ff-cfee-4782-a059-7e0ed215a3a4"",
    ""resource"": ""api://example.com/32e8e5ff-cfee-4782-a059-7e0ed215a3a4""
  }
}
```

## What I Have Tried:

✅ **Checked logs in Teams Console (`F12`)**

- Logs from the app are visible, but the UI doesn’t appear.

✅ **Verified HTTPS Access**

- When I open `https://example.com/index.html#/tab` in a browser, the app loads fine.

✅ **Added Domain to `validDomains`**

- `example.com` is already listed.

✅ **Checked for CSP Errors (`F12` in Teams Console)**

- No explicit CSP errors, but I'm not sure if it's blocked due to iframe restrictions.

✅ **Checked Kubernetes Deployment & Service**

- The pod is running correctly, and logs show that requests are reaching the server.

## Possible Issues & Questions:

1. **Is Microsoft Teams blocking my app due to iframe security policies?**
  - Do I need to explicitly allow `https://teams.microsoft.com` in my `Content-Security-Policy` headers?
2. **Is my Kubernetes ingress configuration missing something?**
  - How can I confirm that the service is properly routing traffic inside Kubernetes?
3. **Is there an issue with `webApplicationInfo.resource`?**
  - Does this value affect the app's visibility in Teams?
4. **Are there any specific Teams permissions required for rendering static tabs?**","reactjs, kubernetes, microsoft-teams, teams-toolkit, microsoft-teams-js",79559187.0,It seems your issue might be related to HTTPS ingress or TLS configuration in Kubernetes. Check out this guide on [Deploying Teams tab apps to Kubernetes](https://learn.microsoft.com/en-us/microsoftteams/platform/toolkit/deploy-teams-app-to-container-service#deploy-teams-tab-app-to-kubernetes).,2025-04-07T06:20:05,2025-03-16T13:14:05
79512339,"Python Flask-Healthz should I use Blueprint, Flask configuration and/or Entension?","It's unclear and confusing to me whether Blueprint and Flask configration is required if I use the extension `Healthz(app)`!?! The confusion comes from the README line:

```
The rest of the configuration is identical.
```

I get the following error!

```
Readiness probe failed: Get ""https://10.1.207.232:443/healthz/ready"": dial tcp 10.1.207.232:443: connect: connection refused
```","kubernetes, flask, python-3.12, readinessprobe, livenessprobe",79549633.0,This is a very trivial / insignificant library and it doesn't work with Quart. I ditched it and implement the heathcheck endpoints using `blueprint`,2025-04-02T03:57:42,2025-03-16T09:14:45
79511095,"When creating a container, the error &quot;secret not found&quot; appears","I'm trying to use Vault CSI Provider to get secrets and HCP Vault

Made the following policy and role on terraform:

```
resource ""vault_policy"" ""n8n"" {
    name = ""n8n""
    policy = <<EOF
path ""secret/data/n8n"" {
    capabilities = [""read""]
}
    EOF
}

resource ""vault_kubernetes_auth_backend_role"" ""n8n"" {
    bound_service_account_names = [""n8n-sa""]
    bound_service_account_namespaces = [""n8n""]
    role_name = ""n8n""
    token_ttl = 3600
    token_policies = [vault_policy.n8n.name]
}
```

Initialization of the CSI provider looks like this:

```
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: n8n-vault-creds
  namespace: n8n
spec:
  provider: vault
  parameters:
    vaultAddress: ""http://vault.vault.svc:8200""
    roleName: ""n8n""
    objects: |
      - objectName: ""DB_POSTGRESDB_DATABASE""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_DATABASE""
      - objectName: ""DB_POSTGRESDB_HOST""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_HOST""
      - objectName: ""DB_POSTGRESDB_PASSWORD""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_PASSWORD""
      - objectName: ""DB_POSTGRESDB_PORT""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_PORT""
      - objectName: ""DB_POSTGRESDB_SCHEMA""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_SCHEMA""
      - objectName: ""DB_POSTGRESDB_USER""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_USER""
      - objectName: ""DB_TYPE""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_TYPE""
      - objectName: ""GENERIC_TIMEZONE""
        secretPath: ""secret/data/n8n""
        secretKey: ""GENERIC_TIMEZONE""
      - objectName: ""TZ""
        secretPath: ""secret/data/n8n""
        secretKey: ""TZ""
  secretObjects:
    - secretName: vault-n8n-creds-secret
      type: Opaque
      data:
        - objectName: DB_POSTGRESDB_DATABASE
          key: DB_POSTGRESDB_DATABASE
        - objectName: DB_POSTGRESDB_HOST
          key: DB_POSTGRESDB_HOST
        - objectName: DB_POSTGRESDB_PASSWORD
          key: DB_POSTGRESDB_PASSWORD
        - objectName: DB_POSTGRESDB_PORT
          key: DB_POSTGRESDB_PORT
        - objectName: DB_POSTGRESDB_SCHEMA
          key: DB_POSTGRESDB_SCHEMA
        - objectName: DB_POSTGRESDB_USER
          key: DB_POSTGRESDB_USER
        - objectName: DB_TYPE
          key: DB_TYPE
        - objectName: GENERIC_TIMEZONE
          key: GENERIC_TIMEZONE
        - objectName: TZ
          key: TZ
```

Deployments + ServiceAccount:

```
---
kind: ServiceAccount
apiVersion: v1
metadata:
  namespace: n8n
  name: n8n-sa
  labels:
    app: n8n

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n-deployment
  namespace: n8n
  labels:
    app: n8n
spec:
  replicas: 1
  selector:
    matchLabels:
      app: n8n
  template:
    metadata:
      labels:
        app: n8n
    spec:
      serviceAccountName: n8n-sa
      containers:
      - name: n8n
        image: n8nio/n8n:1.79.0
        volumeMounts:
          - name: n8n-data
            mountPath: /home/node/.n8n
          - name: 'n8n-vault-creds'
            mountPath: '/mnt/n8n-secrets-store'
            readOnly: true
        ports:
          - containerPort: 5678
            protocol: TCP
        env:
          - name: DB_TYPE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_TYPE
          - name: DB_POSTGRESDB_DATABASE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_DATABASE
          - name: DB_POSTGRESDB_HOST
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_HOST
          - name: DB_POSTGRESDB_PORT
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_PORT
          - name: DB_POSTGRESDB_USER
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_USER
          - name: DB_POSTGRESDB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_PASSWORD
          - name: DB_POSTGRESDB_SCHEMA
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_SCHEMA
          - name: GENERIC_TIMEZONE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: GENERIC_TIMEZONE
          - name: TZ
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: TZ
      volumes:
        - name:  n8n-data
          persistentVolumeClaim:
            claimName: n8n
        - name: n8n-vault-creds
          csi:
            driver: 'secrets-store.csi.k8s.io'
            readOnly: true
            volumeAttributes:
              secretProviderClass: 'n8n-vault-creds'
```

I went through all the documentation from the site, I don't understand what the problem might be:

> UPD: kubernetes authentication is enabled. An error appears when
> creating deployments:

```
Warning  Failed     7s (x2 over 8s)  kubelet            Error: secret ""vault-n8n-creds-secret"" not found
```","kubernetes, devops, hashicorp-vault, vault",79514480.0,"Make sure the Vault CSI provider is correctly mounted and that the Kubernetes auth method is configured.

***Make sure that the Kubernetes auth method is enabled and properly configured in Vault:***

```
vault auth list
```

***Check the secret is being created in Kubernetes:***

```
kubectl get secret vault-n8n-creds-secret -n n8n
```",2025-03-17T11:45:37,2025-03-15T12:04:42
79511095,"When creating a container, the error &quot;secret not found&quot; appears","I'm trying to use Vault CSI Provider to get secrets and HCP Vault

Made the following policy and role on terraform:

```
resource ""vault_policy"" ""n8n"" {
    name = ""n8n""
    policy = <<EOF
path ""secret/data/n8n"" {
    capabilities = [""read""]
}
    EOF
}

resource ""vault_kubernetes_auth_backend_role"" ""n8n"" {
    bound_service_account_names = [""n8n-sa""]
    bound_service_account_namespaces = [""n8n""]
    role_name = ""n8n""
    token_ttl = 3600
    token_policies = [vault_policy.n8n.name]
}
```

Initialization of the CSI provider looks like this:

```
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: n8n-vault-creds
  namespace: n8n
spec:
  provider: vault
  parameters:
    vaultAddress: ""http://vault.vault.svc:8200""
    roleName: ""n8n""
    objects: |
      - objectName: ""DB_POSTGRESDB_DATABASE""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_DATABASE""
      - objectName: ""DB_POSTGRESDB_HOST""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_HOST""
      - objectName: ""DB_POSTGRESDB_PASSWORD""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_PASSWORD""
      - objectName: ""DB_POSTGRESDB_PORT""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_PORT""
      - objectName: ""DB_POSTGRESDB_SCHEMA""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_SCHEMA""
      - objectName: ""DB_POSTGRESDB_USER""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_POSTGRESDB_USER""
      - objectName: ""DB_TYPE""
        secretPath: ""secret/data/n8n""
        secretKey: ""DB_TYPE""
      - objectName: ""GENERIC_TIMEZONE""
        secretPath: ""secret/data/n8n""
        secretKey: ""GENERIC_TIMEZONE""
      - objectName: ""TZ""
        secretPath: ""secret/data/n8n""
        secretKey: ""TZ""
  secretObjects:
    - secretName: vault-n8n-creds-secret
      type: Opaque
      data:
        - objectName: DB_POSTGRESDB_DATABASE
          key: DB_POSTGRESDB_DATABASE
        - objectName: DB_POSTGRESDB_HOST
          key: DB_POSTGRESDB_HOST
        - objectName: DB_POSTGRESDB_PASSWORD
          key: DB_POSTGRESDB_PASSWORD
        - objectName: DB_POSTGRESDB_PORT
          key: DB_POSTGRESDB_PORT
        - objectName: DB_POSTGRESDB_SCHEMA
          key: DB_POSTGRESDB_SCHEMA
        - objectName: DB_POSTGRESDB_USER
          key: DB_POSTGRESDB_USER
        - objectName: DB_TYPE
          key: DB_TYPE
        - objectName: GENERIC_TIMEZONE
          key: GENERIC_TIMEZONE
        - objectName: TZ
          key: TZ
```

Deployments + ServiceAccount:

```
---
kind: ServiceAccount
apiVersion: v1
metadata:
  namespace: n8n
  name: n8n-sa
  labels:
    app: n8n

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: n8n-deployment
  namespace: n8n
  labels:
    app: n8n
spec:
  replicas: 1
  selector:
    matchLabels:
      app: n8n
  template:
    metadata:
      labels:
        app: n8n
    spec:
      serviceAccountName: n8n-sa
      containers:
      - name: n8n
        image: n8nio/n8n:1.79.0
        volumeMounts:
          - name: n8n-data
            mountPath: /home/node/.n8n
          - name: 'n8n-vault-creds'
            mountPath: '/mnt/n8n-secrets-store'
            readOnly: true
        ports:
          - containerPort: 5678
            protocol: TCP
        env:
          - name: DB_TYPE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_TYPE
          - name: DB_POSTGRESDB_DATABASE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_DATABASE
          - name: DB_POSTGRESDB_HOST
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_HOST
          - name: DB_POSTGRESDB_PORT
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_PORT
          - name: DB_POSTGRESDB_USER
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_USER
          - name: DB_POSTGRESDB_PASSWORD
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_PASSWORD
          - name: DB_POSTGRESDB_SCHEMA
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: DB_POSTGRESDB_SCHEMA
          - name: GENERIC_TIMEZONE
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: GENERIC_TIMEZONE
          - name: TZ
            valueFrom:
              secretKeyRef:
                name: vault-n8n-creds-secret
                key: TZ
      volumes:
        - name:  n8n-data
          persistentVolumeClaim:
            claimName: n8n
        - name: n8n-vault-creds
          csi:
            driver: 'secrets-store.csi.k8s.io'
            readOnly: true
            volumeAttributes:
              secretProviderClass: 'n8n-vault-creds'
```

I went through all the documentation from the site, I don't understand what the problem might be:

> UPD: kubernetes authentication is enabled. An error appears when
> creating deployments:

```
Warning  Failed     7s (x2 over 8s)  kubelet            Error: secret ""vault-n8n-creds-secret"" not found
```","kubernetes, devops, hashicorp-vault, vault",79514389.0,"You should check if the secret ""vault-n8n-creds-secret"" really exists in the Namespace ""n8n"". (kubectl get secret -n n8n)

Check also events of the SecretProviderClass and watch out for errors. Maybe secret ""vault-n8n-creds-secret"" cannot be created for some reason.

In addition to that check wether the Role ""n8n"" has permissions to read Secrets. (kubectl get role -n n8n).",2025-03-17T11:05:05,2025-03-15T12:04:42
79510986,how to connect to open telemetry operator in kubernetes,"I have this yaml file defining a `open-telemetry-collector` in a kubernetes cluster. I have already installed `open-telemetry opertor`. This is the file:

```
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  namespace: observability
  name: hangout-collector
spec:
  config:
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318

    processors:
      batch:
        send_batch_size: 10000
        timeout: 10s

    exporters:
      debug:
        verbosity: detailed
      prometheus:
        endpoint: 0.0.0.0:8889
        const_labels:
          origin: otel-collector
      otlp:
        endpoint: tempo:4317
        tls:
          insecure: true
      otlphttp/loki:
        endpoint: ""http://loki:3100/loki/api/v1/push""

    service:
      pipelines:
        metrics:
          receivers: [otlp]
          processors: [batch]
          exporters: [prometheus]
        traces:
          receivers: [otlp]
          processors: [batch]
          exporters: [otlp]
        logs:
          receivers: [otlp]
          exporters: [otlphttp/loki]
```

I see these services being created in my cluster:
[![Running Services](https://i.sstatic.net/ptHYNsfg.png)](https://i.sstatic.net/ptHYNsfg.png)

I'm trying to connect to the collector from a java application using this url:
[http://hangout-collector-collector.observability.svc.cluster.local:4318](http://hangout-collector-collector.observability.svc.cluster.local:4318) but I'm getting errors that connection has been refused.","kubernetes, google-kubernetes-engine, open-telemetry, open-telemetry-collector",,,,2025-03-15T10:41:11
79510780,Python 3 relative imports behave differently between local run and on k8s,"I have the following structure:

```
project/
  |- src/
       |- __init__.py
       |- package/
             |- __init__.py
             |- module1.py
             |- module2.py
```

Let's say `module1.py` tries to import a symbol from `module2`:

```
from .module2 import MySymbol
```

Run it locally with `python <filename>.py` and this hits the error: `ImportError: attempted relative import with no known parent package` whether I run it from `project` or from `project/src/package`. I try:

```
from src.package.module2 import MySymbol
```

and it complains no module named src...

When it runs in a k8s cluster, it hits the error without a `.` prefix:

```
ModuleNotFoundError: No module named 'module1'
```

This is a dilemma for me because importing local module in the same folder using `.` prefix works when the app runs in k8s pod. However, this doesn't run locally. How to get the best of these 2 worlds?","kubernetes, python-module, python-3.12, relative-import",79514939.0,"You should be running your script from the `project` folder - as in:

```
$ cd /path/to/project
$ python -m src.package.module1 # or whatever filename is but *without* the .py
```

Then both absolute and relative import forms will work.",2025-03-17T14:54:12,2025-03-15T07:57:33
79510650,Why does ClientSet in client-go fail to deploy a Deployment?,"When I attempt to use the client-go client to apply a Deployment, I encounter the error ""invalid object type: /, Kind="". Interestingly, if I use the create method instead, the Deployment is created successfully without any issues, and the code compiles perfectly fine. This discrepancy suggests that there might be a problem with how the Apply method is being used or how the object is being configured for Apply.

I’ve thoroughly reviewed the official Kubernetes and client-go documentation but haven’t found any specific guidance or examples that address this particular issue. It’s unclear why the Apply method fails while create works as expected. I suspect it might be related to how the ApplyConfiguration object is structured or how the FieldManager is being set, but I’m unable to pinpoint the exact cause.

If anyone has encountered a similar issue or has insights into why this might be happening, I would greatly appreciate your help. Any suggestions or solutions would be incredibly valuable. Thank you so much in advance for your assistance!

**main.go**

```
package main

import (
    ""context""
    ""flag""
    ""fmt""
    ""log""
    ""path/filepath""
    ""time""

    metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""
    applyappsv1 ""k8s.io/client-go/applyconfigurations/apps/v1""
    applycorev1 ""k8s.io/client-go/applyconfigurations/core/v1""
    applymetav1 ""k8s.io/client-go/applyconfigurations/meta/v1""
    ""k8s.io/client-go/kubernetes""
    ""k8s.io/client-go/tools/clientcmd""
    ""k8s.io/client-go/util/homedir""
    ""k8s.io/utils/pointer""
)

func main() {
    var kubeconfig *string
    if home := homedir.HomeDir(); home != """" {
        kubeconfig = flag.String(""kubeconfig"", filepath.Join(home, "".kube"", ""config""), ""(optional) absolute path to the kubeconfig file"")
    } else {
        kubeconfig = flag.String(""kubeconfig"", """", ""absolute path to the kubeconfig file"")
    }
    flag.Parse()

    config, err := clientcmd.BuildConfigFromFlags("""", *kubeconfig)
    if err != nil {
        panic(err.Error())
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        panic(err.Error())
    }

    deploymentName := ""test-deployment""
    namespace := ""default""

    deploymentApplyConfig := applyappsv1.DeploymentApplyConfiguration{
        ObjectMetaApplyConfiguration: &applymetav1.ObjectMetaApplyConfiguration{
            Name:      &deploymentName,
            Namespace: &namespace,
        },
        Spec: &applyappsv1.DeploymentSpecApplyConfiguration{
            Replicas: pointer.Int32(3),
            Selector: &applymetav1.LabelSelectorApplyConfiguration{
                MatchLabels: map[string]string{
                    ""app"": ""test-app"",
                },
            },
            Template: &applycorev1.PodTemplateSpecApplyConfiguration{
                ObjectMetaApplyConfiguration: &applymetav1.ObjectMetaApplyConfiguration{
                    Labels: map[string]string{
                        ""app"": ""test-app"",
                    },
                },
                Spec: &applycorev1.PodSpecApplyConfiguration{
                    Containers: []applycorev1.ContainerApplyConfiguration{
                        {
                            Name:  pointer.String(""nginx""),
                            Image: pointer.String(""nginx:1.14.2""),
                            Ports: []applycorev1.ContainerPortApplyConfiguration{
                                {
                                    ContainerPort: pointer.Int32(80),
                                },
                            },
                        },
                    },
                },
            },
        },
    }

    fmt.Println(""Applying Deployment..."")
    appliedDeployment, err := clientset.AppsV1().Deployments(namespace).Apply(
        context.TODO(),
        &deploymentApplyConfig,
        metav1.ApplyOptions{FieldManager: ""test-client""},
    )
    if err != nil {
        log.Fatalf(""Error applying Deployment: %s"", err)
    }
    fmt.Printf(""Applied Deployment %q\n"", appliedDeployment.Name)
}
```

**Output:**

```
Applying Deployment...
2025/03/15 12:48:06 Error applying Deployment: invalid object type: /, Kind=
exit status 1
```

**Versions:**

```
go version: 1.23.0
client-go version: 0.32.0
k8s-version: 1.32.0
```","go, kubernetes, client-go",79525968.0,"The simplest way to correct your code is to add `TypeMetaApplyConfiguration` to your struct:

```
deploymentApplyConfig := applyappsv1.DeploymentApplyConfiguration{
    TypeMetaApplyConfiguration: applymetav1.TypeMetaApplyConfiguration{
        Kind:       pointer.String(""Deployment""),
        APIVersion: pointer.String(""apps/v1""),
    },
    ObjectMetaApplyConfiguration: &applymetav1.ObjectMetaApplyConfiguration{
        Name:      &deploymentName,
        Namespace: &namespace,
    },
    ...
```

But, per my comment, there are methods to help with the construction and it may be preferable to use these:

```
deploymentName := ""test-deployment""
namespace := ""default""

labels := map[string]string{
    ""app"": ""test-app"",
}

containerName := ""nginx""
image := ""docker.io/nginx:1.14.2""
port := int32(8080)

deploymentApplyConfig := applyappsv1.Deployment(
    deploymentName,
    namespace,
).WithSpec(
    applyappsv1.DeploymentSpec().WithReplicas(3).WithSelector(
        applymetav1.LabelSelector().WithMatchLabels(labels),
    ).WithTemplate(
        applycorev1.PodTemplateSpec().WithLabels(labels).WithSpec(
            applycorev1.PodSpec().WithContainers(
                applycorev1.Container().WithName(containerName).WithImage(image).WithPorts(applycorev1.ContainerPort().WithContainerPort(port)),
            ),
        ),
    ),
)
```",2025-03-21T15:52:31,2025-03-15T05:11:13
79509708,Azure Kubernetes Services: LoadBalancer Inbound Connection Issues,"fairly new to Kubernetes in general but also Azure Kubernetes Services. I have a single cluster with a telemetry asterix adapter service/pod that is designed to ingest UDP data from ADSB sensors via a public IP circuit. I created a public IP and LoadBalancer service on my cluster in the same namespace using a generic YAML provided by Microsoft (modified slightly for this projects requirements) and deployed. Will post YAML below.

I am able to ping the public IP generated via the YAML and the circuit with the ADSB sensor has been set up via the IP provided via the contractor, but not seeing any packets in logs for my telemetry asterix adapter pod. I am using source port of 1025 and target port of 6000 and that is what the telemetry asterix adapter is using via NettyUDP. I believe the connection between the loadbalancer service, and that pod is set with the selector in the YAML.

Is there something that I am missing? I assume that the loadbalancer service is not connected to the desired pod as I don't see anything in the logs but am able to ping the IP.

```
kind: Service
apiVersion: v1
metadata:
  name: telemetry-asterix-adapter-svc
  namespace: utm
  uid: fac3e2f1-50e1-49f3-9624-2b49fe5bec39
  resourceVersion: '15394560'
  creationTimestamp: '2025-03-04T18:39:58Z'
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: >
      {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""name"":""telemetry-asterix-adapter-svc"",""namespace"":""utm""},""spec"":{""loadBalancerSourceRanges"":[""71.###.###.###/32"",""71.###.###.###/32""],""ports"":[{""port"":1025,""protocol"":""UDP"",""targetPort"":6000}],""selector"":{""app"":""telemetry-asterix-adapter""},""type"":""LoadBalancer""}}
  finalizers:
    - service.kubernetes.io/load-balancer-cleanup
  managedFields:
    - manager: cloud-controller-manager
      operation: Update
      apiVersion: v1
      time: '2025-03-11T16:08:39Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:finalizers:
            .: {}
            v:""service.kubernetes.io/load-balancer-cleanup"": {}
        f:status:
          f:loadBalancer:
            f:ingress: {}
      subresource: status
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: v1
      time: '2025-03-11T20:13:05Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
        f:spec:
          f:allocateLoadBalancerNodePorts: {}
          f:externalTrafficPolicy: {}
          f:internalTrafficPolicy: {}
          f:loadBalancerSourceRanges: {}
          f:ports:
            .: {}
            k:{""port"":1025,""protocol"":""UDP""}:
              .: {}
              f:port: {}
              f:protocol: {}
              f:targetPort: {}
          f:selector: {}
          f:sessionAffinity: {}
          f:type: {}
spec:
  ports:
    - protocol: UDP
      port: 1025
      targetPort: 6000
      nodePort: 31780
  selector:
    app: telemetry-asterix-adapter
  clusterIP: 10.0.203.107
  clusterIPs:
    - 10.0.203.107
  type: LoadBalancer
  sessionAffinity: None
  loadBalancerSourceRanges:
    - 71.###.###.###/32
    - 71.###.###.###/32
  externalTrafficPolicy: Cluster
  ipFamilies:
    - IPv4
  ipFamilyPolicy: SingleStack
  allocateLoadBalancerNodePorts: true
  internalTrafficPolicy: Cluster
status:
  loadBalancer:
    ingress:
      - ip: 62.##.##.###
        ipMode: VIP
```

Pod Manifest:

```
 Name:             telemetry-asterix-adapter-f8bb6f48d-2mqf6
Namespace:        utm
Priority:         0
Service Account:  default
Node:             aks-nodepool1-25615987-vmss000001/10.64.80.12
Start Time:       Thu, 13 Mar 2025 13:09:14 +0000
Labels:           app=telemetry-asterix-adapter
                  pod-template-hash=f8bb6f48d
Annotations:      kubectl.kubernetes.io/restartedAt: 2025-03-13T13:09:13Z
Status:           Running
IP:               10.64.82.134
IPs:
  IP:           10.64.82.134
Controlled By:  ReplicaSet/telemetry-asterix-adapter-f8bb6f48d
Containers:
  telemetry-asterix-adapter:
    Container ID:   containerd://88a01df213e0ec4732dee857798f61d73e9296b9f24ab4b1f61d7a6425c75e93
    Image:          crfusademousgv634.azurecr.us/utm-services/telemetry-asterix:3.5.0
    Image ID:       crfusademousgv634.azurecr.us/utm-services/telemetry-asterix@sha256:4c44d3b8946c6cecaa28d6637104b3f336776a4062f372a33a53238cec3a132f
    Ports:          6000/UDP, 8080/TCP
    Host Ports:     0/UDP, 0/TCP
    State:          Running
      Started:      Thu, 13 Mar 2025 13:09:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  512Mi
    Requests:
      memory:  512Mi
    Environment Variables from:
      telemetry-asterix-adapter  ConfigMap  Optional: false
    Environment:                 <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g6r4q (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-g6r4q:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
```

Logs from pod that I am hoping to ingest UDP data with:

```
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.9)

2025-03-13 13:09:20.002  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Starting AsterixAdapterApp v3.5.0 using Java 11.0.16 on telemetry-asterix-adapter-f8bb6f48d-2mqf6 with PID 1 (/opt/adapter/adapter.jar started by ? in /opt/adapter)
2025-03-13 13:09:20.018 DEBUG 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Running with Spring Boot v2.7.9, Spring v5.3.25
2025-03-13 13:09:20.019  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : No active profile set, falling back to 1 default profile: ""default""
2025-03-13 13:09:26.636  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2025-03-13 13:09:26.682  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-03-13 13:09:26.683  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.71]
2025-03-13 13:09:26.968  INFO 1 --- [           main] a.c.c.C.[.[.[/telemetry-asterix-adapter] : Initializing Spring embedded WebApplicationContext
2025-03-13 13:09:26.969  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 6760 ms
2025-03-13 13:09:28.505  INFO 1 --- [           main] c.f.s.t.asterixadapter.grpc.GrpcClient   : Create gRPC client at address: telemetry-manager-ng.utm.svc.cluster.local:8081
2025-03-13 13:09:38.298  INFO 1 --- [           main] o.a.c.c.s.CamelHttpTransportServlet      : Initialized CamelHttpTransportServlet[name=CamelServlet, contextPath=/telemetry-asterix-adapter]
2025-03-13 13:09:38.304  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path '/telemetry-asterix-adapter'
2025-03-13 13:09:40.204  INFO 1 --- [           main] o.a.c.component.netty.NettyComponent     : Creating shared NettyConsumerExecutorGroup with 3 threads
2025-03-13 13:09:40.551  INFO 1 --- [           main] c.n.SingleUDPNettyServerBootstrapFactory : ConnectionlessBootstrap binding to 0.0.0.0:6000
2025-03-13 13:09:40.837  INFO 1 --- [           main] o.a.camel.component.netty.NettyConsumer  : Netty consumer bound to: 0.0.0.0:6000
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   : Routes startup (total:2 started:2)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   :     Started route1 (netty://UDP://0.0.0.0:6000)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   :     Started route2 (rest://post:telemetry)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   : Apache Camel 3.14.1 (camel-1) started in 2s483ms (build:178ms init:1s560ms start:745ms)
2025-03-13 13:09:40.980  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Started AsterixAdapterApp in 23.126 seconds (JVM running for 25.782)
```

I have tried modifying the YAML and updating the loadbalancer service, removing the whitelist on source IP, tried sending test UDP packets via another device, some modification of NSGs...

Expecting to see at least some data in the logs for the pod showing incoming UDP packets.","azure, kubernetes, load-balancing, kubernetes-ingress, azure-aks",79552610.0,"Azure Load Balancer does not validate UDP health natively. Without a TCP port for health checks, the backend pool may be marked as unhealthy, even if the pod is up. UDP traffic is connectionless, so debugging requires low-level inspection or packet logging. The issue is the pod never actually received the UDP packets due to the above reason.

To overcome this, and to expose a UDP service behind AKS Load Balancer, you can expose a dummy TCP port (like 8080) on the pod. This allows the Azure Load Balancer to consider the backend healthy. Your actual UDP-based app can still bind to 6000 as usual. The TCP port (even if unused by your app) just ensures Azure forwards traffic to the pod.

```
ports:
  - containerPort: 6000
    protocol: UDP
  - containerPort: 8080
    protocol: TCP
```

LoadBalancer service YAML should expose both UDP and TCP

```
apiVersion: v1
kind: Service
metadata:
  name: telemetry-asterix-adapter-svc
  namespace: utm
spec:
  type: LoadBalancer
  selector:
    app: telemetry-asterix-adapter
  externalTrafficPolicy: Cluster
  ports:
    - name: udp-port
      protocol: UDP
      port: 1025
      targetPort: 6000
    - name: health-port
      protocol: TCP
      port: 8080
      targetPort: 8080
```

Since Netty logs may not show raw UDP activity easily, you can validate this using a simple Alpine pod with  `socat`

```
apiVersion: v1
kind: Pod
metadata:
  name: udp-echo-server
  namespace: udp-test
  labels:
    app: udp-echo
spec:
  containers:
  - name: udp-echo
    image: alpine
    command: [""/bin/sh""]
    args: [""-c"", ""apk add --no-cache socat && socat -v UDP-RECV:6000 STDOUT""]
    ports:
      - containerPort: 6000
        protocol: UDP
      - containerPort: 8080
        protocol: TCP
```

![enter image description here](https://i.imgur.com/nMwsJSe.png)

![enter image description here](https://i.imgur.com/qR7reaA.png)

![enter image description here](https://i.imgur.com/EqPffUq.png)

Then expose it with a Load Balancer service

![enter image description here](https://i.imgur.com/iFUQ2Z6.png)

and test the UDP ingestion as below-

```
kubectl run udp-client --rm -it --image=busybox --restart=Never --namespace=udp-test -- /bin/sh
echo ""hello after socat fix"" | nc -u <LB_PUBLIC_IP> 1025
```

![enter image description here](https://i.imgur.com/OGJRQVP.png)

you can confirm the message in pod logs using-

```
kubectl logs -n udp-test udp-echo-server
```

![enter image description here](https://i.imgur.com/11b13Hj.png)

Looks good.

Once you add the TCP port for health probes, UDP packets will start flowing through and your application received them without any other change as you can see in my example.",2025-04-03T10:07:24,2025-03-14T16:37:05
79509450,Apache Spark operator ingress for driver,"Apache Spark operator recently added [ingress](https://github.com/apache/spark-kubernetes-operator/blob/629d16783655aa8c2532b45270b81b4d107db72e/docs/spark_custom_resources.md#enable-additional-ingress-for-driver) for driver.

The example is a just a snippet which when added to `SparkApplication` results in this error:

`Error from server (BadRequest): error when creating ""templates/objective-scala.yaml"": SparkApplication in version ""v1alpha1"" cannot be handled as a SparkApplication: strict decoding error: unknown field ""spec.driverServiceIngressList""`

Does anyone have a full .yaml example? Thx","apache-spark, kubernetes",,,,2025-03-14T15:03:26
79509230,ActiveMQ Artemis: Primary Pod Restart Loop with Shared Store HA,"I am running ActiveMQ Artemis on Kubernetes and trying to configure high availability (HA) with shared storage. However, I am facing an issue where the primary pod goes into a restart loop after enabling the shared store HA policy.

My question is an extension of [this one](https://stackoverflow.com/questions/79018253/activemq-artemis-master-primary-pod-goes-to-restart-loop-after-change-to-share), as I am experiencing the same issue but have also experimented with an alternative setup.

**What I Tried**

Configured HA with shared store:

Primary Pod

```
<ha-policy>
    <shared-store>
        <primary>
            <failover-on-shutdown>true</failover-on-shutdown>
        </primary>
    </shared-store>
</ha-policy>
```

Secondary Pod

```
<ha-policy>
    <shared-store>
        <backup>
            <allow-failback>false</allow-failback>
            <failover-on-shutdown>true</failover-on-shutdown>
        </backup>
    </shared-store>
</ha-policy>
```

Observed Issue:

```
ERROR [org.apache.activemq.artemis.core.server] AMQ222010: Critical IO Error, shutting down the server. file=Lost NodeManager lock, message=NULL
java.io.IOException: lost lock
```

What change I tried:

Tested Running without HA Policy but in a Clustered Mode:

- Instead of defining an HA policy, I simply booted two clustered Artemis nodes using the same PVC (Persistent Volume Claim) for data storage.
- Behavior Observed:
  - One pod becomes active while the other becomes passive.
  - This resembles an active-passive setup, even though no HA policy is explicitly defined.

**Questions:**

1. Why does the shared store HA setup cause the ""Lost NodeManager lock"" error, but a simple clustered setup with shared storage works fine?
2. If I continue using a clustered setup without an HA policy but with shared storage, is this an acceptable and recommended approach?
3. What are the risks of running a clustered ActiveMQ Artemis setup with shared storage but without an HA policy?","kubernetes, activemq-artemis",79509439.0,"You see ""Lost NodeManager lock"" when using a `shared-store` `ha-policy` because that configuration causes the broker to actively monitor the shared file lock while the broker is running.

Without a `shared-store` `ha-policy` your primary broker might lose the shared file lock without realizing it in which case the backup would activate and both the primary and the backup would be operating simultaneously (i.e. split brain). Therefore, I would not recommend a simple clustered setting using shared storage *without* a `shared-store` `ha-policy`.

I recommend you inspect the configuration and features of the shared storage device to ensure it is able to support exclusive shared file locks. I also recommend you monitor the shared storage device to ensure there are no intermittent problems that would cause the primary broker to lose its lock.

You can [enable `TRACE` logging for `org.apache.activemq.artemis.core.server.impl.FileLockNodeManager`](https://activemq.apache.org/components/artemis/documentation/latest/logging.html#configuring-a-specific-level-for-a-logger) to help you identify why the primary broker is losing its shared file lock.",2025-03-14T14:58:45,2025-03-14T13:38:13
79508392,Socket.io Client Not Connecting to AKS-Deployed Socket.io Server (Only Receiving 40 Without sid),"I’m facing an issue where my Socket.io client fails to fully connect to a Socket.io server deployed on AKS.

- When running Socket.io locally, the client correctly receives the 40{""sid"":""<socket_id>""} message, meaning the connection is successful.
[![Local](https://i.sstatic.net/tt8IeAyf.png)](https://i.sstatic.net/tt8IeAyf.png)
- When connecting to the AKS-deployed server, the client only receives 40 without the sid, and the connection does not complete.
[![AKS Server](https://i.sstatic.net/AJfYGKs8.png)](https://i.sstatic.net/AJfYGKs8.png)
- The issue happens over wss://, but the same client works fine when connecting to a local server.

**Server**: Node.js with Socket.io (`socket.io@4.7.5`)

```
  io.on(""connection"", (socket) => {
        console.log(""Socket connected:"", socket.id);
      });
```

**Client**: Angularapp (`socket.io-client@4.7.5`)

```
  this.socket = io(""https://myapplication.com"",
      {
      transports: [""websocket""],
       path: ""/nodeapp/socket.io/""
      });
      this.socket.on(""connect"", (socketdata:any) => {
      console.log(""Connected to server"");
      });
```

**Deployment**: AKS with NGINX Ingress
**Ingress Annotations**:

```
ingress:
  enabled: true
  className: ""nginx""
  annotations:
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/error-log-level: ""debug""
    nginx.ingress.kubernetes.io/enable-websocket: ""true""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/proxy-body-size: ""100m""
  hosts:
   - host:
     paths:
       - path: /nodeapp(/|$)(.*)
         pathType: ImplementationSpecific
```

Why does the Socket.io client only receive 40 (without sid) when connecting via Ingress?

Could this be an issue with Ingress proxy headers stripping the sid?
Any known Azure or NGINX settings that affect WebSocket handshake completion?
Would appreciate any insights or troubleshooting tips!
Someone confirms a known issue with Socket.io, NGINX Ingress and suggests a fix.
I has been tried with load balancer also it's also showing same error it's seems to be connection not establish correctly but socket id is generated after `io.on(""connection"",{})`.","kubernetes, websocket, socket.io, azure-aks, nginx-ingress",,,,2025-03-14T07:36:03
79507486,Key-Vault auth issue with AKS &amp; external-secrets-operator,"I setup a simple setup of external-secret-operator and used a Managed Identity for authentication as shown in the documentation [here](https://external-secrets.io/v0.4.3/provider-azure-key-vault/#managed-identity-authentication).

I used the managed identity's Principal ID when setting in the SecretStore setup.

I setup the secret store and an External Secret (CRD's) and this is what I see in the External Secret (error):

> error processing spec.data[0] (key: my-secret), err:
> azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token
> for request to
> [https://my.vault.azure.net/secrets/my-secret/?api-version=7.0](https://my.vault.azure.net/secrets/my-secret/?api-version=7.0):
> StatusCode=400 -- Original Error: adal: Refresh request failed. Status
> Code = '400'. Response body:
> {""error"":""invalid_request"",""error_description"":""Identity not found""}
> Endpoint
> [http://xxx.xxx.xxx.xxx/metadata/identity/oauth2/token?api-version=2018-02-01&client_id=eeeeee-eeeeeee-eeeeeeee-eeeee-eeeeeeeeeeee&resource=https%3A%2F%2Fvault.azure.net](http://xxx.xxx.xxx.xxx/metadata/identity/oauth2/token?api-version=2018-02-01&client_id=eeeeee-eeeeeee-eeeeeeee-eeeee-eeeeeeeeeeee&resource=https%3A%2F%2Fvault.azure.net)","azure, kubernetes, azure-aks, azure-keyvault",79514444.0,"You error message indicates that External Secrets Operator (ESO) cannot authenticate with Azure Key Vault using Managed Identity. This typically happens due to incorrect identity configuration, missing role assignments, or specifying the wrong identity type. Check if your AKS is using system assigned or user assigned identity

```
az aks show --resource-group \<RESOURCE_GROUP\> --name \<AKS_CLUSTER_NAME\> --query identity
```

If you see type: ""SystemAssigned"", your AKS is using System Assigned Identity. If you see ""userAssignedIdentities"", it is using User Assigned Identity.

Please follow the below steps to set up external secrets operator with your azure key vault using managed identity on your AKS cluster.

Create your cluster with managed identity-

```
az aks create \
\--resource-group arkorg \
\--name myAKSCluster \
\--enable-managed-identity \
\--node-count 2 \
\--generate-ssh-keys
```

Create an Azure Key Vault

```
az keyvault create \
\--name arkoKeyVault \
\--resource-group arkorg \
\--location centralindia \
\--sku standard
```

![enter image description here](https://i.imgur.com/2HwZo9T.png)

Create a Managed Identity to authenticate with Key Vault

```
az identity create --name myIdentity --resource-group arkorg
```

and assign the ""Key Vault Administrator"" Role

```
az role assignment create \
\--assignee \<OBJECT_ID\> \
\--role ""Key Vault Administrator"" \
\--scope /subscriptions/abcdefghijk/resourceGroups/arkorg/providers/Microsoft.KeyVault/vaults/arkoKeyVault
```

Now store your secret. I am just using a sample one for example

```
az keyvault secret set --vault-name arkoKeyVault --name my-secret --value ""SuperSecretValue""
```

![enter image description here](https://i.imgur.com/Fd6I2KE.png)

Now that your Key Vault is ready, next will connect it to AKS using ESO.

```
helm repo add external-secrets https://charts.external-secrets.io
helm repo update
helm install external-secrets external-secrets/external-secrets \
\--namespace external-secrets \
\--create-namespace
```

![enter image description here](https://i.imgur.com/skVBA48.png)

Check it

```
kubectl get pods -n external-secrets
```

![enter image description here](https://i.imgur.com/op1KnJ2.png)

yup! working.

next create a Secret Store that connects to your Azure Key Vault.

```
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: keyvault-secretstore
  namespace: default
spec:
  provider:
    azurekv:
      authType: ManagedIdentity
      vaultUrl: ""https://arkokeyvault.vault.azure.net""
```

![enter image description here](https://i.imgur.com/GjPhLBz.png)

Done, now your Secret Store is now correctly configured to use System Assigned Identity and is in the Ready state.

Now, check if the ExternalSecret is syncing correctly

```
kubectl get externalsecret my-external-secret -o yaml
```

![enter image description here](https://i.imgur.com/D7VsVOk.png)

Check logs- `kubectl logs -n external-secrets deployment/external-secrets`

It's working. Below log snippet confirms that ESO successfully reconciled (synced) the secret from your Key Vault to your AKS.

![enter image description here](https://i.imgur.com/ZvCkHqi.png)

You can even run `kubectl get secrets my-kubernetes-secret -o yaml`

![enter image description here](https://i.imgur.com/k9UEO67.png)

You can even decode the secret to confirm the Value

```
kubectl get secret my-kubernetes-secret -o jsonpath=""{.data.my-secret}"" | base64 --decode
```

![enter image description here](https://i.imgur.com/ZiKE5Dl.png)

[![enter image description here](https://i.sstatic.net/eAlMRtmv.png)]

[![enter image description here](https://i.sstatic.net/871Kp6TK.png)](https://i.sstatic.net/871Kp6TK.png)

[![enter image description here](https://i.sstatic.net/CbBdv2Hr.png)]
[![enter image description here](https://i.sstatic.net/VCb4I00t.png)](https://i.sstatic.net/VCb4I00t.png)",2025-03-17T11:30:28,2025-03-13T19:52:31
79504822,How can I gain access to an AWS SQS queue message using argo events in Kubernetes?,"I have a k3s cluster setup with the Argo Events framework. I have an AWS SQS queue set up.

The event source and sensor are configured properly (Every time I send a message to the queue manually through the AWS dashboard the sensor gets triggered. I verified this by checking the generated pods that the sensor creates)
The trigger is working but the sensor is unable to get the body of the SQS event.

Here is the sensor:

```
apiVersion: argoproj.io/v1alpha1
kind: Sensor
metadata:
  name: s3-sensor
spec:
  template:
    serviceAccountName: argo-events-sa
  dependencies:
    - name: s3-file-upload
      eventSourceName: s3-source
      eventName: s3-file-upload
  triggers:
    - template:
        name: log-trigger
        k8s:
          group: """"
          version: v1
          resource: pods
          operation: create
          source:
            resource:
              apiVersion: v1
              kind: Pod
              metadata:
                generateName: log-pod-
                labels: log-pod
              spec:
                containers:
                  - name: log-container
                    image: alpine
                    command: [""echo""]
                    args: [""S3 file upload event received:\n"", """"]
                restartPolicy: Never
        parameters:
          - src:
              dependencyName: s3-file-upload
              dataKey: body
              dest: spec.containers.0.args.1
```

Here is my event source file:

```
apiVersion: argoproj.io/v1alpha1
kind: EventSource
metadata:
  name: s3-source
  namespace: argo-events
spec:
  sqs:
    s3-file-upload:
      jsonBody: true
      accessKey:
        key: AWS_ACCESS_KEY_ID
        name: aws-credentials
      secretKey:
        key: AWS_SECRET_ACCESS_KEY
        name: aws-credentials
      region: us-east-1
      queue: sqsqueue
      waitTimeSeconds: 5
```

When I try to view the logs for the created pod from the event:
`sudo kubectl --namespace argo-events logs log-pod-6kxd5`
I get:
`S3 file upload event received:`

The event body from the sqs queue is not showing.

Ive tried changing dataKey to 'data' and using busybox instead of alpine to echo the upload event.","kubernetes, amazon-sqs, argo-events",,,,2025-03-12T20:54:55
79503443,Wordpress in kubernetes cluster failed to open stream: No such file or directory,"Trying to run WordPress in kubernetes cluster - However Faling to launch due to error failed to open stream: No such file or directory.

I have wordpress pod using volumes for `/var/www/html`

POD Error logs

```
No 'wp-config.php' found in /var/www/html, but 'WORDPRESS_...' variables supplied; copying '/usr/src/wordpress/wp-config-
[php:error] [pid 56] [client 10.244.2.0:44079] PHP Fatal error:
Uncaught Error: Failed opening required '/var/www/html/wp-load.php' (include_path='.:/usr/local/lib/php') in /var/www/html/wp-blog-header.php:13\nStack trace:\n#0 /var/www/html/index.php(17): require()\n#1 {main}\n
thrown in /var/www/html/wp-blog-header.php on line 13,
referer: http://localhost:8001/
```

NFS Directory
[![enter image description here](https://i.sstatic.net/ZIXQ9dmS.png)](https://i.sstatic.net/ZIXQ9dmS.png)

Kubernetes yaml files.

```
---
# Wordpress deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress-app
  template:
    metadata:
      labels:
        app: wordpress-app
    spec:
      containers:
        - name: wordpress
          image: wordpress:6.2.1-apache
          securityContext:
            capabilities:
              add: [ ""SYS_ADMIN"" ]
          ports:
            - containerPort: 80
              name: http-port
          volumeMounts:
            - name: wordpress-data
              mountPath: /var/www/html
          envFrom:
            - configMapRef:
                name: wordpress-cm
      volumes:
        - name: wordpress-data
          persistentVolumeClaim:
            claimName: wordpress-pvc
---
# Persistent volume - Dynamically provission using nfs server
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: wordpress-pvc
  labels:
    app: wordpress-app
spec:
  storageClassName: nfs-client
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
# ConfigMap
apiVersion: v1
kind: ConfigMap
metadata:
  name: wordpress-cm
  namespace: default
data:
  WORDPRESS_DB_NAME: wordpress
  WORDPRESS_DB_USER: root
  WORDPRESS_DB_PASSWORD: 'Password'
  WORDPRESS_DB_HOST: mysql-0.mysql.default.svc.cluster.local
  WORDPRESS_DEBUG: ""true""
---
# Service
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  namespace: default
spec:
  type: LoadBalancer
  selector:
    app: wordpress-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: http-port
```","php, wordpress, kubernetes, nfs",,,,2025-03-12T11:29:56
79503342,Avoid leadership grant on shutdown (Spring Cloud Kubernetes),"I have an application using the Spring Cloud Kubernetes leader election mechanism. When shutting down the pod, I would like to revoke leadership before the Spring graceful shutdown starts to ensure that the leader is never shutdown. This is to avoid that the instance is shutdown while the leader is doing one of its tasks.

For that, I added a preStop hook which is executed when the pod starts shutting down and before the Spring graceful shutdown is started. But I have observed that sometimes the leadership is granted to the same pod. Is there a way to avoid that?

I have added an internal flag to make the application aware the shutdown is in progress, so if the leadership is granted again, I can avoid to do any leader-related operation. With this what Im implementing is that the leader stops doing its tasks before shutting down, but even though this solves my original problem, its not exactly what I was trying to implement on first place.

Thanks in advance!","spring, kubernetes, spring-cloud, spring-cloud-kubernetes",79664951.0,"[This PR](https://github.com/spring-cloud/spring-cloud-kubernetes/pull/1658), when it's going to be released, should address your concern.

- The `shutdown` endpoint will call one of the `@PreDestroy` methods that we have created, where we will cleanly release the leadership.
- You are correct that after you call `/shutdown`, the deployment will re-create the pod and it can easily happen that the same pod will take leadership again. For that we will have `spring.cloud.kubernetes.leader.election.wait-after-renewal-failure` seconds.

This will be properly documented too.

Disclaimer: I am the author of the PR",2025-06-13T13:50:39,2025-03-12T10:46:49
79502996,How to modify Kubernetes ingress for sending cookie?,"I have a web application running on kubernetes cluster. There are some cookies on the frontend part and I want to add this cookies in the request header. I'm using axios for sending request. I added `withCredentials: true` to axios instance. In the backend there is node.js. I added `app.use(cors({origin: true, credentials: true}));` middleware. When I run the app locally, frontend part is sending the cookies I can see it on the network tab. So I assume that I need to do some modification in kubernetes ingress. Here is my kubernetes ingress for backend and frontend:

Frontend:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: client-ingress
  namespace: {{ .Values.namespace }}
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - {{ .Values.ingress.hostname }}
      secretName: {{ .Values.frontend.name }}-tls
  rules:
  - host: {{ .Values.ingress.hostname }}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: client-cluster-ip-service
            port:
              number: 5000
```

Backend:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: server-ingress
  namespace: {{ .Values.namespace }}
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - {{ .Values.ingress.hostname }}
      secretName: {{ .Values.backend.name }}-tls
  rules:
  - host: {{ .Values.ingress.hostname }}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: server-cluster-ip-service
            port:
              number: 8000
```

What can I do? Do you think I need to add something to this ingress? Or do you think problem is something else?","node.js, kubernetes, cookies, axios, setcookie",,,,2025-03-12T08:26:56
79502760,route.yaml to reach the admin page of a wildfly container deployed in openshift local (CRC),"I have an Openshift Local (CRC) installed on my Windows Notebook
I would like to deploy a simple wildfly server to openshift. The goal is provide two routes to the application port 8080 and the admin port 9990 of quay.io/wildfly/wildfly:33.0.2.Final-jdk21 vie these urls: app-route.example.com adm-route.example.com

I have configured deployment, services and routes. If I port-forward the pod or the services, I can call http://localhost:9990 or http://localhost:8080 for service and pods. It is working. But it is not possible to call the admin page via [http://adm-route.example.com](http://adm-route.example.com) (a request to the application page via [http://app-route.apps-crc.testingexample.com](http://app-route.apps-crc.testingexample.com) is possible)

The deployment for the wildfly image

```
    -----deployment.yaml-----
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: wildfly-deployment
      namespace: test
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: wildfly
      template:
        metadata:
          labels:
            app: wildfly
        spec:
          containers:
            - name: wildfly-container
              image: quay.io/wildfly/wildfly:33.0.2.Final-jdk21
              ports:
                - containerPort: 8080
                - containerPort: 9990
    -----deployment.yaml-----
```

then I describe two services, one for each port

```
    -----service-app.yaml-----
    kind: Service
    apiVersion: v1
    metadata:
      name: app-service
      namespace: test
    spec:
      ipFamilies:
        - IPv4
      ports:
        - name: 8080-tcp
          protocol: TCP
          port: 8080
          targetPort: 8080
      internalTrafficPolicy: Cluster
      type: ClusterIP
      ipFamilyPolicy: SingleStack
      selector:
        app: wildfly
    -----service-app.yaml-----
```

```
    -----service-adm.yaml-----
    kind: Service
    apiVersion: v1
    metadata:
      name: adm-service
      namespace: test
    spec:
      ipFamilies:
        - IPv4
      ports:
        - name: 9990-tcp
          protocol: TCP
          port: 9990
          targetPort: 9990
      internalTrafficPolicy: Cluster
      type: ClusterIP
      ipFamilyPolicy: SingleStack
      selector:
        app: wildfly
    -----service-adm.yaml-----
```

for each service I have a route configured

```
    -----route-app.yaml-----
    kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: app-route
      namespace: test
    spec:
      host: example.com
      to:
        kind: Service
        name: app-service
        weight: 100
      port:
        targetPort: 8080-tcp
      wildcardPolicy: None
    -----route-app.yaml-----
```

```
    -----route-adm.yaml-----
    kind: Route
    apiVersion: route.openshift.io/v1
    metadata:
      name: adm-route
      namespace: test
    spec:
      host: example.com
      to:
        kind: Service
        name: adm-service
        weight: 100
      port:
        targetPort: 9990-tcp
      wildcardPolicy: None
    -----route-adm.yaml-----
```

I create the the project test

```
    oc login -u developer -p developer example.com
    oc new-project test-project
```

```
this is that the server may start with configured jboss user
```

```
    oc login -u kubeadmin -p kubeadmin example.com
    oc adm policy add-scc-to-user anyuid -n test -z default
```

and finally I apply the files

```
    oc login -u developer -p developer example.com
    oc apply -f deployment.yaml
    oc apply -f service-app.yaml
    oc apply -f service-adm.yaml
    oc apply -f route-app.yaml
    oc apply -f route-adm.yaml
```

status after configuration

```
    oc get pod
    NAME                                  READY   STATUS    RESTARTS   AGE
    wildfly-deployment-65d59dbcb5-4ql2x   1/1     Running   0          107s

    oc get svc
    NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
    adm-service   ClusterIP   10.217.4.237   <none>        9990/TCP   61s
    app-service   ClusterIP   10.217.5.148   <none>        8080/TCP   65s

    oc get route
    NAME        HOST/PORT                    PATH   SERVICES      PORT       TERMINATION   WILDCARD
    adm-route   adm-route.apps-crc.testing          adm-service   9990-tcp                 None
    app-route   app-route.apps-crc.testing          app-service   8080-tcp                 None
```

I can port-forward the pod to 8080 and 9990 and also I can do a port-forward for each service: Each time a call to admin page or the application page is working (curl http://localhost:8080 or curl http://localhost:9990)

but if I use the url:
the request to app-route.example.com works fine.
the request to adm-route.example.com will be answered with this error.

This is the error message when I call adm-route.example.com

```
curl http://adm-route.example.com
<html>
  <head>
    <meta name=""viewport"" content=""width=device-width, initial-scale=1"">

  </head>
  <body>
    <div>
      <h1>Application is not available</h1>
      <p>The application is currently not serving requests at this endpoint. It may not have been started or is still starting.</p>

      <div class=""alert alert-info"">
        <p class=""info"">
          Possible reasons you are seeing this page:
        </p>
        <ul>
          <li>
            <strong>The host doesn't exist.</strong>
            Make sure the hostname was typed correctly and that a route matching this hostname exists.
          </li>
          <li>
            <strong>The host exists, but doesn't have a matching path.</strong>
            Check if the URL path was typed correctly and that the route was created using the desired path.
          </li>
          <li>
            <strong>Route and path matches, but all pods are down.</strong>
            Make sure that the resources exposed by this route (pods, services, deployment configs, etc) have at least one pod running.
          </li>
        </ul>
      </div>
    </div>
  </body>
</html>
```

I have also tried to configure two ingress routes

```
    -----ingress-app.yaml-----
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: app-ingress
    spec:
      rules:
      - host: app-ingress.example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: app-service
                port:
                  number: 8080
    -----ingress-app.yaml-----

    -----ingress-adm.yaml-----
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: adm-ingress
    spec:
      rules:
      - host: adm-ingress.example.com
        http:
          paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: adm-service
                port:
                  number: 9990
    -----ingress-adm.yaml-----
```

I can apply both files

```
    oc apply -f ingress-app.yaml
    oc apply -f ingress-adm.yaml

    oc get route
    NAME                HOST/PORT                      PATH   SERVICES      PORT       TERMINATION   WILDCARD
    adm-ingress-mzwx6   adm-ingress.apps-crc.testing   /      adm-service   9990-tcp                 None
    adm-route           adm-route.apps-crc.testing            adm-service   9990-tcp                 None
    app-ingress-bm7j6   app-ingress.apps-crc.testing   /      app-service   8080-tcp                 None
    app-route           app-route.apps-crc.testing            app-service   8080-tcp                 None
```

and here is the same situation: a call to [http://app-ingress.example.com](http://app-ingress.example.com) is working - but not a call to [http://adm-ingress.apps-crc.testingexample.com](http://adm-ingress.apps-crc.testingexample.com)

I also can check the endpoints of the services

```
    oc describe svc adm-service
    Name:              adm-service
    Namespace:         test
    Labels:            <none>
    Annotations:       <none>
    Selector:          app=wildfly
    Type:              ClusterIP
    IP Family Policy:  SingleStack
    IP Families:       IPv4
    IP:                10.217.4.237
    IPs:               10.217.4.237
    Port:              9990-tcp  9990/TCP
    TargetPort:        9990/TCP
    Endpoints:         10.217.0.46:9990
    Session Affinity:  None
    Events:            <none>

    D:\Daten.wrk\00_PROJEKTE\BArch\openshift.local\barch\test>oc describe svc app-service
    Name:              app-service
    Namespace:         test
    Labels:            <none>
    Annotations:       <none>
    Selector:          app=wildfly
    Type:              ClusterIP
    IP Family Policy:  SingleStack
    IP Families:       IPv4
    IP:                10.217.5.148
    IPs:               10.217.5.148
    Port:              8080-tcp  8080/TCP
    TargetPort:        8080/TCP
    Endpoints:         10.217.0.46:8080
    Session Affinity:  None
    Events:            <none>
```

If I do a curl to the endpoint of the application service - I will get a normal answer

```
    tmp-shell  ~  curl http://10.217.0.46:8080
    -> HTML of the normal application

     tmp-shell  ~ 
```

If I do a curl to the endpoint of the admin service - I receive an error

```
    tmp-shell  ~  curl http://10.217.0.46:9990
    curl: (7) Failed to connect to 10.217.0.46 port 9990 after 0 ms: Couldn't connect to server
```

It seems there is something wrong - perhaps with the service?

```
------------------
```","kubernetes, openshift",79504539.0,"The core issue probably lies in how WildFly handles its management interface (port 9990).

In the WildFly configuration file, specifically the management interface, check if it's bound to `127.0.0.1` (Loopback).

If it is then that's the problem. It only listens for connections originating from within the same container (or the host if port-forwarded).

You have to change the management interface to bind to `0.0.0.0` (All Interfaces), to listen on all available network interfaces, allowing connections from any IP address that can reach the container.

- `127.0.0.1` is the loopback address. Services bound to this address are only accessible from within the same machine or container.
- `0.0.0.0` means ""listen on all available network interfaces"". Services bound to this address are accessible from any IP address that can reach the machine or container.

When you use Docker's `-p` flag, you're essentially creating a network bridge that forwards traffic from your host machine to a port inside the container. Even if the Wildfly container has port 8080 bound to 127.0.0.1, the docker port forwarding makes it accessible from the outside.
Because of this proxy behavior, the application port appears to be working, while the management port does not.

The management interface (port 9990) is designed for administrative tasks, it is intentionally restricted for security reasons. Docker port forwarding alone doesn't bypass the binding restrictions of the management interface.
When the management interface is bound to 127.0.0.1, it only accepts connections originating from within the container. Therefore, even if you forwarded port 9990, the WildFly management interface would reject the connection because it's coming from an external source.",2025-03-12T18:17:21,2025-03-12T06:10:34
79502676,How to pass values from one generator to another and use as a filter In Argo appset,"I am trying to pass value from git generator using files and use it as filter for my other child generator, but it doesn’t work.
Can someone suggest what is the correct way or is it not possible?

matrix generator

```
generators:
  - matrix:
      generators:
      - git:
          repoURL: https://github.com/JCodeX-Dev/JCodeXClusterPublic
          revision: main
          files:
          - path: helm-charts/override.yaml
      - list:
          elementsYaml: {{ .clusterList | toYaml }}
        selector:
          matchLabels:
            cluster: {{ .environmentType }}
```

override.yaml

```
environmentType: green
clusterList:
     - cluster: blue
       overrideTargetRevision: 18.3.6
     - cluster: green
       overrideTargetRevision: 18.3.6
```","kubernetes, kubernetes-helm, argocd",,,,2025-03-12T05:05:40
79498642,How to modify container filesystem after pod creation but before execution in Kubernetes?,"I have a Kubernetes cluster, and I need to ensure that every time a new pod is created, I can run a command that reads the container's filesystem and modifies certain files (e.g., replacing the configuration of nginx) before the main application in the pod starts executing.

The challenge is that I don't know in advance which files need modification — I must inspect the container's filesystem first.

# What I've tried so far:

- Init containers: These run before the container is created, so they don't have access to the actual container's filesystem.
- MutatingWebhookConfiguration: This modifies the pod spec before the container is created, which doesn't help in my case.
- Sidecars: They run in parallel with the main container, but I need modifications before execution.

# What are my options?

Is there a Kubernetes-native way to achieve this? Or would I need a custom solution like a wrapper entrypoint script or a custom runtime modification?

Any suggestions would be appreciated!","kubernetes, sidecar",,,,2025-03-10T16:53:26
79497226,Set ENV Variable Dynamically in a Dockerfile,"I am creating a multi-arch docker image in GA using the `Build and Push` Action as:

```
      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: my-dir
          push: true
          tags: myrepo.io/my-registry/my-app:${{ github.event_name == 'repository_dispatch' && github.event.client_payload.latest_ver || github.event.inputs.certified_version }}-multiarch
          file: Dockerfile_multiarch
          platforms: linux/amd64,linux/arm64
          provenance: false
```

In the dockerfile, I want to set the value of `LD_PRELOAD` as `ENV LD_PRELOAD=/usr/lib/${ARCH}-linux-gnu/libjemalloc.so.2` where `ARCH` can be with `x86_64` or `aarch64`. Setting this env variable inside a `RUN` block does not work because that variable is lost on that block finishes. How can I set this env var dynamically?

Here's my Dockerfile:

```
FROM ubuntu:20.04

ENV DEBIAN_FRONTEND noninteractive

# Install required packages and dependencies for RPM
RUN apt-get update && \
    apt-get install -y wget curl vim less git python3 linux-tools-common sysstat procps libjemalloc-dev gnupg && \
    rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=""/usr/lib/jvm/java-21-amazon-corretto""
ENV PATH=$JAVA_HOME/bin:$PATH

//todo: How can I do this?
ENV LD_PRELOAD=/usr/lib/${ARCH}-linux-gnu/libjemalloc.so.2
```","linux, docker, kubernetes, github-actions",79497719.0,"[**ld.so**(8)](https://linux.die.net/man/8/ld.so) documents:

> **LD_PRELOAD**: A list of additional, user-specified, ELF shared libraries to be loaded before all others. ... The libraries are searched for using the rules given under DESCRIPTION.

Those rules are:

> - Using the directories specified in the DT_RPATH dynamic section attribute of the binary....
> - Using the environment variable **LD_LIBRARY_PATH**....
> - Using the directories specified in the DT_RUNPATH dynamic section attribute of the binary if present.
> - From the cache file */etc/ld.so.cache*....
> - In the default path */lib*, and then */usr/lib*.

That is: if you just put a library name in `$LD_PRELOAD`, it will look for the library in the usual places.  (The cache file is populated by **ldconfig**(8), which honors the extended paths in `/etc/ld.so.conf`.)  For your use it should work to just put the library name in that variable, without trying to figure out its exact path.

```
ENV LD_PRELOAD=libjemalloc.so.2
```",2025-03-10T10:36:49,2025-03-10T06:57:14
79497226,Set ENV Variable Dynamically in a Dockerfile,"I am creating a multi-arch docker image in GA using the `Build and Push` Action as:

```
      - name: Build and push
        uses: docker/build-push-action@v6
        with:
          context: my-dir
          push: true
          tags: myrepo.io/my-registry/my-app:${{ github.event_name == 'repository_dispatch' && github.event.client_payload.latest_ver || github.event.inputs.certified_version }}-multiarch
          file: Dockerfile_multiarch
          platforms: linux/amd64,linux/arm64
          provenance: false
```

In the dockerfile, I want to set the value of `LD_PRELOAD` as `ENV LD_PRELOAD=/usr/lib/${ARCH}-linux-gnu/libjemalloc.so.2` where `ARCH` can be with `x86_64` or `aarch64`. Setting this env variable inside a `RUN` block does not work because that variable is lost on that block finishes. How can I set this env var dynamically?

Here's my Dockerfile:

```
FROM ubuntu:20.04

ENV DEBIAN_FRONTEND noninteractive

# Install required packages and dependencies for RPM
RUN apt-get update && \
    apt-get install -y wget curl vim less git python3 linux-tools-common sysstat procps libjemalloc-dev gnupg && \
    rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME=""/usr/lib/jvm/java-21-amazon-corretto""
ENV PATH=$JAVA_HOME/bin:$PATH

//todo: How can I do this?
ENV LD_PRELOAD=/usr/lib/${ARCH}-linux-gnu/libjemalloc.so.2
```","linux, docker, kubernetes, github-actions",79497427.0,"I would just make the path constant.

```
RUN <code to get arch> && ln -s /usr/lib/${ARCH}-linux-gnu/libjemalloc.so.2 /
ENV LD_PRELOAD=/libjemalloc.so.2
```",2025-03-10T08:36:45,2025-03-10T06:57:14
79496573,How to configure HorizontalPodAutoscaler?,"I currently have my HPA configured like this:

```
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-deployment-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app-deployment
  minReplicas: 2
  maxReplicas: 5
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 133  # Scale up when CPU usage exceeds 133% of resources.requests. Must be less than resources.limits.
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 133
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 10
    scaleDown:
      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down
      policies:
        - type: Pods
          value: 1
          periodSeconds: 120  # Scale down by 1 pod per 2 minutes
```

I'm new to HPAs. From what I've seen on the internet, most people seem to `averageUtilization` to like 50-80%. I'm trying to understand why.

As I understand it, it's a % of the deployment's `resources.requests` and `requests` is ""the minimum amount of compute resources required""

Why would I want to scale up when my utilization exceeds 50% of *the minimum required*?

I've configured `resources.requests` a tiny bit below above my normal usage. It's got no need to scale up if it's operating under normal conditions. Normal utilization should be like 90%/R, am I wrong?

So I've set `averageUtilization` to 133%, that way if utilization is *high*, then it should start adding more pods.",kubernetes,79496734.0,"I'll start by saying: if your combination of resource requests, resource limits, node sizes, and HPA settings has the effect you want, then it's correct.  Are you scaling up and down when you want to, and not getting evicted more than you can tolerate?  There's nothing ""wrong"" *per se* with the configuration you're showing here.

I think the one big thing I'd change in your wording is to say that resource requests are the minimum the cluster is guaranteed to allocate for you.  This is not necessarily the same as the minimum the application needs.

Say the application needs 512 MiB of memory to start up; but usually 1 GiB in the steady state; but it can burst up to 2 GiB under load.  I'd probably set the memory requests to 1 GiB (the steady-state value) and the limits to 2 GiB (the peak value).  The risk here is, the further apart these two values are, the more likely it is that your node will run out of memory under maximum load, and there's an argument to set the two values to equal, to guarantee that you'll have the memory available and will never get evicted.

If you've set the resource requests to the steady-state expected utilization, then you'll probably want to set the HPA to target about 100% of the requests; if actual usage is significantly higher (or lower) then it's time to scale up (or down).  If you've set resource requests to guaranteed allocation (requests == limits) then that's probably where you're seeing that 50-80% target.

There are also challenges with both of the built-in resource autoscaling, depending on your language runtime.

For memory autoscaling, are you using a garbage-collected language (pretty much everything other than C/C++/Rust), and if so, does it ever give memory back to the OS?  You might find yourself never being able to scale down.

For CPU autoscaling, how much of your application time is going to a database or something else, and you're mostly in I/O wait?  Sometimes you might need to scale up anyways, especially if you have thread-pool constraints.

I've had the best results attaching the HPA to other resources – thread-pool utilization, queue length – but these require some substantial administrator setup to make them accessible.",2025-03-09T22:58:34,2025-03-09T20:42:49
79494514,Losing logs when using a tunnel in aws,"I'm using DocumentDB with an EC2 attached, using it as a bastion, and Jenkins and Kubernetes for the cicd

I'm doing a tunnel to handle the communication between EC2 and documentdb

```
ssh -o StrictHostKeyChecking=no -i ""/opt/app/src/mongo-key-pair.pem""
    -L 27017:mongo-cluster.cluster-asdf.us-east-2.docdb.amazonaws.com:27017
             ec2-user@ec2-asdf.us-east-2.compute.amazonaws.com -N -f
```

The database works, but the tests (coverage) stopped working. Leaving aside the possibility of my application not being prepared, I now don't see any trace of them when I execute them on Jenkins.

However, if I enter in one of the pods where the application is being run and use

```
coverage run --source=src -m pytest --junit-xml=test_result.xml -m typetest src/tests
```

the tests are executed, and the logs are shown correctly.

What's going on here? Is the tunnel ""hiding"" the logs? What could I do if so to see the logs in Jenkins?","amazon-web-services, kubernetes, jenkins, ssh-tunnel, documentdb",,,,2025-03-08T14:15:42
79492029,How do I set Ingress to use WS connection in Kubernetes?,"Problem is without ingress and setting my service as `NodePort` I can access my node.js server outside of the Kubernetes Cluster via WebSocket connection in the client side which is running locally e.g., KX Platform using their q as programming language.

However, part of the architecture design we are required to use Ingress. But, using Ingress I cannot connect to the Pod with the server by WS connection.

Keep in mind, I'm running it locally not through `minikube` but using `Rancher Desktop` as context, as it was recommended by our SSE.

ingress-service.yaml

```
# Ingress
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: app-ingress
  annotations:
    # nginx.ingress.kubernetes.io/rewrite-target: /
    # nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    # nginx.ingress.kubernetes.io/secure-backends: ""true""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    # nginx.ingress.kubernetes.io/configuration-snippet: |
    #   proxy_http_version 1.1;
    #   proxy_set_header Upgrade ""websocket"";
    #   proxy_set_header Connection ""Upgrade"";
    nginx.org/websocket-services: ""server-service""
spec:
  ingressClassName: nginx
  rules:
    - host: example.com
      http:
        paths:
        - path: /ws/
          pathType: Prefix
          backend:
            service:
              name: server-service
              port:
                number: 5005
```

server-service.yaml

```
# Service exposing pod internally to other pods
apiVersion: v1
kind: Service
metadata:
  name: server-service
spec:
  # type: NodePort
  type: ClusterIP
  selector:
      nodejs-server: server
  ports:
    - protocol: TCP
      port: 5005
      targetPort: 5005
      # nodePort: 30005
```

Terminal to check default hosts file content

```
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
10.42.0.178     server-deployment-8b9cf4977-mc779
```

What's the problem here?",kubernetes,79492672.0,"Problem relied in not properly installed and configured `nginx` ingress controller. By installing it through Helm and deleting all references to `traefik` to avoid `LoadBalancer` collision and setting `nginx.org/websocket-services: server-service` and/or `nginx.ingress.kubernetes.io/websocket-services: server-service`, we successfully established 101 Switching Protocols with the client.

As the ingress did not have an address, therefore it was never being routed.",2025-03-07T14:45:49,2025-03-07T10:50:12
79491704,How to create a new ILM policy for Filebeat with Elasticsearch on Kubernetes?,"I'm using [ECK helm chart](https://artifacthub.io/packages/helm/elastic/eck-operator) and created a Filebeat with Beat CRD:

```
apiVersion: beat.k8s.elastic.co/v1beta1
kind: Beat
metadata:
  name: quickstart
  namespace: default
spec:
  type: filebeat
  version: 8.17.3
  elasticsearchRef:
    name: quickstart
  config:
    filebeat.inputs:
    - type: container
      paths:
      - /var/log/containers/*.log
    output.elasticsearch:
      ilm:
        enabled: true
        policy_name: ""filebeat_policy""
  ......
```

I created a `filebeat_policy.json` policy:

```
{
    ""policy"":{
       ""phases"":{
          ""hot"":{
             ""actions"":{
                ""rollover"":{
                    ""max_age"": ""1d"",
                    ""max_docs"": 10000,
                    ""max_size"": ""10gb""
                }
             }
          },
          ""delete"":{
             ""min_age"":""30d"",
             ""actions"":{
                ""delete"":{

                }
             }
          }
       }
    }
}
```

But after I create policy this way:

```
curl -X PUT -k -u elastic:$ELASTIC_PASSWORD ""https://localhost:9200/_ilm/policy/filebeat_policy"" -H 'Content-Type: application/json' -d @./values/elastic/filebeat-policy.json
```

I can't find the `filebeat_policy` in the current Elasticsearch indices:

```
curl -X GET -k -u elastic:$ELASTIC_PASSWORD ""https://localhost:9200/_data_stream/filebeat-*?pretty""
```

If I do this way, it works:

```
curl -X PUT -k -u elastic:$ELASTIC_PASSWORD ""https://localhost:9200/_component_template/filebeat-settings"" -H 'Content-Type: application/json' -d '
{
  ""template"": {
    ""settings"": {
      ""index.lifecycle.name"": ""filebeat_policy""
    }
  }
}'
curl -X PUT -k -u elastic:$ELASTIC_PASSWORD ""https://localhost:9200/_index_template/filebeat-8.15.3"" -H 'Content-Type: application/json' -d '
{
  ""index_patterns"": [""filebeat-*""],
  ""data_stream"": {},
  ""composed_of"": [""filebeat-settings""]
}'
```

So my question is, if use Beat CRD, doesn't it work in `config` section below?

```
    output.elasticsearch:
      ilm:
        enabled: true
        policy_name: ""filebeat_policy""
```","elasticsearch, kubernetes, filebeat",,,,2025-03-07T08:54:55
79491630,apache spark-submit to local k8s cluster fails despite having the necessary edit rbac setup,"I have created the necessary `serviceaccount` and `clusterrolebinding` with `edit` clusterrole but still fails to submit to a local k8s cluster:

```
$ spark-submit --master k8s://https://127.0.0.1:16443 --conf spark.executor.instances=1 --conf spark.kubernetes.container.image=spark:python3 --conf spark.kubernetes.file.upload.path=/tmp --conf spark.kubernetes.authenticate.driver.serviceAccountName=sa-apache-spark --deploy-mode cluster TextFile.py
25/03/07 16:20:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/07 16:20:57 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
25/03/07 16:20:58 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
25/03/07 16:20:58 INFO KubernetesUtils: Uploading file: /usr/src/Python/PySpark/TextFile.py to dest: /tmp/spark-upload-a551a9f4-6433-4674-bdc2-d117401c50ee/TextFile.py...
25/03/07 16:21:39 ERROR Client: Please check ""kubectl auth can-i create pod"" first. It should be yes.
Exception in thread ""main"" io.fabric8.kubernetes.client.KubernetesClientException: An error has occurred.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:129)
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:122)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:44)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:1108)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.create(BaseOperation.java:92)
    at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:153)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6(KubernetesClientApplication.scala:256)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$6$adapted(KubernetesClientApplication.scala:250)
    at org.apache.spark.util.SparkErrorUtils.tryWithResource(SparkErrorUtils.scala:48)
    at org.apache.spark.util.SparkErrorUtils.tryWithResource$(SparkErrorUtils.scala:46)
    at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:94)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:250)
    at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:223)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:1034)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:199)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:222)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1125)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1134)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.IOException: timeout
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:515)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:535)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleCreate(OperationSupport.java:340)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:703)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleCreate(BaseOperation.java:92)
    at io.fabric8.kubernetes.client.dsl.internal.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:42)
    ... 17 more
Caused by: java.io.InterruptedIOException: timeout
    at okhttp3.RealCall.timeoutExit(RealCall.java:108)
    at okhttp3.RealCall$AsyncCall.execute(RealCall.java:205)
    at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
    at java.base/java.lang.Thread.run(Thread.java:1583)
Caused by: java.net.SocketException: Socket closed
    at java.base/sun.nio.ch.NioSocketImpl.endConnect(NioSocketImpl.java:531)
    at java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:604)
    at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
    at java.base/java.net.Socket.connect(Socket.java:751)
    at okhttp3.internal.platform.Platform.connectSocket(Platform.java:129)
    at okhttp3.internal.connection.RealConnection.connectSocket(RealConnection.java:247)
    at okhttp3.internal.connection.RealConnection.connect(RealConnection.java:167)
    at okhttp3.internal.connection.StreamAllocation.findConnection(StreamAllocation.java:258)
    at okhttp3.internal.connection.StreamAllocation.findHealthyConnection(StreamAllocation.java:135)
    at okhttp3.internal.connection.StreamAllocation.newStream(StreamAllocation.java:114)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.java:42)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.java:93)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.java:127)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:147)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.java:121)
    at okhttp3.RealCall.getResponseWithInterceptorChain(RealCall.java:257)
    at okhttp3.RealCall$AsyncCall.execute(RealCall.java:201)
    ... 4 more
25/03/07 16:21:39 INFO ShutdownHookManager: Shutdown hook called
25/03/07 16:21:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-36873a8d-eb37-43e5-bacc-032e639773eb
```

```
$ kubectl auth can-i create pod
yes
```","apache-spark, kubernetes, pyspark, python-3.12, kubernetes-rbac",79690681.0,"I faced the same issue when I was trying to do a spark-submit to the local cluster from outside. However using the below command helped me.

```
kubectl proxy
```

The above command will create an authenticating proxy, to communicate to the Kubernetes API.

Local proxy will be running at localhost:8001, `--master k8s://http://127.0.0.1:8001` can be used as the argument to spark-submit instead of -`-master k8s://https://127.0.0.1:6443`

Full example command:

```
$SPARK_HOME/bin/spark-submit \
--master k8s://http://127.0.0.1:8001 \
--deploy-mode cluster \
--name spark-pi \
--class org.apache.spark.examples.SparkPi \
--conf spark.executor.instances=3 \
--conf spark.kubernetes.container.image=ghcr.io/dulshanr/spark-py:1.0 \
--conf spark.kubernetes.namespace=spark \
local:///opt/spark/examples/jars/spark-examples_2.13-4.0.0.jar
```

Source :

[https://spark.apache.org/docs/latest/running-on-kubernetes.html#cluster-mode](https://spark.apache.org/docs/latest/running-on-kubernetes.html#cluster-mode)

I believe the **rbac** resources you created originally will not get utilized properly unless you create the proxy.",2025-07-04T22:04:10,2025-03-07T08:27:54
79490655,How to restore postgres backup using kubernetes exec command,"I'm trying to restore the PostgreSQL backup using this command

```
kubectl exec -i cnpg-cluster-1 -n lc -- psql --username=""postgres"" --dbname=""data"" --password < ""/tmp/backup-2025-03-05-20-30.sql""
```

The restore starts and after about 5 minutes or so, it fails with this error:

```
E0306 20:41:40.484825  749825 v2.go:104] write tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
E0306 20:41:40.484957  749825 websocket.go:499] Websocket Ping failed: write tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
E0306 20:41:40.485028  749825 websocket.go:499] Websocket Ping failed: write tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
E0306 20:41:40.687396  749825 v2.go:167] next reader: read tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
E0306 20:41:40.687453  749825 v2.go:129] next reader: read tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
E0306 20:41:40.687463  749825 v2.go:150] next reader: read tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
error: error reading from error stream: next reader: read tcp 172.31.20.8:33792->54.198.88.223:443: i/o timeout
```

I don't really understand why timeout issue would happen, because the network connection is quite good and other database operations are happening properly.

Is there any way to increase the timeout OR any keep alive settings?","postgresql, kubernetes, psql",79493592.0,"First thing you could try is to upgrade (if not using the updated) the `kubectl` to the latest [version 1.32](https://kubernetes.io/releases/), usually this solves the timeout/disconnection.

The 5 minute timeout is essentially the timeout setting of the load balancer that provides high availability for the cluster.

`keepalive` for `exec` is set to ping every 5s to send a periodic keep alive signal to the API server. This is most likely not the culprit as it is enabled by default and not configurable. See the following [Feature request](https://github.com/kubernetes/kubernetes/issues/94301) | [Pull request](https://github.com/kubernetes/kubernetes/pull/97083) for more information.

To increase the timeout, you can use [--request-timeout=0](https://kubernetes.io/docs/reference/kubectl/kubectl/#:%7E:text=%2D%2Drequest%2Dtimeout%20string%C2%A0%C2%A0%C2%A0%C2%A0%C2%A0Default%3A%20%220%22)

```
The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests.
```

If the issue persists, I would recommend [raising an issue](https://issuetracker.google.com/issues/new?component=187077&template=1162666) with GKE engineers since this seems to be an isolated case and better handled by them.",2025-03-07T22:46:27,2025-03-06T20:48:44
79489621,How to Use spring-boot-starter-actuator Without spring-boot-starter-web for Health Checks and Prometheus Metrics?,"I have two services running in Kubernetes, and I need to configure:

```
1.  A health check (using httpGet)
2.  Prometheus metrics (via /actuator/prometheus)
```

My services use either:

```
•   org.springframework.cloud:spring-cloud-stream
•   org.springframework.grpc:spring-grpc-spring-boot-starter
```

I do not use spring-boot-starter-web and would prefer not to add it just for health checks and metrics.

How can I expose actuator endpoints
(/actuator/health, /actuator/prometheus) without adding spring-boot-starter-web? Are there alternative ways to achieve this in a lightweight manner?

Thanks in advance! Spring Boot 3.4.3","spring, spring-boot, kubernetes, kubernetes-helm, actuator",79561025.0,"Why not use `spring-boot-starter-actuator` dependency? That will give you the option of exposing plenty of endpoints.

As for `prometheus`, you will have to use `io.micrometer:micrometer-registry-prometheus` dependency in addition to enable `/actuator/prometheus`. It is plug-and-play so it will start exposing your JVM metrics.

The following config must be added in the base `application.yml` of the spring-boot service:

```
management:
  endpoints:
    web:
      exposure:
        include: * # health,prometheus
    health:
      show-details: always
```

You can be more granular on what endpoints you want to expose by replacing the `*`  with something more specific as per Spring's doco: [Endpoints :: Spring Boot](https://docs.spring.io/spring-boot/reference/actuator/endpoints.html)",2025-04-08T01:49:37,2025-03-06T14:07:59
79489188,How to attach Job Labels to Job Events in Fluent Bit Kubernetes Events,"I'm using Fluent Bit's [kubernetes_events](https://docs.fluentbit.io/manual/pipeline/inputs/kubernetes-events) plugin to collect Kubernetes events directly from the API server. I'm aiming to ingest these events into an OpenSearch cluster.

A key requirement is to enrich each event with the labels of the Kubernetes object that triggered it. Specifically, for Job events, I need to include the labels defined in the Job's configuration.

I attempted to achieve this by configuring the kubernetes_events plugin and the [kubernetes Filter](https://docs.fluentbit.io/manual/pipeline/filters) with the expectation that it would automatically attach the labels. However, this didn't work as anticipated.

Here's the config map that I used. I'm using rancher desktop for the kubernetes test enviroment

```
[SERVICE]
    flush           1
    log_level       info

[INPUT]
    name            kubernetes_events
    tag             k8s_events
    kube_url        https://kubernetes.default.svc

[FILTER]
    Name rewrite_tag
    Match k8s_events
    Rule $involvedObject['kind'] ^(Job)$ $TAG.pod.$involvedObject['name'].$involvedObject['namespace'].event false

[FILTER]
    Name kubernetes
    Buffer_Size 32MB
    K8S-Logging.Parser On
    K8S-Logging.Exclude On
    Keep_Log Off
    Match k8s_events.pod.*
    Owner_References On
    Kube_Tag_Prefix k8s_events.Job.
    Merge_Log On
    tls.verify Off
    Use_Kubelet true

# Output for job events
[OUTPUT]
    name            stdout
    match           k8s_events.Job.*
    format          json_lines
```

This displays the events as [JSON](https://gist.github.com/RavinduWeerakoon/8fc4dbb9685facacf74f8e85553ea948) but no labels about the Jobs.

What am I missing here?","kubernetes, fluentd, fluent-bit",79500309.0,"If the labels are not showing up then it means maybe fluent bit’s kubernetes filter is not configured correctly. For this you need to manually enrich the events using a custom [Lua filter](https://docs.fluentbit.io/manual/pipeline/filters/lua) if the default kubernetes metadata collection isn’t sufficient.

Regarding your query, whether it requires direct calls to the Kubernetes API server via Lua scripts. **Yes, Lua plugin** would require direct API calls to the k8s API server to fetch Job labels. But the **Fluent Bit Lua filter plugin** has some limitations, like the Lua plugin does not include  the necessary HTTP modules to fetch job metadata from the k8s API. To resolve this you need to enrich the data through an external processor. Refer to this [How to configure Fluent Bit to collect logs for your K8s cluster](https://isitobservable.io/observability/kubernetes/how-to-configure-fluent-bit-to-collect-logs-for-your-k8s-cluster) blog by Giulia Di Pietro, which will be helpful to resolve the issue.

**Note: If you intend to use Lua to interact with kubernetes API directly you will need to implement  HTTP requests within Lua however this may require additional modules that aren't included by default fluent bit’s Lua plugin.**",2025-03-11T09:57:46,2025-03-06T11:29:38
79488688,"Pod not using LVM volumes created by TopoLVM, defaulting to node&#39;s main disk in my kubernetes cluster","I am facing an issue where my pod is not using the LVM volumes created by TopoLVM. Instead, the pod is defaulting to using the main disk of the node, even though the correct StorageClass is set, and the LVM volumes are created correctly.
Here’s a summary of the situation:
The LVM volumes are properly created and available.
The correct StorageClass is set up in the Kubernetes deployment.
The pod doesn't seem to write to the LVM volumes, and instead, it writes to the node's main disk.
I have verified that the StorageClass and PVC are correctly defined, and the volumes are correctly created by topolvm. However, the pod still doesn't mount and use the LVM volumes.
Has anyone encountered a similar issue or have suggestions on what I might be missing?
Any help would be greatly appreciated!

PS: I used microk8s to create the cluster",kubernetes,,,,2025-03-06T08:30:45
79488435,.NET Aspire w/ Aspir8 - build substitutes periods for hyphens in docker image tag,"I have a simple Aspire app with an API (Using ASP.NET WebAPIs on .NET 9), a DB Migrater project and a Worker service.  When I run:

`aspirate build -crp 'neodevacr.azurecr.io/neoadj'`

It generates all the images properly, except for the webapi project, which gets the periods in the ACR registry name converted to hyphens, like so:

[![enter image description here](https://i.sstatic.net/T1nPOGJj.png)](https://i.sstatic.net/T1nPOGJj.png)

I cannot find anywhere in my config where that project is getting special treatment.
Does anyone know what's causing it and how I fix it?",".net, docker, kubernetes, dotnet-aspire, kompose",,,,2025-03-06T06:27:07
79487937,KeyDB in Kubernetes receives SIGTERM and restarts unexpectedly,"I’m running KeyDB v6.3.2 in a Kubernetes sts, and after an extended uptime (39+ days), the pod receives SIGTERM and restarts, even though there was no manual shutdown or scaling event

Here is the relevant log before termination:

> 1:signal-handler (1740559193) Received SIGTERM scheduling shutdown...
> 1:22:S 26 Feb 2025 08:39:53.711 # User requested shutdown...
> 1:22:S 26 Feb 2025 08:39:53.711 * Saving the final RDB snapshot before exiting.
> 1:22:S 26 Feb 2025 08:39:54.027 * DB saved on disk
> 1:22:S 26 Feb 2025 08:39:54.027 * Removing the pid file.
> 1:22:S 26 Feb 2025 08:39:54.029 # KeyDB is now ready to exit, bye bye...

Pods lastState shows:

```
    lastState:
  terminated:
    containerID: containerd://8cb24568ab14bd0579427dd04505ad2fea2a0ea685ef63e4c64579b8f5f92888
    exitCode: 0
    finishedAt: ""2025-02-26T08:39:54Z""
    reason: Completed
    startedAt: ""2025-01-18T07:30:35Z""
restartCount: 2
```

Pod was running for over a month before this restart

**What I’ve checked so far:**

1. SIGTERM was received (kubectl logs  --previous confirms it)
2. No manual shutdown or scaling event (no Helm or StatefulSet update)
3. No eviction or OOMKill (kubectl describe pod/keydb shows no resource issues)
4. No liveness probe failures (kubectl describe pod/keydb)
5. PVC is stable (kubectl get pvc shows volumes are bound)

**Deployment Details:**

1. KeyDB Version: v6.3.2
2. Kubernetes Version: v1.21
3. Helm Chart: keydb-0.48.0
4. StatefulSet with PVC: Yes

**Liveness Probe:**

```
livenessProbe:
  exec:
    command:
    - sh
    - -c
    - /health/ping_liveness_local.sh 5
  failureThreshold: 5
  initialDelaySeconds: 20
  periodSeconds: 5
  timeoutSeconds: 6
```

**Readiness Probe:**

```
readinessProbe:
  exec:
    command:
    - sh
    - -c
    - /health/ping_readiness_local.sh 1
  failureThreshold: 5
  initialDelaySeconds: 20
  periodSeconds: 5
  timeoutSeconds: 2
```

Are there any keydb specific conditions where it could request its own shutdown? Any debugging steps to track the origin of the SIGTERM signal? I've tried to set loglevel to debug/verbose, but there are no results","kubernetes, redis, sigterm, redis-server, keydb",,,,2025-03-05T23:49:17
79486138,PostgreSQL database isn&#39;t created automatically in kubernetes deployment,"I want to deploy my postgreSQL database with kubernetes but the database is not created...I put POSTGRES_DB in env of the deployment.

the kubernetes deployment of postgres work very well but no ""qr_auth"" database created.

deployment:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-deployment
  labels:
    app: postgres
spec:
  serviceName: ""postgres-service""
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:17.4
        env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: qr-auth-secret
                key: DB_PASSWORD
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: qr-auth-secret
                key: DB_USER
          - name: POSTGRES_DB
            value: qr_auth
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""250m""
          limits:
            memory: ""512Mi""
            cpu: ""1000m""
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 5Gi

---

apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
```

configmap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: qr-auth-config
data:
  DB_HOST: ""postgres-service""
  DB_PORT: ""5432""
  DB_NAME: ""qr_auth""
```

secret:

```
apiVersion: v1
kind: Secret
metadata:
  name: qr-auth-secret
type: Opaque
data:
  DB_USER: cG9zdGdyZXM=  # postgres en base64
  DB_PASSWORD: cm9vdA==  # root en base64
```","postgresql, kubernetes",79494560.0,"I solved the problem by changing:

```
volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 5Gi
```

to:

```
volumes:
            - name: postgres-storage
              persistentVolumeClaim:
                claimName: postgres-pvc
```

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgres
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```",2025-03-08T15:03:10,2025-03-05T10:22:57
79482446,Cannot install Longhorn in Rancher running using Docker,"I installed `Rancher` as outlined in their [github Quick Start](https://github.com/rancher/rancher?tab=readme-ov-file#quick-start):

```
sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher
```

I then went ahead and tried to install `Longhorn`, however I'm unable to install it and keep getting this error.

```
helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-crd-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn-crd /home/shell/helm/longhorn-crd-105.1.0-up1.7.2.tgz
Release ""longhorn-crd"" does not exist. Installing it now.
NAME: longhorn-crd
LAST DEPLOYED: Tue Mar  4 00:18:14 2025
NAMESPACE: longhorn-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
---------------------------------------------------------------------
SUCCESS: helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-crd-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn-crd /home/shell/helm/longhorn-crd-105.1.0-up1.7.2.tgz
---------------------------------------------------------------------
helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn /home/shell/helm/longhorn-105.1.0-up1.7.2.tgz
Release ""longhorn"" does not exist. Installing it now.
Error: context deadline exceeded
```

I cannot run the [longhorn CLI checker](https://longhorn.io/docs/1.8.0/advanced-resources/longhornctl/) that longhorn has either since the instructions does not bind the `/boot/config/` to the docker container so I keep getting this error:

```
> ./longhornctl check preflight
INFO[2025-03-04T00:39:42Z] Initializing preflight checker
INFO[2025-03-04T00:39:42Z] Cleaning up preflight checker
INFO[2025-03-04T00:39:42Z] Running preflight checker
ERRO[2025-03-04T00:40:09Z] Failed to run preflight checker: failed DaemonSet condition check: pod container is in crash loop. View the logs using ""kubectl -n default logs longhorn-preflight-checker-gldg9 -c init-longhornctl""
```

I have already satisfied all the requirements mentioned in [Longhorn prerequisites](https://longhorn.io/docs/1.8.0/deploy/install/install-with-rancher/#prerequisites), please note that it says [Rancher already has Mount Propagation enabled by default](https://longhorn.io/docs/1.8.0/deploy/install/#notes-on-mount-propagation).

Versions:

```
Rancher: 2.10.2
Kubernetes: v1.31.1+k3s1
```","docker, kubernetes, rancher, longhorn",79485217.0,"This is kind of embarrassing but apparently I failed to read the documentation properly. The issue was that `rancher` is for testing/development purposes only in `docker` container mode. Once I installed a local `k3s` cluster and added `rancher` on top of that, I was able to install `longhorn` easily as specified in the docs.

1. Install `k3s`
  - [https://docs.k3s.io/quick-start#install-script](https://docs.k3s.io/quick-start#install-script)
2. Install `rancher`
  - [https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster](https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/install-upgrade-on-a-kubernetes-cluster)
3. Install `longhorn` (make sure you have met the pre-requisites)
  - [https://longhorn.io/docs/1.8.0/deploy/install/install-with-rancher/](https://longhorn.io/docs/1.8.0/deploy/install/install-with-rancher/)

Here's the note on `docker` installation:

- [https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/other-installation-methods](https://ranchermanager.docs.rancher.com/getting-started/installation-and-upgrade/other-installation-methods)
  - `The Docker installation is for development and testing environments only.`",2025-03-05T01:03:49,2025-03-04T00:45:41
79482446,Cannot install Longhorn in Rancher running using Docker,"I installed `Rancher` as outlined in their [github Quick Start](https://github.com/rancher/rancher?tab=readme-ov-file#quick-start):

```
sudo docker run -d --restart=unless-stopped -p 80:80 -p 443:443 --privileged rancher/rancher
```

I then went ahead and tried to install `Longhorn`, however I'm unable to install it and keep getting this error.

```
helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-crd-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn-crd /home/shell/helm/longhorn-crd-105.1.0-up1.7.2.tgz
Release ""longhorn-crd"" does not exist. Installing it now.
NAME: longhorn-crd
LAST DEPLOYED: Tue Mar  4 00:18:14 2025
NAMESPACE: longhorn-system
STATUS: deployed
REVISION: 1
TEST SUITE: None
---------------------------------------------------------------------
SUCCESS: helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-crd-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn-crd /home/shell/helm/longhorn-crd-105.1.0-up1.7.2.tgz
---------------------------------------------------------------------
helm upgrade --install=true --labels=catalog.cattle.io/cluster-repo-name=rancher-charts --namespace=longhorn-system --timeout=10m0s --values=/home/shell/helm/values-longhorn-105.1.0-up1.7.2.yaml --version=105.1.0+up1.7.2 --wait=true longhorn /home/shell/helm/longhorn-105.1.0-up1.7.2.tgz
Release ""longhorn"" does not exist. Installing it now.
Error: context deadline exceeded
```

I cannot run the [longhorn CLI checker](https://longhorn.io/docs/1.8.0/advanced-resources/longhornctl/) that longhorn has either since the instructions does not bind the `/boot/config/` to the docker container so I keep getting this error:

```
> ./longhornctl check preflight
INFO[2025-03-04T00:39:42Z] Initializing preflight checker
INFO[2025-03-04T00:39:42Z] Cleaning up preflight checker
INFO[2025-03-04T00:39:42Z] Running preflight checker
ERRO[2025-03-04T00:40:09Z] Failed to run preflight checker: failed DaemonSet condition check: pod container is in crash loop. View the logs using ""kubectl -n default logs longhorn-preflight-checker-gldg9 -c init-longhornctl""
```

I have already satisfied all the requirements mentioned in [Longhorn prerequisites](https://longhorn.io/docs/1.8.0/deploy/install/install-with-rancher/#prerequisites), please note that it says [Rancher already has Mount Propagation enabled by default](https://longhorn.io/docs/1.8.0/deploy/install/#notes-on-mount-propagation).

Versions:

```
Rancher: 2.10.2
Kubernetes: v1.31.1+k3s1
```","docker, kubernetes, rancher, longhorn",79482943.0,"Try

```
helm repo add longhorn https://charts.longhorn.io
helm repo update
```

then

```
kubectl create namespace longhorn-system
helm install longhorn longhorn/longhorn --namespace longhorn-system
```

Verify

```
kubectl -n longhorn-system get pod
```",2025-03-04T07:31:30,2025-03-04T00:45:41
79481161,OOM kills pod when setting the resource limits,"Below is the stateful-set that I use. If I run it in `minicube (with 2000M, 4Gi config)` without `resources.limits`, then it runs fine. But if I specify `resources.limits`, which are equal to the same number of resources that minikube can provide, then the pod either does not work, or I get an error like: `Unable to connect to the server: net/http: TLS handshake timeout`. Why is this happening if, logically, this pod should have a similar resource limit without specifying `resources.limits`?

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  serviceName: cassandra
  replicas: 1
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      containers:
        - name: cassandra
          image: sevabek/cassandra:latest
          ports:
            - containerPort: 9042
          volumeMounts:
            - mountPath: /var/lib/cassandra
              name: cassandra-storage

          livenessProbe:
            exec:
              command:
                - cqlsh
                - -e
                - ""SELECT release_version FROM system.local;""
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 2

          resources:
            requests:
              memory: ""3500Mi""
              cpu: ""1700m""
            limits:
              memory: ""4Gi""
              cpu: ""2000m""

  volumeClaimTemplates:
    - metadata:
        name: cassandra-storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 3Gi
```","kubernetes, cassandra",79508013.0,"I suspect the container is using more memory than you anticipated because you've configured the liveness probe to run `cqlsh`:

```
          livenessProbe:
            exec:
              command:
                - cqlsh
                - -e
                - ""SELECT release_version FROM system.local;""
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 2
```

`cqlsh` is a full-fledged Python application so it means that it consumes a significant amount of resources to run. It is a little excessive to use it just to check that Cassandra is ""alive"" every 30 seconds.

Cassandra is considered operational if it is listening for client connections on the CQL port (default is `9042`). If something goes wrong for whatever reason (disk failure for example), Cassandra will automatically stop accepting connections and shutdown the CQL port.

Instead of running a CQL `SELECT` statement through `cqlsh`, I would suggest using a low-level TCP check using Linux utilities like `netstat`:

```
$ netstat -ltn | grep 9042
```

If you use a lightweight liveness probe, the Cassandra containers should use significantly less resources. Cheers!",2025-03-14T02:06:57,2025-03-03T14:04:36
79480632,how to get filtered nodes in k8s scheduler api,"I am implementing a custom scheduler for k8s, which works in the scoring stage. A certain function needs to first obtain the nodes that have been filtered by various default filters. I did not find any related functions or methods in the code [https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/](https://github.com/kubernetes/kubernetes/blob/master/pkg/scheduler/)",kubernetes,79480721.0,"You can use clientset from Go Client library, including nodes client and list method with for example LabelSelector. Here is the example:

```
nodes, err := clientset.CoreV1().Nodes().List(
    context.TODO(),
    metav1.ListOptions{LabelSelector: labelSelector},
)
```

Here just need to add metav1 List Options and you can do specific search.",2025-03-03T10:07:50,2025-03-03T09:33:35
79480514,Terraform: Error: Provider produced inconsistent result after apply -- rabbitmqCluster default_user.conf,"I faced this issue when deploying a `Secret` resource using `kubernetes_manifest` in terraform, which I am using for a rabbitmqCluster deployment.

```
╷
│ Error: Provider produced inconsistent result after apply
│
│ When applying changes to kubernetes_manifest.default_user_config, provider
│ ""provider[\""registry.opentofu.org/hashicorp/kubernetes\""]"" produced an
│ unexpected new value: .object.stringData: was
│ cty.MapVal(map[string]cty.Value{""default_user.conf"":cty.StringVal(""default_user
│ = user\ndefault_pass = password\n"")}), but now null.
│
│ This is a bug in the provider, which should be reported in the provider's
│ own issue tracker.
```

in this `kubernetes_manifest`

```
resource ""kubernetes_manifest"" ""default_user_config"" {
  manifest = yamldecode(<<EOF
apiVersion: v1
kind: Secret
metadata:
  name: ""default-user-config""
  namespace: ${var.namespace}
type: Opaque
stringData:
  default_user.conf: |
    default_user = user
    default_pass = password
  # host: dmF1bHQtZGVmYXVsdC11c2VyLmRlZmF1bHQuc3Zj
  # username: my-admin
  # password: super-secure-password
  # port: ""5672""
  # provider: rabbitmq
  # type: rabbitmq
EOF
  )
}
```","kubernetes, terraform",79480515.0,"Adding the `computed_fields` field in the manifest resource and appending the `stringData` solved the issue. The resulting `kubernetes_manifest` is

```
resource ""kubernetes_manifest"" ""default_user_config"" {
  computed_fields = [""stringData""]
  manifest = yamldecode(<<EOF
apiVersion: v1
kind: Secret
metadata:
  name: ""default-user-config""
  namespace: ${var.namespace}
type: Opaque
stringData:
  default_user.conf: |
    default_user = user
    default_pass = password
  # host: dmF1bHQtZGVmYXVsdC11c2VyLmRlZmF1bHQuc3Zj
  # username: my-admin
  # password: super-secure-password
  # port: ""5672""
  # provider: rabbitmq
  # type: rabbitmq
EOF
  )
}
```

reference: [https://github.com/hashicorp/terraform-provider-kubernetes/issues/1769#issuecomment-1176795535](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1769#issuecomment-1176795535)",2025-03-03T08:40:10,2025-03-03T08:40:10
79479584,Kubernetes Cronjob and SMTP,"Situation:
I have a php-apache web browser in a Kubernetes namespace that is controlled by an Ingress. The ingress exposes the web browser to the public IP within the organization, so its intranet and not internet.
The website works well. The organization has smpt server which is configured into php.ini on the web server. The website can trigger email to members of the organization on both the application or logging into the pod and executing the PHP script. The PHP's mail funciton uses sendmail and sends the email.

Problem:
I configured a Cronjob and there are a few PHP scripts executed during this process. These scripts similar to the PHP scripts on the web server, will trigger email depending on the conditions.
The php.ini is configured with organization's smtp host and port as for the web server, however, email could not be sent outside of the cluster, so the smtp server doesn't post it to the ingress from my understanding. I tried linking the web service to Cronjob and a few configurations broke the website, but no success in sending email from the Cronjob.

However, Cronjob scripts can communicate with organization's API as the web server, except email.

Request:
Any example of enabling email functionality within the cronjob would be much appreciated. This doesn't include any separate mail server to be configured for the application within the Kubernetes namespace or cluster.","kubernetes, cron, smtp",,,,2025-03-02T19:05:48
79479368,Pycharm remote debugger is stuck on waiting for connection when running from Pycharm `Cloud Code: Kubernetes` run configuration,"I'm trying to run in debug mode, a Python FastAPI application running in a container using PyCharm's Cloud Code plugin, but I can't get breakpoints to work.

### Setup:

- PyCharm with Cloud Code plugin
- Python 3.11 FastAPI application using base docker image from `tiangolo/uvicorn-gunicorn-fastapi:python3.11`
- docker-desktop local deployment via `Cloud Code: Kubernetes` run configuration

### What happens:

1. The container deploys successfully
2. Port forwarding works (both for app and debug ports)
3. The application runs normally
4. Breakpoints are never hit - I see a background task on debugger which says ""waiting for connection""
[![waiting for connection in debugger](https://i.sstatic.net/WxmU6Gow.png)](https://i.sstatic.net/WxmU6Gow.png)

### Logs show this warning (not sure if relevant):

```
[elon] WARN[0000] not a python launcher: unable to determine python version from ""/usr/bin/env"": exit status 125
```

### From container inspection:

```
$ kubectl exec -it pod-name -- ps -ef
root         1     0  0 13:39 pts/0    00:00:00 /dbg/python/launcher --mode pydevd --port 5678 -- /start-reload.sh
root        13     1  0 13:39 pts/0    00:00:01 /usr/local/bin/python3.11 /usr/local/bin/uvicorn --reload --host 0.0.0.0 --port 8080 --log-level info app.web:app
```

### The container can successfully reach my host machine:

```
kubectl exec -it pod-name -- wget -O- http://host.docker.internal:63341
HTTP request sent, awaiting response... 404 Not Found
```

### Port forwarding is active

```
Port forwarding pod/pod-name in namespace default, remote port 5678 -> http://127.0.0.1:5678
```

### Environment details:

- PyCharm Professional `2024.3.2`
- Cloud Code plugin version `24.11.1-233-api-version-223`
- macOS Sequoia `15.3.1`
- Docker Desktop version `4.38.0 (181591)`

[![debugger settings](https://i.sstatic.net/zKMMwp5n.png)](https://i.sstatic.net/zKMMwp5n.png)
[![run configuration](https://i.sstatic.net/tCCXaDay.png)](https://i.sstatic.net/tCCXaDay.png)","python, docker, kubernetes, pycharm, google-cloud-code",,,,2025-03-02T16:09:22
79478161,Unexpected subPath Behavior in Kubernetes: Auto-Created Directories with Root Ownership and Permission Issues,"I’m observing unexpected behavior when using subPath in a Kubernetes Pod’s volume mount.

Pod Definition:

```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - name: main-container
    image: busybox
    command: [""sh"", ""-c"", ""while true; do echo Running; sleep 60; done""]
    securityContext:
      runAsUser: 1001
      runAsGroup: 20
    volumeMounts:
    - mountPath: /work-dir
      name: workdir
      subPath: app/data/my-pod-data
  volumes:
  - name: workdir
    persistentVolumeClaim:
      claimName: nfspvc
```

Note: `app/data` directory already exists in the Persistent Volume.

Observed Behavior:

If my-pod-data does not exist, it is automatically created—but with root ownership:

```
drwxr-xr-x. 2 root root   0 Mar  1 18:56 my-pod-data
```

This was observed from another pod (Let's call it other-pod) mounting app/data from the same PV.
I cannot create files within my-pod-data from either my-pod or other-pod, which is expected since write permissions are only available to the root user.
However, I can delete my-pod-data from other-pod, even though it is running with a non-root security context.

Nested Directories Behavior:

If the subPath includes multiple non-existent nested directories (e.g., app/data/a/b/c), the behavior changes. This time, I cannot delete a, b, or c from other-pod.

This behavior is confusing, and I couldn’t find much documentation about it:
[https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath](https://kubernetes.io/docs/concepts/storage/volumes/#using-subpath)

Can someone clarify why this happens?","kubernetes, nfs, persistent-volumes",,,,2025-03-01T19:39:31
79477584,In helm hooks annotation the previous job is not getting deleted,"Im using latest helm version v3.17.1. but while trying to upgrade it always throws Error:

> UPGRADE FAILED: cannot patch ""populate-ifsc-job"" with kind Job:
> Job.batch ""populate-ifsc-job"" is invalid: spec.template: Invalid
> value.

```
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include ""helm-generic-template.fullname"" . }}-job
  labels:
    {{- include ""helm-generic-template.labels"" . | nindent 4 }}-job
  annotations:
    ""helm.sh/hook-weight"": ""100""
    ""helm.sh/hook-delete-policy"": before-hook-creation
spec:
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include ""helm-generic-template.selectorLabels"" . | nindent 8 }}
  backoffLimit: 1
  template:
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      containers:
        {{- range .Values.job.containers }}
          - name: {{ .name }}
            image: ""{{ .image.repository }}:{{ .image.tag }}""
            imagePullPolicy: ""{{ .image.pullPolicy }}""
            {{- if .args }}
            args:
              {{- toYaml .args | nindent 12 }}
            {{- end }}
      restartPolicy: Never
```

when i dispatch this job from the github actions it works, but on new commits it fails to upgrade.","kubernetes, kubernetes-helm",,,,2025-03-01T12:43:32
79476126,Kubernetes Weighted Node Affinity always prioritized over higher weight pod affinity,"I have the following affinity defined on a pod. When this pod gets scheduled, it is always respecting the NodeAffinity first. Even if both pod affinities are satisfied, they are ignored. Are weights between node and pod affinity always ignored and node affinity always wins? or am I doing something else wrong?

K3d v5.7.5

Kubernetes Version: v1.30.6+k3s1

```
   spec:
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: example.com/capacity-remaining
                operator: Gt
                values:
                - ""2457600""
            weight: 1
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: example.com/lead-pod
                  operator: In
                  values:
                  - ""receiver-0""
              topologyKey: kubernetes.io/hostname
            weight: 100
          - podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: example.com/imagery-destination
                  operator: In
                  values:
                  - live-stitcher
              topologyKey: kubernetes.io/hostname
            weight: 80
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: example.com/regime
                operator: In
                values:
                - ingest
            topologyKey: kubernetes.io/hostname
```","kubernetes, k3d",79476836.0,"Pod affinity are not ignored, it's just that weights for **soft affinity** are important and respected first because it influences the scheduler to find a node that meets the rule as concept of [node affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity):

```
preferredDuringSchedulingIgnoredDuringExecution
```

Configs are good. I suggest removing your weights with your pod affinity as it is only applicable with preferred affinity config and experiment with your affinity. Setting pod affinity as **hard affinity** for stricter conditions wherein if pod conditions aren’t met it will not be scheduled at all:

```
requiredDuringSchedulingIgnoredDuringExecution
```",2025-02-28T22:43:46,2025-02-28T16:19:44
79475686,How to Properly Run a gRPC Service in AKS? Getting HTTP/1.x Request Sent to HTTP/2 Only Endpoint Error,"Can someone help me? How can I run a gRPC service in AKS? I am currently facing an issue with my gRPC service.

Below are the configuration details I am using:

dockerfile

```
# Use base image for running the application
FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 3042

# Use SDK image for building the application

FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
ARG BUILD_CONFIGURATION=Release
WORKDIR /src

# Copy the entire solution to the build context (important for
dependencies)

COPY . .

# Restore dependencies

WORKDIR /src/src/Account/CC.Account.Queries.API
RUN dotnet restore ""CC.Account.Queries.API.csproj""

# Build the application

RUN dotnet build ""CC.Account.Queries.API.csproj"" -c
$BUILD_CONFIGURATION -o /app/build

# Publish the application

FROM build AS publish
ARG BUILD_CONFIGURATION=Release
RUN dotnet publish ""CC.Account.Queries.API.csproj"" -c
$BUILD_CONFIGURATION -o /app/publish /p:UseAppHost=false

# Final image for running the application

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .

# Start the application

ENTRYPOINT [""dotnet"", ""CC.Account.Queries.API.dll""\]
```

**Deployment.yaml**

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: accountqueries-api
  namespace: admin-backend
spec:
  replicas: 1
  selector:
    matchLabels:
       app: accountqueries-api
  template:
     metadata:
     labels:
        app: accountqueries-api
     spec:
       containers:
        - name: accountqueries-api
          image: 824/accountqueriesapi:8 # Replace with your
          actual image
           ports:
             - containerPort: 3042
             - containerPort: 80
           env:
             - name: ASPNETCORE_ENVIRONMENT
               value: ""Development""
             - name: ASPNETCORE_URLS
               value: ""http://+""
```

**Service.yaml**

```
apiVersion: v1
kind: Service
metadata:
  name: accountqueries-api
  namespace: admin-backend
spec:
  selector:
     app: accountqueries-api
  ports:
    - name: grpc
      protocol: TCP
      port: 3042
      targetPort: 3042
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP
```

Issue

When I forward the pod to localhost using kubectl port-forward, I see this response:

An HTTP/1.x request was sent to an HTTP/2 only endpoint.

This indicates that the request is being sent to the gRPC service using HTTP/1.1, whereas gRPC expects HTTP/2.

Additionally, when testing using Postman with the provided .proto file and authentication, I am not receiving any response.

What am I missing in my configuration? How can I properly expose and test my gRPC service in AKS?","asp.net, .net, kubernetes, grpc",,,,2025-02-28T13:28:10
79475508,"controller runtime: GrandPa, Son, GrandSon and setting OwnerReferences","I write a Kubernetes controller with controller-runtime.

Imagine there are three CRDs: `GrandPa`, `Son`, and `GrandSon`. Each has its own controller in a separate deployment.

I am writing the `GrandPa` controller. When the user creates a `GrandPa` object, my controller creates a `Son` object and uses `SetControllerReference(grandpa, son)`.

The `Son` controller detects the new `Son` object, creates a `GrandSon` object, and uses `SetControllerReference(son, grandson)`.

My `GrandPa` controller needs its `Reconcile()` function to be triggered when a `GrandSon` object changes.

Afaik `.Owns()` does not work for `GrandPa` watching `GrandSon` because the ownership chain is indirect (`GrandPa -> Son -> GrandSon`).

I can only change the source code of GrandPa controller.

How to get Reconcile() of GrandPa get triggered when GrandSon changes?","go, kubernetes, kubernetes-custom-resources, controller-runtime",79497058.0,"Your question is pretty generic, however from what I can understand, you should use `Watches()` to make `GrandPa` reconcile when a `GrandSon` changes. Since `GrandPa` isn’t a direct owner, manually map `GrandSon` to `GrandPa` through `Son`, like so:

```
err = ctrl.NewControllerManagedBy(mgr).
    For(&GrandPa{}).
    Owns(&Son{}).
    Watches(
        &source.Kind{Type: &GrandSon{}},
        handler.EnqueueRequestsFromMapFunc(func(grandson client.Object) []reconcile.Request {
            var sonRef *metav1.OwnerReference
            for _, owner := range grandson.GetOwnerReferences() {
                if owner.Kind == ""Son"" {
                    sonRef = &owner
                    break
                }
            }
            if sonRef == nil {
                return nil
            }

            son := &Son{}
            if err := mgr.GetClient().Get(context.TODO(), types.NamespacedName{
                Name:      sonRef.Name,
                Namespace: grandson.GetNamespace(),
            }, son); err != nil {
                return nil
            }

            var grandpaRef *metav1.OwnerReference
            for _, owner := range son.GetOwnerReferences() {
                if owner.Kind == ""GrandPa"" {
                    grandpaRef = &owner
                    break
                }
            }
            if grandpaRef == nil {
                return nil
            }

            return []reconcile.Request{{
                NamespacedName: types.NamespacedName{
                    Name:      grandpaRef.Name,
                    Namespace: son.GetNamespace(),
                },
            }}
        }),
    ).Complete(r)
```",2025-03-10T05:05:50,2025-02-28T12:24:25
79475508,"controller runtime: GrandPa, Son, GrandSon and setting OwnerReferences","I write a Kubernetes controller with controller-runtime.

Imagine there are three CRDs: `GrandPa`, `Son`, and `GrandSon`. Each has its own controller in a separate deployment.

I am writing the `GrandPa` controller. When the user creates a `GrandPa` object, my controller creates a `Son` object and uses `SetControllerReference(grandpa, son)`.

The `Son` controller detects the new `Son` object, creates a `GrandSon` object, and uses `SetControllerReference(son, grandson)`.

My `GrandPa` controller needs its `Reconcile()` function to be triggered when a `GrandSon` object changes.

Afaik `.Owns()` does not work for `GrandPa` watching `GrandSon` because the ownership chain is indirect (`GrandPa -> Son -> GrandSon`).

I can only change the source code of GrandPa controller.

How to get Reconcile() of GrandPa get triggered when GrandSon changes?","go, kubernetes, kubernetes-custom-resources, controller-runtime",79496562.0,"The `.Owns()` method in a Kubernetes controller allows you to observe changes to an object if it owns the object or if the object has a direct dependency on the controller (i.e. owns it). However, in your case, the ownership chain is indirect: the `GrandPa` controller does not directly own the `GrandSon` object, but rather owns `Son`, which in turn owns `GrandSon`.

In order to call `GrandPa` controller's `Reconcile()` when there's a change to `GrandSon`, you could follow the pattern of event watching and filtering `GrandSon` objects for `GrandPa`.

You need to create a `watch` for `GrandSon` in the `GrandPa controller` and check whether `GrandSon` updates apply to some `GrandPa` through `Son`. Wherever you do encounter a match, you can initiate an update of `GrandPa` manually so it receives the changes entered in `GrandSon`.

```
func (r *GrandPaReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {

    // we will try to create a list of objects in the grandson
    var grandsons mygroupv1.GrandSonList
    if err := r.List(ctx, &grandsons, client.InNamespace(req.Namespace)); err != nil {
        return ctrl.Result{}, err
    }

    // -> now we go through each object of this grandson
    for _, grandson := range grandsons.Items {

        // and we check if there is an indication of the owner (son)
        if owner := metav1.GetControllerOf(&grandson); owner != nil {
            if owner.Kind == ""Son"" {

                // checking if there is a connection between grandson and grandpa through son
                if grandson.Labels[""grandpa-id""] == req.Name {

                    // if a connection is found, trigger reprocessing for grandpa
                    return ctrl.Result{}, nil // (here you can implement the grandpa update logic)
                }
            }
        }
    }

    // return the controller to the standard processing loop
    return ctrl.Result{}, nil
}
```",2025-03-09T20:34:56,2025-02-28T12:24:25
79475258,Multicluster mesh with admiral,"I am trying to install Admiral to create multi-cluster mesh, [here](https://github.com/istio-ecosystem/admiral/blob/master/docs/Examples.md) is the proccess of installation, but I faced with an issue.

`admiral-sync` namespace is empty after installation and there are some .go errors in admiral's pod logs:

```
reflector.go:539] pkg/mod/k8s.io/client-go@v0.29.2/tools/cache/reflector.go:229: failed to list *v1alpha3.Sidecar: Get

   https://11.11.11.11/apis/networking.istio.io/v1alpha3/sidecars?
    limit=500&resourceVersion=0: tls: failed to verify certificate: x509: certificate signed by unknown authority
```

The connection between main and remote clusters is based on kubeconfigs ([here](https://github.com/istio-ecosystem/admiral/blob/master/install/scripts/cluster-secret.sh) is the script that creates the secret). So as I understand my main cluster is unavailable to connect to remote clusters because of x509 error. Unfortunately it is not working even if I try to add `insecure-skip-tls-verify: true`

Maybe somebody have an idea how to fix it?","go, kubernetes, tls1.2, istio",,,,2025-02-28T10:43:35
79474815,Why Godror make Oracle ACL recognizes the program name as `main@&lt;hostname&gt; (TNS V1-V3)`?,"I am working on a Go project that connects to an Oracle database using the `godror` driver. The Oracle DB has an ACL that includes the program name. However, the K8s pods with some replicas cannot fix the hostname to a unique value, which prevents registering a unique program name on the ACL.

The program name, the Oracle DB recognizes

> `<go-init-name>@<hostname> (TNS V1-V3)`

I have set the program name using the `ALTER SESSION SET` commands, but Oracle ACL does not recognize it.

```
params.SetSessionParamOnInit(""CLIENT_INFO"", ""main"")
params.SetSessionParamOnInit(""MODULE"", ""main"")
params.SetSessionParamOnInit(""ACTION"", ""main"")
params.SetSessionParamOnInit(""PROGRAM"", ""main"")
```

ChatGPT guided option strings, but it seems godror does not support it.

```
url := os.Getenv(""ORACLE_URL"") + ""?appname=main&module=main&program=main&connectionClass=main&standaloneConnection=1""
```

I have checked the godror repo and found that the `(TNS V1-V3)` part is not explicitly set in the code. It seems to be automatically inserted by the Oracle client library.

**Questions:**

1. What is causing Oracle to recognize the program name as `main@<hostname> (TNS V1-V3)`?
2. How can I ensure that Oracle ACL recognizes the program name as `main` without the `(TNS V1-V3)` suffix?

Any insights or solutions would be greatly appreciated. Thank you!","oracle-database, go, kubernetes",79477032.0,"Godror sits on [ODPI-C](https://github.com/oracle/odpi) which uses [Oracle Call Interface](https://docs.oracle.com/en/database/oracle/oracle-database/23/lnoci/index.html).  OCI does not provide an API to change the PROGRAM, HOST or similar values. I expect that the original Oracle designers intended those columns to be immutable so that DBAs could properly track what was connecting - even recently DBAs have mentioned this as a concern.

At some stage JDBC thin mode and now [python-oracledb thin mode](https://python-oracledb.readthedocs.io/en/latest/release_notes.html#id4) and [node-oracledb thin mode](https://node-oracledb.readthedocs.io/en/latest/release_notes.html#node-oracledb-v6-7-0-18-nov-2024) have allowed these additional values to be set.  Currently, general OCI-based solutions are to fiddle with the application binary names (e.g. `node --title xxx` with node-oracledb thick mode) but the `(TNS V1-V3)` will likely be appended internally by OCI. Or you can make use of another attribute.

You could open an request on the [Godror repo](https://github.com/godror/godror) to make [DriverName](https://github.com/godror/godror/blob/67aeda6400907d4ea0fcf9b0730741ea79b8206c/drv.go#L115) be settable since this will be passed to OCI as the CLIENT_DRIVER of the V$SESSION_CONNECT_INFO view.",2025-03-01T02:25:18,2025-02-28T07:39:16
79473923,Updating k8s operator admission webhook code does not take,"After adding some additional validation logic to one of my admission webhooks that I created via [kubebuilder instructions here](https://book.kubebuilder.io/cronjob-tutorial/webhook-implementation), and re-running `make` (which is successful, below output):

```
/Users/kp/my-operator/bin/controller-gen rbac:roleName=manager-role crd:allowDangerousTypes=true webhook paths=""./..."" output:crd:artifacts:config=config/crd/bases
/Users/kp/my-operator/bin/controller-gen object:headerFile=""hack/boilerplate.go.txt"" paths=""./...""
go fmt ./...
go vet ./...
go build -o bin/manager cmd/main.go
```

...the code changes (some additional validation statements, logging output) are not taking, meaning that the old logging output and validation code is still being executed. I've tried building a new docker image with `--no-cache` and applying it to my local cluster, but that does not work either. Commands being ran (in order):

```
kind create cluster --name test-cluster
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.yaml
docker build -t my-operator:latest .
kind load docker-image my-operator:latest --name test-cluster
make deploy IMG=my-operator:latest
```

I'm looking for suggestions on causes regarding why my code changes would not be applied after rebuilding the operator and applying it to a brand new cluster spun up w/ kind (or if anyone else has ran into this before).","kubernetes, webhooks, kubebuilder",79473979.0,running `docker system prune -a` and then building a new image seemed to fix this issue.,2025-02-27T21:33:11,2025-02-27T20:56:25
79473080,sidebase-auth issue in kuberentes,"I have a dockerized nuxt3 app with sidebase-auth using AzureAD provider running on local K8S using rancher desktop. I'm utilising port forwarding to map service with localhost port.

Deployment works fine, I can see `Listening on http://0.0.0.0:3000` in pod logs but when I go to http://localhost:63729 it loads for a moment and then there is an error

```
500 [GET] ""http://localhost:63729/api/auth/session"": <no response> fetch failed
```

When I copy that exact link and visit it from the browser I get response with session data so I presume the issue is coming from the node server not being able to fetch some data from that address. Which makes sense cause inside the cluster there is nothing running on port **63729**. I tried configuring `NuxtAuthHandler` to detect if the request comes from the client or from the server and to use different URLs respectively but it doesn't seem to work

server/api/auth/[...].ts

```
import AzureADProvider from 'next-auth/providers/azure-ad';
import { NuxtAuthHandler } from '#auth';

function getAuthBaseUrl() {
  const config = useRuntimeConfig();
  if(import.meta.client) {
    return 'http://localhost:63729';
  } else {
    return 'http://service-name';
  }
}

export default NuxtAuthHandler({
  secret: useRuntimeConfig().auth.secret,
  baseURL: getAuthBaseUrl(),
  providers: [
    AzureADProvider.default({...})
  ]
})
```

The app is a starter application with landing page for signing in / singing out. The auth setup is directly copy-pasted from sidebase-auth docs.

Dockerfile:

```
FROM node:22-slim AS base
WORKDIR /src

FROM base AS build
COPY --link . .
RUN npm install -g pnpm
RUN pnpm install
RUN pnpm run build

FROM base
COPY --from=build /src/.output /src/.output
ENV PORT 3000
ENV HOST 0.0.0.0
ENV NODE_ENV=production
CMD [ ""node"", "".output/server/index.mjs"" ]
EXPOSE 3000
```

Versions:

```
nuxt: 3.15.4
@sidebase/nuxt-auth: 0.6.7
nuxt-auth: 4.21.1
```","kubernetes, nuxt3.js, nuxt-auth",,,,2025-02-27T15:02:44
79473008,Unable to connect to the server: dial tcp 192.168.64.3:8443: i/o timeout,"I want to run k8s cluster locally with help of mimikube.
My instruments

```
MacOS 14.7.3
minikube v1.30.1 on Darwin 14.7.3
kubectl version --client   --- Kustomize Version: v4.5.7
```

My actios

```
# Started minikube (looks fine)
minikube start
😄  minikube v1.30.1 on Darwin 14.7.3
🎉  minikube 1.35.0 is available! Download it: https://github.com/kubernetes/minikube/releases/tag/v1.35.0
💡  To disable this notice, run: 'minikube config set WantUpdateNotification false'

✨  Using the hyperkit driver based on existing profile
👍  Starting control plane node minikube in cluster minikube
🔄  Restarting existing hyperkit VM for ""minikube"" ...

# run kubectl
kubectl apply -f pod.yaml
Unable to connect to the server: dial tcp 192.168.64.3:8443: i/o timeout
```

pod.yaml

```
apiVersion: v1
kind: Pod
metadata:
  name: test-pod
  labels:
    app: test-app
spec:
  containers:
  - name: nginx
    image: nginx:latest
    ports:
    - containerPort: 80
```

How to fix the error?",kubernetes,79474086.0,"To fix the error, [verify kubectl configuration](https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/#verify-kubectl-configuration) to see if kubectl is correctly configured to access your cluster using:

```
kubectl cluster-info dump
```",2025-02-27T22:34:53,2025-02-27T14:33:10
79472851,"Getting org.quartz.JobPersistenceException error while creating Quartz Job, Trigger on SpringBoot running on Kubernetes","Getting org.quartz.JobPersistenceException error while creating Quartz Job, Trigger on SpringBoot running on Kubernetes and JobDatamap is maintained on PostgresDB / Aurora MySQL running on AWS.

It works for some time like 24 hours then it start failing. And then keeps failing constantly.

```
@Service
@Slf4j
@Component
public class XYZJobScheduler {

  @Autowired
  private Scheduler scheduler;

  /**
   * Schedule a new job.
   *
   * @param jobName The name of the job
   * @param jobClass The class representing the job
   * @param jobDataMap Data map for job details
   * @throws SchedulerException If there is an issue with scheduling the job
   */
  public void scheduleJobDetail(
      String jobName, Class<? extends Job> jobClass, JobDataMap jobDataMap) {
    try {

      JobKey jobKey = new JobKey(jobName);

      if(scheduler.checkExists(jobKey)){
        log.error(""Job with name, jobname={} already exists"",jobName);
        return;
      }

      JobDetail jobDetail =
          JobBuilder.newJob(jobClass)
              .withIdentity(jobName)
                  .storeDurably()
              .usingJobData(jobDataMap)
              .build();
      scheduler.addJob(jobDetail,false);
      scheduleJobTrigger(jobDataMap, jobDetail, jobName);
    } catch (Exception e) {
      log.error(""Error occurred while creating a job, jobName={} error={}"", jobName, e);
    }
  }

  /**
   * Schedule a new job with a default trigger that starts now.
   *
   * @param jobDataMap Data map for job details
   * @param jobDetail
   * @param jobName
   */
  private void scheduleJobTrigger(JobDataMap jobDataMap, JobDetail jobDetail, String jobName) {
    try {
      Trigger trigger =
          TriggerBuilder.newTrigger()
              .forJob(jobDetail)
              .withIdentity(jobName)
              .usingJobData(jobDataMap)
              .startNow()
              .build();
      scheduler.scheduleJob(trigger);
    } catch (Exception e) {

      log.error(""Exception occurred while executing trigger={}, e={}"", jobName, e);
    }
  }
```

[![Quartz Configuration](https://i.sstatic.net/lpbYqV9F.png)](https://i.sstatic.net/lpbYqV9F.png)","spring-boot, kubernetes, quartz",,,,2025-02-27T13:37:40
79472562,Upgrading apiextensions-apiserver to v0.30.10 bumps k8s deps to v0.32.2; how to keep them at v0.30.10 (no replace)?,"In my project, all the k8s-related libraries are currently at version v0.26.6:

- k8s.io/api v0.26.6
- k8s.io/apiextensions-apiserver v0.26.6
- k8s.io/apimachinery v0.26.6
- k8s.io/apiserver v0.26.6
- k8s.io/client-go v0.26.6

However, when I run:

```
go get -u k8s.io/apiextensions-apiserver@v0.30.10
```

the k8s.io/apiextensions-apiserver package is upgraded to v0.30.10, but other k8s modules (such as k8s.io/api, k8s.io/apimachinery, k8s.io/apiserver, and k8s.io/client-go) get upgraded to the latest versions (e.g. v0.32.2).

Why does upgrading k8s.io/apiextensions-apiserver to v0.30.10 result in the other modules being pulled in at a higher version? And how can I upgrade all these k8s libraries uniformly to v0.30.10?","go, kubernetes, go-modules, kubernetes-go-client",,,,2025-02-27T11:44:07
79472167,Zeppelin k8s dots in pod name,"I've configured my zeppelin on kubernetes according to [documentation](https://zeppelin.apache.org/docs/0.12.0/quickstart/kubernetes.html)

I've also configured [trino interpreter](https://trino.io/docs/current/overview.html)
I had used to use scoped per user mode in standard Zeppelin setup, so I tried it in my k8s deployment. It leads Zeppelin to create a Pod and Service with the same name like `<interpreter_group>-<username>-<RANDOM_STRING>` (for example `trino-mr.user-zpxesi`).

But Kubernetes doesn't allow creating Service name with dots in it (more information in [issue](https://github.com/kubernetes/kubernetes/issues/3752#issuecomment-234752944)).

If username contains dots, it cause error:

```
Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/zeppelin/services. Message: Service ""trino-mr.user-zpxesi"" is invalid: metadata.name: Invalid value: ""trino-mr.user-zpxesi"": a DNS-1035 label must consist of lower case alphanumeric characters or '-', start with an alphabetic character, and end with an alphanumeric character (e.g. 'my-name',  or 'abc-123', regex used for validation is '[a-z]([-a-z0-9]*[a-z0-9])?')
```

I can change Service name template in interpreter Pod definition, but in this case Zeppelin server still tries to resolve invalid interpreter Pod dns name with dot and this cause error:

```
java.lang.RuntimeException
    at org.apache.zeppelin.interpreter.remote.PooledRemoteClient.callRemoteFunction(PooledRemoteClient.java:123)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.callRemoteFunction(RemoteInterpreterProcess.java:100)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreterProcess.init(RemoteInterpreterProcess.java:104)
    at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:72)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:106)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:156)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:128)
    at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:273)
    at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:428)
    at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:68)
    at org.apache.zeppelin.scheduler.Job.run(Job.java:187)
    at org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:136)
    at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:186)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
```

Is there any way to fix it or change rules for resolving interpreter Pod dns name?","kubernetes, apache-zeppelin, trino",,,,2025-02-27T09:22:14
79471629,"Kubernetes scheduler, empty node while scheduling","Problem: how to achieve exactly opposite behaviour for bin packing strategy in k8s?

Details: we have one app hosted on k8s. We run a couple of pods which we need to scale down and up very frequently. Scale count is anywhere between 2-400.

With affinity, we achieved bin packing scheduling very well.

```
  affinity:
    podAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: some-key
              operator: In
              values:
              - some-value
          topologyKey: kubernetes.io/hostname
        weight: 100
```

But while scaling down, k8s scheduler ended up with removing pods from all nodes.

We are looking for configs/settings by which scheduler can remove pods from specific node first (not forcefully but best try) So that my cloud provider can remove empty nodes.

Can default scheduler has some settings to achieve such thing? Can any other tool can support this case?

Thanks in advance.","kubernetes, scheduling, bin-packing",,,,2025-02-27T04:52:13
79471107,Is there a way to restrict access to directories within a Kubernetes container?,"If I have two groups that are **not root users** that will access a container's directory structure, is there a way to fine tune permissions such that **Group 1** can have WRITE permissions on /DIR1, but **Group 2** only has READ or even NO ACCESS permissions on /DIR1? Assuming that this **/DIR1 is NOT A MOUNTED VOLUME?**

Does the answer change if the directory IS a mounted volume?

I am unable to find an absolute answer online, but I think I might be touching on something called a security context, though I can't quite wrap my head around it, so I don't know if I am understanding it correctly as the examples always show a root, and a non-root user. But never two non-root users.

I have considered the following avenues:

- **RoleBindings**, but I am unable to find how I can limit or tweak something like the existing Read-Only role to point to specific directories? It seems to read K8 resources.
- I cannot completely remove all roles from **Group 2** as they will have to access the pods at some point to troubleshoot. Maybe.
- I know you can chmod / chown in the dockerfile during image build, but.... not sure how this would tie into users that log in and a variety of groups that may need to access the same directory. Like what if Group 1 and Group 3 need access? Can you chown 2 groups? Does it even work like that?","kubernetes, containers, rbac",79471131.0,"In your Dockerfile, create groups/users and set strict permissions:

```
RUN groupadd group1 && groupadd group2 && \
useradd -g group1 user1 && useradd -g group2 user2 && \
mkdir /DIR1 && \
chown user1:group1 /DIR1 && \  # Owned by user1 and group1
chmod 770 /DIR1  # rwx for owner/group, no access for others
```

In the pod’s YAML, set the runtime identity:

```
securityContext:
runAsUser: 1000
runAsGroup: 1000
```

Use fsGroup to set volume group:

```
securityContext:
fsGroup: 1000
```

(if you want to) Use an initContainer to fix permissions:

```
initContainers:
 - name: fix-permissions
   image: busybox
   command: [""sh"", ""-c"", ""chmod 770 /DIR1""]
   volumeMounts:
    - name: my-volume
      mountPath: /DIR1
```",2025-02-26T22:10:06,2025-02-26T21:53:07
79469947,How to properly escape symbols in ansible command?,"I want to execute the command on each node:

```
docker ps --format '{{.Names}}' -a | egrep -v '^k8s.*$'
```

I tried millions of variants to execute command in ansible, including:

```
- hosts: kubernetes
  tasks:
    - name: check docker
      command:
        cmd: docker ps --format '{{.Names}}' -a | egrep -v '^k8s.*$'
      register: doc

    - debug: var=doc.stdout_lines
```

I tried to escape characters.
But nothing works. So, how to make ansible execute the my docker command on each host?

PS I want to list containers that ain't controlled by k8s","kubernetes, ansible",79470575.0,"You have several problems with your task:

1. you are using the [`command` module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/command_module.html) with shell syntax (pipelining multiple commands)

The error in this case:

> unknown shorthand flag: 'v' in -v See 'docker ps --help'.

As you can see, the pipe is ignored and the `-v` parameter is passed to the Docker command.
2. double curly braces in a string. Double curly braces are interpreted as Jinja2 syntax in Ansible and attempts to templatize.

The error in this case:

> template error while templating string: unexpected '.'.

### Solution for problem 1

- You can use the [`shell` module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html), here you can execute all commands that are possible in a shell.
- Alternatively, you can simply execute the Docker command with the [`command` module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/command_module.html) and filter the resulting data in Ansible.

*See below for examples.*

### Solution for problem 2

- Define the curly brackets as variables and insert them at the appropriate place.

```
- name: check docker
  command:
    cmd: ""docker ps --format '{{ cbo }} .Names {{ cbc }}' -a""
  vars:
    cbo: ""{{ '{{' }}""  # curly braces open
    cbc: ""{{ '}}' }}""  # curly braces close
  register: doc
```
- You can define the entire string as [raw string with marker `!unsafe`](https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_advanced_syntax.html#unsafe-or-raw-strings). Then it is not templated by Ansible with Jinja2, but then you can't templatize other variables in this string.

```
- name: check docker
  command:
    cmd: !unsafe ""docker ps --format '{{.Names}}' -a""
  register: doc
```

### Final resulting task

- **using [`command` module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/command_module.html), filter data in Ansible and `!unsafe` marker**

```
- name: check docker
  command:
    cmd: !unsafe ""docker ps --format '{{.Names}}' -a""
  register: doc

- name: filter docker container names
  set_fact:
    container_names: ""{{ doc.stdout_lines | reject('match', '^k8s') }}""

- debug: var=container_names
```

With the [`reject` filter](https://jinja.palletsprojects.com/en/stable/templates/#jinja-filters.reject), all list elements that match `^k8s` are discarded.
- **using [`shell` module](https://docs.ansible.com/ansible/latest/collections/ansible/builtin/shell_module.html) and `!unsafe` marker**

```
- name: check docker
  shell:
    cmd: !unsafe ""docker ps --format '{{.Names}}' -a | egrep -v '^k8s.*$'""
  register: doc

- debug: var=doc.stdout_lines
```",2025-02-26T17:25:37,2025-02-26T13:41:08
79469659,Quarkus-Helm: Adding custom templates to Helm-Chart possible?,"I wrote a Quarkus 3.15.0 app that also makes use of the Quarkus-Helm plugin. Now I am required to write network policies and would like to add some more templates to the Chart. I want to do this by copying files into the chart before it is compressed and pushed. However, the files always get overwritten during the Gradle build process.

This is the template file as an example:

```
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: {{ .Values.namespace }}
spec:
  podSelector: {}
  policyTypes:
    - Egress
```

And this is the Gradle-Copy-Task:

```
tasks.register('copy', Copy) {
    from file (""src/main/helm/templates/networkpolicy-deny-all.yaml"")
    into layout.buildDirectory.dir(""helm/kubernetes/my-app/templates"")
}
```

I also tried to add these to kubernetes/common.yml but it does not accept Helm variables.

My goal is to have a single packaged helm chart that contains everything I need for my application and that I can seamleassly integrate into the existing build-process with Gradle.","kubernetes, gradle, kubernetes-helm, quarkus",,,,2025-02-26T12:08:18
79469513,How read a file from a pod in Azure Kubernetes Service (AKS) in a Pythonic way?,"I have a requirement to read a file which is located inside a particular folder in a pod in AKS.

My manual flow would be to:

1. exec into the pod with kubectl.
2. cd to the directory where the file is located.
3. cat the file to see it's contents.

I want to automate all this purely using python. I am able to do it with [subprocess](https://docs.python.org/3/library/subprocess.html) but that would work only on a machine which has azure and kubectl setup.

Thus, I am looking for a purely pythonic way of doing this. I have looked into the [Kubernetes client for Python](https://github.com/kubernetes-client/python) but I am not able to find a way to do everything which I listed above.","python, kubernetes, azure-aks",79488905.0,"To read a file which is located inside a particular folder in a pod in AKS via Python script, follow the below steps

Assuming you have a valid aks cluster up and running, deploy a pod with your desired file.

For example -

```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: my-app
spec:
  containers:
  - name: my-container
    image: busybox
    command: [""/bin/sh"", ""-c"", ""echo 'Hello from AKS' > /data/file.txt && sleep 3600""]
    volumeMounts:
    - name: data-volume
      mountPath: ""/data""
  volumes:
  - name: data-volume
    emptyDir: {}
```

```
kubectl apply -f pod.yaml
kubectl get pods
```

![enter image description here](https://i.imgur.com/KtDWRDh.png)

![enter image description here](https://i.imgur.com/lNQeWzO.png)

This one says `Hello from AKS' and it should reflect the same when you read the file from the pod using python.

Install / update the necessary dependencies

`pip install kubernetes`

Here's the script-

```
from kubernetes import client, config, stream

def read_file_from_pod(namespace: str, pod_name: str, container_name: str, file_path: str) -> str:
    try:
        config.load_incluster_config()
    except config.config_exception.ConfigException:
        config.load_kube_config()

    api_instance = client.CoreV1Api()
    command = [""cat"", file_path]

    try:
        exec_response = stream.stream(
            api_instance.connect_get_namespaced_pod_exec,
            name=pod_name,
            namespace=namespace,
            command=command,
            container=container_name,
            stderr=True,
            stdin=False,
            stdout=True,
            tty=False,
        )
        return exec_response
    except Exception as e:
        return f""Error reading file from pod: {str(e)}""

if __name__ == ""__main__"":
    namespace = ""default""
    pod_name = ""my-pod""
    container_name = ""my-container""
    file_path = ""/data/file.txt""

    file_contents = read_file_from_pod(namespace, pod_name, container_name, file_path)
    print(""File Contents:"", file_contents)
```

Save and run the script. Now you can read a file from a pod in AKS in a Pythonic way.

![enter image description here](https://i.imgur.com/9WaMGqx.png)",2025-03-06T09:47:39,2025-02-26T11:19:18
79469477,Localstack DynamoDB : PutItem operation exceeded maximum number of attempts,"I set up a DynamoDB table on a Kubernetes pod using LocalStack, which is exposed via a Kubernetes service. I'm trying to make a `PutItem` request to this table using a service configured with the following DynamoDB client setup:

```
var client *dynamodb.Client
var err error

httpClient := awshttp.NewBuildableClient().WithTransportOptions(
    func(tr *http.Transport) {
        tr.IdleConnTimeout = cfg.DDB.IdleConnTimeout
    }).WithTimeout(cfg.DDB.ReqTimeout)

ddbClientConfig, err = awsconfig.LoadDefaultConfig(
    context.Background(),
    awsconfig.WithRetryer(func() aws.Retryer {
        return retry.NewStandard(func(options *retry.StandardOptions) {
            options.MaxAttempts = cfg.DDB.MaxRetryCount
            options.MaxBackoff = cfg.DDB.MaxBackoffDelay
        })
    }),
    awsconfig.WithHTTPClient(httpClient),
)

ddbClient = dynamodb.NewFromConfig(clientCfg, func(o *dynamodb.Options) {
    o.BaseEndpoint = ""http://<localstack-service-name>.<kubernetes-namespace-name>.svc.cluster.local:91""
})
```

However, when making a `PutItem` request using this DynamoDB client, I encounter the following error:

```
operation error DynamoDB: PutItem, exceeded maximum number of attempts, 3, https response error StatusCode: 503, RequestID: , api error UnknownError: UnknownError
```

I have explored several solutions, including:

1. Verifying network connectivity between the pods.
2. Increasing the memory and CPU allocations for the LocalStack pod.
3. Upgrading the `aws-sdk-go-v2/dynamodb` version used in the client application.

Despite making these changes, the issue persists. The table was set up in LocalStack with the following command:

```
awslocal dynamodb create-table \
 --endpoint-url http://localhost:4566 \ # The Kubernetes service directs this localhost 4566 port to expose port 91
 --region us-east-1 \
 --table-name transaction-data \
 --attribute-definitions \
     AttributeName=key1,AttributeType=S \
     AttributeName=key2,AttributeType=S \
 --key-schema AttributeName=key1,KeyType=HASH \
              AttributeName=key2,KeyType=RANGE \
 --billing-mode PAY_PER_REQUEST
```

Interestingly, I am able to successfully execute a `curl` request from the client pod to the LocalStack pod to put an item into the table. However, the `PutItem` request made from the client application using the Go AWS SDK results in the error mentioned above.

I also attempted using the `--sharedDb` parameter when starting the LocalStack container in my Dockerfile, and also setting it in my kubernetes deployment help chart but without success:

```
FROM localstack/localstack:latest

# Set environment variables for test AWS credentials
ENV AWS_ACCESS_KEY_ID=test_key_id
ENV AWS_SECRET_ACCESS_KEY=test_key
ENV AWS_REGION=us-east-1

COPY /execute-scripts/scripts/ /etc/localstack/init/ready.d/

RUN chmod +x /etc/localstack/init/ready.d/executeScript.sh

# Starting localstack process with --shareDb parameter
CMD [""localstack"", ""start"", ""--sharedDb""]
```

Can anyone help identify potential reasons for this error and suggest solutions?

UPDATE :

I also am passing in the --sharedDb flag to localstack container as env variables my kubernetes deployment helm chart, but I still see the DDB service in the localstack container is setting --sharedDb to false. Am I using --sharedDb parameter incorrectly?

```
            env:
              - name: JAVA_OPTS
                value: ""-Xms512m -Xmx512m""
              - name: SERVICES
                value: ""dynamodb,s3,sqs""
              - name: DEBUG
                value: ""1""
              - name: LOG_LEVEL
                value: ""TRACE""
              - name: LOG_FORMAT
                value: ""json""
            args: [""start"",""-sharedDb"",""--async-response-enabled"", ""true"", ""--async-response-threads"", ""30"", ""--no-request-journal"", ""--jetty-acceptor-threads"",""10""]

localstack container logs

Initializing DynamoDB Local with the following configuration:
Port:   54919
InMemory:   false
Version:    2.5.4
SharedDb:   false
```

Update 2

I updated my application's DDB client to use the same access key values which the localstack docker container is using. This also did not help with the issue

```
client = dynamodb.NewFromConfig(clientCfg, func(o *dynamodb.Options) {
                o.BaseEndpoint = &cfg.LocalStackEndpoint
                o.Credentials = awsCredentials.NewStaticCredentialsProvider(""test_key_id"", ""test_key"", """")
            })
```","amazon-web-services, kubernetes, amazon-dynamodb, localstack, amazon-dynamodb-local",79469527.0,"When you create DynamoDB local pass the table `-sharedDb` parameter which you can also set as an env variable for local stack.

Because you create the tables using CLI which has different access keys it creates a different database file to the one from your code. shareDb uses a shared database file so you can access it from any local code.

> If you use the -sharedDb option, DynamoDB creates a single database file named shared-local-instance.db. Every program that connects to DynamoDB accesses this file. If you delete the file, you lose any data that you have stored in it.

[https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.UsageNotes.html](https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DynamoDBLocal.UsageNotes.html)

In localstack that translates to the following:

DYNAMODB_SHARE_DB=1

```
CMD [""DYNAMODB_SHARE_DB=1"",""localstack"", ""start""]
```",2025-02-26T11:23:52,2025-02-26T11:08:08
79468727,How can I reset Docker Desktop and Kubernetes on Windows 11/WSL2,"I setup Kubernetes in Docker Desktop with WSL2

Everything was working until I tried to setup a new context for GCloud. That setup was too fragile locally, so now I'm trying to reset config back to only the `docker-desktop` context

I used the cli to remove the context, which didn't take it out of Docker Desktop (DD), so I uninstalled and reinstalled DD, which also didn't remove it.

In WSL2 I tried to force a full reinstall by deleting `~/.kube` and reinstalling DD in Windows (bad mistake)

This has left everything broken.

- Docker Desktop still lists the old contexts (docker-desktop & gke_xxxxx)
- WSL2 has no contexts (see below)

How do I fully reset/reinstall Docker Desktop & Kubernetes, forcing a reinstall inside WSL2 as well, or is there a command like `init kubectl`?

Is there a way to get the standard version of the `.kube/config file` that I can use to reset things myself?

```
mike@Mike:~$ kubectl version
Client Version: v1.31.4
Kustomize Version: v5.4.2
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

```
mike@Mike:~/.kube$ kubectl get pods
E0225 19:56:34.345048    1347 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""http://localhost:8080/api?timeout=32s\"": dial tcp 127.0.0.1:8080: connect: connection refused""
E0225 19:56:34.346521    1347 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""http://localhost:8080/api?timeout=32s\"": dial tcp 127.0.0.1:8080: connect: connection refused""
E0225 19:56:34.347936    1347 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""http://localhost:8080/api?timeout=32s\"": dial tcp 127.0.0.1:8080: connect: connection refused""
E0225 19:56:34.349328    1347 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""http://localhost:8080/api?timeout=32s\"": dial tcp 127.0.0.1:8080: connect: connection refused""
E0225 19:56:34.350654    1347 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: Get \""http://localhost:8080/api?timeout=32s\"": dial tcp 127.0.0.1:8080: connect: connection refused""
The connection to the server localhost:8080 was refused - did you specify the right host or port?
```

`~/.kube/config`

```
apiVersion: v1
clusters: null
contexts:
- context:
    cluster: """"
    user: """"
  name: docker-desktop
current-context: """"
kind: Config
preferences: {}
users: null
```","kubernetes, docker-desktop, wsl2",79469075.0,"To fully reset/reinstall Docker Desktop & Kubernetes, forcing a reinstall inside WSL2

1. Select the Docker menu and then **Troubleshoot** option.
2. Select the **Troubleshoot** icon near the top-right corner of **Docker Dashboard**.
3. Choose **Reset to factory defaults option** to reset all options on Docker Desktop to their initial state, the same as when Docker Desktop was first installed.

To get the standard version of the `.kube/config` file  follow the below steps:

1. Open the Docker Desktop Dashboard and go to Settings.
2. Select the **Kubernetes** tab.
3. Toggle on **Enable** Kubernetes.
4. Click on **Apply & Restart** to save the settings.

This will regenerate the `.kube/config` file with default settings. You can also run the below command to get the standard version.

```
$ Kubectl config init
```

Refer to this document on [Deploy kubernetes with docker desktop](https://docs.docker.com/desktop/features/kubernetes/) for more detailed information.",2025-02-26T08:58:12,2025-02-26T06:33:40
79468239,How to stream from dynamically created Selenium Moon browser pods to RTMP using ffmpeg?,"I'm running Selenium Moon (from Aerokube) in my minikube cluster for automated browser testing, and I need to stream the browser sessions to an RTMP endpoint. Here's my current setup:

```
$ kubectl get all -n my-namespace
NAME                                    READY   STATUS    RESTARTS   AGE
pod/minio-5b85cc6cd6-nznwl              1/1     Running   1 (48m ago)   2d8h
pod/moon-7db8b9c76f-8jzzm               4/4     Running   0             34m
pod/moon-7db8b9c76f-kp78j               4/4     Running   0             34m

NAME                     TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/minio            NodePort    10.99.231.25     <none>        9000:31147/TCP,9001:31125/TCP          2d8h
service/moon             NodePort    10.100.244.255   <none>        4444:30683/TCP,9090:32243/TCP,8080:32571/TCP   34m

NAME                               READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/minio              1/1     1            1           2d8h
deployment.apps/moon               2/2     2            2           34m

NAME                                          DESIRED   CURRENT   READY   AGE
replicaset.apps/minio-5b85cc6cd6              1         1         1       2d8h
replicaset.apps/moon-7db8b9c76f               2         2         2       34m
```

## The challenge:

1. Moon dynamically creates browser pods when tests run
2. I need to stream the browser activity to an RTMP endpoint using ffmpeg
3. Moon was installed with Helm

## What I've tried to understand:

- How to integrate ffmpeg with browser pods that are created on-demand
- How to modify the Moon Helm chart to inject ffmpeg capabilities
- How to capture display from the browser pods
- How to stream to RTMP endpoints

Has anyone implemented something similar? I'm looking for a solution that works with the dynamic nature of Moon's browser pod creation. Any examples, Helm chart modifications, or approaches would be greatly appreciated.

Thanks in advance!","kubernetes, ffmpeg, video-streaming, kubernetes-helm, rtmp",79473184.0,"Since Moon uses the Kubernetes API to launch the browser pod , there are two methods to stream the browser session to rtmp endpoints:

1. Inject a sidecar ffmpeg container in the creted browser pod (after extraction the ip address of the pod or use metadata labels )
2. Use a standalone ffmpeg pod that catch the new browser pod created by moon (not recommended)",2025-02-27T15:36:05,2025-02-26T00:45:41
79466282,Spring Outh2 client authorization code flow for multiple instances/pods,"I'm having issues understanding the client side of the oauth2 authorization code flow when there are multiple instances of the service.

As far as I understand the OAuath2AuthorizationRequest is a statefull request since it uses the state parameter which is passed to authorization server. Before sending it, spring security stores this state in memory on the pod that initiated the call. After the user authenticates, the authentication server responds with the same state parameter which reaches spring security and is checked against what was previously stored in memory. If there is no match an authorization_request_not_found exception is thrown and the flow is aborted. If there is a match the flow is successfull.

What I'm trying is to store some extra information in the state parameter such that when the flow is finished successfully I can use the information in my app. If the request is state full, is it safe to store the extra information in spring security http session instead of sending it in the state parameter?

How is this flow working if there are multiple pods? Is the initial http connection kept alive and reused until the flow finishes?","spring, kubernetes, oauth",79467750.0,"When using multiple instances of a stateful Spring application, you should share the session between these instances (applications using `formLogin` or `oauth2Login` are stateful). [Spring Session](https://spring.io/projects/spring-session) serves this purpose and stores session data in Redis by default. Be aware that [Back-Channel Logout won't work unless you write your own `OidcSessionRegistry`](https://github.com/spring-projects/spring-session/issues/3341).",2025-02-25T20:02:57,2025-02-25T10:56:57
79466282,Spring Outh2 client authorization code flow for multiple instances/pods,"I'm having issues understanding the client side of the oauth2 authorization code flow when there are multiple instances of the service.

As far as I understand the OAuath2AuthorizationRequest is a statefull request since it uses the state parameter which is passed to authorization server. Before sending it, spring security stores this state in memory on the pod that initiated the call. After the user authenticates, the authentication server responds with the same state parameter which reaches spring security and is checked against what was previously stored in memory. If there is no match an authorization_request_not_found exception is thrown and the flow is aborted. If there is a match the flow is successfull.

What I'm trying is to store some extra information in the state parameter such that when the flow is finished successfully I can use the information in my app. If the request is state full, is it safe to store the extra information in spring security http session instead of sending it in the state parameter?

How is this flow working if there are multiple pods? Is the initial http connection kept alive and reused until the flow finishes?","spring, kubernetes, oauth",79466731.0,Seems that there is a sticky session configuration done on the ALB on the JSESSIONID cookie name.,2025-02-25T13:27:56,2025-02-25T10:56:57
79465457,How to access environment variables defined in dockerfile in helm chart value file,"I have defined CATALINA_OPTS with default values in the Dockerfile and built a Docker image. While deploying to a Kubernetes cluster using a Helm chart, it seems that additional options cannot be appended to CATALINA_OPTS. The only option is to override CATALINA_OPTS entirely in the Helm chart values file.

Docker file :

```
ENV CATALINA_OPTS=""$CATALINA_OPTS -XshowSettings:vm \
    --add-opens=java.base/java.io=ALL-UNNAMED \
    --add-opens=java.base/java.lang.invoke=ALL-UNNAMED \
    --add-opens=java.base/java.lang.reflect=ALL-UNNAMED""
```

helm value file :

```
envVars:
  - name: CATALINA_OPTS
    value: ""$CATALINA_OPTS -XX:+UseG1GC \
           -XX:+UnlockExperimentalVMOptions \
           -XX:InitialRAMPercentage=60.0 \
          -XX:MinRAMPercentage=60.0 \
          -XX:MaxRAMPercentage=60.0""
```

Error :

```
  Error: Could not find or load main class ${CATALINA_OPTS} Caused by: java.lang.ClassNotFoundException: ${CATALINA_OPTS} Could not load Logmanager ""org.apache.juli.ClassLoaderLogManager"" java.lang.ClassNotFoundException: org.apache.juli.ClassLoaderLogManager
```

Ok. Shell script string concatenations can't be used in helm value files.

Helm templates are processed before the container starts look like only override is possible. How to access environment variables defined in dockerfile in helm chart value file and amend additional jvm arguments? Thanks","docker, kubernetes, environment-variables, kubernetes-helm, jvm-arguments",79465924.0,"Tell me if I understand correctly. You have already built docker image with env var set to default values and you want to add additional options to same env var at runtime using helm?

If so, then I can think of two ways. First is to fully override env var with helm (that you try to avoid). Second option is to introduce new env var as @Lukman suggested but you will probably need to handle it with custom entrypoint inside docker image. Something like this:

```
#!/bin/bash

set -e

if [[ -n $CATALINA_OPTS_EXTRA ]]; then
  echo ""[INFO] Adding additional opts from CATALINA_OPTS_EXTRA""
  export CATALINA_OPTS=""$CATALINA_OPTS $CATALINA_OPTS_EXTRA""
fi

echo ""[INFO] Executing container command""

exec ""$@""
```

Remember to first check if base image you are using already has one, then you will need to ""extend"" it.

There is also slight change that this Catalina app you are using has support for additional opts using another env var out of the box so you could use that with helm. I could not find any though.

//EDIT

There is two more options I can think of. First is to use a script to exec Catalina then use it as container command (or default command).

```
export CATALINA_OPTS=""$CATALINA_OPTS -XshowSettings:vm
--add-opens=java.base/java.io=ALL-UNNAMED
--add-opens=java.base/java.lang.invoke=ALL-UNNAMED
--add-opens=java.base/java.lang.reflect=ALL-UNNAMED
$CATALINA_OPTS_EXTRA""

exec my-actual-catalina-command
```

Not Tomcat expert but another option could probably be to set default options in Tomcat config file inside container. It should read config file first and then `CATALINA_OPTS` as additional options so you could set this env var only in helm.",2025-02-25T08:55:10,2025-02-25T05:00:44
79464712,Image name not resolving properly during Helm Upgrade/Install for Elastic Kibana: InvalidImageName error,"I am attempting to deploy Kibana to my Amazon EKS cluster via Jenkins and am encountering the error InvalidImageName and can't seem to figure out why the image name isn't resolving properly.

Inside my Jenkinsfile I believe i'm providing everything needed to the Helm Upgrade command so that it points to my private repository (Sonatype Nexus Repository). I am using a local copy of the Helm chart that exists in my project and I got it from the following URL: [https://helm.elastic.co/helm/kibana/kibana-8.5.1.tgz](https://helm.elastic.co/helm/kibana/kibana-8.5.1.tgz)

What I am noticing is that the image is being returned as `map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1` and I am unsure why the left hand side is an object/map? The right hand side is the default value for the image tag found in the values.yaml file of the Kibana Helm chart instead of the value I passed as an argument.

ElasticSearch doesn't seem to be giving me an issue and its deployed using the same loop so i'm not sure why Kibana is behaving differently.

When I look at the image within Nexus Repository it gives me the following docker command

```
docker pull bitnami/kibana:8-debian-12
```

The stage within Jenkins that performs this work has the following in it:

```
def helmCharts = [
    [image_repository:'bitnami/elasticsearch', image_tag:'8-debian-12', helm_release_name:'elasticsearch', helm_chart_directory:'charts/bitnami/elasticsearch',namespace:'logging'],
    [image_repository:'bitnami/kibana', image_tag:'8-debian-12', helm_release_name:'kibana', helm_chart_directory:'charts/bitnami/kibana', namespace:'logging'],
    // [image_repository:'bitnami/fluentd', image_tag:'', helm_release_name:'fluentd', helm_chart_directory:'charts/bitnami/fluentd'],
]

helmCharts.each { chart ->
    // Define the Helm command
    def helmCommand = """"""
        helm upgrade $chart.helm_release_name /workspace/$chart.helm_chart_directory \\
        --install \\
        --namespace $chart.namespace \\
        --create-namespace \\
        --cleanup-on-fail \\
        --timeout 2m0s \\
        --set image.registry=${DOCKER_REGISTRY} \\
        --set image.repository=$chart.image_repository \\
        --set image.tag=$chart.image_tag \\
        --set global.imagePullSecrets[0].name=${params.NEXUS_IMAGE_PULL_SECRET} \\
        --set global.defaultStorageClass=gp2 \\
        --set global.security.allowInsecureImages=true \\
        --kubeconfig /workspace/kubeconfig \\
        --debug
    """"""
    // Run Helm commands using Docker
    sh """"""
        docker run --rm \\
            -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
            -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
            -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
            -e HTTP_PROXY=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e HTTPS_PROXY=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e http_proxy=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e https_proxy=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -v ${JENKINS_WORKSPACE}:/workspace \\
            ${HELM_AWS_CLI_IMAGE} sh -c '${helmCommand}'
    """"""
}
```

The following is the output when looking at the pod that is giving me issues:

```
PS C:\Users\******> kubectl get pods -n logging
NAME                              READY   STATUS             RESTARTS   AGE
elasticsearch-master-0            0/1     Pending            0          2m55s
pre-install-kibana-kibana-jkj7h   0/1     InvalidImageName   0          2m51s
PS C:\Users\******> kubectl describe pod pre-install-kibana-kibana-jkj7h -n logging
Name:             pre-install-kibana-kibana-jkj7h
Namespace:        logging
Priority:         0
Service Account:  pre-install-kibana-kibana
Node:             ip-**-***-***-***.***-***-west-1.compute.internal/**.***.**.***
Start Time:       Mon, 24 Feb 2025 13:33:59 -0600
Labels:           batch.kubernetes.io/controller-uid=15cea76c-4fa1-4a12-b44b-0f81130a1b64
                  batch.kubernetes.io/job-name=pre-install-kibana-kibana
                  controller-uid=15cea76c-4fa1-4a12-b44b-0f81130a1b64
                  job-name=pre-install-kibana-kibana
Annotations:      <none>
Status:           Pending
IP:               **.***.**.***
IPs:
  IP:           **.***.**.***
Controlled By:  Job/pre-install-kibana-kibana
Containers:
  create-kibana-token:
    Container ID:
    Image:         map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1
    Image ID:
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/share/kibana/node/bin/node
    Args:
      /usr/share/kibana/helm-scripts/manage-es-token.js
      create
    State:          Waiting
      Reason:       InvalidImageName
    Ready:          False
    Restart Count:  0
    Environment:
      ELASTICSEARCH_USERNAME:                    <set to the key 'username' in secret 'elasticsearch-master-credentials'>  Optional: false
      ELASTICSEARCH_PASSWORD:                    <set to the key 'password' in secret 'elasticsearch-master-credentials'>  Optional: false
      ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES:  /usr/share/kibana/config/certs/ca.crt
    Mounts:
      /usr/share/kibana/config/certs from elasticsearch-certs (ro)
      /usr/share/kibana/helm-scripts from kibana-helm-scripts (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lngm8 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  elasticsearch-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-master-certs
    Optional:    false
  kibana-helm-scripts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kibana-kibana-helm-scripts
    Optional:  false
  kube-api-access-lngm8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason         Age                   From               Message
  ----     ------         ----                  ----               -------
  Normal   Scheduled      3m16s                 default-scheduler  Successfully assigned logging/pre-install-kibana-kibana-jkj7h to ip-**-***-**-***.***-***-west-1.compute.internal
  Warning  Failed         66s (x12 over 3m16s)  kubelet            Error: InvalidImageName
  Warning  InspectFailed  52s (x13 over 3m16s)  kubelet            Failed to apply default image tag ""map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1"": couldn't parse image name ""map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1"": invalid reference format
```

Any help would be greatly appreciated. Thank you

EDIT:
The following is what is inside the values.yaml file for Kibana with regards to the image

```
image: ""docker.elastic.co/kibana/kibana""
imageTag: ""8.5.1""
imagePullPolicy: ""IfNotPresent""
```

EDIT:
The following is taken from the deployment manifest with regards to the image

```
      containers:
      - name: kibana
        securityContext:
{{ toYaml .Values.securityContext | indent 10 }}
        image: ""{{ .Values.image }}:{{ .Values.imageTag }}""
        imagePullPolicy: ""{{ .Values.imagePullPolicy }}""
        env:
          {{- if .Values.elasticsearchURL }}
          - name: ELASTICSEARCH_URL
            value: ""{{ .Values.elasticsearchURL }}""
          {{- else if .Values.elasticsearchHosts }}
          - name: ELASTICSEARCH_HOSTS
            value: ""{{ .Values.elasticsearchHosts }}""
          {{- end }}
          - name: ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES
            value: ""{{ template ""kibana.home_dir"" . }}/config/certs/{{ .Values.elasticsearchCertificateAuthoritiesFile }}""
          - name: SERVER_HOST
            value: ""{{ .Values.serverHost }}""
          - name: ELASTICSEARCH_SERVICEACCOUNTTOKEN
            valueFrom:
              secretKeyRef:
                name: {{ template ""kibana.fullname"" . }}-es-token
                key: token
                optional: false
```","docker, kubernetes, kubernetes-helm, kibana",79467898.0,"When you set the image tag in the resulting YAML manifest

```
image: ""{{ .Values.image }}:{{ .Values.imageTag }}""
```

you expect `image` in the Helm values to be a string.  This is true in the default Helm values, but when you run the install command

```
helm upgrade ... \
  --set image.registry=${DOCKER_REGISTRY} \
  --set image.repository=$chart.image_repository \
  --set image.tag=$chart.image_tag \
  ...
```

that particular `--set` syntax turns `image` into an object, with embedded fields `registry`, `repository`, and `tag`.  What you're seeing in the output is a default Go-template serialization of string-keyed maps, which isn't usually useful in a Helm context.

Probably the easiest fix here is to change the pipeline code to match the structure that's in the Helm values

```
helm upgrade ... \
  --set image=""${DOCKER_REGISTRY}/$chart.image_repository"" \
  --set imageTag=$chart.image_tag \
  ...
```

It would also work to change the Helm template to match the values that are being passed in.  (Do one or the other, not both!)

```
{{- $i := .Values.image }}
image: ""{{ $i.registry }}/{{ $i.repository }}:{{ $i.tag }}""
```",2025-02-25T21:16:24,2025-02-24T20:20:52
79464629,kubectl rollout status for scale down - doesn&#39;t work,"I'm using `kubectl rollout status` to monitor the scale up/down of statefulset. But on scale down it doesn't work the prompt/script right away continues even tho there are still pod out which are in the process of terminating.

Example scale up:

```
kubectl scale sts my-cnn-box --replicas=20
##wait for most replicas to be deployed
kubectl rollout status sts my-cnn-box --timeout=15m
```

output

```
statefulset.apps/my-cnn-box scaled
Waiting for 20 pods to be ready...
Waiting for 19 pods to be ready...
Waiting for 19 pods to be ready...
Waiting for 18 pods to be ready...
Waiting for 18 pods to be ready...
```

Example scale down:

```
##clean stop
kubectl scale sts -n my-ns --replicas=0 --all
##wait for replicas to be terminated
kubectl rollout status sts my-cnn-box --timeout=15m
```

output, cmd prompt comes back right away so I can run the cmd few times.

```
partitioned roll out complete: 11 new pods have been updated...
tilo@my:/mnt/c/temp$ kubectl rollout status sts my-cnn-box --timeout=15m
partitioned roll out complete: 6 new pods have been updated...
tilo@my:/mnt/c/temp$ kubectl rollout status sts my-cnn-box --timeout=15m
partitioned roll out complete: 4 new pods have been updated...
tilo@my:/mnt/c/temp$ kubectl rollout status sts my-cnn-box --timeout=15m
partitioned roll out complete: 2 new pods have been updated...
```

is there a tweak to the rollout status or should it work for scale down?","kubernetes, kubernetes-statefulset",79467404.0,"`kubectl rollout status` isn't primarily designed to track the removal of pods during a scale down of a StatefulSet. It primarily focuses on observing updates and new pod deployments as it is more focused on the progress of pod creation and updates.

If you want to see or watch the realtime changes of pods, you can use `–watch` or `-w` to see pods transition through statuses like *Terminating* and eventually disappear from the output :

```
kubectl get pod <your pod name> -w
```

See this additional information that might help :

- [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-status-em-](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-status-em-)
- [https://github.com/kubernetes/kubectl/issues/1628](https://github.com/kubernetes/kubectl/issues/1628)
- [https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#scale](https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#scale)",2025-02-25T17:28:57,2025-02-24T19:35:33
79464533,passing env variable to docker image from k8 secret store,"How to expand environment variables coming from  a secret store and pass them inside a docker container?. Said docker container does not have a shell, therefore it is not possible to run a script. This is the sample yaml file

```
        envFrom:
        - secretRef:
            name: secret
        command: [""my-command""]
        args:
          - ""--env=ENV1=${MY_ENV_VAR1}""
          - ""--env=env2=${MY_ENV_VAR2}""
```",kubernetes,79464639.0,"You can pass environment variables to your arguments by using parentheses `()` instead of braces `{}`

```
    envFrom:
    - secretRef:
        name: secret
    command: [""my-command""]
    args:
      - ""--env=ENV1=$(MY_ENV_VAR1)""
      - ""--env=env2=$(MY_ENV_VAR2)""
```

Kubernetes docs have an example here for reference: [https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)",2025-02-24T19:41:06,2025-02-24T18:41:21
79463887,FileNotFoundException After Making Container Read-Only File System,"I am new to Docker/Kubernetes/Helm Charts. I was asked to make the file system as read only for an environment. After reading up a bit.

I made the changes in the helm charts like:

```
containers:
- name: {{ template ""name"" . }}
  image: ""{{ .Values.images.repository }}/com.gmt.comp.aid.dev/aid:{{ .Values.images.aidTag }}""
  #imagePullPolicy: {{ .Values.images.pullPolicy }}
  imagePullPolicy: {{ .Values.images.pullPolicy }}
  securityContext:
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
  command:
    - sh
    - -c
    - >
      .
      .
      .
      .
      .
      .
      .
      keytool -import -trustcacerts -alias gtt_internal_root -file /tmp/gtt_internal_root.crt -keystore /opt/java/openjdk/lib/security/cacerts --storepass changeit -noprompt;
      keytool -import -trustcacerts -alias gttinternalrootv2_1 -file /tmp/gttinternalrootv2_1.crt -keystore /opt/java/openjdk/lib/security/cacerts --storepass changeit -noprompt;
      keytool -delete -alias digicertglobalrootca -keystore /opt/java/openjdk/lib/security/cacerts -storepass changeit -noprompt;
      keytool -import -trustcacerts -alias digicertglobalrootca -file /tmp/digicertglobalrootca.crt -keystore /opt/java/openjdk/lib/security/cacerts --storepass changeit -noprompt;
      keytool -delete -alias digicertglobalrootg2 -keystore /opt/java/openjdk/lib/security/cacerts -storepass changeit -noprompt;
      keytool -import -trustcacerts -alias digicertglobalrootg2 -file /tmp/digicertglobalrootg2.crt -keystore /opt/java/openjdk/lib/security/cacerts --storepass changeit -noprompt;
      .
      .
      .
      .
      .
      .
```

After deleting the pod. During the restart the POD status show `CrashLoopBackOff`. And I see in the logs:

```
Certificate stored in file </opt/app/aafcertman/ca_aaf_0.crt>
Certificate stored in file </opt/app/aafcertman/ca_aaf_1.crt>
Certificate stored in file </opt/app/aafcertman/ca_aaf_2.crt>
Certificate stored in file </opt/app/aafcertman/ca_aaf_3.crt>
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Certificate stored in file </opt/app/aafcertman/verisigng3_ca.crt>
Certificate stored in file </opt/app/aafcertman/digicertsha2secureserverca.crt>
Certificate stored in file </opt/app/aafcertman/verisigng5_ca.crt>
Certificate stored in file </opt/app/aafcertman/verisigng4_ca.crt>
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
keytool error: java.lang.Exception: Alias <digicertglobalrootca> does not exist
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Warning: use -cacerts option to access cacerts keystore
keytool error: java.lang.Exception: Alias <digicertglobalrootg2> does not exist
Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
Starting AAI KeyStore creation.
Importing keystore /opt/app/aafcertman/aai-client-cert.p12 to /opt/app/aafcertman/vid-aai.jks...
keytool error: java.lang.NullPointerException: invalid null input
keytool error: java.lang.Exception: Certificate not imported, alias <ca_aaf_2> already exists
keytool error: java.lang.Exception: Certificate not imported, alias <ca_aaf_1> already exists
keytool error: java.lang.Exception: Certificate not imported, alias <ca_aaf_0> already exists
keytool error: java.lang.Exception: Certificate not imported, alias <ca_aaf_3> already exists
Ended AAI KeyStore creation.
Importing TAPM certs
Existing entry alias vid_chain exists, overwrite? [no]:  Enter new alias name   (RETURN to cancel import for this entry):  Warning: use -cacerts option to access cacerts keystore
Certificate was added to keystore
keytool error: java.io.FileNotFoundException: /opt/java/openjdk/lib/security/cacerts (Read-only file system)
/usr/local/tomcat/bin/setenv.sh: line 9: /tmp/aaf_pass: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 10: /tmp/aaf_pass: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 11: /tmp/cadi_truststore_password: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 12: /tmp/cadi_keystore_password: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 13: /tmp/tomcat_ssl_port: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 14: /tmp/ajp_port: Read-only file system
/usr/local/tomcat/bin/setenv.sh: line 15: /tmp/catalina_opts_in_startup_start: Read-only file system
```

Can you please help me with this? Did I do the helm charts right? Thanks.","docker, kubernetes, kubernetes-helm",,,,2025-02-24T14:41:32
79459923,Jenkins Pipeline With Kubernetes Deployment,"I've a following groovy code in my Jenkins pipeline:

```
pipeline {
    agent any

    stages {

       .....
       .....

        stage('Deploy To Kubernetes') {
            steps {
                    withCredentials([string(credentialsId: 'kubernetes-secret', variable: 'K8S_TOKEN')]) {
                        sh """"""
                            kubectl --token=$K8S_TOKEN get pods
                        """"""
                    }
            }
        }

        .......
        .......
    }
}
```

In the stage deploy to kubernetes, im using a token which i manually created using:

```
kubectl create token default
```

I'm not sure what i'm missing. I keep getting following error:

```
+ kubectl --token=**** get pods
E0222 22:06:55.856387  321714 memcache.go:265] ""Unhandled Error"" err=<
    couldn't get current server API group list: <html><head><meta http-equiv='refresh' content='1;url=/login?from=%2Fapi%3Ftimeout%3D32s'/><script id='redirect' data-redirect-url='/login?from=%2Fapi%3Ftimeout%3D32s' src='/static/ffec7a5e/scripts/redirect.js'></script></head><body style='background-color:white; color:white;'>
    Authentication required
    <!--
    -->

    </body></html>
 >
E0222 22:06:55.858786  321714 memcache.go:265] ""Unhandled Error"" err=<
    couldn't get current server API group list: <html><head><meta http-equiv='refresh' content='1;url=/login?from=%2Fapi%3Ftimeout%3D32s'/><script id='redirect' data-redirect-url='/login?from=%2Fapi%3Ftimeout%3D32s' src='/static/ffec7a5e/scripts/redirect.js'></script></head><body style='background-color:white; color:white;'>
    Authentication required
    <!--
    -->

    </body></html>
 >
```","kubernetes, jenkins-pipeline",79465398.0,"Probably you are connecting to localhost:8080 to access the Kubernetes API server as that is the default if nothing else is configured.
This happens to be the Jenkins server/UI itself and generates the response you see.

Configure the Kubernetes API endpoint in addition to the credentials.",2025-02-25T04:15:42,2025-02-22T16:29:40
79458542,Kubernetes connection refused after volume populated,"I'm trying to follow the Kubernetes tutorial for [Deploying WordPress and MySQL with Persistent Volumes](https://kubernetes.io/docs/tutorials/stateful-application/mysql-wordpress-persistent-volume/). I get through the tutorial just fine and my WordPress site comes up and shows the installation wizard as expected.

But, when I go through the installation wizard and then restart my pods and run `minikube service Wordpress` to verify that the volumes are working, I get a connection refused error and it seems that my browser is redirecting to the previously used port which is no longer the assigned port after I run `minikube service Wordpress`. It works fine if I don't run `minikube service Wordpress` and just keep it open while I restart the pods.

I believe the problem is that WordPress stores the URL I used to install it in the database. But when I delete a pod and run minikube service Wordpress, the port is different so that's why the connection is refused.

Here's what I'm working with:

mysql-deployment.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  ports:
    - port: 3306
  selector:
    app: wordpress
    tier: mysql
  clusterIP: None
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mysql-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress-mysql
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: mysql
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: mysql
    spec:
      containers:
      - image: mysql:latest
        name: mysql
        env:
        - name: MYSQL_ROOT_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        - name: MYSQL_DATABASE
          value: wordpress
        - name: MYSQL_USER
          value: wordpress
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
      volumes:
      - name: mysql-persistent-storage
        persistentVolumeClaim:
          claimName: mysql-pv-claim
```

wordpress-deployment.yaml

```
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: LoadBalancer
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:latest
        name: wordpress
        env:
        - name: WORDPRESS_DB_HOST
          value: wordpress-mysql
        - name: WORDPRESS_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        - name: WORDPRESS_DB_USER
          value: wordpress
        ports:
        - containerPort: 80
          name: wordpress
        volumeMounts:
        - name: wordpress-persistent-storage
          mountPath: /var/www/html
      volumes:
      - name: wordpress-persistent-storage
        persistentVolumeClaim:
          claimName: wp-pv-claim
```

kustomization.yaml

```
secretGenerator:
- name: mysql-pass
  literals:
  - password=example
resources:
  - mysql-deployment.yaml
  - wordpress-deployment.yaml
```

Is there a way I can use a dynamic URL so that the port updates in the Wordpress volume to the current port in use?

The Kubernetes tutorial only goes so far as bringing the site up the first time which works fine. I wanted to go a step further and actually verify that the data persists. Any help is much appreciated, thank you.","wordpress, kubernetes",,,,2025-02-21T20:16:35
79457631,How to restore default docker destop&#39;s KUBECONFIG for demo-k8s,"In my local windows dev environment, I've run
`az aks get-credentials --resource-group $RG --name $AKS --overwrite-existing`

`kubectl` now connects to AKS cluster.

How do I revert the KUBECONFIG to connect to my localhost environment again?","kubernetes, kubectl, docker-desktop, kubeconfig",79458272.0,"1. List available contexts to check if the desired context exists:

```
kubectl config get-contexts
```

1. Select the context you wish to use. For example, to use docker-desktop, run:

```
kubectl config use-context docker-desktop
```

1. Verify the active context. The output should display docker-desktop:

```
kubectl config current-context
```

[Optional] Unset the `KUBECONFIG `variable if needed:

If you've set the `KUBECONFIG` environment variable manually (e.g., pointing to a specific file), you can unset it to revert to the default kubeconfig:

- [https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable)
- [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/kubectl_config_unset/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_config/kubectl_config_unset/)",2025-02-21T18:02:31,2025-02-21T14:06:34
79457479,How to set multiple kubeconfigs using powershell,"The [kubectl documentation](https://kubernetes.io/docs/reference/kubectl/quick-reference/#kubectl-context-and-configuration) says, we should use `:` to separate paths to multiple kubeconfig files, but it does not work in powershell:

```
> $env:KUBECONFIG=""C:\Users\me\.kube\staging-config:C:\Users\me\.kube\demo-k8s-config""
> kubectl config get-contexts
error: error loading config file ""C:\Users\me\.kube\staging-config:C:\Users\me\.kube\demo-k8s-config"": open C:\Users\dturanme.kube\staging-config:C:\Users\me\.kube\demo-k8s-config: The filename, directory name, or volume label syntax is incorrect.
```

I've tried with newline separator as well.","powershell, kubernetes, kubectl, kubeconfig",79457830.0,"You are correct **( : )** does not work in powershell to separate paths to multiple kubeconfig files instead it uses **semicolon** **( ; )** to separate paths.

For Example:

```
$env:KUBECONFIG=""C:\Users\me\.kube\staging-config;C:\Users\me\.kube\demo-k8s-config""
```

In Powershell, the colon ( : ) is used to separate the drive letter from the rest of the path.

Refer to the official kubernetes document on [Set the KUBECONFIG environment variable](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/#set-the-kubeconfig-environment-variable) for more information.",2025-02-21T15:17:29,2025-02-21T13:16:12
79456703,Automerge workflow not triggering on PRs,"I have two GitHub workflows working together to automate cluster list updates in my repository:

1. `cluster-list.yaml` workflow

This workflow runs every 3 hours and:

  1. Retrieves a list of all clusters from a tool (ArgoCD).
  2. Compares it with the existing `clusters-list.yaml` file.
  3. If there are differences, it:

    - Creates a new branch.
    - Updates `clusters-list.yaml`.
    - Creates a pull request (PR) with the changes.
    - Labels the PR with ""automerge"".

```
name: Check for new cluster

on:
  workflow_dispatch:
  schedule:
    - cron: '0 */3 * * *'

permissions:
  contents: write
  pull-requests: write

jobs:
  check-for-new-cluster:
    runs-on: ubuntu-latest
    steps:
      - name: Check for new cluster
        uses: actions/checkout@v4
        with:
          ref: main

      - name: Install ArgoCD CLI
        run: |
          curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/latest/download/argocd-linux-amd64
          sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
          sudo rm argocd-linux-amd64

      - name: Login to ArgoCD
        run: argocd login XXXXXX --username ${{ secrets.ARGOCD_USER }} --password ${{ secrets.ARGOCD_PW }} --insecure

      - name: Get clusters list
        run: |
          cd ./helm-charts
          argocd cluster list -o json | jq 'map({name, labels, annotations, shortName: (.name | gsub(""-k8s""; """"))}) | sort_by(.name)' | yq eval '{""argocdClusters"": .}' -P > clusters-list-new.yaml

      - name: Compare cluster list
        run: |
          cd ./helm-charts
          diff -u clusters-list.yaml clusters-list-new.yaml > clusters-list.diff || true
          if [ -s clusters-list.diff ]
          then
            echo ""Cluster list has changed""
            cat clusters-list.diff
            branchname=$(echo ""maint/update-clusters-list-$(date +'%Y%m%d%H%M%S')"")
            cp clusters-list-new.yaml clusters-list.yaml
            git config user.name github-actions
            git config user.email github-actions@github.com
            git checkout -b $branchname
            git add clusters-list.yaml
            git commit -m ""JOP-000: Update clusters-list.yaml""
            git push origin $branchname
            gh pr create -B main -H $branchname -t ""JOP-000: Update clusters-list.yaml"" -b ""JOP-000: Update clusters-list.yaml"" --label automerge
          else
            echo ""Cluster list has not changed""
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```
2. `automerge.yaml` workflow

This workflow should automatically merge PRs under specific conditions. It triggers on PR events (labeled, synchronize, opened, ready_for_review, and review_submission) and:

  - If only `helm-charts/clusters-list.yaml` is modified, it merges the PR without requiring review.
  - If other files are modified, it requires an approved review before merging.

```
name: Automerge

on:
  pull_request_target:
    types:
      - labeled
      - synchronize
      - opened
      - ready_for_review
  pull_request_review:
     types:
      - submitted

permissions:
  contents: write
  pull-requests: write

jobs:
  automerge:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      - name: Check if checks passed and pr is correctly labeled
        run: |
          git config user.name github-actions
          git config user.email github-actions@github.com

          # check if the PR is labeled automerge
          if ! gh pr view ${{ github.event.number }} --json labels --jq '.labels[].name' | grep -q automerge; then
            echo ""PR is not labeled automerge, exiting""
            exit 0
          fi
          echo ""PR is labeled automerge""

          # Get list of modified files
          MODIFIED_FILES=$(gh pr view ${{ github.event.number }} --json files --jq '.files[].path')

          # Check if only clusters-list.yaml in helm-charts is modified
          if [ ""$(echo ""$MODIFIED_FILES"" | wc -l)"" -eq 1 ] && [ ""$MODIFIED_FILES"" = ""helm-charts/clusters-list.yaml"" ]; then
            echo ""Only helm-charts/clusters-list.yaml is modified, proceeding without review""
          else
            # For other files, check for review approval
            if ! gh pr view ${{ github.event.number }} --json reviews --jq '.reviews[].state' | grep -iq approved; then
              echo ""PR modifies other files and does not have an approved review, exiting""
              exit 0
            fi
            echo ""PR has an approved review""
          fi

          #merge
          gh pr merge ${{ github.event.number }} --auto --squash --delete-branch
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

How the two workflows should work together:

1. `cluster-list.yaml` detects new clusters and creates a PR with updates.
2. This PR is labeled ""automerge"" by the workflow.
3. `automerge.yaml` should trigger on this PR and automatically merge it (since it only modifies `clusters-list.yaml`).
4. Other PRs (modifying different files) should still require review before merging.

The `cluster-list.yaml` workflow successfully creates the PR and applies the ""automerge"" label. However, the `automerge.yaml` workflow never runs for this PR, and the merge does not happen automatically.

I've:

1. Confirmed that the `automerge.yaml` workflow is correctly configured to trigger on PR events.
2. Manually checked that the PR is labeled ""automerge.""
3. Verified that only `helm-charts/clusters-list.yaml` is modified in the PR.

Has anyone encountered a similar issue with GitHub Actions? How can I debug why the `automerge.yaml` workflow does not trigger for the PRs created by `cluster-list.yaml`?","kubernetes, github-actions",,,,2025-02-21T08:10:32
79455014,kubernetes multitenancy - tenant creation through API,"I and trying to setup Kubernetes cluster in my org and want tenant creation for each customer in kubernetes cluster. Each customer tenant pods should be isolated from another customer tenant.

Also Each customer can have multiple nodes with pod( same containers) for each device.

How this can be designed.

is there kubernetes API to create tenant
is there kubernetes API to create node.
What should be authentication mechanism - access token or cert and how it can be implemented?",kubernetes,79455479.0,"There is no direct Kubernetes API to create a tenant. The approach in Kubernetes to deal with multi-tenancy is to use or create [namespace](https://kubernetes.io/docs/concepts/security/multi-tenancy/#namespace-per-tenant) for each customer.

- Create a [namespace](https://kubernetes.io/docs/concepts/security/multi-tenancy/#namespace-per-tenant) for each tenant; each customer can have their own namespace.
- Use [RBAC roles](https://kubernetes.io/docs/concepts/security/multi-tenancy/#access-controls) for access control. You can create `Roles` or `RoleBindings` to give tenants access to their Namespace and limit to others.
- Configure [Network policies](https://kubernetes.io/docs/concepts/security/multi-tenancy/#network-isolation) to ensure isolation. You can restrict communication between pods using namespace labels or IP address ranges.

For further details you can check these documentation :

- [https://kubernetes.io/docs/concepts/security/multi-tenancy/#tenants](https://kubernetes.io/docs/concepts/security/multi-tenancy/#tenants)
- [Kubernetes Multi-Tenancy: Best Practices and Implementation](https://medium.com/@platform.engineers/kubernetes-multi-tenancy-best-practices-and-implementation-5d2df18f83dc)",2025-02-20T18:16:11,2025-02-20T15:29:40
79454507,Kubernetes disabling pod auto-restart using yaml for pods created using deployment,"I want to disable the pod auto-restart in AKS using yaml file for pods created using kind=deployment.
From looking at the documentation it seems the restartPolicy can have value = ""Always"" only for the kind=deployment.
For kind=pod, the restartPolicy supports these value=""Always"", ""Never"", ""onFailure"". So by setting the restartPolicy=""Never"" we can disable a pod from restarting if crashes for some reason or there is a error on it.

But is there a way we can do it using kind=Deployment. That is disable auto restarting of Pods created using kind=Deployment.

I understand that with kind=Deployment, we are adding a deployment so ideally we would want pods to be restarted, but my requirement is for a lower testing env, where we would rather have a pod not restart and let devs look into the issue and fix it.

Any help is appreciated.","azure, kubernetes, azure-aks",79463029.0,"Deployments are designed to check the application stays available by automatically restarting failed pods, and it **cannot be disabled using a Deployment**

- The restartPolicy field is a standard part of the Pod specification. For Pods, the allowed values are ""Always,"" ""OnFailure,"" and ""Never."" When set to ""Never,"" Kubernetes will not restart the container if it fails.

```
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
spec:
  restartPolicy: Never
  containers:
  - name: debug-container
    image: your-image:tag
    command: [""sh"", ""-c"", ""exit 1""]
```

The container will exit and remain in a terminated state, allowing developers to inspect logs and diagnose issues without the Pod being automatically restarted.

![enter image description here](https://i.imgur.com/mL1siRk.png)

**Note:** While standalone Pods or Jobs honor the specified restartPolicy, Deployments always execute a restartPolicy of ""Always"" for their Pods.",2025-02-24T09:35:46,2025-02-20T12:26:56
79452116,jmap in kubectl debug container returns immediately; heap dump file is missing,"I'm trying to get a Java heap dump out of a GKE container.

First I connect to the container:

```
kubectl debug -it my-pod --image adoptopenjdk/openjdk11:latest --target=my-container -- bash
```

Then I run:

```
jmap -dump:live,format=b,file=heap_dump.hprof <pid of my java program>
```

The program returns instantly, having outputted:

```
Heap dump file created
```

However there is no file to be found.","kubernetes, jmap",79452117.0,"`jmap` creates the heap dump file in the file system of the original container, found at `/proc/1/root`.",2025-02-19T16:34:00,2025-02-19T16:32:16
79448794,FATAL: password authentication failed for user &quot;postgres&quot; in Kubernetes,"I can connect the database through docker-compose.yml with its username as postgres and its password 111111 but I cannot handle with the process through Kubernetes with Postgres.

I got this error shown below

```
FATAL:  password authentication failed for user ""postgres""
DETAIL:  Connection matched file ""/var/lib/postgresql/data/pg_hba.conf"" line 128: ""host all all all scram-sha-256
```

How can I fix it?

Here is the **postgres-secret.yml**

```
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
type: Opaque
data:
  POSTGRES_USER: cG9zdGdyZXM=   # Base64 encoded ""postgres""
  POSTGRES_PASSWORD: MTExMTEx    # Base64 encoded ""111111""
```

Here is the **postgres-config.yml**

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: default
data:
  POSTGRES_DB: ""weatherapianalysisdatabase""
  POSTGRES_PORT: ""5432""
```

Here is the **postgres-pv.yml**

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
  namespace: default
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgresql

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

Here is the **postgres-statefulset.yml**

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: default
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:latest
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_USER
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_PASSWORD
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: postgres-config
                  key: POSTGRES_DB
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-data
      spec:
        accessModes: [ ""ReadWriteOnce"" ]
        resources:
          requests:
            storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: default
spec:
  selector:
    app: postgres
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
  clusterIP: None
```

I just look through postgres pod inside

```
kubectl exec -it postgres-0 -n default -- /bin/bash

root@postgres-0:/# env | grep POSTGRES
POSTGRES_PASSWORD=111111
POSTGRES_USER=postgres
POSTGRES_DB=weatherapianalysisdatabase
```

Next I enter postgres-0 through bash

```
kubectl exec -it postgres-0 -n default -- /bin/bash
root@postgres-0:/# psql -h $(hostname -i) -U postgres
Password for user postgres:
psql: error: connection to server at ""10.244.0.62"", port 5432 failed: FATAL:  password authentication failed for user ""postgres""
```

I get the same error again.","postgresql, kubernetes, passwords",79452714.0,"After I defined `POSTGRES_INITDB_ARGS` in **postgres-statefulset.yml**, the issue disappeared.

Here is the **code** block shown below

```
- name: POSTGRES_INITDB_ARGS
  value: ""--auth-host=scram-sha-256""
```",2025-02-19T20:31:41,2025-02-18T15:36:04
79448794,FATAL: password authentication failed for user &quot;postgres&quot; in Kubernetes,"I can connect the database through docker-compose.yml with its username as postgres and its password 111111 but I cannot handle with the process through Kubernetes with Postgres.

I got this error shown below

```
FATAL:  password authentication failed for user ""postgres""
DETAIL:  Connection matched file ""/var/lib/postgresql/data/pg_hba.conf"" line 128: ""host all all all scram-sha-256
```

How can I fix it?

Here is the **postgres-secret.yml**

```
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
type: Opaque
data:
  POSTGRES_USER: cG9zdGdyZXM=   # Base64 encoded ""postgres""
  POSTGRES_PASSWORD: MTExMTEx    # Base64 encoded ""111111""
```

Here is the **postgres-config.yml**

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: default
data:
  POSTGRES_DB: ""weatherapianalysisdatabase""
  POSTGRES_PORT: ""5432""
```

Here is the **postgres-pv.yml**

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
  namespace: default
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgresql

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

Here is the **postgres-statefulset.yml**

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: default
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:latest
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_USER
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_PASSWORD
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: postgres-config
                  key: POSTGRES_DB
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-data
      spec:
        accessModes: [ ""ReadWriteOnce"" ]
        resources:
          requests:
            storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: default
spec:
  selector:
    app: postgres
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
  clusterIP: None
```

I just look through postgres pod inside

```
kubectl exec -it postgres-0 -n default -- /bin/bash

root@postgres-0:/# env | grep POSTGRES
POSTGRES_PASSWORD=111111
POSTGRES_USER=postgres
POSTGRES_DB=weatherapianalysisdatabase
```

Next I enter postgres-0 through bash

```
kubectl exec -it postgres-0 -n default -- /bin/bash
root@postgres-0:/# psql -h $(hostname -i) -U postgres
Password for user postgres:
psql: error: connection to server at ""10.244.0.62"", port 5432 failed: FATAL:  password authentication failed for user ""postgres""
```

I get the same error again.","postgresql, kubernetes, passwords",79449769.0,"It looks like the issue you're facing is related to [PostgreSQL's password authentication](https://www.postgresql.org/docs/14/auth-password.html#:%7E:text=To%20upgrade%20an,sha%2D256.) method `line 128: ""host all all all scram-sha-256""` configured in the `pg_hba.conf`. The file trying to connect using the `SCRAM-SHA-256` method does not match the expected method. Upgrade with existing installation and authentication methods in `pg_hba.conf` to match the `SCRAM-SHA-256`.

Try to set `POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256` otherwise check appropriate solutions provided in this [community issue](https://github.com/docker-library/postgres/issues/726) that suit your case.",2025-02-18T22:32:53,2025-02-18T15:36:04
79446984,k8s network. nginx with apache,"I need help with my **nginx-apache** configuration in **k8s**

I'm getting troubles with communications between nginx-pod and apache-pod

I have these configs:

```
server {
    listen 8080;
    listen [::]:8080;

    server_name _;

    location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
    }

    location /redblue/ {
        rewrite ^/redblue/(.*)$ /$1 break;

        proxy_pass http://redblue;
    }

    location /apache/ {
        rewrite ^/apache/(.*)$ /$1 break;

        proxy_pass http://apache:8080;
    }

    if ($host = ${DOMAIN}) {
        return 301 https://$host$request_uri;
    }
}

server {
    listen 8003;

    server_name red-virtual-server-redblue;

    location / {
        root /usr/share/nginx/html/redblue/red;
        index index.html index.htm;
    }
}

server {
    listen 8004;

    server_name blue-virtual-server-redblue;

    location / {
        root /usr/share/nginx/html/redblue/blue;
        index index.html index.htm;
    }
}

upstream redblue {
    server 127.0.0.1:8003;
    server 127.0.0.1:8004;
}
```

nginx's static folder structure

```
nginx/html/
├── index.html
├── music
│   └── sunflower.mp3
├── redblue
│   ├── blue
│   │   └── index.html
│   └── red
│       └── index.html
└── sunflower.html
```

apache-pod.yml

```
apiVersion: v1
kind: Pod
metadata:
  name: apache-pod
  labels:
    app: apache
spec:
  containers:
    - name: apache
      image: my-registry:apache
      ports:
        - containerPort: 8080
          name: apache-port
```

nginx-pod.yml

```
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
spec:
  containers:
    - name: nginx
      image: my-registry:apache
      ports:
        - containerPort: 8080
          name: nginx-port
```

apache-service.yml

```
apiVersion: v1
kind: Service
metadata:
  name: apache
spec:
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 8080
      targetPort: apache-port
```

nginx-service.yml

```
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: NodePort
  selector:
    app: apache
  ports:
    - protocol: TCP
      port: 80
      targetPort: nginx-port
      nodePort: 30081
```

So my nginx is available at ip:30081

I have index page with links to my static content
sunflower.html and /music/sunflower.mp3 are getting normally
but

I cant get proxied to my redblue servers and apache server

Locally I'm using docker compose and with it everything is working fine. All proxy_pass works as needed

But when I'm using k8s i have access to only my static content (nginx is running inside of container by `nginx` user and access rights to static folder are set to nginx:nginx)

Also I have no redirect 301 moved permanently logs to my `redblue` and `apache` servers in kubectl logs

Only static content entries

Inside nginx-pod I can ping/curl to http://apache:8080 and can obtain apache's index.html

But request to nginx doesn't even log when reaching ip:30081/apache or ip:30081/redblue :(

What am I doing wrong?
Should I have to use ingress class?","apache, kubernetes, nginx",,,,2025-02-18T02:32:51
79446847,k8s - Stop Trying To Run Pod If x Time Elapsed,I have a CronJob in my k8s cluster which I run every 5 minutes. I occasionally have issues where the pod cannot be started in this time (let's not get into that). Is it possible to configure the Job to stop trying after a specific amount of time?,kubernetes,79457210.0,"There are multiple ways to achieve this.

1. activeDeadlineSeconds: xxx  # Pod will stop after xxx seconds.
This will abruptly terminate the pod regardless of the status. Job might be retain back but not the pod.
2. ttlSecondsAfterFinished: xxx  # Deletes job and pod xxx seconds after completion. This is applicable only if you can predict the amount of time job might run to completion and time when you want to terminate the pod and job.
3. Last I can think of K8s way would be, use postStart lifecycle hook and kill the PID 1 to terminate the pod.",2025-02-21T11:19:00,2025-02-17T23:56:25
79446847,k8s - Stop Trying To Run Pod If x Time Elapsed,I have a CronJob in my k8s cluster which I run every 5 minutes. I occasionally have issues where the pod cannot be started in this time (let's not get into that). Is it possible to configure the Job to stop trying after a specific amount of time?,kubernetes,79456685.0,"Did you try startingDeadlineSeconds ?

According to the documentation:

The .spec.startingDeadlineSeconds field defines a deadline (in whole seconds) for starting the Job, if that Job misses its scheduled time for any reason.

After missing the deadline, the CronJob skips that instance of the Job (future occurrences are still scheduled).

For example, if it is set to 200, it allows a Job to be created for up to 200 seconds after the actual schedule.

For Jobs that miss their configured deadline, Kubernetes treats them as failed Jobs. If you don't specify startingDeadlineSeconds for a CronJob, the Job occurrences have no deadline.",2025-02-21T07:59:18,2025-02-17T23:56:25
79445863,How can I prevent the &quot;Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to&quot; log in a confluent-kafka-python consumer?,"When the consumer (which is a very simple confluent-kafka-python consumer) starts, we see this log message after the assignment

> %6|1739802885.947|GETSUBSCRIPTIONS|<consumer id>#consumer-1| [thrd:main]: Telemetry client instance id changed from AAAAAAAAAAAAAAAAAAAAAA to <some random string>

- I tried running the consumer locally (in contrast to the Kubernetes cluster) and see no such logs.
- I tried googling for this log message but found no bugs or help avoiding this (though I am not the only person with such logs)","python, kubernetes, apache-kafka, confluent-kafka-python",79446313.0,"As per my previous comment,

```
from confluent_kafka import Consumer

conf = {
    'bootstrap.servers': 'your broker',
    'group.id': 'your group',
    'enable.metrics.push': False
}

consumer = Consumer(conf)
```",2025-02-17T18:37:45,2025-02-17T15:26:09
79442385,K8 custom controller could not found CR,"I am developing a k8s controller with kubebuilder which reconciles my CustomResource object 'test-pod-monitor-cr' this is a cluster level resource and I get below error when the controller goes thru the reconcile process. As per the error, looks like the issue occurs in controller runtime code itself.

The service account the controller pod is using has the needed privileges to get the CR(Tested with kubectl GET using the service account token).

```
2025-02-15T23:13:32Z    INFO    starting server {""name"": ""health probe"", ""addr"": ""[::]:8081""}
2025-02-15T23:13:32Z    INFO    Starting EventSource    {""controller"": ""podlogmonitor"", ""controllerGroup"": ""monitoring.mydomain.com"", ""controllerKind"": ""PodLogMonitor"", ""source"": ""kind source: *v1.PodLogMonitor""}
2025-02-15T23:13:32Z    INFO    Starting Controller     {""controller"": ""podlogmonitor"", ""controllerGroup"": ""monitoring.mydomain.com"", ""controllerKind"": ""PodLogMonitor""}
2025-02-15T23:13:32Z    INFO    Starting workers        {""controller"": ""podlogmonitor"", ""controllerGroup"": ""monitoring.mydomain.com"", ""controllerKind"": ""PodLogMonitor"", ""worker count"": 1}
2025-02-15T23:13:32Z    ERROR   Reconciler error        {""controller"": ""podlogmonitor"", ""controllerGroup"": ""monitoring.mydomain.com"", ""controllerKind"": ""PodLogMonitor"", ""PodLogMonitor"": {""name"":""test-pod-monitor-cr""}, ""namespace"": """", ""name"": ""test-pod-monitor-cr"", ""reconcileID"": ""66f143a5-a211-4eab-911c-f0109f0661d7"", ""error"": ""podlogmonitors.monitoring.mydomain.com \""test-pod-monitor-cr\"" not found""}
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).reconcileHandler
        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.0/pkg/internal/controller/controller.go:332
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).processNextWorkItem
        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.0/pkg/internal/controller/controller.go:279
sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller[...]).Start.func2.2
        /go/pkg/mod/sigs.k8s.io/controller-runtime@v0.20.0/pkg/internal/controller/controller.go:240
```

Below is my controller code. Could you help me troubleshoot this issue.

```
package controller

import (
        ""context""
        ""strings""
        ""time""

        corev1 ""k8s.io/api/core/v1""
        metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""

        ""k8s.io/client-go/kubernetes""
        ""k8s.io/apimachinery/pkg/runtime""
        ""k8s.io/client-go/rest""
        ctrl ""sigs.k8s.io/controller-runtime""
        ""sigs.k8s.io/controller-runtime/pkg/client""
        ""sigs.k8s.io/controller-runtime/pkg/log""
        ""sigs.k8s.io/controller-runtime/pkg/reconcile""
        ""github.com/go-logr/logr""

        monitoringv1 ""mydomain.com/m/api/v1""
)

// PodLogMonitorReconciler reconciles a PodLogMonitor object
type PodLogMonitorReconciler struct {
        client.Client
        Scheme *runtime.Scheme
        Log logr.Logger
        clientset *kubernetes.Clientset
}

// +kubebuilder:rbac:groups=monitoring.mydomain.com,resources=podlogmonitors,verbs=get;list;watch;create;update;patch;delete
// +kubebuilder:rbac:groups=monitoring.mydomain.com,resources=podlogmonitors/status,verbs=get;update;patch
// +kubebuilder:rbac:groups=monitoring.mydomain.com,resources=podlogmonitors/finalizers,verbs=update

// Reconcile is part of the main kubernetes reconciliation loop which aims to
// move the current state of the cluster closer to the desired state.
// TODO(user): Modify the Reconcile function to compare the state specified by
// the PodLogMonitor object against the actual cluster state, and then
// perform operations to make the cluster state reflect the state specified by
// the user.
//
// For more details, check Reconcile and its Result here:
// - https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/reconcile
func (r *PodLogMonitorReconciler) Reconcile(ctx context.Context, req ctrl.Request) (ctrl.Result, error) {
        _ = log.FromContext(ctx)

        log := r.Log.WithValues(""podlogmonitor"", req.NamespacedName)
        log.Info(""Line - 1"")
        // Fetch the PodLogMonitor resource
        var podLogMonitor monitoringv1.PodLogMonitor
        if err := r.Get(ctx, req.NamespacedName, &podLogMonitor); err != nil {
                log.Error(err, ""unable to fetch PodLogMonitor"")
                return reconcile.Result{}, client.IgnoreNotFound(err)
        }

        log.Info(""Line - 2"")
        // Fetch Pod logs from the specified namespace
        podList := &corev1.PodList{}
        if err := r.List(ctx, podList, client.MatchingLabels{""someLabel"": podLogMonitor.Spec.Namespace}); err != nil {
        //if err := r.List(ctx, podList, client.InNamespace(podLogMonitor.Spec.Namespace)); err != nil {
                log.Error(err, ""unable to list pods"")
                return reconcile.Result{}, err
        }

        log.Info(""Line - 3"")
        // Iterate over the pods and check logs
        for _, pod := range podList.Items {
                log.Info(""In the for loop"")
                if err := r.checkPodLogs(ctx, &pod, podLogMonitor.Spec.LogMessage, &podLogMonitor); err != nil {
                log.Error(err, ""unable to check pod logs"", ""pod"", pod.Name)
                continue
                }
        }

        log.Info(""Line - 4"")
        // Save the updated status
        if err := r.Status().Update(ctx, &podLogMonitor); err != nil {
                log.Error(err, ""unable to update PodLogMonitor status"")
                return reconcile.Result{}, err
        }
        log.Info(""Line - 5"")
        // Return after processing
        return reconcile.Result{}, nil
}

func (r *PodLogMonitorReconciler) checkPodLogs(ctx context.Context, pod *corev1.Pod, logMessage string, podLogMonitor *monitoringv1.PodLogMonitor) error {
   log := r.Log.WithValues(""pod"", pod.Name)
   log.Info(""Line - 6"")
   // Check if we have the clientset; if not, create it
    if r.clientset == nil {
        log.Info(""Creating a new clientset..."")
        config, err := rest.InClusterConfig()
        if err != nil {
            log.Error(err, ""unable to create in-cluster config"")
            return err
        }
        clientset, err := kubernetes.NewForConfig(config)
        if err != nil {
            log.Error(err, ""unable to create Kubernetes clientset"")
            return err
        }
        r.clientset = clientset
    }
   // Get the logs of the pod using the Kubernetes API
   req := r.clientset.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &corev1.PodLogOptions{})
   podLogs, err := req.Stream(ctx)
   if err != nil {
       log.Error(err, ""unable to stream pod logs"")
       return err
   }
   defer podLogs.Close()
   // Check if the log message exists in the pod logs
   buf := make([]byte, 2000)
   _, err = podLogs.Read(buf)
   if err != nil {
       log.Error(err, ""error reading pod logs"")
       return err
   }
   logStr := string(buf)
   if strings.Contains(logStr, logMessage) {
       log.Info(""Found the specified log message, restarting pod"")
       // Restart the pod by deleting it (will be recreated by deployment/statefulset/etc.)
       err = r.clientset.CoreV1().Pods(pod.Namespace).Delete(ctx, pod.Name, metav1.DeleteOptions{})
       if err != nil {
           log.Error(err, ""unable to delete pod"", ""pod"", pod.Name)
           return err
       }
       log.Info(""Pod restarted"", ""pod"", pod.Name)
       // Update the PodLogMonitor status
       podLogMonitor.Status.LastRestartedPodName = pod.Name
       podLogMonitor.Status.LastRestartTime = metav1.NewTime(time.Now())

                if err := r.Status().Update(ctx, podLogMonitor); err != nil {
           log.Error(err, ""unable to update PodLogMonitor status"")
           return err
       }
   }
   return nil
}

// SetupWithManager sets up the controller with the Manager.
func (r *PodLogMonitorReconciler) SetupWithManager(mgr ctrl.Manager) error {
        return ctrl.NewControllerManagedBy(mgr).
                For(&monitoringv1.PodLogMonitor{}).
                Complete(r)
}
```

Below is the CRD definition for 'podlogmonitors.monitoring.mydomain.com'

```
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: podlogmonitors.monitoring.mydomain.com
spec:
  group: monitoring.mydomain.com
  names:
    kind: PodLogMonitor
    listKind: PodLogMonitorList
    plural: podlogmonitors
    singular: podlogmonitor
  scope: Cluster
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                namespace:
                  type: string
                logMessage:
                  type: string
            status:
              type: object
              properties:
                lastRestartedPodName:
                  type: string
                lastRestartTime:
                  type: string
```

Below is the CR definition for 'test-pod-monitor-cr'

```
apiVersion: monitoring.mydomain.com/v1
kind: PodLogMonitor
metadata:
  name: test-pod-monitor-cr
spec:
  namespace: hello-devops
  logMessage: ""error restart test""
```","go, kubernetes, kubebuilder",,,,2025-02-15T23:31:03
79441482,Specific instructions for AWS fargate metrics using opentelemetry-collector sidecar,"Does anybody know of an example or an article giving instructions on how to collect memory, cpu, perhaps k8s behaviour etc stats/metrics on AWS fargate pods running the otel collector as a sidecar and, preferably, using at most the opentelemetry-collector-contrib image?

Background is that we run a lot of our service within Fargate and we use New Relic to help monitor the various kubernetes clusters. We've been using one of New Relic's own agents to collect the metrics to view. However we are starting to add otel collectors into some of the pods because we are adding otel metric and trace generation into some of the containers. (The otel collectors are running as sidecars) The otel stuff currently goes to an Elastic environment where we hold our logs. For obvious(?) reasons, I'd like to rationalise this and ditch the New Relic agent - sending the metrics to both Elastic and NR - but would need to generate those metrics that the NR agent obtains, and I am not totally clear how to.

My guess was that the thing to use would be the kubeletstats receiver. Following a few hints I tried adding via:

```
      kubeletstats:
        collection_interval: 30s
        auth_type: ""none""
        endpoint: ""http://${env:MY_NODE_NAME}:10255""
```

but no extra metrics were dispatched. Quite possibly the auth type is wrong but then again I might be barking up the wrong tree.

Any suggestions?","kubernetes, aws-fargate, newrelic, open-telemetry-collector",,,,2025-02-15T12:08:23
79437967,K8S: Invalid value for configuration broker.id for kafka pod,"I'm trying to bring up kafka with two replicas but facing the issue of broker.id not INT. Would you please give me a hint what is wrong with my deployment yaml:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
  namespace: monitoring
spec:
  serviceName: ""kafka""
  replicas: 2
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/hostname
                operator: In
                values:
                - docker-desktop
      containers:
      - name: kafka
        image: bitnami/kafka:latest
        command: [""/bin/bash"", ""-c""]
        args:
          - |
            export KAFKA_BROKER_ID=$(echo ${POD_NAME} | grep -o ""[0-9]*$"")
            exec /opt/bitnami/scripts/kafka/run.sh
        ports:
        - containerPort: 9092
          name: kafka-0
        - containerPort: 9093
          name: kafka-1
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: KAFKA_BROKER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.annotations['statefulset.kubernetes.io/pod-index']
        - name: KAFKA_BROKER_ID_FALLBACK
          value: '$(echo ${POD_NAME} | grep -o ""[0-9]*$"")'
        - name: KAFKA_BROKER_ID_FINAL
          value: ""$(KAFKA_BROKER_ID:-$(KAFKA_BROKER_ID_FALLBACK))""
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: zookeeper.monitoring.svc.cluster.local:2181
        - name: KAFKA_LISTENERS
          value: CLIENT://:9092,BROKER://:9093
        - name: KAFKA_ADVERTISED_LISTENERS
          value: CLIENT://$(POD_NAME).kafka.monitoring.svc.cluster.local:9092,BROKER://$(POD_NAME).kafka.monitoring.svc.cluster.local:9093
        - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
          value: CLIENT:PLAINTEXT,BROKER:PLAINTEXT
        - name: KAFKA_CFG_INTER_BROKER_LISTENER_NAME
          value: BROKER
        volumeMounts:
        - name: kafka-data
          mountPath: /bitnami/kafka/data
  volumeClaimTemplates:
  - metadata:
      name: kafka-data
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 10Gi
      storageClassName: hostpath-delayed-binding
---
apiVersion: v1
kind: Service
metadata:
  name: kafka
  namespace: monitoring
spec:
  clusterIP: None
  ports:
  - port: 9092
    name: kafka-0
  - port: 9093
    name: kafka-1
  selector:
    app: kafka

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-external
  namespace: monitoring
spec:
  type: NodePort
  ports:
  - port: 9092
    targetPort: 9092
    nodePort: 30092
  selector:
    app: kafka

alex@Mac k8s % kubectl apply -f kafka-broker.yaml -n monitoring

alex@Mac k8s % kubectl get pods -n monitoring
NAME                         READY   STATUS             RESTARTS        AGE
kafka-0                      0/1     CrashLoopBackOff   7 (4m27s ago)   15m
kafka-1                      0/1     CrashLoopBackOff   7 (4m46s ago)   15m
zookeeper-569cb468c4-mcghr   1/1     Running            0               24h
```

Check logs of pod:

```
alex@Mac k8s % kubectl logs -f kafka-0 -n monitoring
kafka 23:16:10.65 INFO  ==>
kafka 23:16:10.65 INFO  ==> Welcome to the Bitnami kafka container
kafka 23:16:10.65 INFO  ==> Subscribe to project updates by watching https://github.com/bitnami/containers
kafka 23:16:10.65 INFO  ==> Did you know there are enterprise versions of the Bitnami catalog? For enhanced secure software supply chain features, unlimited pulls from Docker, LTS support, or application customization, see Bitnami Premium or Tanzu Application Catalog. See https://www.arrow.com/globalecs/na/vendors/bitnami/ for more information.
kafka 23:16:10.65 INFO  ==>
kafka 23:16:10.65 INFO  ==> ** Starting Kafka setup **
kafka 23:16:10.67 WARN  ==> The KAFKA_ZOOKEEPER_PROTOCOL environment variable does not configure SASL and/or SSL, this setting is not recommended for production environments.
kafka 23:16:10.68 WARN  ==> Kafka has been configured with a PLAINTEXT listener, this setting is not recommended for production environments.
kafka 23:16:10.69 INFO  ==> Initializing Kafka...
kafka 23:16:10.69 INFO  ==> No injected configuration files found, creating default config files
kafka 23:16:10.74 INFO  ==> ** Kafka setup finished! **

kafka 23:16:10.75 INFO  ==> ** Starting Kafka **
Picked up JAVA_TOOL_OPTIONS:
[2025-02-13 23:16:10,969] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
[2025-02-13 23:16:11,043] ERROR Exiting Kafka due to fatal exception (kafka.Kafka$)
org.apache.kafka.common.config.ConfigException: Invalid value  for configuration broker.id: Not a number of type INT
    at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:776)
    at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:531)
    at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:524)
    at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:115)
    at org.apache.kafka.server.config.AbstractKafkaConfig.<init>(AbstractKafkaConfig.java:70)
    at kafka.server.KafkaConfig.<init>(KafkaConfig.scala:182)
    at kafka.server.KafkaConfig.<init>(KafkaConfig.scala:185)
    at kafka.server.KafkaConfig$.fromProps(KafkaConfig.scala:101)
    at kafka.Kafka$.buildServer(Kafka.scala:71)
    at kafka.Kafka$.main(Kafka.scala:90)
    at kafka.Kafka.main(Kafka.scala)
```

I see the broker.id blank accordingly to the log (double space between 'Ibvalid value' and the rest of warning:

```
org.apache.kafka.common.config.ConfigException: Invalid value  for configuration broker.id: Not a number of type INT
```","kubernetes, apache-kafka, yaml",,,,2025-02-13T23:28:05
79437965,How to Automatically Provision CDN(CLOUDFRONT) which points to LB(ELB) with Kubernetes and External-DNS?,"I'm learning Kubernetes (K8s) and have a question regarding cloud infrastructure provisioning with Kubernetes and external-dns.

```
ROUTE 53 ---> CLOUDFRONT (SSL TERMINATION) ----> ELB --------> INGRESS -|---> VARIOUS_PODS
ROUTE 53 <--------UPDATE ROUTES-----------------------------------------|---- external-dns
```

**My question is, is it possible to automatically provision `Route53` to `cloudfront` to `ELB`?** (see bellow)

```
ROUTE 53 ---> CLOUDFRONT (SSL TERMINATION & enforce HTTPS) ----> ELB
```

Currently the **default behaviour** is to create a `Route53` record that points to `ELB`

```
ROUTE 53 ---> ELB
```","kubernetes, amazon-cloudfront, amazon-elb, amazon-route53, external-dns",,,,2025-02-13T23:26:16
79437722,how to get cluster certificate authority and JWT token after assuming different aws account,"Hello this is my code below, I'm basically trying to assume role into different aws account and get the list of clusters using aws cli command, later update kubeconfig based on the cluster name selected and get the jwt token and kube ca cert, but I'm unable to retrieve those values, any help would be appreciated.

```
#!/bin/bash

# Set AWS region
AWS_REGION=""us-east-1""
INPUT_FILE=""inputfile""

# Function to assume role and get temporary credentials
assume_role() {
    local account_id=$1
    local account_name=$2
    local role_arn=""arn:aws:iam::${account_id}:role/support-admin""
    local session_name=""${account_id}""

    echo ""Assuming role for account: ${account_name} (${account_id})""

    # Assume role and get temporary credentials
    credentials=$(aws sts assume-role --role-arn ""${role_arn}"" --role-session-name ""${session_name}"")

    if [ $? -ne 0 ]; then
        echo ""Cannot assume role for account: ${account_id} (${account_name})""
        echo ""--------------------------------------------------------------------""
        return
    fi

    export AWS_ACCESS_KEY_ID=$(echo ""${credentials}"" | jq -r '.Credentials.AccessKeyId')
    export AWS_SECRET_ACCESS_KEY=$(echo ""${credentials}"" | jq -r '.Credentials.SecretAccessKey')
    export AWS_SESSION_TOKEN=$(echo ""${credentials}"" | jq -r '.Credentials.SessionToken')

    list_eks_clusters ""${account_name}""
    echo ""--------------------------------------------------------------------""
}

# Function to list EKS clusters
list_eks_clusters() {
    local account_name=$1

    echo ""Listing EKS clusters for account: ${account_name}""

    clusters=$(aws eks list-clusters --region ""${AWS_REGION}"" | jq -r '.clusters[]')

    if [ $? -ne 0 ]; then
        echo ""Error listing EKS clusters for account: ${account_name}""
        return
    fi

    for cluster in ${clusters}; do
        echo ""Cluster: ${cluster}""
        # describe_cluster ""${cluster}""
    done
}

# Function to describe EKS cluster, update kubeconfig, and get CA certificate
describe_cluster() {
    local cluster_name=$1

    echo ""Describing cluster: ${cluster_name}""

    cluster_info=$(aws eks describe-cluster --name ""${cluster_name}"" --region ""${AWS_REGION}"")

    if [ $? -ne 0 ]; then
        echo ""Error describing cluster: ${cluster_name}""
        return
    fi

    cluster_endpoint=$(echo ""${cluster_info}"" | jq -r '.cluster.endpoint')
    echo ""Cluster ${cluster_name} endpoint: ${cluster_endpoint}""

    # Update kubeconfig for the cluster and write to custom location
    CUSTOM_KUBECONFIG=""kubeconfig""
    aws eks --region ""${AWS_REGION}"" update-kubeconfig --name ""${cluster_name}"" --kubeconfig ""${CUSTOM_KUBECONFIG}""

    Get the authentication token for the cluster

    export KUBECONFIG=kubeconfig
    kubectl config view --kubeconfig=""${KUBECONFIG}""

    token=$(aws eks get-token --cluster-name ""${cluster_name}"" --region ""${AWS_REGION}"" | jq -r '.status.token')
    echo ""Token: ${token}""
    kubectl config set-credentials arn:aws:eks:${AWS_REGION}:${account_id}:cluster/${cluster_name} --token=""${token}""

    # Ensure the kubeconfig context is set correctly
    kubectl config use-context arn:aws:eks:${AWS_REGION}:${account_id}:cluster/${cluster_name}

    # Get all namespaces from the cluster using the token - works this here
    kubectl --kubeconfig=""${KUBECONFIG}"" get namespaces

    # Get the CA certificate from the vault-token secret in the kube-system namespace - dont get exact ca cert of cluster selected
    KUBE_CA_CERT=$(kubectl get secret vault-token -n kube-system -o json --token=""${token}"" | jq -r '.data | .""ca.crt""' | base64 --decode)
    echo ""KUBE_CA_CERT: ${KUBE_CA_CERT}""
    echo ""--------------------------------------------------------------------""
}

# Read accounts from input file and assume role for each account
if [ ! -f ""${INPUT_FILE}"" ]; then
    echo ""Input file not found: ${INPUT_FILE}""
    exit 1
fi

while IFS=, read -r account_id account_name; do
    assume_role ""${account_id}"" ""${account_name}""
done < ""${INPUT_FILE}""
```

Error:

Switched to context ""arn:aws:eks:us-east-1:*********:cluster/eks7-***-***"".
E0213 16:15:03.105457   57722 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: the server has asked for the client to provide credentials""
E0213 16:15:03.188119   57722 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: the server has asked for the client to provide credentials""
E0213 16:15:03.290776   57722 memcache.go:265] ""Unhandled Error"" err=""couldn't get current server API group list: the server has asked for the client to provide credentials""
error: You must be logged in to the server (Unauthorized)","amazon-web-services, kubernetes, aws-cli, amazon-eks",,,,2025-02-13T21:06:04
79437693,How can I configure a Kubernetes startupProbe without blocking inbound traffic needed for cluster formation?,"I'm deploying Apache Ignite cluster in Kubernetes and using a startupProbe to ensure each node has joined the Ignite baseline topology. It's necessary to make sure that Server Node is completly ready for work (it's connected to Topology, caches are loaded..). For this I created custom bash script that utilizes Ignite REST Api to check whether the Node is part of Topology or not.

```
#!/bin/bash
apk add --no-cache curl jq
NODE_HOSTNAME=$(hostname)
echo ""Hostname: $NODE_HOSTNAME""

NODE_ID=$(curl -s ""http://localhost:8080/ignite?cmd=top"" | \
         jq -r --arg node_hostname ""$NODE_HOSTNAME"" '.response[] | select(.tcpHostNames[] | startswith($node_hostname)) | .consistentId')
echo ""NODE_ID: $NODE_ID""

BASELINE=$(curl -s ""http://localhost:8080/ignite?cmd=baseline"" | \
          jq -r --arg node_id ""$NODE_ID"" '.response.baseline[] | select(. == $node_id)')
echo ""Baseline result: $BASELINE""

if [ -z ""$BASELINE"" ]; then
  echo ""Node is NOT part of the baseline topology yet.""
  exit 1
else
  echo ""Node is part of the baseline topology.""
  exit 0
fi
```

And my Kubernetes deployment's startupProbe is defined as:

```
startupProbe:
  exec:
    command:
      - /bin/sh
      - -c
      - /scripts/check_baseline.sh
  initialDelaySeconds: 10
  periodSeconds: 10
  failureThreshold: 60
```

I tested in containers and it works as expected, but when I tried to apply it as startupProbe, I faced the problem, that Readiness Probe is not started until StartUp succeeds, therefore the incoming traffic can not reach the Pod and Ignite Client can not join and form Topology.
So I have a deadlock: StartupProbe waits for Node to join cluster, but it can not do it, since incoming traffic is blocked due to this probe.

Is there any way to workaround it? or may be other way how to add such a health check.","kubernetes, ignite",,,,2025-02-13T20:55:20
79436997,createNamespacedJob Fails with “namespace was null or undefined”,"**Summary**:

When calling BatchV1Api.createNamespacedJob with a hard-coded namespace (“audio-processing”)—both as the first argument and in the job manifest metadata—the client throws a RequiredError stating that the namespace parameter is null or undefined. This occurs even though the KubeConfig is loaded with a default namespace and the manifest includes a hard-coded namespace in its metadata.

**Steps to Reproduce**:

1. Set up a Kubernetes (EKS) cluster and ensure that a namespace named “audio-processing” exists.
2. Create a minimal Node.js script that:Loads a KubeConfig using kc.loadFromOptions() (or an equivalent method).

- Creates an API client using kc.makeApiClient(BatchV1Api).
- Prepares a job manifest with hard-coded namespace in its metadata.
- Calls createNamespacedJob(""audio-processing"", jobManifest).

1. Run the script and observe that an error is thrown indicating that the namespace parameter is null or undefined.

**Minimal Viable Example:**

```
// File: minimal-example.js

import * as k8s from ""@kubernetes/client-node"";

async function main() {
  // Set up a dummy KubeConfig
  const kc = new k8s.KubeConfig();
  kc.loadFromOptions({
    clusters: [
      {
        name: ""dummyCluster"",
        server: ""https://<your-cluster-endpoint>"",
        caData: ""<your-ca-data>"",
        skipTLSVerify: false,
      },
    ],
    contexts: [
      {
        name: ""dummyContext"",
        cluster: ""dummyCluster"",
        user: ""dummyUser"",
        namespace: ""audio-processing"",   // Default namespace provided here
      },
    ],
    users: [
      {
        name: ""dummyUser"",
        // For this minimal example, no exec configuration is needed.
      },
    ],
    currentContext: ""dummyContext"",
  });

  // Create the BatchV1Api client from the loaded KubeConfig
  const batchV1Api = kc.makeApiClient(k8s.BatchV1Api);

  // Job manifest with the namespace explicitly declared in metadata
  const jobManifest = {
    apiVersion: ""batch/v1"",
    kind: ""Job"",
    metadata: {
      name: ""test-job"",
      namespace: ""audio-processing"", // Hard-coded namespace in the manifest
    },
    spec: {
      template: {
        spec: {
          containers: [
            {
              name: ""test-container"",
              image: ""busybox"",
              command: [""echo"", ""Hello World""],
            },
          ],
          restartPolicy: ""Never"",
        },
      },
    },
  };

  console.log(""Job manifest:"", JSON.stringify(jobManifest, null, 2));

  try {
    // Explicitly pass the namespace as the first argument to createNamespacedJob
    const response = await batchV1Api.createNamespacedJob(""audio-processing"", jobManifest);
    console.log(""Job created successfully:"", response.body);
  } catch (error) {
    console.error(""Error creating job:"", error);
  }
}

main();
```

**Expected Behavior**:

The call to: `await batchV1Api.createNamespacedJob(""audio-processing"", jobManifest);`

should succeed and create the job in the “audio-processing” namespace.

**Observed Behavior:**

The API call fails with the error:

```
RequiredError: Required parameter namespace was null or undefined when calling BatchV1Api.createNamespacedJob.
    at BatchV1ApiRequestFactory.createNamespacedJob (…/node_modules/@kubernetes/client-node/dist/gen/apis/BatchV1Api.js:84:19)
    ...
```

**Even though**:

- The namespace is provided explicitly as ""audio-processing"" in the method call.
- The job manifest also includes ""namespace"": ""audio-processing"" in its metadata.
- The KubeConfig is loaded with a context that has a default namespace.

**Environment Details**:

- @kubernetes/client-node version: (""@kubernetes/client-node"": ""^1.0.0"",)
- Node.js version: (runtime: nodejs20.x)
- Deployment Environment: (AWS Lambda Function triggering a Fargate job on EKS)
- Cluster: EKS (or Kubernetes) with the namespace “audio-processing” created.

**Additional Context**:

- The error originates from a validation check in the BatchV1Api source (see BatchV1Api.ts #Line84), which throws a RequiredError if the namespace parameter is null or undefined.
- We have attempted multiple configuration and manifest-based solutions (using both kc.makeApiClient and ensuring the namespace is present in both the API call and the job manifest) without success.
- The issue persists even after verifying that the hard-coded namespace string is correctly provided.

**Request for Help**:

We request assistance to understand why, despite the explicit provision of the namespace, the API call still fails. Is there an expected alternative approach to instantiating the client (e.g., using createConfiguration instead of kc.makeApiClient) that might resolve this issue? Any guidance or fixes would be greatly appreciated.

this is how the job manifest is generated:

```
/**
 * Generates a Kubernetes Job manifest for the audio conversion task.
 *
 * @param {Object} params - Parameters for the job manifest.
 * @param {string} params.inputS3Path - The S3 path of the input MP3 file.
 * @param {string} params.outputS3Path - The S3 path for the output PCM file.
 * @returns {Object} - The Kubernetes Job manifest.
 */
function generateJobManifest({ inputS3Path, outputS3Path, recordId }) {
  const jobManifest = {
    apiVersion: ""batch/v1"",
    kind: ""Job"",
    metadata: {
      name: `audio-conversion-${recordId}`, // FIXME: Make sure this is correct job name!
      namespace: ""audio-processing"",
    },
    spec: {
      parallelism: 5, // Specifies how many pods can run at once
      completions: 5, // Specifies how many pod successes are needed
      template: {
        spec: {
          containers: [
            {
              name: ""audio-processing"",
              image: `${process.env.ECR_REPO_PCM_CONVERTER}:latest`,
              env: [
                { name: ""INPUT_S3_PATH"", value: inputS3Path },
                { name: ""OUTPUT_S3_PATH"", value: outputS3Path },
                {
                  name: ""AWS_ACCOUNT_REGION"",
                  value: process.env.AWS_ACCOUNT_REGION,
                },
              ],
              command: [""/convert.sh""],
              resources: {
                requests: {
                  cpu: ""4000m"", // Reserve 4 vCPU
                  memory: ""8Gi"", // Reserve 8 GiB of memory
                  ""ephemeral-storage"": ""175Gi"",
                },
                limits: {
                  cpu: ""4000m"", // Allow up to 4 vCPU
                  memory: ""8Gi"", // Allow up to 8 GiB of memory
                  ""ephemeral-storage"": ""175Gi"",
                },
              },
            },
          ],
          restartPolicy: ""Never"",
        },
      },
      backoffLimit: 5,
    },
  };

  console.log(""Generated Job Manifest:"", JSON.stringify(jobManifest));
  return jobManifest;
}
```

```
/**
 *
 * Retrieves the EKS cluster details (endpoint & CA data) so we can create a kubeconfig.
 *
 *
 */
async function getEksClusterDetails(clusterName, region) {
  const eksClient = new EKSClient({ region });
  const command = new DescribeClusterCommand({ name: clusterName });
  const response = await eksClient.send(command);
  console.log(""EKS Cluster details: "", response);
  return response.cluster;
}

/**
 * Builds a kubeconfig object for the given EKS cluster using exec authentication.
 */
export async function loadKubeConfig() {
  const clusterName = process.env.CLUSTER_NAME; // e.g. ""dev-eks-med-transcribe""

  console.log(""CLUSTER NAME: "", clusterName);
  const region = process.env.AWS_ACCOUNT_REGION;

  console.log(""CLUSTER REGION: "", region);
  const cluster = await getEksClusterDetails(clusterName, region);

  console.log(""CLUSTER DETAILS: "", cluster);
  const kc = new k8s.KubeConfig();

  console.log(""KUBE CONFIG: "", kc);

  kc.loadFromOptions({
    clusters: [
      {
        name: cluster.name,
        server: cluster.endpoint,
        caData: cluster.certificateAuthority.data,
        skipTLSVerify: false,
      },
    ],
    contexts: [
      {
        name: ""aws"",
        cluster: cluster.name,
        user: ""aws"",
        namespace: ""audio-processing"",
      },
    ],
    users: [
      {
        name: ""aws"",
        exec: {
          apiVersion: ""client.authentication.k8s.io/v1beta1"",
          command: ""aws"",
          args: [
            ""eks"",
            ""get-token"",
            ""--cluster-name"",
            cluster.name,
            ""--region"",
            region,
          ],
        },
      },
    ],
    currentContext: ""aws"",
  });

  console.log(""KUBE LOAD FROM OPTIONS: "", kc);

  return kc;
}
```

**ERROR LOGGED**

```
ERROR   Error creating EKS job: RequiredError: Required parameter namespace was null or undefined when calling BatchV1Api.createNamespacedJob.
    at BatchV1ApiRequestFactory.createNamespacedJob (file:///var/task/node_modules/@kubernetes/client-node/dist/gen/apis/BatchV1Api.js:84:19)
    at ObservableBatchV1Api.createNamespacedJobWithHttpInfo (file:///var/task/node_modules/@kubernetes/client-node/dist/gen/types/ObservableAPI.js:7621:59)
    at ObservableBatchV1Api.createNamespacedJob (file:///var/task/node_modules/@kubernetes/client-node/dist/gen/types/ObservableAPI.js:7646:21)
    at ObjectBatchV1Api.createNamespacedJob (file:///var/task/node_modules/@kubernetes/client-node/dist/gen/types/ObjectParamAPI.js:2840:25)
    at Runtime.main [as handler] (file:///var/task/triggerConversionJob.js:219:24)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5) {
  api: 'BatchV1Api',
  method: 'createNamespacedJob',
  field: 'namespace'
}
```","node.js, kubernetes, namespaces",79501367.0,"Seems like v1.0.0 expects an params object instead of the listed parameters (until v0.22.3 it worked like your code).

In your case:

```
await batchV1Api.createNamespacedJob({namespace: ""audio-processing"", body: jobManifest});
```

should do the trick.",2025-03-11T15:53:18,2025-02-13T16:14:58
79433398,how can I inject a secret on my helm overlays?,"Im new with helm charts but I created a deployment template, the template will need to include 2 secrets, so, inside the deployment.yaml file I have this: (this is for 1 secret)

env:

```
{{- range $name, $value := .Values.env}}
 - name: {{ name }}
   value: ""{{ value }}""
{{- end }}
 - name: SECRET_PASSWORD
   valueFrom:
      secretKeyRef:
      name: {{ .Values.env.secret.secretPassword.name }}
      key: {{ .Values.env.secret.secretPassword.key }}
```

That is on the template, if I add example values for the secretKey I can do:

```
env:
  secret:
    secretPassword:
      name: passname
      key: passkey
```

However, I know there is a missing part, I saw I can also create a template for secrets, something like this:

```
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metada:
   name: {{ .name }}
spec:
   encryptedData:
{{-range $key, $value := .encryptedData}}
{{$key}} : | -
{{$value}}
{{- end}}
```

what parts am I missing? did I understood it correctly? and how can I create an overlay for this? any tutorial or reference will be appreciated.","kubernetes, kubernetes-helm",79433714.0,"You have two different configuration structures for your environment variables.  If your Helm values say

```
env:
  SIMPLE_STRING_VALUE: string
  secret: { secretPassword: { ... } }
```

then the output will contain an additional entry for `secret` that might not make sense: you'll get a default Go serialization of the nested structure.

One option is just to skip over the special `secret` key in your loop

```
{{- range $name, $value := .Values.env}}
{{- if ne $name ""secret"" }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
{{- end }}
```

A better option would be to separate out the specific configuration for the value you're injecting as a Secret.

```
# values.yaml
env:
  SIMPLE_STRING_VALUE: string
secretPassword:
  name: secretName
  key: password
```

```
# deployment.yaml
{{- range $name, $value := .Values.env }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ .Values.secretPassword.name }}
      key: {{ .Values.secretPassword.key }}
```

I might go even further, though.  If the SealedSecret is generated in your chart, then its name doesn't need to be configurable.  In the Helm values, you can apply some structure around what you expect to be present, instead of allowing totally free-form data.  If the Helm values say

```
encryptedData:
  password: ""...""
```

and then the SealedSecret says specifically

```
metadata:
  name: {{ include ""mychart.name"" . }}
spec:
  encryptedData:
    password: {{ .Values.encryptedData.password }}
```

now you know the Secret name *and* the key within the Secret, and you don't need to make any of it configurable at all.

```
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ include ""mychart.name"" . }}
      key: password
```

Notice that this last block in the Deployment spec contains no references to `.Values`, and there is no provision for an administrator to change the Secret name (beyond the generic boilerplate settings for changing all of the object names) or the specific key; but since all of these objects are being created in the chart itself, an administrator also doesn't need to create them.",2025-02-12T16:01:39,2025-02-12T14:19:10
79433030,Load balancing HTTP traffic with istio from non-sidecar-enabled pods to sidecar-enabled pods,"My use case:

We have a kubernetes `namespace-a`, with 2 deployments `deployment-a` and `deployment-b`.

Pods of the `deployment-a` are sidecar enabled with envoy proxy. Pods of `deployment-b` are not.

Is there a way to configure istio so that HTTP traffic coming from `deployment-b` pods -> `deployment-a` pods is being load balanced by istio? Currently it seems, that load balancing is done by plain kubernetes service instead of istio.","kubernetes, istio, istio-gateway, istio-sidecar",79447591.0,External services can access services managed by Istio via a `Gateway`. They can use internal IP of ingress gateway and provide a host header for which server the request is sent to. Or add a DNS entry for ingress gateway IP.,2025-02-18T08:40:39,2025-02-12T12:23:47
79433030,Load balancing HTTP traffic with istio from non-sidecar-enabled pods to sidecar-enabled pods,"My use case:

We have a kubernetes `namespace-a`, with 2 deployments `deployment-a` and `deployment-b`.

Pods of the `deployment-a` are sidecar enabled with envoy proxy. Pods of `deployment-b` are not.

Is there a way to configure istio so that HTTP traffic coming from `deployment-b` pods -> `deployment-a` pods is being load balanced by istio? Currently it seems, that load balancing is done by plain kubernetes service instead of istio.","kubernetes, istio, istio-gateway, istio-sidecar",79433548.0,"If you're hitting the k8s service directly then it will round robin the requests (k8s default), and as your deployment is not sidecar injected, you can't use load balancing algorithms from the [DR](https://istio.io/latest/docs/reference/config/networking/destination-rule/#LoadBalancerSettings-SimpleLB) to configure the client.",2025-02-12T15:05:09,2025-02-12T12:23:47
79433005,Kubernetes PostStartHook fails with curl,"I am trying to get a postStart hook working in a container but it keeps failing. The error I get is the following:

```
kubelet[1057]: E0212 11:07:20.205922    1057 handlers.go:78] ""Exec lifecycle hook for Container in Pod failed"" err=<
kubelet[1057]:         command 'curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted' exited with 2: curl: (2) no URL specified
kubelet[1057]:         curl: try 'curl --help' or 'curl --manual' for more information
kubelet[1057]:  > execCommand=[curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted] containerName=""srsran-cu-du"" pod=""srsran/srsran-project-cudu-chart-78f658b865-pjvt2"" message=<
kubelet[1057]:         curl: (2) no URL specified
kubelet[1057]:         curl: try 'curl --help' or 'curl --manual' for more information
kubelet[1057]:  >
```

The hook in my manifest looks like this:

```
lifecycle:
  postStart:
    exec:
      command: [ ""curl"", ""-H"",  ""'Content-Type: application/json'"", ""-d"", ""'{ \""restarted\"": True}'"", ""-X"", ""POST http://localhost:5000/restarted"" ]
```

which renders to `curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted`.

If I run the curl command as it renders in the container directly its working fine. But when running it via the posStart hook it doesn't work. What am I doing wrong?

I have tried replacing the `'` with `\\\""` but that also didnt work.","kubernetes, curl",79433074.0,"When you use an array-form `command:`, or pass a command as a container's `args:`, you need to pass exactly one shell word per YAML list item.  The most immediate cause of your error is that there are two words in the last list item, so `curl` interprets this as a single parameter requesting an HTTP method `POST http://...` including a space, and then there is no following parameter with the URL.

You will also possibly get an error from the `Content-Type:` header: because you have a set of single quotes inside the double-quoted YAML string, `curl` will see these quotes as part of the `-H` argument, and it may send an invalid HTTP header or reject the header syntax itself.

Splitting this out into one argument per word, using YAML block sequence syntax, and using only YAML quoting and only where required, I might write:

```
command:
  - curl
  - -H
  - 'Content-Type: application/json'  # quotes required, else `key: value` looks like a mapping
  - -d
  - '{ ""restarted"": true }'           # YAML single quoting; double quotes do not need to be escaped; some quoting required else this looks like a JSON object
  - -X
  - POST
  - http://localhost:5000/restarted   # note two separate words
```

Or you could repack this into an inline list (""flow sequence""), using the same quoting rules

```
command: [curl, -H, 'Content-Type: application/json', -d, '{ ""restarted"": true }', -X, POST, http://localhost:5000/restarted]
```

Again, note that there is only one set of quotes for the `Content-Type:` header, and that `POST` and `http://...` are separate list items.  You can quote the other words too if you'd like, but it's only required for the couple of things that could be mistaken for other YAML syntax.",2025-02-12T12:36:39,2025-02-12T12:15:33
79430402,How to use ephemeral container for debug pod in k8s 1.18,"I learned from the [information](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md#sig-cli-introduces-kubectl-debug) that k8s1.18 can also support ephemeral containers.

I enabled this feature by modifying the api-server

```
- --feature-gates=EphemeralContainers=true
```

However, when I executed the debug command, I did not achieve the expected interactive debugging of the business pod effect.  The terminal seemed to be stuck.

```
$ kubectl alpha debug <biz_pod> -i -t --image=busybox -- /bin/bash
Defaulting debug container name to debugger-ttxgk.
<stuck forever>
```

I checked the business pod and found that the container had been injected.

```
$ kubectl get pod <biz_pod> -o yaml
...
- image: busybox
    imagePullPolicy: IfNotPresent
    name: debugger-pg4vf
    resources: {}
    stdin: true
    terminationMessagePolicy: File
    tty: true
  - command:
    - /bin/sh
    image: busybox
    imagePullPolicy: IfNotPresent
    name: debugger-45fzq
    resources: {}
    stdin: true
    terminationMessagePolicy: File
  - command:
    - /bin/sh
    image: busybox
    imagePullPolicy: IfNotPresent
    name: debugger-sjgh7
    resources: {}
    stdin: true
    terminationMessagePolicy: File
    tty: true
...
```

Is it because I am missing some configuration or the command is wrong? Is there a way to delete the ephemeral container without affecting the pod? Looking forward to your reply.

> PS: due to business reasons, it is impossible to upgrade the k8s cluster to 1.25 of the ephemeral container feature stable",kubernetes,79430934.0,"Based on the yaml file you shared you're using shell `/bin/sh` but in the command you run you use `/bin/bash`. Try to run the :

```
kubectl alpha debug <biz_pod> -i -t --image=busybox -- /bin/sh
```

For additional workaround confirm that the ephemeral container was successfully injected. Look for the injected ephemeral container if it’s running or falling :

```
kubectl describe pod <biz_pod>
```

You can also check the logs that will help you understand if the debug container is running or failing.

```
kubectl logs <biz_pod> -c debugger-<name>
```

> Is there a way to delete the ephemeral container without affecting the pod?

You can't delete the ephemeral container after you have added it to a pod.

For additional information see the documentation below :

- [Remove Ephemeral Container](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/#what-is-an-ephemeral-container:%7E:text=Like%20regular%20containers%2C%20you%20may%20not%20change%20or%20remove%20an%20ephemeral%20container%20after%20you%20have%20added%20it%20to%20a%20Pod)
- [https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/277-ephemeral-containers#removing-ephemeral-containers](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/277-ephemeral-containers#removing-ephemeral-containers)",2025-02-11T17:58:05,2025-02-11T14:54:31
79426473,Implementing Spring application with Keycloak OAauth2 running behind gateway on several Pods,"I have the following structure of Kubernetes cluster.

There is Gateway: Spring app with net.bretti.openapi-route-definition-locator to dispatch requests to several microservices.

There are business applications (microservices in Spring and Quarkus). One of Spring apps uses Oidc Keycloak authentication. Keycloak server is running in the same cluster.

Each of apps (and the Gateway) have replica count 2 or 3.

The problem is in Spring OAuth2 app (let's name it aaa). The gateway has ingress rule to redirect requests with /aaa prefix to the aaa application. The Service object of this application has property: `sessionAffinity: ClientIP` in its spec.

As I understand, after a user request from the browser goes to the gateway, it redirects to the Service, it then redirects to one of working pods.
The pod checks user - if not authenticated, Spring redirects to aaa/oauth2/authorization, then to keycloak (keycloak/realms/my-realm) with redirect-uri query param (aaa/login/oauth2/code).

The callback from Keycloak through the Gateway returns back to Service and should go to the same Pod. But sometimes this does not occur. The redirect arrives to the second pod. And I got ""The page isn't redirecting properly"" and see keycloak?error page.

Here are some related configs:

application.yaml in Spring aaa application

```
spring:
  security:
    oauth2:
      client:
        provider:
          keycloak:
            user-name-attribute: preferred_username
            issuer-uri: https://my-keycloak/realms/my-realm
        registration:
          keycloak:
            client-id: ${OIDC_CLIENT_ID}
            client-secret: ${OIDC_CLIENT_SECRET}
            scope: openid
            authorization-grant-type: authorization_code
            redirect-uri: ""{baseUrl}/aaa/login/oauth2/code/keycloak""
            client-authentication-method: client_secret_post
```

SecurityConfig.java

```
 @Bean
    public SecurityFilterChain filterChain(HttpSecurity http) throws Exception {
        http
                .authorizeHttpRequests(authorize -> authorize
                        .requestMatchers(""/actuator/*"").permitAll()
                        .requestMatchers(""/openapi.yaml"").permitAll()
                        .anyRequest().authenticated()
                )
                .oauth2Login(oauth2 -> oauth2
                        .authorizationEndpoint(authorization -> authorization
                                .baseUri(""/aaa/oauth2/authorization"")
                        )
                        .loginPage(""/aaa/oauth2/authorization/keycloak"")
                        .redirectionEndpoint(redirection -> redirection
                                .baseUri(""/aaa/login/oauth2/code/*"")
                        )
                );
        return http.build();
    }
```

The flow works correctly if I set replicas: 1.

What may be the problem and how to diagnose and fix it?","kubernetes, spring-security, keycloak, session-affinity",79427735.0,"When running a clustered Spring application with sessions, you should share these sessions across instances. The [Spring Session project](https://spring.io/projects/spring-session) serves that purpose.

This is true when using any stateful app (like those configured with `oauth2Login` or `formLogin`), not for stateless apps (those configured with `oauth2ResourceServer` or `Basic` auth for instance).",2025-02-10T16:42:45,2025-02-10T08:29:15
79423776,Cant create secret via kubernetes node client,"I try to create a secret in a namespace.

```
import { KubeConfig, CoreV1Api } from ""@kubernetes/client-node"";

const kc = new KubeConfig();
kc.loadFromDefault();

const k8sCoreApi = kc.makeApiClient(CoreV1Api);
const namespace = ""a316e0ae-19cc-46ce-84ae-b8236ba20ffc"";

const secretManifest = {
    apiVersion: 'v1',
    kind: 'Secret',
    namespace,
    metadata: {
        name: 'environment',  // Secret name
        namespace
    },
    data: {
        username: Buffer.from('hans.hubert@example.com').toString('base64'),
        password: Buffer.from('Pa$$w0rd').toString('base64'),
    },
    type: 'Opaque',
};

// Call createNamespacedSecret with the correct body structure
k8sCoreApi.createNamespacedSecret(namespace, secretManifest).then((secret) => {
    console.log(""Secret created"", secret)
}).catch(console.error);
```

No matter what i do, i allways get the error ""RequiredError: Required parameter namespace was null or undefined when calling CoreV1Api.createNamespacedSecret."":

```
RequiredError: Required parameter namespace was null or undefined when calling CoreV1Api.createNamespacedSecret.
    at CoreV1ApiRequestFactory.createNamespacedSecret (file:///home/marc/projects/kubernets/open-haus.cloud/node_modules/@kubernetes/client-node/dist/gen/apis/CoreV1Api.js:2850:19)
    at ObservableCoreV1Api.createNamespacedSecretWithHttpInfo (file:///home/marc/projects/kubernets/open-haus.cloud/node_modules/@kubernetes/client-node/dist/gen/types/ObservableAPI.js:12243:59)
    at ObservableCoreV1Api.createNamespacedSecret (file:///home/marc/projects/kubernets/open-haus.cloud/node_modules/@kubernetes/client-node/dist/gen/types/ObservableAPI.js:12268:21)
    at ObjectCoreV1Api.createNamespacedSecret (file:///home/marc/projects/kubernets/open-haus.cloud/node_modules/@kubernetes/client-node/dist/gen/types/ObjectParamAPI.js:4652:25)
    at file:///home/marc/projects/kubernets/open-haus.cloud/deploy.mjs:41:12
    at ModuleJob.run (node:internal/modules/esm/module_job:218:25)
    at async ModuleLoader.import (node:internal/modules/esm/loader:329:24)
    at async loadESM (node:internal/process/esm_loader:28:7)
    at async handleMainPromise (node:internal/modules/run_main:113:12) {
  api: 'CoreV1Api',
  method: 'createNamespacedSecret',
  field: 'namespace'
}
```

According to the documentation: [https://kubernetes-client.github.io/javascript/classes/CoreV1Api.html#createNamespacedSecret](https://kubernetes-client.github.io/javascript/classes/CoreV1Api.html#createNamespacedSecret) it looks right to me.

I use k3s, versoin v1.31.3+k3s1, and Kubernetes node client v1.0.0.","node.js, kubernetes, k3s",79424885.0,"Got it working.
It seems like the documentation is outdated:

```
const secretManifest = {
  namespace,
  body: {
    apiVersion: 'v1',
    kind: 'Secret',
    namespace,
    metadata: {
        name: 'environment',  // Secret name
        namespace
    },
    data: {
        username: Buffer.from('hans.hubert@example.com').toString('base64'),
        password: Buffer.from('Pa$$w0rd').toString('base64'),
    },
    type: 'Opaque',
  },
};

// Call createNamespacedSecret with the correct body structure
k8sCoreApi.createNamespacedSecret(secretManifest).then((secret) => {
    console.log(""Secret created"", secret)
}).catch(console.error);
```

[https://github.com/kubernetes-client/javascript/issues/2208#issuecomment-2646002431](https://github.com/kubernetes-client/javascript/issues/2208#issuecomment-2646002431)",2025-02-09T12:34:35,2025-02-08T19:11:32
79423775,Deploy a pod with envoy as sidecar. How can I access it?,"I have created the following simple `Dockerfile`

```
FROM envoyproxy/envoy:v1.19.0
COPY envoy.yaml /etc/envoy/envoy.yaml
RUN chmod go+r /etc/envoy/envoy.
```

For which I build locally a custom envoy image with the name `envoy:my-test-image`

I created the following deployment file (`envoy_sidecar_deployment.yml`) with the intention to try to imitate istio i.e. create a sidecar envoy in a pod.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: citizenstig/httpbin
      initContainers:
        - name: envoy-proxy
          image: envoy:my-test-image
          restartPolicy: Always
```

I did:

```
$ kubectl apply -f envoy_sidecar_deployment.yml
deployment.apps/myapp created
```

and then I can see the pods:

```
$ kubectl get pods
NAME                     READY   STATUS    RESTARTS   AGE
myapp-759fd49fdd-sqxjh   2/2     Running   0          35s
myapp-759fd49fdd-wtpqc   2/2     Running   0          35s
```

The problem is I don't know how to test the connection. My expectation is that I would do something like `http://HOST:15001/header` and `15001` is the port for envoy and I would get a response by the `httpbin` as I have configured envoy like that in the image already. But I don't know what hostname to use.

I thought I need a loadbalancer service so I created the file:

```
apiVersion: v1
kind: Service
metadata:
  name: lb-svc
  labels:
    app: myapp
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    targetPort: 15001
    protocol: TCP
  selector:
    app: myapp
```

and I can see:

```
$ kubectl get svc
NAME         TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
lb-svc       LoadBalancer   10.105.14.50   localhost     8080:30448/TCP   81s
```

But when I try to access `http://localhost:8080/headers` I get an error. I also don't understand why don't I see in the ports section `8080:15001` and instead I see `30448`.

How can I make the pod accessible via that envoy proxy?","kubernetes, kubernetes-ingress, envoyproxy, kubernetes-service",,,,2025-02-08T19:11:12
79423757,Using ingress controller with local kind deployment: is it feasible to access all services under one root URL?,"I have a local kind deployment which deploys 3 services, MLflow, Katib and a K8s dashboard. I can deploy these as services and access them via port forwarding to each service and visiting the url in the browser without problem. I will for the sake of brevity only consider one from here on out.

Now what I wanted to be able to do is access  all services from outside the cluster through the url - localhost - followed by a path leading to the service; localhost/mlflow for the  mlflow service.  Such that I can avoid having to port-forward to each service individually. From my understanding I needed to implement an ingress object for each service and deploy an ingress controller to handle routing. Now I’ve deployed these objects and applications but still:

1. I cannot access the service via the URL http://localhost/mlflow through the browser without port forwarding
2. When I do port-forward to the nginx-ingress service and visit the url followed by the path /mlflow it  does not lead to the supposedly configure service of mlflow.

What have I tried?

1. I noticed the nginx ingress controller was of type loadBalancer and since I was deploying locally I patched this to be of type nodePort instead. This didn’t resolve issue
2. To the kind cluster configuration I have added extra hostMappingPorts corresponding to  the nginx ingress controller ports. This didn’t resolve anything.

End goal: I want to be able to deploy this cluster and the application in one go, and be able to visit http://localhost/mlflow to find the service

Here is my minimal reproducible example with all the necessary manifests and script

The bash `deploy.sh` script to deploy the cluster and application stack

```
#!/usr/bin/env bash

set -e

# Get the directory of the script
SCRIPT_DIR=$( cd -- ""$( dirname -- ""${BASH_SOURCE[0]}"" )"" &> /dev/null && pwd )

# Default values
DEFAULT_CLUSTER_NAME=""kmlflow-local-v1""
DEFAULT_HOST_VOLUME_PATH=""$(dirname ""$SCRIPT_DIR"")/volume""

# Color formatting
GREEN='\033[0;32m'
RESET='\033[0m'

# Prompt user to accept default values or set new ones
read -p ""Enter cluster name (default: $DEFAULT_CLUSTER_NAME): "" CLUSTER_NAME
CLUSTER_NAME=""${CLUSTER_NAME:-$DEFAULT_CLUSTER_NAME}""

read -p ""Enter volume path (default: $DEFAULT_HOST_VOLUME_PATH): "" HOST_VOLUME_PATH
HOST_VOLUME_PATH=""${HOST_VOLUME_PATH:-$DEFAULT_HOST_VOLUME_PATH}""

echo ""Using cluster name: $CLUSTER_NAME""
echo ""Using volume path: $HOST_VOLUME_PATH""

# Set environment variables for Kubernetes
export CLUSTER_NAME
export HOST_VOLUME_PATH

# Deploy the Kind cluster
echo ""Deploying local multi-node Kubernetes cluster using kind ...""
kind create cluster --config ""$SCRIPT_DIR/cluster.yaml"" || echo ""cluster already exists. Continuing on with application deployment ... ""
# Set the kubectl context
echo ""Setting context for kubectl cli tool ...""
kubectl cluster-info --context ""kind-$CLUSTER_NAME""
echo """"

# Install Ingress Controller (NGINX)
echo ""Installing NGINX Ingress controller ...""
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/cloud/deploy.yaml
kubectl wait --for=condition=available --timeout=60s -n ingress-nginx deploy/ingress-nginx-controller
echo ""NGINX Ingress controller installed successfully.""
echo """"

echo ""Install Mlflow ...""

kubectl apply -f ""$SCRIPT_DIR/mlflow.yaml""

# Apply the Ingress objects to expose services
echo ""Creating Ingress objects for services ...""
kubectl apply -f ""$SCRIPT_DIR/ingress.yaml""
echo ""Ingress objects created successfully.""
echo """"

# patching the ingress-controller to have type node port
kubectl patch svc ingress-nginx-controller -n ingress-nginx -p '{""spec"":{""type"":""NodePort""}}'

# Print the Ingress URLs for the services with color formatting

echo ""To access MLFlow's user interface head to:""
echo -e ""${GREEN}https://localhost/mlflow${RESET}""

# Complete the deployment
echo ""Deployment complete!""

exit 0
```

The `cluster.yaml` file

```
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: kmlflow-local-v1
nodes:
  - role: control-plane
    kubeadmConfigPatches:
    - |
      kind: InitConfiguration
      nodeRegistration:
        kubeletExtraArgs:
          node-labels: ""ingress-ready=true""
    extraPortMappings:
    - containerPort: 80
      hostPort: 80
      protocol: TCP
    - containerPort: 443
      hostPort: 443
      protocol: TCP

    extraMounts:
      # Mount GPU drivers and necessary files
      - hostPath: /usr/lib/x86_64-linux-gnu/nvidia  # Adjust the path based on your environment
        containerPath: /usr/local/nvidia
      - hostPath: /dev/nvidia*  # Mount all the necessary GPU device files
        containerPath: /dev/nvidia*
      - hostPath: /home/namedProfile/Code/kmlflow/volume  # Your volume path
        containerPath: /data
```

And `mlflow.yaml` file

```
apiVersion: v1
kind: ServiceAccount
metadata:
  name: mlflow-sa
  namespace: mlflow
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mlflow-deployment
  namespace: mlflow
  labels:
    app: mlflow
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mlflow
  template:
    metadata:
      labels:
        app: mlflow
    spec:
      serviceAccountName: mlflow-sa

      volumes:
      - name: mlflow-data
        hostPath:
          path: /home/namedProfile/Code/kmlflow/volume

      containers:
      - name: mlflow
        image: akinolawilson/mlflow:latest
        imagePullPolicy: Always
        ports:
          - containerPort: 5001

        volumeMounts:
        - name: mlflow-data
          mountPath: /usr/src/app
---
apiVersion: v1
kind: Namespace
metadata:
  name: mlflow
---
apiVersion: v1
kind: Service
metadata:
  name: mlflow-service
  namespace: mlflow
spec:
  type: ClusterIP
  selector:
    app: mlflow
  ports:
    - protocol: TCP
      port: 5000
      targetPort: 5001
```

And `ingress.yaml` file

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: mlflow-ingress
  namespace: mlflow
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
    - host: localhost
      http:
        paths:
          - path: /mlflow
            pathType: Prefix
            backend:
              service:
                name: mlflow-service
                port:
                  number: 5000
```

But this doesn’t work. What am I doing wrong? Is it even feasible to do what I am trying to? I just want this for a local deployment.

Thank you for your time and help. I really appreciate it.","kubernetes, nginx, url-routing, nginx-ingress, kind",79424775.0,"Yes it is feasible and the [KIND Ingress documentation](https://kind.sigs.k8s.io/docs/user/ingress) shows exactly how to do it. Maybe you need to break it down into stages:

- Start with the foo and bar examples that the above document provides and get routing to those working first.
- In your example, see if you can simplify initially - e.g. avoid the use of both port and targetPort. Just use the same port for both the Service and the Deployment.

If you run `kubectl get svc -n ingress-nginx` you will see output like this if you are using port mapping. The `<pending>` external IP address is normal for this type of local deployment.

```
ingress-nginx   ingress-nginx-controller             LoadBalancer   10.96.39.16     <pending>     80:30777/TCP,443:31099/TCP   13m
```

From your question it is hard to know what your environment does not work. You do not indicate whether the pods are healthy, what host OS you are running etc. If you fail to get the foo and bar services working then maybe post back with more about your environment.",2025-02-09T10:49:42,2025-02-08T18:58:38
79423739,ArgoCD not recognizing ApplicationSets,"I'm trying to wrap my head around Argo Application Sets, but I can't get my setup to work.

Here's my directory structure

```
.
├── kubernetes-deployments
│   └── core
│       ├── argo-cd
│       │   ├── Chart.yaml
│       │   └── values.yaml
│       └── cilium
│           ├── Chart.yaml
│           └── values.yaml
└── README.md
```

Here's my values file:

```
argo-cd:
  enabled: true
  dex:
    enabled: false
  notifications:
    enabled: false
  applicationSet:
    enabled: true
  server:
    extraArgs:
      - --insecure
  namespaceOverride: ""argo-cd""
  server:
    service:
      type: NodePort
      nodePort: 32080
applicationsets:
  core:
    goTemplate: true
    generators:
      - git:
          repoURL: https://mygitrepo.git
          revision: HEAD
          directories:
            - path: kubernetes-deployments/core/*
    template:
      metadata:
        name: '{{path.basename}}'
        labels: {}
      spec:
        project: default
        source:
          repoURL: https://mygitrepo.git
          targetRevision: HEAD
          path: ""{{ .path.path }}""
          helm: &appsets-helm
            valueFiles:
              - values.yaml
        destination: &appsets-destination
          server: https://kubernetes.default.svc
          namespace: ""{{ base .path.path }}""
        revisionHistoryLimit: 5
        syncPolicy:
          syncOptions: &appsets-sync-options
            - ApplyOutOfSyncOnly=true
            - CreateNamespace=true
            - RespectIgnoreDifferences=true
            - PruneLast=true
        ignoreDifferences: []
    syncPolicy:
      preserveResourcesOnDeletion: true
      applicationsSync: sync
```

Here's the chart file:

```
apiVersion: v2
description: A Helm chart for Argo CD, a declarative, GitOps continuous delivery tool for Kubernetes.
name: argo-cd
version: 7.8.2
home: https://github.com/argoproj/argo-helm
icon: https://argo-cd.readthedocs.io/en/stable/assets/logo.png
sources:
  - https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd
  - https://github.com/argoproj/argo-cd
dependencies:
  - name: argo-cd
    version: 7.8.2
    repository: https://argoproj.github.io/argo-helm
    condition: argo-cd.enabled

  - name: argocd-apps
    version: 2.0.0
    repository: https://argoproj.github.io/argo-helm
    condition: argocd-apps.enabled
```

What I'm doing is applying the above values file. Argo CD gets deployed. I go through the initial setup of entering the admin password and connecting my GitHub repository. I don't see any apps in the Argo UI. Based on my directory structure above, I should see Cilium app, and the agro app, right?","kubernetes, argocd",79424969.0,"I figured this out. My values file wasn't structured properly. Here's the corrected values file:

```
argo-cd:
  enabled: true
  dex:
    enabled: false
  notifications:
    enabled: false
  applicationSet:
    enabled: true
  server:
    resources:
      limits:
        cpu: 250m
        memory: 128Mi
      requests:
        cpu: 25m
        memory: 48Mi
    extraArgs:
      - --insecure
  namespaceOverride: ""argocd""
  server:
    service:
      type: NodePort
      nodePort: 32080
argocd-apps:
  enabled: true
  applicationsets:
    core:
      goTemplate: true
      generators:
        - git:
            repoURL: REPO.git
            revision: HEAD
            directories:
              - path: kubernetes-deployments/core/*
      template:
        metadata:
          name: '{{path.basename}}'
          labels: {}
        spec:
          project: default
          source:
            repoURL: REPO.git
            targetRevision: HEAD
            path: ""{{ .path.path }}""
            helm: &appsets-helm
              valueFiles:
                - values.yaml
          destination: &appsets-destination
            server: https://kubernetes.default.svc
            namespace: ""{{ base .path.path }}""
          revisionHistoryLimit: 5
          syncPolicy:
            syncOptions: &appsets-sync-options
              - ApplyOutOfSyncOnly=true
              - CreateNamespace=true
              - RespectIgnoreDifferences=true
              - PruneLast=true
          ignoreDifferences: []
      syncPolicy:
        preserveResourcesOnDeletion: true
        applicationsSync: sync
```

What led me to this was the fact that when I was rendering locally, the applicationSets weren't being rendered, which is why they were appearing in the UI. After making the above change and rendering locally, my ApplicationSets are are now being rendered correctly and the applications are now appearing in the argo UI",2025-02-09T13:32:12,2025-02-08T18:45:55
79421875,Failing to get authorization of kubernetes cluster using python sdk when the same config works with kubectl,"I have kubernetes configuration file set up to access multiple clusters which looks like the following with `clusters`, `users` and `contexts` sections for each cluster.

`myk8sconfig.yaml`

```
apiVersion: """"
kind: """"
clusters:
    - name: mycluster
      cluster:
        server: https://<mycluster IP>:443
        certificate-authority-data: <my cluster certificate>

--------------------------------------</snip>------------------------------------------

users:
    - name: mycluster-user
      user:
        exec:
            apiVersion: client.authentication.k8s.io/v1beta1
            command: ./mycustom_token_generator_cmd
            args:
                - token_gen_args
            env: []

--------------------------------------</snip>------------------------------------------

contexts:
    - name: mycluster-context
      context:
        cluster: mycluster
        user: mycluster-user
```

I can use the above config to run any `kubectl` command as the following without any errors

```
kubectl get pod --all-namespaces -o json  \
    --kubeconfig ~/.kube/myk8sconfig.yaml  \
    --context mycluster-context
```

But the same configuration file fails to get authorization when used in python sdk like the following

```
from kubernetes import client, config

config.load_kube_config(
    config_file=HOME_DIR + ""/.kube/myk8sconfig.yaml"",
    context=""mycluster-context""
)

config.debug = True

v1=client.CoreV1Api()
ret = v1.list_pod_for_all_namespaces(watch=False)
```

It gives me the following error

```
Reason: Unauthorized
HTTP response headers: HTTPHeaderDict({'Audit-Id': '172c4e92-7e7a-45a1-blah-blah', 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 07 Feb 2025 18:19:28 GMT', 'Content-Length': '129'})
HTTP response body: {""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Unauthorized"",""reason"":""Unauthorized"",""code"":401}
```

As you can see I tried `config.debug = True` but that didn't yield any additional information.

I am not sure how to debug this further and need some help.

Thank you in advance for your help.","python, authentication, kubernetes",,,,2025-02-07T18:43:16
79421578,Using /actuator/health when Spring application is deployed on k8s?,"I wonder what the drawbacks of the Spring Boot application are that it has a `/health` actuator endpoint defined as liveness and readiness probes.

Are there any issues related to graceful shutdown or rolling updates?","spring-boot, kubernetes, spring-boot-actuator, readinessprobe, livenessprobe",79421672.0,"Interesting question. I found this [answer on Reddit](https://www.reddit.com/r/kubernetes/comments/wayj42/comment/ii479g7/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button).

> Here is the big one that most people miss, though. Handling of SIGTERM
> events is a must in Kubernetes. Evictions, pod autoscalers, and just
> regular operation (like kubectl apply ...) can lead to pods getting
> killed prematurely. This happens by sending a SIGTERM event to the
> application. **The readiness probe MUST respond differently than a
> liveness probe after the SIGTERM event has been received but before
> the app is able to exit**. The readiness probe must return failure,
> while the liveness probe must return success. In this way, no new
> requests are sent to the terminated replica, but existing requests
> that are currently being processed are able to complete. After the
> liveness probe returns failure, Kubernetes may send a SIGKILL event,
> terminating your app immediately. In order for this to be handled
> properly, your liveness probe MUST remain healthy and your readiness
> probe MUST return failure. Of course, after the request in flight have
> been completed, your liveness probe should return failure, but only
> after all requests in flight have been completed.",2025-02-07T17:13:04,2025-02-07T16:32:58
79420894,Jest Encountered Unexpected Token &#39;export&#39; in Node Module &quot;@kubernetes/client-node&quot; while Running Tests,"I am running a Jest(Version:^29.7.0) test suite in a Node.js v22 environment using TypeScript. Used ""@kubernetes/client-node"" library version: ""^1.0.0"".
When I try to execute my tests, I encounter the following error:

```
FAIL  src/modules/_common/k8s/services/k8s-storage-class.service.test.ts
  ● Test suite failed to run

  Jest encountered an unexpected token

  Jest failed to parse a file. This happens e.g. when your code or its dependencies use non-standard JavaScript syntax, or when Jest is not configured to support such syntax.

  By default ""node_modules"" folder is ignored by transformers.

  Here's what you can do:
   • If you are trying to use ECMAScript Modules, see https://jestjs.io/docs/ecmascript-modules for how to enable it.
   • If you are trying to use TypeScript, see https://jestjs.io/docs/getting-started#using-typescript
   • To have some of your ""node_modules"" files transformed, you can specify a custom ""transformIgnorePatterns"" in your config.
   • If you need a custom transformation specify a ""transform"" option in your config.
   • If you simply want to mock your non-JS modules (e.g. binary assets) you can stub them out with the ""moduleNameMapper"" config option.

  You'll find more details and examples of these config options in the docs:
  https://jestjs.io/docs/configuration
  For information about custom transformations, see:
  https://jestjs.io/docs/code-transformation

  Details:

  D:\project\node_modules\@kubernetes\client-node\dist\index.js:1
  ({""Object.<anonymous>"":function(module,exports,require,__dirname,__filename,jest){export * from './config.js';
                                                                                      ^^^^^^

  SyntaxError: Unexpected token 'export'
    at Runtime.createScriptFromCode (node_modules/jest-runtime/build/index.js:1505:14)
    at Object.<anonymous> (src/modules/_common/k8s/k8s.config.ts:2:1)
    at Object.<anonymous> (src/modules/_common/k8s/services/k8s-storage-class.service.test.ts:3:1)
```

tsconfig.json

```
{
  ""compilerOptions"": {
    ""module"": ""commonjs"",
    ""target"": ""es2020"",
    ""allowJs"": true,
    ""outDir"": ""dist"",
    ""strict"": true,
    ""moduleResolution"": ""node"",
    ""esModuleInterop"": true,
    ""skipLibCheck"": true
  }
}
```

jest config

```
{
    ""verbose"": true,
    ""transform"": {
      ""^.+\\.tsx?$"": ""ts-jest"",
      ""^.+\\.js$"": ""babel-jest"",
      ""\\.(yaml|yml)$"": ""jest-yaml-transform"",
      ""node_modules/@kubernetes/client-node/dist/index.js"": [
        ""ts-jest"",
        {
          ""isolatedModules"": true
        }
      ]
    },
    ""transformIgnorePatterns"": [
      ""/node_modules/(?!@kubernetes/client-node)/"",
      ""/node_modules/(?!.*.js$)"",
      ""/dist/""
    ],
    ""testEnvironment"": ""node"",
    ""testRegex"": ""(/__tests__/.*|(\\.|/)(test|spec))\\.(tsx?)$"",
    ""moduleNameMapper"": {
      ""^@root(.*)$"": ""<rootDir>$1"",
      ""^@configs(.*)$"": ""<rootDir>/configs$1"",
      ""^@constants(.*)$"": ""<rootDir>/src/constants$1"",
      ""^@common(.*)$"": ""<rootDir>/src/common$1"",
      ""^@core(.*)$"": ""<rootDir>/src/core$1"",
      ""^@middleware(.*)$"": ""<rootDir>/src/middleware$1"",
      ""^@modules(.*)$"": ""<rootDir>/src/modules$1"",
      ""^@utils(.*)$"": ""<rootDir>/src/utils$1""
    },
    ""moduleFileExtensions"": [
      ""ts"",
      ""tsx"",
      ""js"",
      ""jsx"",
      ""json"",
      ""node""
    ],
    ""setupFiles"": [
      ""./test.config.ts""
    ]
}
```

Jest is trying to execute a file that uses ES module (import/export) syntax, but Jest is not properly configured to handle ES modules, leading to the `Unexpected token 'export'` error.","javascript, node.js, typescript, kubernetes, jestjs",79602567.0,"I found a workaround for the error by updating `transformIgnorePatterns` to explicitly allow ESM packages like `@kubernetes/client-node` and all of its ESM dependants like `openid-client` and sub-dependents like `jose` to be preprocessed by `ts-jest`.

`tsconfig.json` in `commonjs` mode and `esModuleInterop: true` should allow for ESM compatibility, and created a `tsconfig.test.json` with `allowJs: true` so that ts-jest could handle the `.js` files in `node_modules`. With this setup, I can import ESM libraries normally (e.g., `import * as k8s from '@kubernetes/client-node'`) and run my tests without issues.

Firstly, install the newest version of `jest` (for me it only works with `29.3.2` but not with `29.3.1`).

So this is the `jest.config.js`:

```
module.exports = {
  testEnvironment: 'node',
  preset: 'ts-jest',
  moduleFileExtensions: ['js', 'json', 'ts', 'tsx', 'jsx', 'node'],
  transform: {
    // (Added support for .tsx files but not required)
    '^.+\\.(t|j)sx?$': [
      'ts-jest',
      {
        tsconfig: 'tsconfig.test.json',
      },
    ],
    // (No need for module specific ts-jest configurations)...
  },
  transformIgnorePatterns: [
    // Added jose library (probably this ESM dependency was added in the newer versions)
    // This will work for @kuberenets/client-node version 1.1.2
    '/node_modules/(?!(@kubernetes/client-node|openid-client|oauth4webapi|jose)/)',
  ],
};
```

This is the `tsconfig.json`:

```
{
  ""compilerOptions"": {
    ""module"": ""commonjs"" /* Specify what module code is generated. */,

    // THIS IS IMPORTANT!
    ""esModuleInterop"": true /* Emit additional JavaScript to ease support for importing CommonJS modules. This enables 'allowSyntheticDefaultImports' for type compatibility. */
  },
  ""include"": [
    ""src/**/*"" // Include all files in the 'src' folder
  ]
}
```

And this is `tsconfig.test.json`:

```
{
  ""compilerOptions"": {
    ""allowJs"": true
  },
  ""extends"": ""./tsconfig.json""
}
```

Finally, you can import the library like this:

```
import * as k8s from '@kubernetes/client-node';
```

I also set up a [stackblitz](https://stackblitz.com/edit/stackblitz-starters-un7zge3j?file=jest.config.js) with a minimal sample which works for `@kuberenets/client-node` version `1.1.2`. Run `npm run test` to see it wor.",2025-05-01T21:34:52,2025-02-07T12:35:36
79420094,"Check if resource, version apiGroup is known in Scheme","Input: resource, version apiGroup is known in Scheme

For example:

```
    resource := ""pods""
    version := ""v1""
    apiGroup := """"
```

How can I check if that is known in the scheme?

Background: I want to validate the resource string in CI.

I do not have a connection to an api-server.

I found that solution, but it feels very ugly because it does lower-case the string and adds an `s`.

How to check if the resource is registered without ""magic"" string manipulation?

```
package main

import (
    ""fmt""
    ""log""
    ""strings""

    ""k8s.io/apimachinery/pkg/runtime/schema""
    ""k8s.io/client-go/kubernetes/scheme""
)

func main() {
    // Define the resource, version, and API group
    resource := ""pods""
    version := ""v1""
    apiGroup := """"

    // Check if the GroupVersion is registered in the scheme
    gvkList := scheme.Scheme.AllKnownTypes()
    found := false

    for gvk := range gvkList {
        if gvk.Group == apiGroup && gvk.Version == version {
            plural := strings.ToLower(gvk.Kind) + ""s"" // Simple pluralization, adjust as needed
            if plural == resource {
                fmt.Printf(""Resource %s with version %s in group %s is registered in the scheme\n"", resource, version, apiGroup)
                found = true
                break
            }
        }
    }

    if !found {
        log.Fatalf(""Resource %s with version %s in group %s is not registered in the scheme"", resource, version, apiGroup)
    }
}
```","go, kubernetes",79547722.0,"Without using string manipulation, I think you can use a hard-coded map to map `resource` value with expected corresponding kubernetes schema `Kind` like says `pods` -> `Pod`

```
package main

import (
    ""fmt""
    ""log""
    ""strings""

    ""k8s.io/apimachinery/pkg/runtime/schema""
    ""k8s.io/client-go/kubernetes/scheme""
)

func main() {
    // Define the resource, version, and API group
    resource := ""pods""
    version := ""v1""
    apiGroup := """"

    resourceMapper := map[string]string{
      ""pods"":  ""Pod"",
    }

    // Check if the GroupVersion is registered in the scheme
    gvkList := scheme.Scheme.AllKnownTypes()
    found := false

    for gvk := range gvkList {
        if gvk.Group == apiGroup && gvk.Version == version && gvk.Kind == resourceMapper[resource] {
            fmt.Printf(""Resource %s with version %s in group %s is registered in the scheme\n"", resource, version, apiGroup)
            found = true
            break
    }

    if !found {
        log.Fatalf(""Resource %s with version %s in group %s is not registered in the scheme"", resource, version, apiGroup)
    }
}
```",2025-04-01T04:38:54,2025-02-07T07:45:12
79419771,Kubernetes external-secret.io operator can not unmarshal GCP secret value stored as plain text,"Created GCP secret and stored plain text value e.g. `userpassword`

Created and applied external-secrets.io yaml manifest as shown below

```
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: test-ext-secret-gcp
  namespace: myplatform
spec:
  secretStoreRef:
    kind: ClusterSecretStore
    name: secret-store
  target:
    name: db-readonly-userpass
  dataFrom:
      # GCP Secrets Manager secret name
    - extract:
        key: gcp-db-readonly-userpass
```

When doing `kubectl describe`, it gives error `unable to unmarshal secret: invalid character 'u' looking for beginning of value`

How to read the plain text GCP secret value and load as kubernetes secret using external-secrets.io operator?","kubernetes, google-cloud-platform, google-kubernetes-engine, external-secrets-operator, external-secrets",,,,2025-02-07T01:22:12
79418984,Using Istio ServiceEntry to map a local Postgresql,"I am trying to access a Postgresql database from within a Kubernets cluster with the following configuration (inspired by the documentation from [https://istio.io/latest/docs/reference/config/networking/service-entry/](https://istio.io/latest/docs/reference/config/networking/service-entry/)).

The database that I am trying to use runs on my localhost at address 192.168.1.177 port 54322, outside the cluster (Minikube).

```
---
# Source: external-postgresql/templates/service-entry.yaml
apiVersion: networking.istio.io/v1
kind: ServiceEntry
metadata:
  name: sample-external-postgresql
  namespace: sample
  labels:
    helm.sh/chart: external-postgresql-0.1.0
    app.kubernetes.io/name: external-postgresql
    app.kubernetes.io/instance: sample
    app.kubernetes.io/version: ""0.0.1""
    app.kubernetes.io/managed-by: Helm
spec:
  hosts:
    - postgres
  addresses:
    - 192.168.1.177/16
  ports:
    - number: 54322
      name: postgresql
      protocol: TCP
  location: MESH_EXTERNAL
  resolution: STATIC
---
# Source: external-postgresql/templates/virtual-service.yaml
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: sample-external-postgresql
  namespace: sample
  labels:
    helm.sh/chart: external-postgresql-0.1.0
    app.kubernetes.io/name: external-postgresql
    app.kubernetes.io/instance: sample
    app.kubernetes.io/version: ""0.0.1""
    app.kubernetes.io/managed-by: Helm
spec:
  hosts:
    - postgres
  http:
    - name: ""external-postgresql""
      match:
        - uri:
            prefix: ""/""
      route:
        - destination:
            host: postgres
```

The consuming Java (minimalistic SpringBoot) based Restful application gets configured to open the database connection with 'jdbc:postgresql://postgres:54322/postgres'. I am deploying that container with the following object

```
# Source: training-restful/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-training-restful
  namespace: sample
  labels:
    helm.sh/chart: training-restful-0.0.2
    app.kubernetes.io/name: training-restful
    app.kubernetes.io/instance: sample
    app.kubernetes.io/namespace: sample
    app.kubernetes.io/version: ""0.3""
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: training-restful
      app.kubernetes.io/instance: sample
      app.kubernetes.io/namespace: sample
  template:
    metadata:
      labels:
        helm.sh/chart: training-restful-0.0.2
        app.kubernetes.io/name: training-restful
        app.kubernetes.io/instance: sample
        app.kubernetes.io/namespace: sample
        app.kubernetes.io/version: ""0.3""
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: sample-training-restful
      securityContext:
        {}
      restartPolicy: Always
      containers:
        - name: training-restful
          securityContext:
            {}
          image: ""training-app:0.3""
          imagePullPolicy: IfNotPresent
          env:
            - name: DATABASE_USER
              value: postgres
            - name: DATABASE_PASSWORD
              value: ChangeMe$191
            - name: DATABASE_URL
              value: jdbc:postgresql://postgres:54322/postgres
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 1000
            failureThreshold: 1
            periodSeconds: 60
            terminationGracePeriodSeconds: 60
            httpGet:
              host: 127.0.0.1
              path: /v1/api/index
              port: 8080
```

When the pod starts-up, it writes the following traces in the log file

```
2025-02-06 17:53:36,670 INFO o.s.b.StartupInfoLogger [main] Starting SampleApplication v0.0.2-SNAPSHOT using Java 21.0.6 with PID 1 (/opt/app/Sample.jar started by root in /opt/app)
2025-02-06 17:53:36,674 DEBUG o.s.b.StartupInfoLogger [main] Running with Spring Boot v3.4.1, Spring v6.2.1
2025-02-06 17:53:36,674 INFO o.s.b.SpringApplication [main] The following 1 profile is active: ""dev""
2025-02-06 17:53:40,975 ERROR o.h.e.j.s.SqlExceptionHelper [main] The connection attempt failed.
2025-02-06 17:53:42,506 DEBUG c.m.w.s.c.SecurityConfiguration [main] Configure the security platform
2025-02-06 17:53:42,866 INFO o.s.b.StartupInfoLogger [main] Started SampleApplication in 6.84 seconds (process running for 7.61)
```

After deploying the SE and VS, running 'istioctl analyze -n sample' yields no validation issues in the log.

```
>istioctl analyze -n sample

✔ No validation issues found when analyzing namespace: sample.
```

However, the Restful application cannot connect to the external database.

I have an extra hint from Kiali, which analyzing the Virtual Service, it tells me that the host (spec.http.route.destination.host) cannot be resolved. ![Screenshot from Kiali](https://i.sstatic.net/3rHcqzlD.png)

To test that the actual Postgres database is connectable from the Kubernetes cluster, if I hardcode its IP directly into the environment variable DATABASE_URL (as shown in the code snippet below), the connection is established right, and my sample Restful app can do all needed operation (SELECT, INSERT, UPDATE ...) without restriction

```
# Source: training-restful/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-training-restful
  namespace: sample
  labels:
    helm.sh/chart: training-restful-0.0.2
    app.kubernetes.io/name: training-restful
    app.kubernetes.io/instance: sample
    app.kubernetes.io/namespace: sample
    app.kubernetes.io/version: ""0.3""
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: training-restful
      app.kubernetes.io/instance: sample
      app.kubernetes.io/namespace: sample
  template:
    metadata:
      labels:
        helm.sh/chart: training-restful-0.0.2
        app.kubernetes.io/name: training-restful
        app.kubernetes.io/instance: sample
        app.kubernetes.io/namespace: sample
        app.kubernetes.io/version: ""0.3""
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: sample-training-restful
      securityContext:
        {}
      restartPolicy: Always
      containers:
        - name: training-restful
          securityContext:
            {}
          image: ""training-app:0.3""
          imagePullPolicy: IfNotPresent
          env:
            - name: DATABASE_USER
              value: postgres
            - name: DATABASE_PASSWORD
              value: ChangeMe$191
            - name: DATABASE_URL
              value: jdbc:postgresql://192.168.1.177:54322/postgres
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          livenessProbe:
            initialDelaySeconds: 1000
            failureThreshold: 1
            periodSeconds: 60
            terminationGracePeriodSeconds: 60
            httpGet:
              host: 127.0.0.1
              path: /v1/api/index
              port: 8080
```

How can I configure the external database to access so that I can late make use of the goodies from Istio?

Thank you for any useful hit","kubernetes, istio",,,,2025-02-06T18:24:03
79418570,Pods not able to communicate via service url (created with Kustomize),"I have a single node with microk8s running. And the [DNS plugin](https://microk8s.io/docs/addon-dns) is defenitly enabled. But still pods cannot communitcate via the services, direct access via pod IP is working.

I read in [kubernetes cannot ping another service](https://stackoverflow.com/questions/50852542/kubernetes-cannot-ping-another-service) that pinging a service doesn't work. Since the connection problem is with a Postgres container I'm testing with psql from inside another pod
:

```
psql -h service-name -U postgres -d db_name   # doesn't work with service name
psql -h 10.152.183.98 -U postgres -d db_name  # doesn't work with service ClusterIP
psql -h 10.1.100.73 -U postgres -d db_name    # but works with pod IP
```

If I do `nslookup service-name` the service IP is detected. But the problem seems to be to forward from the service to the pod. Not even from within the postgres pod itself a connection to the service works.","kubernetes, kustomize",79428382.0,"Damn I found the error. I set a commonLabel with Kustomize for all 4 services/deployments/pods of my stack.

```
commonLabels:
  app: myapp
```

That overwrote the app labels from all 4 services and the whole selector mechanism matching services to pods broke because of that. Removing the common Label app fixed it.",2025-02-10T21:47:59,2025-02-06T15:58:46
79418244,How to configure kubernetes&#39; haproxy-ingres for host or path?,"With this setup I was able to get 'global' settings for haproxy ingress controller. Can I modify this settings to change configuration on a `host` or `path` level?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: gn-ingress
  namespace: gn
  annotations:
    kubernetes.io/ingress.class: haproxy
    haproxy.org/backend-config-snippet: |
      timeout server 600s
      timeout client 600s
      timeout http-request 60s
spec:
  rules:
    - host: ""myhost1.org""
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: myhost
                port:
                  number: 80
```","kubernetes, kubernetes-ingress, haproxy",79419410.0,[Annotations](https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/) can only be applied to the whole kubernetes resource because they are part of the resource [metadata](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata). One way to solve this is to create separate Ingress resources for each host or path with specific settings.,2025-02-06T21:35:09,2025-02-06T14:07:14
79417456,nginx ingress rewrite rule containing question mark not working,"I'm trying to create rewrite rules for nginx ingress for directing the following URLs to the same service:

*example.com/ws?list=a,b,c*

*example.com/ws/?list=a,b,c*

Pasting in the regexps bellow from my helm chart in the context with the other rules that need to work.

So basically my question is: **How to I write a nginx compatible regexp that contains a questionmark?**

I thought this would solve it:

```
- path: /api/(.*)  <----- THIS IS FOR ANOTHER SERVICE
  pathType: ImplementationSpecific
  component: ""{{ .Values.backend.component }}""
  port: ""{{ .Values.backend.service.port }}""
- path: /ws/(.*) <------- WORKS
  pathType: ImplementationSpecific
  component: ""{{ .Values.broadcaster.component }}""
  port: ""{{ .Values.broadcaster.service.port }}""
- path: /ws(\?.*) <-------- NOT WORKING
  pathType: ImplementationSpecific
  component: ""{{ .Values.broadcaster.component }}""
  port: ""{{ .Values.broadcaster.service.port }}""
- path: /(.*)
  pathType: ImplementationSpecific
  component: ""{{ .Values.frontend.component }}""
  port: ""{{ .Values.frontend.service.port }}""
```","kubernetes, nginx, nginx-ingress",79418015.0,"The question mark in the URL is part of the query string and not the path but however **you can use the * modifier in regex to make the ? optional.** Since the query string (anything starting with?) is not a part of the normalised URI, you cannot match it in location and rewrite expressions. Refer to official  document on [Module ngx_http_core_module](https://nginx.org/en/docs/http/ngx_http_core_module.html#location) for more details.

You could test for the presence of a query string then use rewrite to remove them:

```
if ($args) {
    rewrite ^/$ /? permanent;
}
```

And also If you have a number of URIs to redirect, you should consider using a **map**.To process many URIs, use a map directive, for example:

```
map $request_uri $redirect {
    default 0;
    /somepath/somearticle.html?p1=v1&p2=v2  /some-other-path-a;
    /somepath/somearticle.html              /some-other-path-b;
}

server {
    ...
    if ($redirect) {
        return 301 $redirect;
    }
    ...
}
```

Note: You can also use **regular expressions in the map**, for example, if the URIs also contain optional unmatched parameters. See this [official Nginx document](http://nginx.org/en/docs/http/ngx_http_map_module.html#map) for more info.",2025-02-06T13:09:10,2025-02-06T09:58:51
79416996,Unable to connect to the server: net/http: TLS handshake timeout on kubectl get pods command,"kubectl get pods
Unable to connect to the server: net/http: TLS handshake timeout

Earlier I had faces some issues while I run the command kubectl get pods it was ""ErrImageNeverPull"" and it was because of Missing image pull secrets for private registries and I resolved it but after that when I run the kubectl get pods i'm getting error Unable to connect to the server: net/http: TLS handshake timeout

I wanted to resolve this issue I don't know why it happens.","kubernetes, minikube",,,,2025-02-06T06:49:33
79416561,Helmfile Secrets Not Created in Namespace During Deployment,"I'm using Helmfile to deploy my own Helm chart along with PostgreSQL, but I'm running into an issue where secrets are not being created in the namespace at all. This causes PostgreSQL and Redis to fail because they expect existing secrets for credentials.

Setup
My deployment consists of the following:

- A primary Helm chart
- A bitnami PostgreSQL database chart
- Secret values managed using Helm Secrets with SOPS

```
helm/
  project/
    helmfile.yaml
    environments/
      base/
        secrets.yaml
        another-secret.yaml
        values.yaml
      prd/
        secrets.yaml
        another-secret.yaml
        values.yaml
      acc/
        secrets.yaml
        another-secret.yaml
        values.yaml
      dev/
        secrets.yaml
        another-secret.yaml
        values.yaml
    charts/
    templates/
    .sops.yaml
    Chart.lock
    Chart.yaml
```

- The base/ directory holds default values.
- Each environment-specific directory contains its own secrets and values.

# Issue

- I specify the existingSecret and key name in values.yaml.
- Secrets are encrypted using helm secrets (SOPS) and should be decrypted at runtime.
- However, when deploying with helmfile sync, the secrets do not get created in the namespace.
- Even if the pods fail due to missing secrets, the secrets are still absent.
- Running helm secrets decrypt works fine, and I can see the decrypted values.

# Example Values, Secrets and helmfile

```
#base values
MyPostgres:
  auth:
    existingSecret: ""database-credentials-mypostgres""
    secretKeys:
      adminPasswordKey: ""admin-password""
      userPasswordKey: ""user-password""
    username: ""myusername""
    database: ""my_db""
```

```
#environment values
MyPostgres:
  primary:
    resources:
      requests:
        ...
      limits:
        ...
```

```
#decrypted secret file
apiVersion: v1
kind: Secret
metadata:
    name: database-credentials-mypostgres
type: Opaque
stringData:
    admin-password: admin-pass
    user-password: user-pass
```

```
#helm file
environments:
  dev: {}
  acc: {}
  prd: {}

releases:
...
    values:
      - environments/base/values.yaml
      - environments/{{ .Environment.Name }}/values.yaml
    secrets:
      - environments/base/secrets.yaml
      - environments/{{ .Environment.Name }}/secrets.yaml
...
```

# Question

Why are my secrets not being created at all? How can I make sure Helmfile properly initializes them before dependent charts like PostgreSQL and Redis attempt to use them?

Any insights or debugging tips would be greatly appreciated!","kubernetes, devops, kubernetes-helm, cicd, helmfile",,,,2025-02-06T01:03:10
79416195,Are both RBAC and IAM permissions needed for GKE authorization?,"I am trying to authorize to a GKE cluster using a service account with the following permissions

```
cluster.deployments.get
container.clusters.get
```

I create a kubeconfig by making API calls to get cluster info like cluster certificate, endpoint and service account token and then use this kubeconfig to list deployments using the command below

`kubectl get deployments --kubeconfig=kubeconfig.yaml`

Is this supposed to work or will I need to setup RBAC too? I am doing this whole thing through go code.","kubernetes, google-cloud-platform, google-kubernetes-engine, kubernetes-rbac, gcp-iam",79421497.0,"For authorization you can use either IAM or Kubernetes RBAC or you can use both. To authorize an action, GKE checks for an RBAC policy first. If there isn't an RBAC policy, GKE checks for IAM permissions. For additional information you can check [Interaction with Identity and Access Management](https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#iam-interaction).

In addition, Kubernetes RBAC is built into Kubernetes, and grants granular permissions to objects within Kubernetes clusters. IAM manages Google Cloud resources, including clusters, and types of objects within clusters. See [About RBAC and IAM](https://cloud.google.com/kubernetes-engine/docs/concepts/access-control#:%7E:text=Kubernetes%20RBAC%20is,a%20good%20choice.)",2025-02-07T16:06:36,2025-02-05T21:01:27
79415957,When configuring kubernetes gateway api specifying the hostname causes istio to error when using TLS,"I have a manually deployed istio gateway (from [this](https://github.com/istio/istio/tree/master/manifests/charts/gateway) helm chart)  and I'm trying to configure a kubernetes api gateway resource using TLS.  However whenever I set the `hostname:` field on the gateway resource istio responds with `2025-02-05T18:51:09.844430Z   debug   envoy conn_handler external/envoy/source/common/listener_manager/active_stream_listener_base.cc:45  closing connection from XXX.XXX.XXX.0:39292: no matching filter chain found thread=21`.  My Gateway and HTTPRoute are configures as follows. NOTE: I'm running the manually deployed gateway on ports 81 and 444 for testing purposes.  The service port redirects to 443 on the istio-ingress pod

```
---
apiVersion: gateway.networking.k8s.io/v1
kind: Gateway
metadata:
  name: gateway
  namespace: istio-ingress
spec:
  gatewayClassName: istio
  listeners:
  - name: default
    port: 81
    protocol: HTTP
    allowedRoutes:
      namespaces:
        from: All
  - name: default-tls
    hostname: ""httpbin.example.com""
    port: 444
    protocol: HTTPS
    tls:
      certificateRefs:
      - kind: Secret
        name: httpbin-example
        namespace: default
    allowedRoutes:
      namespaces:
        from: All
  addresses:
  - value: istio-ingress.istio-ingress.svc.cluster.local
    type: Hostname
---
apiVersion: gateway.networking.k8s.io/v1beta1
kind: ReferenceGrant
metadata:
  name: allow-istio-ingress-to-ref-secrets
  namespace: default
spec:
  from:
  - group: gateway.networking.k8s.io
    kind: Gateway
    namespace: istio-ingress
  to:
  - group: """"
    kind: Secret
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: http
  namespace: default
spec:
  parentRefs:
  - name: gateway
    namespace: istio-ingress
  hostnames:
  - httpbin.example.com
  rules:
  - matches:
    - path:
        type: PathPrefix
        value: /get
    backendRefs:
    - name: httpbin
      port: 8000
```

The response I receive from a curl request is

```
$ curl -kLvv -H ""Host: httpbin.example.com""  https://XXX.XXX.XXX.XXX:444/get?foo=bar
* TCP_NODELAY set
* Connected to XXX.XXX.XXX.XXX (XXX.XXX.XXX.XXX) port 444 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to XXX.XXX.XXX.XXX:444
* Closing connection 0
curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to XXX.XXX.XXX.XXX:444

```

Adding `--resolve httpbin.example.com:444:XXX.XXX.XXX.XXX` does not seem to help either.

When I print out the envoy listener config for the istio-ingress pod for I see the following.

```
  .....
  {
    ""name"": ""0.0.0.0_443"",
    ""address"": {
      ""socketAddress"": {
        ""address"": ""0.0.0.0"",
        ""portValue"": 443
      }
    },
    ""filterChains"": [
      {
        ""filterChainMatch"": {
          ""serverNames"": [
            ""httpbin.example.com""
          ]
        },
        ""filters"": [
          {
            ""name"": ""envoy.filters.network.http_connection_manager"",
            ""typedConfig"": {
              ""@type"": ""type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager"",
              ""statPrefix"": ""outbound_0.0.0.0_443"",
              ""rds"": {
                ""configSource"": {
                  ""ads"": {},
                  ""initialFetchTimeout"": ""0s"",
                  ""resourceApiVersion"": ""V3""
                },
                ""routeConfigName"": ""https.444.default.gateway-istio-autogenerated-k8s-gateway-default-tls.istio-ingress""
              },
              ""httpFilters"": [
                {
                  ""name"": ""istio.metadata_exchange"",
                  ""typedConfig"": {
                    ""@type"": ""type.googleapis.com/udpa.type.v1.TypedStruct"",
                    ""typeUrl"": ""type.googleapis.com/io.istio.http.peer_metadata.Config"",
                    ""value"": {
                      ""upstream_discovery"": [
                        {
                          ""istio_headers"": {}
                        },
                        {
                          ""workload_discovery"": {}
                        }
                      ],
                      ""upstream_propagation"": [
                        {
                          ""istio_headers"": {}
                        }
                      ]
                    }
                  }
                },
....
```

If I remove `hostname:` from the `Gateway` resource.  Istio removes thes `filterChainMatch` from the listener and traffic flows properly.  How can I specify the hostname in the `Gateway` resource and have istio route traffic properly?

The version of istio is `1.21.6`","kubernetes, istio, envoyproxy, kubernetes-gateway-api",79416127.0,"It seems the issue was with my curl command. and the resolve

```
curl -kLvv  --resolve httpbin.example.com:444:XXX.XXX.XXX.XXX https://httpbin.example.com:444/get?foo=bar
```

Use the resolve option and changing from the IP address to the actual hostname works.  Not sure what the difference is.",2025-02-05T20:31:04,2025-02-05T19:20:26
79415693,Question about connector plugin versions in Strimzi&#39;s Kafka Connect,"I was playing around with Kafka Connect's functionality to build images, and I have some questions. I used Strimzi operator version 0.45.0 in all examples.

When I create a Strimzi Kafka Connect instance with no image specified by applying this configuration:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster-connect-build
  namespace: first-kafka-namespace
labels:
  my-connect: my-connect-cluster-connect-build
annotations:
  strimzi.io/use-connector-resources: ""true""
spec:
  version: 3.8.0
  replicas: 1
  bootstrapServers: ""first-kafka-cluster-kafka-bootstrap:9092""
config:
  group.id: connect-cluster
  offset.storage.topic: connect-cluster-offsets
  config.storage.topic: connect-cluster-configs
  status.storage.topic: connect-cluster-status
  config.storage.replication.factor: 1
  offset.storage.replication.factor: 1
  status.storage.replication.factor: 1
```

Connectors that are available are these (by running curl -s http://localhost:8083/connector-plugins | jq . command):

```
[
  {
    ""class"": ""org.apache.kafka.connect.mirror.MirrorCheckpointConnector"",
    ""type"": ""source"",
    ""version"": ""3.8.0""
  },
  {
    ""class"": ""org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"",
    ""type"": ""source"",
    ""version"": ""3.8.0""
  },
  {
    ""class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
    ""type"": ""source"",
    ""version"": ""3.8.0""
  }
]
```

And this makes sense to me, the Kafka version is 3.8.0, and the connector's versions are 3.8.0.
Even though, it is strange to me why FileStream connectors are not listed considering that they are also part of the default Kafka Connect.

Some time later, on a completely different environment I created Kafka Connect instance in order to build image and push it to GHCR.
Configuration of that Kafka Connect instance looked like this:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: kafka-connect-cluster
  namespace: kafka
annotations:
  strimzi.io/use-connector-resources: ""true""
spec:
  version: 3.9.0
  replicas: 1
  bootstrapServers: ""kafka-cluster-kafka-bootstrap:9092""
config:
  group.id: connect-cluster
  offset.storage.topic: connect-cluster-offsets
  config.storage.topic: connect-cluster-configs
  status.storage.topic: connect-cluster-status
  config.storage.replication.factor: 1
  offset.storage.replication.factor: 1
  status.storage.replication.factor: 1
build:
  output:
    type: docker
    image: <GHCR Registry>
    pushSecret: ghcr-push-secret
  plugins:
    - name: file-source-connector
      artifacts:
        - type: jar
          url: https://repo1.maven.org/maven2/org/apache/kafka/connect-file/3.6.0/connect-file-3.6.0.jar

```

And the image is successfully created and pushed to the storage.

Then, I wanted to test the image, so I created a testing Kafka Connect instance with this config:

```
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster-connect-build
  namespace: first-kafka-namespace
labels:
  my-connect: my-connect-cluster-connect-build
annotations:
  strimzi.io/use-connector-resources: ""true""
spec:
  image: <image from GHCR>
  replicas: 1
  bootstrapServers: ""first-kafka-cluster-kafka-bootstrap:9092""
config:
  group.id: connect-cluster
  offset.storage.topic: connect-cluster-offsets
  config.storage.topic: connect-cluster-configs
  status.storage.topic: connect-cluster-status
  config.storage.replication.factor: 1
  offset.storage.replication.factor: 1
  status.storage.replication.factor: 1
  template:
    pod:
      imagePullSecrets:
        - name: ghcr-pull-secret
```

So now when I list connectors I expected to see three mirrormaker connectors with versions 3.9.0, considering that that was the Kafka version of Kafka Connect instance that created the image and I expected to see FileStream connectors of the version 3.6.0 considering that that was the version of the connector's that I specified in the build section.

But what I got was this:

```
[
  {
    ""class"": ""org.apache.kafka.connect.file.FileStreamSinkConnector"",
    ""type"": ""sink"",
    ""version"": ""3.9.0""
  },
  {
    ""class"": ""org.apache.kafka.connect.file.FileStreamSourceConnector"",
    ""type"": ""source"",
    ""version"": ""3.9.0""
  },
  {
    ""class"": ""org.apache.kafka.connect.mirror.MirrorCheckpointConnector"",
    ""type"": ""source"",
    ""version"": ""3.9.0""
  },
  {
    ""class"": ""org.apache.kafka.connect.mirror.MirrorHeartbeatConnector"",
    ""type"": ""source"",
    ""version"": ""3.9.0""
  },
  {
   ""class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
   ""type"": ""source"",
   ""version"": ""3.9.0""
  }
]
```

What I don't understand is, why are FileStream connectors of version 3.9.0?

Maybe considering that the Kafka version of the Kafka Connect instance that created the image is 3.9.0, it somehow overrode Filestream connectors of version 3.6.0 that I wanted to put in the image, and instead put those of version 3.9.0.

Could that be the reason?

Also, I don't understand why GET connector-plugins did not give me FileStream connectors in the output the first time, but now it did.
I would like to know what I am doing wrong here?","kubernetes, apache-kafka, apache-kafka-connect, strimzi",79416217.0,"The `FileStream` source and sync connectors are not part of the Kafka's default classpath. So they will not show up by default. This is different from the Kafka Mirror Maker 2 connectors. That is why they show up while the file connectors don't.

As the version of the FileStream connector you added, this is because of the way how the connector *loads* the version -> the `FileStream` is the Kafka version. But you added it to Kafka 3.9.0. So it will show 3.9.0 as the version. I think there is no reason to use the 3.6.0 connector with Kafka 3.8.0. You should really use the 3.9.0 version. But the connector is listed there because you added it to the container image in `spec.build`.

So what you are seeing is from my perspective as expected.",2025-02-05T21:12:07,2025-02-05T17:44:12
79414325,Seed MongoDB in local minikube cluster using skaffold,"I am using a skaffold to deploy mongodb to my local minikube cluster.

sample files below:

skaffold.yaml

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/mongodb.yaml""
    defaultNamespace: ""mongodb""
```

config/namespace.yaml

```
kind: Namespace
apiVersion: v1
metadata:
  name: mongodb
  labels:
    name: mongodb
```

config/mongodb.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-mongo-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb-mongo
  template:
    metadata:
      labels:
        app: mongodb-mongo
    spec:
      containers:
        - name: mongodb-mongo
          image: mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-mongo-srv
spec:
  selector:
    app: mongodb-mongo
  ports:
    - name: db
      protocol: TCP
      port: 27017
      targetPort: 27017
```

It successfully creates a mongodb instance in my minikube cluster.

I would also like to seed the db with some json data.

Is there are way to do this using skaffold ?

Update:

I have also created a configmap and a job to seed the database so my config is as follows:

skaffold.yaml

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/configmap.yaml""
    - ""config/mongodb.yaml""
    - ""config/mongo-seed-job.yaml""
    defaultNamespace: ""mongodb""
```

congigmap.yaml

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-data
  namespace: mongodb
data:
  init.json: |
    [{""name"":""Joe Smith"",""email"":""jsmith@gmail.com"",""age"":40,""admin"":false},{""name"":""Jen Ford"",""email"":""jford@gmail.com"",""age"":45,""admin"":true}]
```

mongo-seed-job.yaml

```
apiVersion: batch/v1
kind: Job
metadata:
  name: mongo-seed
spec:
  template:
    spec:
      containers:
      - name: seed
        image: mongo:latest
        command: [""sh"", ""-c"", ""mongoimport --uri mongodb://mongodb:27017/mydb --collection accounts --type json --file '/init.json' --jsonArray""]
        volumeMounts:
        - name: seed-data
          mountPath: /data
      volumes:
      - name: seed-data
        configMap:
          name: seed-data
          items:
          - key: init.json
            path: init.json
      restartPolicy: Never
```

now the mongo-seed pod wont start. I am getting ContainerCannotRun","mongodb, kubernetes, minikube, skaffold",79417726.0,"I managed to get it working. Here is the working code if anyone else needs it:

`skaffold.yaml`

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/mongodb-credentials.yaml""
    - ""config/configmap.yaml""
    - ""config/mongodb.yaml""
    - ""config/mongo-seed-job.yaml""
    defaultNamespace: ""mongodb""
```

`config/namespace.yaml`

```
kind: Namespace
apiVersion: v1
metadata:
  name: mongodb
  labels:
    name: mongodb
```

`config/mongodb-credentials.yaml`

Note: username: admin password: password

Please change this to whatever you want

```
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
```

`config/configmap.yaml`

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-data
data:
  init.json: |
    [{""name"":""Joe Smith"",""email"":""jsmith@gmail.com"",""age"":40,""admin"":false},{""name"":""Jen Ford"",""email"":""jford@gmail.com"",""age"":45,""admin"":true}]
```

`config/mongodb.yaml`

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: mongo:latest
          ports:
          - containerPort: 27017
          volumeMounts:
          - name: mongo-data
            mountPath: /data/db
          env:
            - name: MONGO_INITDB_ROOT_USERNAME
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: username
            - name: MONGO_INITDB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: password
      volumes:
      - name: mongo-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
spec:
  ports:
    - port: 27017
  selector:
    app: mongodb
```

`config/mongo-seed-job.yaml`

```
apiVersion: batch/v1
kind: Job
metadata:
  name: mongo-seed
spec:
  template:
    spec:
      initContainers:
      - name: init-copy
        image: busybox
        command: ['sh', '-c', 'cp /config/init.json /data/']
        volumeMounts:
        - name: config-volume
          mountPath: /config
        - name: data-volume
          mountPath: /data
      containers:
      - name: seed
        image: mongo:latest
        command: [""sh"", ""-c"", ""mongoimport --uri mongodb://$(MONGO_USERNAME):$(MONGO_PASSWORD)@mongodb:27017/mydb --collection accounts --type json --file /data/init.json --jsonArray --authenticationDatabase=admin""]
        env:
          - name: MONGO_USERNAME
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: username
          - name: MONGO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: password
        volumeMounts:
        - name: data-volume
          mountPath: /data
      restartPolicy: Never
      volumes:
      - name: config-volume
        configMap:
          name: seed-data
      - name: data-volume
        emptyDir: {}
```

If anyone has any alternate solutions it would be good to know.

Thanks @imran-premnawaz for your help",2025-02-06T11:29:25,2025-02-05T09:51:31
79414122,Zen server in Unreal container has insufficient storage,"I'm running Unreal 5.5 as a Docker container on Kubernetes using the official dev-slim-5.5 image. For security reasons I set the file system to read-only and mounted following volumes to the Unreal directory for writing data:

- /tmp as emptyDir
- /home/ue4/.config/Epic as PVC with 3GB
- /home/ue4/app/Unreal/Projects/Test/Project/Saved as PVC with 200MB
- /home/ue4/app/Unreal/Projects/Test/Project/DerivedDataCache/VT as emptyDir
- /home/ue4/UnrealEngine/Engine/Binaries/ThirdParty/USD/UsdResources/Linux/plugins as PVC with 5GB
- /home/ue4/app/Unreal/Projects/Test/Project/Intermediate/PipInstall as PVC with 2GB

Once I start Unreal and import a file I receive several error messages like:

> 2025-02-04T15:52:19.047Z [log]: stdout: [2025.02.04-15.52.19:034][  0]LogDerivedDataCache: Display: ZenLocal: Error response received from PutCacheValues RPC: from POST http://[::1]:8558//z$/$rpc -> 507
>
>
> 2025-02-04T15:52:19.510Z [log]: stdout: [2025.02.04-15.52.19:510][
> 0]LogDerivedDataCache: Display: ZenLocal: Error response received from
> PutCacheValues RPC: from POST http://[::1]:8558//z$/$rpc -> 507
>
>
> 2025-02-04T15:52:25.730Z [log]: stdout: [2025.02.04-15.52.25:729][
> 0]LogStaticMesh: Display: Building static mesh
> product-0efae793-589e-4e1f-ac35-7a05ddbc4373-body (Required Memory
> Estimate: 5.133849 MB)...
>
>
> 2025-02-04T15:52:26.013Z [log]: stdout: [2025.02.04-15.52.26:012][
> 0]LogDerivedDataCache: Display: ZenLocal: Error response received from
> PutCacheRecords RPC: from POST http://[::1]:8558//z$/$rpc -> 507

Can someone tell which directory / volume runs out of space?
Thanks","docker, kubernetes, unreal-engine5",79762782.0,"This worked for me:

[https://forums.unrealengine.com/t/zen-server-in-unreal-container-has-insufficient-storage/2320320/3](https://forums.unrealengine.com/t/zen-server-in-unreal-container-has-insufficient-storage/2320320/3)

""Hi, I had the same issue on a standard (non docker) machine,

changing the zen storage location solved it for me:

- Open `BaseEngine.ini` (found in `UnrealEngineVersion\Engine\Config\`).
- Locate the `[Zen.AutoLaunch]` section.
- Change the line: DataPath=%ENGINEVERSIONAGNOSTICINSTALLEDUSERDIR%Zen/Data
- to: DataPath=%GAMEDIR%LocalDerivedDataCache

This will ensure that the cache is stored within your project folder instead of the default location.""",2025-09-12T09:46:08,2025-02-05T08:42:16
79412200,How to preserve quotes in YAML when processing annotations data using PyYAML,"I'm working with a YAML file that contains annotations with specific values that need to be quoted. Here's an example of my input:

```
example.com/:
 1. catalog-item-20='server1.test.local:80'
 2. network-policy-version=v14.yaml
openshift.io/:
 3. sa.scc.mcs='s0,c107,c49'
collectord.io/:
 4. logs-index=channel_1
 5. logs-override.11-match='^.*(%SENSITIVE%).*$'
```

I need to process this YAML and output it with proper quoting for values containing special characters. The desired output should look like this:

```
annotations:
    example.com/catalog-item-20: 'server1.test.local:80'
    bnhp.co.il/network-policy-version: v14.yaml
    openshift.io/sa.scc.mcs: 's0,c107,c49'
    collectord.io/logs-index: channel_1
    collectord.io/logs-override.11-match: ^.*(%SENSITIVE%).*$
```

Notice that:

1. Values that come with quotes in the input should remain quoted in the output
2. Values without quotes should remain unquoted
3. Quote preservation should be independent of the value's content (special characters, commas, colons, etc.)

Here's my current code:

```
import yaml

with open('annotations.yaml', 'r') as f:
    raw_annotations = yaml.safe_load(f)

annotations = {}
for annotations_prefix, annotations_body in raw_annotations.items():
    prefix = annotations_prefix if annotations_prefix.endswith('/') else f""{annotations_prefix}/""
    for value in annotations_body:
        if '=' in value:
            annotation_key, annotation_value = value.split('=', 1)
            if annotation_value.startswith(""'"") and annotation_value.endswith(""'""):
                annotation_value = annotation_value[1:-1]
            full_key = f""{prefix}{annotation_key}""
            annotations[full_key] = annotation_value

namespace_content = {
    'apiVersion': 'v1',
    'kind': 'Namespace',
    'metadata': {
        'annotations': annotations
    }
}

with open('namespace.yaml', 'w') as f:
    yaml.dump(namespace_content, f, default_flow_style=False)
```

But this produces output without proper quotes:

```
annotations:
    example.com/catalog-item-20: server1.test.local:80
    openshift.io/sa.scc.mcs: s0,c107,c49
```

I've tried:

1. Using default_style=""'"" but this quotes everything
2. Using ruamel.yaml with preserve_quotes=True but it didn't help
3. Using yamlcore package but it also didn't preserve quotes

How can I make PyYAML preserve the quotes exactly as they appear in the input file?","python, kubernetes, yaml, pyyaml, ruamel.yaml",79412458.0,"Your input is valid YAML, but there is no way the putput you present comes from the input and the program you specify:

- there is no `aPIVersion` key at the root level of your expected output (and other stuff missing)
- nothing in your code removes the numbering `1.` to `5.`
- you don't call splitlines on annotations_body, which is a multi-line string value

That is a bit too much to correct your program, but in general in `ruamel.yaml` setting `.preserve_quotes` only affects loaded strings and
not newly created Python strings. You will have to create the special ruamel.yaml string subclasses that give you single quotes:

```
import sys
import ruamel.yaml
from pathlib import Path

def SQ(s):
    return ruamel.yaml.scalarstring.SingleQuotedScalarString(s)

data = {'annotations': {
    'example.com/catalog-item-20': SQ('server1.test.local:80'),
    'bnhp.co.il/network-policy-version': 'v14.yaml',
    'openshift.io/sa.scc.mcs': SQ('s0,c107,c49'),
    'collectord.io/logs-index': 'channel_1',
    'collectord.io/logs-override.11-match': '^.*(%SENSITIVE%).*$',
    }}

output = Path('namespace.yaml')

yaml = ruamel.yaml.YAML()
yaml.indent(mapping=4)

yaml.dump(data, output)
sys.stdout.write(output.read_text())
```

which gives:

```
annotations:
    example.com/catalog-item-20: 'server1.test.local:80'
    bnhp.co.il/network-policy-version: v14.yaml
    openshift.io/sa.scc.mcs: 's0,c107,c49'
    collectord.io/logs-index: channel_1
    collectord.io/logs-override.11-match: ^.*(%SENSITIVE%).*$
```

But you only have to do that if you process the output with a broken YAML parser (or some non-YAML tool), as these quotes are superfluous.",2025-02-04T16:57:07,2025-02-04T15:26:14
79411984,Kubernetes Ingress Controller. X-Cache-Status is always Bypass,"Configured proxy_cache_path for keys_zone=static-cache via ConfigMap. I will check that this is exactly applied in the configuration. It does:

```
$ kubectl -n ingress-nginx exec -it pod/$POD_IC_NAME -- cat /etc/nginx/nginx.conf | grep proxy_cache_path
        proxy_cache_path /tmp/nginx_cache levels=1:2 keys_zone=static-cache:10m max_size=10g  inactive=60m use_temp_path=off;
        proxy_cache_path /tmp/nginx/nginx-cache-auth levels=1:2 keys_zone=auth_cache:10m max_size=128m inactive=30m use_temp_path=off;
```

Now I'm configuring Ingress. I get a helm template like this:

```
{{ if .Values.createIngress }}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ntp-integration-bff-2-{{ .Release.Name }}
  annotations:
    {{ if .Values.http2Enable }}
    nginx.ingress.kubernetes.io/enable-http2: ""true""
    {{ end }}
    {{ if and .Values.httpCache .Values.services.ntp.integrationBff.httpCache }}
    nginx.ingress.kubernetes.io/server-snippet: |
      location ~* ^/api/v2/ {
        proxy_buffering on;
        proxy_cache static-cache;
        proxy_cache_valid 200 30m;
        proxy_cache_methods GET;
        proxy_cache_key ""$scheme$request_uri"";
        proxy_ignore_headers ""Cache-Control"" ""Expires"" ""Set-Cookie"" ""Vary"";
        proxy_hide_header Cache-Control;
        proxy_cache_bypass off;
        proxy_no_cache off;
        proxy_cache_lock on;
        proxy_cache_use_stale updating error timeout http_500 http_502 http_503 http_504;
        proxy_cache_convert_head off;
        proxy_cache_min_uses 1;
        proxy_cache_background_update on;
        add_header X-Cache-Status $upstream_cache_status;
        # Директивы для проксирования заголовков
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_pass http://ntp-integration-bff-{{ .Release.Name }}.{{ .Release.Namespace }}.svc.cluster.local:8000;
      }
    {{ end }}
spec:
  ingressClassName: nginx
{{ if .Values.tls.enable }}
  tls:
  - hosts:
    - {{ .Values.ingress.api.publicDomain2 }}
    secretName: platform-tls-secret
{{ end }}
  rules:
  - host: {{ .Values.ingress.api.publicDomain2 }}
    http:
      paths:
      - path: /api/v3
        pathType: Prefix
        backend:
          service:
            name: ntp-integration-bff-{{ .Release.Name }}
            port:
              number: 8000
{{ end }}
```

I'm getting this configuration :

```
location ~* ^/api/v2/ {
        proxy_buffering on;
        proxy_cache static-cache;
        proxy_cache_valid 200 30m;
        proxy_cache_methods GET;
        proxy_cache_key ""$scheme$request_uri"";
        proxy_ignore_headers ""Cache-Control"" ""Expires"" ""Set-Cookie"" ""Vary"";
        proxy_hide_header Cache-Control;
        proxy_cache_bypass off;
        proxy_no_cache off;
        proxy_cache_lock on;
        proxy_cache_use_stale updating error timeout http_500 http_502 http_503 http_504;
        proxy_cache_convert_head off;
        proxy_cache_min_uses 1;
        proxy_cache_background_update on;
        add_header X-Cache-Status $upstream_cache_status;
        # Директивы для проксирования заголовков
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-For $remote_addr;
        proxy_pass http://ntp-integration-bff-platform.platform.svc.cluster.local:8000;
}
```

But in the browser I see that all requests are returned with x-cache-status: BYPASS

REQUEST = [http://dev190.local/api/v2/query/ru/system-banner](http://dev190.local/api/v2/query/ru/system-banner)

Request Headers

```
GET /api/v2/query/ru/system-banner HTTP/1.1
Accept: application/json, text/plain, */*
Accept-Encoding: gzip, deflate
Accept-Language: ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7,id;q=0.6,zh-TW;q=0.5,zh;q=0.4,fr;q=0.3,bg;q=0.2
Authorization: Bearer eyJhbGciOiJIUzUxMiJ9.eyJqdGkiOiJjOTlhMzc0Yi02YjliLTQzZTQtYjYxNy1hYmJkZDNhMjkyNjciLCJzdWIiOiJGQU1JTElZQV9JLk8iLCJleHAiOjE3Mzk1NDAxMjQsImlhdCI6MTczODY3NjEyNH0.cn7OS0yDQtQMjUYXnCDFlJtVoSSjuGpbhh_167geQ6YtuPHIotI507X-pFl2w3eIIzGZkfzmbEBjMe8oALVZ9Q
Connection: keep-alive
Cookie: username=...; backend=...; rt_a=eyJhbGciOiJIUzI1NiIsIn....; Authorization=Bearer%20eyJhbGciOiJIUz...; rt_r=qjkWLS8xkfcw...
Host: dev190.local
Referer: http://dev190.local/gp-frontend-gp-main/
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36
```

Response Headers

```
HTTP/1.1 200 OK
Date: Tue, 04 Feb 2025 13:53:51 GMT
Content-Type: application/json; charset=utf-8
Content-Length: 447
Connection: keep-alive
X-Cache-Status: BYPASS
```

I don't understand what else can be configured to make these Nginx requests cached?  According to my logic caching should already work.","kubernetes, nginx, caching, kubernetes-ingress",,,,2025-02-04T14:09:57
79411729,Kustomize patching multiple path with same value,"I am trying to see if there are other ways to run patches with multiple paths with the same value.

This is an example of my Kustomization where I am replacing it with the same value. Is there a way to have a variable that I can use to refer to replace it instead of typing the same value multiple times?

```
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: common-base
    patch: |-
      - op: replace
        path: /metadata/name
        value: ""svc1""
      - op: replace
        path: /metadata/labels/app
        value: ""svc1""
```","kubernetes, kustomize",79414257.0,"You can use **ConfigMaps** and **Secrets** to hold configuration or sensitive data that are used by other Kubernetes objects, such as **Pods**. The source of  ConfigMaps or Secrets are usually external to a cluster, such as a **.properties** file or an **SSH keyfile**. Kustomize has **secretGenerator** and **configMapGenerator**, which generate Secret and ConfigMap from files or literals.

To run patches with multiple paths with the same value,you need to store the value in a [ConfigMap or Secret](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) and reference it in your resources.

**Define a ConfigMap in your kustomization.yaml:**

```
configMapGenerator:
  - name: app-config
    literals:
      - appName=svc1   #(your path / value )
```

**Reference the ConfigMap in your patch:**

```
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: common-base
    patch: |-
      - op: replace
        path: /metadata/name
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
      - op: replace
        path: /metadata/labels/app
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
```

So by following the above process by referencing the **ConfigMap** in your patch you will be able to achieve patching multiple paths with the same value in the Kustomize and you can also use a  [strategic merge patch](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md) and  [JSON patch](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/jsonpatch.md) which might also help you to resolve your issue.

For more information check this [Github Link](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/patchMultipleObjects.md) which might be helpful for you.",2025-02-05T09:33:07,2025-02-04T12:42:24
79411590,Command fails in probe but works in kubectl exec,"```
kubectl exec pod/my-pod -- wget -q -O- 127.0.0.1:3000
```

The above `kubectl exec` works.

But, when I run the same as a `livenessProbe`,

```
          livenessProbe:
            exec:
              command:
                - /usr/bin/wget
                - -q
                - -O-
                - 127.0.0.1:3000
```

it fails with `wget: can't connect to remote host (127.0.0.1): Connection refused`:

```
  Warning  Unhealthy  4s    kubelet            Liveness probe failed: wget: can't connect to remote host (127.0.0.1): Connection refused
```

I was expecting the same result from `kubectl exec` and the probe's `exec:`.

What differences in the execution environment are causing this?","kubernetes, livenessprobe",,,,2025-02-04T11:51:48
79410911,server side dryRun using create_namespaced_custom_object of kubernetes library for python,"I want to create a custom resource using python's kubernetes library which has **create_namespaced_custom_object** function to achieve this. Before applying the resources I want to make a **server side dry run** as well to make sure my CRDs are not faulty.
However, I'm seeing only two possible values for **dry_run** argument in **create_namespaced_custom_object**.
Can someone help please?

Tried getting references from the following resources but got no help:

[https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/CustomObjectsApi.md](https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/CustomObjectsApi.md)

[https://kubernetes.readthedocs.io/en/latest/](https://kubernetes.readthedocs.io/en/latest/)","python, kubernetes, kubernetes-python-client",,,,2025-02-04T07:44:25
79409619,Unable to connect to service in same namespace in kubernetes,"I have 2 apps. One is config-server and other is business-logic-app that consumes data from config-server. Both are running on same namespace in Kubernetes (kubectl on my laptop). However, am getting connection timed out exception when business-logic-app is connecting to config-server which is leading to livenessProbe and readinessProbe failures. What am I missing?

***config-server.yaml***

```
# Config server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-learning-config-server
  namespace: kubernetes-learning
  labels:
    app: kubernetes-learning-config-server
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: kubernetes-learning-config-server
  template:
    metadata:
      name: kubernetes-learning-config-server
      labels:
        app: kubernetes-learning-config-server
    spec:
      containers:
        - name: kubernetes-learning-config-server
          image: ghcr.io/kubernetes/learning.config-server
          imagePullPolicy: Always
          ports:
            - containerPort: 8888
              protocol: TCP
            - containerPort: 48888
              protocol: TCP
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: ""50""
            - name: BPL_DEBUG_ENABLED
              value: ""true""
            - name: BPL_DEBUG_PORT
              value: ""48888""
            - name: GITHUB_CONFIG_DATA_URL
              value: https://github.com/kubernetes/config-data
            - name: GITHUB_CONFIG_DATA_USERNAME
              value: github_user
            - name: GITHUB_CONFIG_DATA_PERSONAL_ACCESS_TOKEN
              value: github_sampletoken
          livenessProbe:
            httpGet:
              path: /alpha-app/local
              port: 8888
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /alpha-app/local
              port: 8888
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3

      restartPolicy: Always

# Expose Config server
---
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-learning-config-server
  labels:
    app: kubernetes-learning-config-server
spec:
  type: ClusterIP
  selector:
    app: kubernetes-learning-config-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8888
```

**alpha-app.yaml**

```
# app applications
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-learning-app
  namespace: kubernetes-learning
  labels:
    app: kubernetes-learning-app
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: kubernetes-learning-app
  template:
    metadata:
      name: kubernetes-learning-app
      labels:
        app: kubernetes-learning-app
    spec:
      containers:
        - name: kubernetes-learning-alpha-app
          image: ghcr.io/kubernetes/learning.alpha-app
          imagePullPolicy: Always
          ports:
            - containerPort: 8441
              protocol: TCP
            - containerPort: 48441
              protocol: TCP
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: ""50""
            - name: BPL_DEBUG_ENABLED
              value: ""true""
            - name: BPL_DEBUG_PORT
              value: ""48441""
            - name: SPRING_PROFILES_ACTIVE
              value: kube
            - name: SPRING_CLOUD_CONFIG_FAIL_FAST
              value: ""true""
            - name: SPRING_CLOUD_CONFIG_RETRY_INITIAL_INTERVAL
              value: ""1000""
            - name: SPRING_CLOUD_CONFIG_RETRY_MAX_INTERVAL
              value: ""10000""
            - name: SPRING_CLOUD_CONFIG_RETRY_MULTIPLIER
              value: ""2""
            - name: SPRING_CLOUD_CONFIG_RETRY_MAX_ATTEMPTS
              value: ""5""
            - name: SPRING_CLOUD_CONFIG_URI
              value: http://kubernetes-learning-config-server:8888
          livenessProbe:
            httpGet:
              path: /info
              port: 8441
            initialDelaySeconds: 60
            timeoutSeconds: 15
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /info
              port: 8441
            initialDelaySeconds: 60
            timeoutSeconds: 15
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3

      restartPolicy: Always

# Expose Config server
---
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-learning-app
  labels:
    app: kubernetes-learning-app
spec:
  type: ClusterIP
  selector:
    app: kubernetes-learning-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8441

```

**exception**

```
Caused by: org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://kubernetes-learning-config-server:8888/alpha-app/kube"": Connect timed out
    at org.springframework.web.client.RestTemplate.createResourceAccessException(RestTemplate.java:926) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:906) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:801) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:683) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.cloud.config.client.ConfigServerConfigDataLoader.getRemoteEnvironment(ConfigServerConfigDataLoader.java:349) ~[spring-cloud-config-client-4.2.0.jar:4.2.0]
    at org.springframework.cloud.config.client.ConfigServerConfigDataLoader.doLoad(ConfigServerConfigDataLoader.java:130) ~[spring-cloud-config-client-4.2.0.jar:4.2.0]
    ... 37 common frames omitted
Caused by: java.net.SocketTimeoutException: Connect timed out
    at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source) ~[na:na]
    at java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.Socket.connect(Unknown Source) ~[na:na]
    at java.base/sun.net.NetworkClient.doConnect(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.openServer(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.openServer(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.<init>(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.New(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.New(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(Unknown Source) ~[na:na]
    at org.springframework.http.client.SimpleClientHttpRequest.executeInternal(SimpleClientHttpRequest.java:79) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.http.client.AbstractStreamingClientHttpRequest.executeInternal(AbstractStreamingClientHttpRequest.java:71) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:81) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:900) ~[spring-web-6.2.1.jar:6.2.1]
    ... 41 common frames omitted
```","spring-boot, kubernetes, kubectl, spring-cloud-config-server",79412097.0,"That is because the service is listening on port 80 that will be routed to port 8888 on the target pods.

```
ports:
  - protocol: TCP
    port: 80
    targetPort: 8888
```

Therefore, you need to point `kubernetes-learning-app` to `http://kubernetes-learning-config-server:80/alpha-app/kube`.

N.B. as mentioned by a comment to the question, `http://kubernetes-learning-config-server` works too because an `http://` URL without port number will default to port 80 which coincidentally what you have set as the service port, not because the service decides the port. Should you use another port like `8080`, the URL without a port number would not work.",2025-02-04T14:50:33,2025-02-03T17:56:40
79408606,Kubernetes deployment on Gitlab not working after credential rotation,"The credentials for my project on GCP were automatically rotated. I use them on Gitlab as pipeline variables to deploy the updates. I updated the variables (url and certificate authority) but now the deploy is giving an error asking to provide credentials. Do I have to change the TOKEN as well (see pipeline below)? I don't even know where to find the `gitlab-admin` user (it's not in IAM or Service Accounts).

I have no experience with kubernetes and little with GCP. This was setup before I came into this project and currently we don't have anyone to manage this. I'm pretty lost here.

Any help would be appreciated!

Gitlab pipeline:

```
deploy_k8s_uat:
  stage: deploy-uat
  only: [uat]  #build and push images only for stating branch commits
  image: google/cloud-sdk:latest
  environment:
    name: staging
  script:
    - kubectl config set-cluster k8s --server=""${KUBE_URL}"" --certificate-authority=""${KUBE_CA}""
    - kubectl config set-credentials gitlab-admin --token=""${KUBE_TOKEN}""
    - kubectl config set-context default --cluster=k8s --user=gitlab-admin --namespace=uat
    - kubectl config use-context default
    - kubectl config view
    - kubectl cluster-info
    - sed -i ""s#__IMAGETAG__#${LOWERCASE_CONTAINER_IMAGE_WEBSERVER}#g"" kubernetes/server.yaml
    - sed -i ""s#__ENVIRONMENT__#prod#g"" kubernetes/server.yaml
    - kubectl apply -f kubernetes/server.yaml
    - kubectl rollout status -n uat -w ""deployment/sensor-staging-server-deployment""
    - sed -i ""s#__IMAGETAG__#${LOWERCASE_CONTAINER_IMAGE_WEBSERVER}#g"" kubernetes/server-teresa.yaml
    - sed -i ""s#__ENVIRONMENT__#prod#g"" kubernetes/server-teresa.yaml
    - kubectl apply -f kubernetes/server-teresa.yaml
    - kubectl rollout status -n uat -w ""deployment/sensor-staging-server-deployment-teresa""
  when: manual
```

Deployment log:

```
$ kubectl cluster-info
E0203 10:59:31.740402      42 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0203 10:59:32.001373      42 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0203 10:59:32.266110      42 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0203 10:59:32.528532      42 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
E0203 10:59:32.790974      42 memcache.go:265] couldn't get current server API group list: the server has asked for the client to provide credentials
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
error: You must be logged in to the server (the server has asked for the client to provide credentials)
Cleaning up project directory and file based variables
00:01
ERROR: Job failed: exit code 1
```","kubernetes, google-cloud-platform, deployment, gitlab, gitlab-ci",79409510.0,"#### Assumption 1

Since only these commands are used to configure **kubectl** in a clean container, I assume that we're talking about **Kubernetes Service Account (KSA)**[1][2] rather than IAM Service Account.

```
$ kubectl config set-cluster k8s --server=""${KUBE_URL}"" --certificate-authority=""${KUBE_CA}""
$ kubectl config set-credentials gitlab-admin --token=""${KUBE_TOKEN}""
$ kubectl config set-context default --cluster=k8s --user=gitlab-admin --namespace=uat
```

#### Assumption 2

Judging by the issue description, token hasn't been updated regularly before the issue, thus I assume it's a **legacy long-lived secret-based KSA token**[3].

#### Updated issue summary based on assumptions

GKE credentials have been rotated and a legacy secret-based KSA token is not working anymore.

#### Potential solution

1. Get the current `KUBE_TOKEN` value *(from the description I see you have an access to values)*.
2. Use the following shell commands to decode token data *(`jq` is only for formatting, if not installed just remove last part)*:
```
$ echo -n ""${KUBE_TOKEN}"" | cut -d'.' -f2 | base64 -d | jq .
```
3. See the value of the **sub** key in decoded JSON.
4. Using an associated service account name, search for existing secrets using your own access:
  - If secret has been create before K8s v1.24 release (should be create automatically along with KSA)
```
$ kubectl get secrets -n uat | grep ""<SERVICE_ACCOUNT_NAME>-token-""
```
  - If after (should be created manually and linked to KSA using annotation)
```
$ kubectl get secret -n uat -o=jsonpath-as-json='{.items[?(@.metadata.annotations.kubernetes\.io\/service-account\.name==""<SERVICE_ACCOUNT_NAME>"")]}'
```
5. Based on what you see, you'll need to decide what to do next:
  - If secret exists, and token is different that one you're using - check if it's new and valid;
  - If secret exists, and token is the same as you're using (invalid) - delete the old secret, create a new one and use it's token [4];
  - If you'll not find any secrets (try to adjust shell commands above first) - also try to create a token as explained in [4].

Notes:

1. I assume you meant GKE cluster credentials[5] by «credentials for my project»? This is not the same.
> [...] credentials for my project on GCP were automatically rotated. I use them on Gitlab as pipeline variables [...]
2. Keep in mind that using legacy KSA tokens (if I was right, ofc) is not the best way. Instead, learn more about using IAM Service Accounts for this[6].

[1] [https://kubernetes.io/docs/concepts/security/service-accounts/](https://kubernetes.io/docs/concepts/security/service-accounts/)

[2] [https://cloud.google.com/kubernetes-engine/docs/how-to/service-accounts#kubernetes-service-accounts](https://cloud.google.com/kubernetes-engine/docs/how-to/service-accounts#kubernetes-service-accounts)

[3] [https://kubernetes.io/docs/concepts/security/service-accounts/#authenticating-credentials](https://kubernetes.io/docs/concepts/security/service-accounts/#authenticating-credentials)

[4] [https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-long-lived-api-token-for-a-serviceaccount](https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#manually-create-a-long-lived-api-token-for-a-serviceaccount)

[5] [https://cloud.google.com/kubernetes-engine/docs/how-to/credential-rotation](https://cloud.google.com/kubernetes-engine/docs/how-to/credential-rotation)

[6] [https://cloud.google.com/docs/authentication#auth-decision-tree](https://cloud.google.com/docs/authentication#auth-decision-tree)",2025-02-03T17:07:15,2025-02-03T11:11:51
79406483,How to move shared configuration and sensitive data to a global config and secrets in Helm?,"I have the following project structure:

```
some-module1
  templates
    deployment.yaml
  values.yaml

some-module2
  templates
    deployment.yaml
  values.yaml

some-module3
  templates
    deployment.yaml
  values.yaml

global-config
  templates
    secrets.yaml
  values.yaml
```

The values.yaml files for some-module1, some-module2, and some-module3 contain the same section:

```
shards:
  - name: ""db1""
    url: ""url1""
    login: ""login1""
    password: ""password1""
  - name: ""db2""
    url: ""url2""
    login: ""login2""
    password: ""password2""
```

The deployment.yaml files for some-module1, some-module2, and some-module3 contain the following identical part:

```
{{- range $name, $val := .Values.shards }}
env:
  - name: DB_URL
    value: {{ $val.url }}
  - name: DB_LOGIN
    value: {{ $val.login }}
  - name: DB_PASSWORD
    value: {{ $val.password }}
{{- end }}
```

I have two tasks:

Move the shards configuration to global-config/values.yaml.
Move all sensitive data from global-config/values.yaml (under shards) to secrets in global-config/secrets.yaml.
As a result, some-module1, some-module2, and some-module3 should read the shards data from the global config (i.e., the loop should look like {{- range $name, $val := .Values.global.shards }}), and all sensitive data should be sourced from the secrets.

Is this possible? If yes, how can I achieve this?","kubernetes, kubernetes-helm, kubernetes-secrets",79406560.0,"There's a couple of parts here, and some are easier than others.

**The duplicated output block.** If you have the same block of output YAML that you're repeating in several places, you can split that into a [named template](https://docs.helm.sh/docs/chart_template_guide/named_templates/).  For example,

```
{{/* env.db generates part of an env: block that sets database-specific
     parameters.  Takes a shard object as a parameter.  Unindented, ends
     with a newline. */}}
{{- define ""env.db"" -}}
- name: DB_URL
  value: {{ .url }}
- name: DB_LOGIN
  value: {{ .login }}
- name: DB_PASSWORD
  value: {{ .password }}
{{ end -}}

{{- range $name, $val := .Values.shards }}
env:
{{ include ""env.db"" $val | indent 2 }}
{{- end -}}
```

Having done that, you can move the template definition into a [library chart](https://docs.helm.sh/docs/topics/library_charts/).  This could be something like moving the `{{ define }}...{{ end }}` block into your existing `global-secrets/templates/_helpers.tpl` file.  The main charts would need to include a dependency on the library chart in their `Chart.yaml` files, but then they could call the template in exactly the same way.

Note that, if you define a function in a library chart, settings like `.Values` come from the place it's *called*, not the place it's defined.  So it won't work to put the credential in the library chart too, it needs to be defined in the per-service chart.

**The duplicated input settings.** You can pass arbitrarily many `helm install -f` options to add values files when you deploy a chart, and these are used in addition to the chart's `values.yaml` file.  One possible setup here is that your CD system can extract the credentials from a secret store and write them out as a YAML (or JSON) file, and then run

```
helm upgrade --install mod1 ./some-module1 --namespace mod1 \
  -f values-credentials.yaml
```

I would avoid putting these values under the [special `global:` top-level key](https://docs.helm.sh/docs/topics/charts/#global-values); that's only useful when you have multiple levels of nested charts or settings that are the same across multiple components being installed in the same Helm invocation.  Also note (see same documentation link) that a dependency's `global:` settings do not propagate upward to the main chart: if the credentials are in `global-config/values.yaml` and the main chart depends on them, the main chart (including the `include` call above) will not be able to see those values.

**Using a Secret instead of Helm values.**  Well, you can, I guess, there's a supported path in Helm to do this.  I'd avoid having external dependencies like this if you can.  In the previous section I suggested using some sort of credential store and having automation generate parts of the values, and it could `kubectl get secret` and base64-decode the values if you don't have anything better available.

Helm does have a [`lookup` function](https://docs.helm.sh/docs/chart_template_guide/functions_and_pipelines/#using-the-lookup-function) that can find arbitrary resources in the cluster.  You can, in principle, get a Secret value like

```
{{- $secret := lookup ""v1"" ""Secret"" .Release.Namespace ""secret-name"" -}}
{{- $password := $secret.data.password | b64dec -}}
```

You could reuse the `$secret` but you'd have to do something like the second line for each value you want to get out of the Secret.  Helm doesn't have a couple of standard functional-programming tools like ""map"" that would make a more generic solution possible.  You'd also have to tolerate cases where the Secret doesn't exist, and figure out how to develop and test the chart without access to the production Secret.  If the Secret changes in the cluster, the Helm chart won't redeploy itself, and you'll need to remember to do this by hand.

(If you want the deployment to be driven by Kubernetes resources, consider writing a custom operator, maybe in Go using [Kubebuilder](https://kubebuilder.io).  It is not harder than writing a really robust Helm chart once you start getting into complexities around library charts, and Go is a much more mainstream language that includes a unit-testing framework.)

**Use a Secret as a source for Helm values.** Helm doesn't have a way to do this.  If the credential was only in a Secret, and you didn't have automation to pull it out, then nothing in Helm itself would let you refer to it as part of `.Values`.  You'd need to invoke `lookup` as above.

To reiterate:

1. I would break the repeated environment block out into a library chart that you control.
2. I would not try to do anything to avoid repeating blocks of `values.yaml` settings.
3. I would have your deployment system get the right credentials from somewhere and inject them with a `helm install -f` option.
4. I would not try to put settings into a Kubernetes Secret.",2025-02-02T12:19:03,2025-02-02T11:16:10
79405712,How to have a (FastAPI) GKE deployment handle multiple requests?,"I have a FastAPI deployment in GKE that has an end-point `/execute` that reads and parses a file, something like below:

```
from fastapi import FastAPI

app = FastAPI()

@app.post(""/execute"")
def execute(
    filepath: str
):
    res = 0
    with open(filepath, ""r"") as fo:
         for line in fo.readlines():
              if re.search(""Hello"", line):
                   res += 1
         return {""message"": f""Number of Hello lines = {res}.""}
```

The GKE deployment has 10 pods with a load balancer and service exposing the deployment.

Now, I would like to send 100 different file paths to this deployment. In my mind, I have the following options, and related questions:

1. Send all 100 requests at the same time and not wait for a response, either using threading, `asyncio` and `aiohttp`, or something hacky like this:

```
for filepath in filepaths:
    try:
        requests.post(""http://127.0.0.1:8000/execute?filepath=filepath"",timeout=0.0000000001)
    except requests.exceptions.ReadTimeout:
        pass
```

Ref: [https://stackoverflow.com/a/45601591/6824949](https://stackoverflow.com/a/45601591/6824949)

In this case, what does the GKE load balancer do when it receives a 100 requests - does it deliver 10 requests to each pod at the same time (in which case I would need to make sure a pod has enough resources to handle all the incoming requests at the same time), **OR** does it have a queuing system delivering a request to a pod only when it is available?

1. Send 10 requests at a time, so that no pod is working on more than 1 request at any given time. That way, I can have predictable resource usage in a pod and not crash it. But how do I accomplish this in Python? And do I need to change anything in my FastAPI application or GKE deployment configuration?

Any help would be greatly appreciated!","python, kubernetes, fastapi, google-kubernetes-engine",,,,2025-02-01T21:30:38
79405580,Make ngnix image accessible in kubernetes pod in localhost,"I have the following deployment file:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-deploy
spec:
  replicas: 10
  selector:
    matchLabels:
      app: hello-world
  revisionHistoryLimit: 5
  progressDeadlineSeconds: 300
  minReadySeconds: 10
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    metadata:
      labels:
        app: hello-world
    spec:
      containers:
      - name: hello-pod
        image: nginx
        ports:
        - containerPort: 8080
        resources:
          limits:
            memory: 128Mi
            cpu: 0.1
```

I check and all the pods are running:

```
$ kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
hello-deploy-7c547fdb4-2tpnc   1/1     Running   0          18m
hello-deploy-7c547fdb4-4kjtt   1/1     Running   0          18m
hello-deploy-7c547fdb4-6cvmt   1/1     Running   0          18m
hello-deploy-7c547fdb4-9gvj6   1/1     Running   0          18m
hello-deploy-7c547fdb4-ctjxx   1/1     Running   0          18m
hello-deploy-7c547fdb4-ggl6n   1/1     Running   0          18m
hello-deploy-7c547fdb4-m528n   1/1     Running   0          18m
hello-deploy-7c547fdb4-mg9gf   1/1     Running   0          18m
hello-deploy-7c547fdb4-t9srq   1/1     Running   0          18m
hello-deploy-7c547fdb4-zcjkt   1/1     Running   0          18m
```

I also have the following service defined:

```
apiVersion: v1
kind: Service
metadata:
  name: lb-svc
  labels:
    app: hello-world
spec:
  type: LoadBalancer
  ports:
  - port: 8080
    protocol: TCP
  selector:
    app: hello-world
```

I check and it is also running:

```
$ kubect get svc lb-svc
NAME     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
lb-svc   LoadBalancer   10.100.237.251   localhost     8080:31527/TCP   10m
```

But when I do in my chrome browser `http://localhost:8080` I get `This page isn't working ERR_EMPTY_RESPONSE`

Kubernetes version

```
$ kubectl version
Client Version: v1.32.1
Kustomize Version: v5.5.0
Server Version: v1.30.5
WARNING: version difference between client (1.32) and server (1.30) exceeds the supported minor version skew of +/-1
```

What am I doing wrong?","docker, kubernetes, nginx",79452083.0,by default nginx image is running on port `80` not `8080`,2025-02-19T16:21:59,2025-02-01T19:38:16
79405404,How to solve HTTP01 challenge when domain is pointing to different IP?,"So, we have a domain which is pointing to IP and serving traffic. I am creating new cluster with ingress-nginx with new IP and trying to add cert to it using cert-manager.

Issue is I can't point the domain to this new IP unless ssl is enabled and without pointing domain to this new IP I am having trouble with acme challenges.

For now I have created an additional sub-domain and added redirects on the original service to these new ones for path `/.well-known/acme-challenge/*`. According to Let's Encrypt docs, they allow up to 10 redirects - [here](https://letsencrypt.org/docs/challenge-types/#http-01-challenge).

Redirect is working fine, it's just that the ingress rules which cert-manager created for the solvers are for original domains and I can't seem to find any config which lets me configure additional rules.

Is there any way solve this?

I am thinking of editing the ingress which cert-manager created but worried it would cause some issue when I switch to production issuer. Or that there might be better way to do this.

Edit: I tried updating the ingress manually, but the server which was created by cert manager to handle the request is checking for the hostname and throwing error since they don't match.","kubernetes, cert-manager, ingress-nginx",79406623.0,"So, cert manager solver checks for the `HOST` header before serving the key. In order for this to work, I ended up creating another service with another sub-domain (redirect.original.com) which had nginx running and updated the host using `proxy_set_header HOST original.com;` and used proxy pass to send the request to new domain pointing to where the solver is running `proxy_pass http://new.original.com;`.

And this how the whole flow looked like -

```
GET original.com/.well-known/acme-challenge/...
 -> original.com
 - redirect -> redirect.original.com
 - update HOST and proxy pass -> new.original.com
```

Since I had couple of sub-domains which I wanted to do this for, I added a unique prefix path which were handled by matching location blocks and used a rewrite.",2025-02-02T12:59:55,2025-02-01T17:40:55
79402942,How do I use the EKSPodOperator with in_cluster set to True with a ServiceAccount without getting an Error,"Looking for help on using the EKSPodOperator.
My set up is as follows:
Airflow Version: 2.6.2 deployed with the official helm chart v1.15.0
Kubernetes Cluster: EKS 1.30
Executor: LocalExecutor
Postgres Database is accessed through AWS secrets backend connection.

My intention is to authenticate to the cluster through the scheduler's service account which has been annotated with the appropriate IAM role and policies.

Issue
When I triggered the DAGs, I got a permission error relating to kubernetes_default and aws_default secrets which I didn't even create in the first place. To get past this, I granted the permission to the Scheduler's IAM role, and also created both secrets with the following content to facilitate the connection:

```
kubernetes_default: kubernetes://?extra__kubernetes__namespace=airflow&extra__kubernetes__in_cluster=True
aws_default: aws://?region_name=eu-west-1
```

Result:
`""ERROR - Invalid connection configuration. Options kube_config_path, kube_config, in_cluster are mutually exclusive. You can only use one option at a time.`
I do not have kube_config_path and kube_config set anywhere.

If I set in_cluster to false, I get the error - `'NoneType' object has no attribute 'metadata'.`
I get the same errors when I delete the secrets just in case they are causing some sort of conflict.

My preference is to use the in_cluster configuration since the tasks will be executed within the cluster and I'd like to use a service account for authentication.

Has anyone successfully used EKSPodOperator with in-cluster auth on EKS? What steps did you follow? Any help or guide will be much appreciated. Thank you.","amazon-web-services, kubernetes, airflow, amazon-eks, kubernetespodoperator",79513811.0,"Don't pass the in_cluster parameter.

Instead provide

`cluster_name=""you_cluster_name"",`

`aws_conn_id=""your_aws_conn""`

`service_account_name=""your_service_account""`

You will still be able to use the service account for authentication",2025-03-17T06:53:39,2025-01-31T13:53:29
79402755,Spring Boot Admin Server Unauthorized error when trying to connect to Kubernetes API while startup,"**application.yml**

```
spring:
  application:
    name: springbootmonitoring-app
  cloud:
    kubernetes:
      discovery:
        enabled: true
        all-namespaces: false  # Optional: Set to true if you need to discover services across all namespaces
        discovery-server-url: https://kubernetes.default.svc.cluster.local
```

This app runs on a kubernetes cluster and I want it to monitor all the apps in the cluster.

When the app runs it fails with below error

```
2025-01-31 13:40:56.566 DEBUG [o.s.web.client.RestTemplate,,main] HTTP GET https://kubernetes.default.svc.cluster.local/apps
2025-01-31 13:40:56.661 DEBUG [o.s.web.client.RestTemplate,,main] Accept=[application/json, application/*+json]
2025-01-31 13:40:57.268 DEBUG [o.s.web.client.RestTemplate,,main] Response 401 UNAUTHORIZED
2025-01-31 13:40:57.448 ERROR [o.s.boot.SpringApplication,,main] Application run failed
2025-01-31T11:40:57.451618044Z org.springframework.web.client.HttpClientErrorException$Unauthorized: 401 Unauthorized: ""{""kind"":""Status"",""apiVersion"":""v1"",""metadata"":{},""status"":""Failure"",""message"":""Unauthorized"",""reason"":""Unauthorized"",""code"":401}<EOL>""
```

This is because as you can see in the logs the Bearer token isn't there in the header.
But if I run the curl command inside the Spring boot admin pod it works fine.
`curl --cacert /var/run/secrets/kubernetes.io/serviceaccount/ca.crt -H ""Authorization: Bearer $(cat /var/run/secrets/kubernetes.io/serviceaccount/token)"" https://kubernetes.default.svc.cluster.local/apps`

This means the token is mounted fine and the Role and Role Bindings are in place. I am not sure why the kubernetes discovery isn't automatically using the token while making the discovery API call.
Also is there a way to force kubernetes discovery to use the token","spring-boot, kubernetes, spring-boot-admin",79413778.0,"I don't think you should specify `discovery-server-url` with Kubernetes API server URL as it is intended to connect to a separated discovery server that acts as middle layer discovery client Spring Boot app and Kubernetes API server. You should build the discovery server from [spring-cloud-kubernetes-discoveryserver image](https://hub.docker.com/r/springcloud/spring-cloud-kubernetes-discoveryserver) and deploy to Kubernetes. Afterwards, you can configure `discovery-server-url` with the URL of the deployed discovery server. You can see the detailed documentation on setting up spring boot discovery server [here](https://spring.io/blog/2021/10/26/new-features-for-spring-cloud-kubernetes-in-spring-cloud-2021-0-0-m3#discovery-server-and-client-for-kubernetes).",2025-02-05T05:53:59,2025-01-31T12:33:40
79402669,Istio TLS termination and mTLS,"I have a number of services in a k8s cluster with Istio.  I want the services to internally communicate with automatic mTLS and externally using a web-browser certificate from Let's Encrypt.

To accmplish the former, I have a peer authentication is the `istio-system` namespace:

```
apiVersion: security.istio.io/v1
kind: PeerAuthentication
metadata:
  name: peer-authentication
  namespace: istio-system
spec:
  mtls:
    mode: STRICT
```

This is working fine for internal communication (my service pods are installed with label `sidecar.istio.io/inject: ""true""`).

I have configured an ingress gateway and a gateway

```
apiVersion: networking.istio.io/v1
kind: Gateway
metadata:
  name: gateway
  namespace: istio-ingress
spec:
  selector:
    istio: gateway
  servers:
  - port:
      name: http
      number: 80
      protocol: HTTP
    hosts:
    - ""*.customer.ocs.nu""
    tls:
      httpsRedirect: true
  - port:
      name: https
      number: 443
      protocol: HTTPS
    hosts:
    - ""*.customer.ocs.nu""
    tls:
      credentialName: ""istio-ingress/star-customer-ocs-nu-crt""
      mode: SIMPLE
```

I have multiple applications I wish to expose; currently, I have this one:

```
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: application
  namespace: customer-application
spec:
  gateways:
  - istio-ingress/gateway
  - mesh
  hosts:
  - application.customer.ocs.nu
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: application.customer-application.svc.cluster.local
        port:
          number: 8000
```

(I've removed some irrelevant annotations + changed the name of the application + namespace, so typos in them are irrelevant)

Problem is now, I can connect on http:

```
# curl -kv http://application.customer.ocs.nu/
* Host application.customer.ocs.nu:80 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:80...
* Connected to application.customer.ocs.nu (IP) port 80
> GET / HTTP/1.1
> Host: application.customer.ocs.nu
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
< HTTP/1.1 301 Moved Permanently
< location: https://application.customer.ocs.nu/
< date: Fri, 31 Jan 2025 11:39:09 GMT
< server: istio-envoy
< content-length: 0
```

In the log, I see

```
[2025-01-31T11:39:09.561Z] ""GET / HTTP/1.1"" 301 - direct_response - ""-"" 0 0 0 - ""10.244.0.165"" ""curl/8.7.1"" ""b05ac233-eea7-46d6-9fee-e9f9c9cf8bb8"" ""application.customer.ocs.nu"" ""-"" - - 10.244.0.200:80 10.244.0.165:22533 - -
```

However, connecting via TLS, I get

```
# curl -kv https://application.customer.ocs.nu/
* Host application.customer.ocs.nu:443 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:443...
* Connected to application.customer.ocs.nu (IP) port 443
* ALPN: curl offers h2,http/1.1
* (304) (OUT), TLS handshake, Client hello (1):
* LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to application.customer.ocs.nu:443
* Closing connection
curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to application.customer.ocs.nu:443
```

and get nothing in the log.  Trying to do HTTP on the HTTPs port, I get

```
# curl -kv http://application.customer.ocs.nu:443/
* Host application.customer.ocs.nu:443 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:443...
* Connected to application.customer.ocs.nu (IP) port 443
> GET / HTTP/1.1
> Host: application.customer.ocs.nu:443
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server
```

but this at least shows up in the log

```
[2025-01-31T11:43:01.227Z] ""- - -"" 0 NR filter_chain_not_found - ""-"" 0 0 0 - ""-"" ""-"" ""-"" ""-"" ""-"" - - 10.244.0.200:443 10.244.0.165:44729 - -
```

If I reconfigure the gateway + virtualservice to use HTTP, everything works as expected.  All Bing results suggest setting up redirection from HTTP to HTTPS, but I already have this.

`istioctl analyze -A` lists nothing significant (some services outside the mesh with illegal names + some namespaces without injection annotations), whereas I get

```
# istioctl pc secret istio-gateway-76676d4954-l5498.istio-ingress
RESOURCE NAME                                                 TYPE           STATUS      VALID CERT     SERIAL NUMBER                        NOT AFTER                NOT BEFORE
kubernetes://istio-ingress/star-customer-ocs-nu-crt                          WARMING     false
default                                                       Cert Chain     ACTIVE      true           12c998930e47b4c9df3f5ae259fb1a92     2025-02-01T03:04:23Z     2025-01-31T03:02:23Z
ROOTCA                                                        CA             ACTIVE      true           c6b587095c06abdabc53c84b1af924d3     2035-01-18T12:59:47Z     2025-01-20T12:59:47Z
```

The certificate is provisioned by Let's Encrypt using certbot with DNS authentication and is perfectly valid.  I assume the issue is that Istio is using its own CA for trust and does not trust my public certificate.

```
# kubectl get certificate -n istio-ingress
NAME                         READY   SECRET                           AGE
star-customer-ocs-nu         True    star-customer-ocs-nu-crt         23h
# kubectl get certificaterequest -n istio-ingress
NAME                           APPROVED   DENIED   READY   ISSUER             REQUESTER                                    AGE
star-customer-ocs-nu-1         True                True    letsencrypt-prod   system:serviceaccount:default:cert-manager   23h
```

Does anybody have an idea to work around this?  I'd prefer to use a Let's Encrypt certificate publicly (I don't want to issue certificated manually) without mTLS, and I'd prefer to use automatic internal mTLS.

E: Changing the tls section of the gateway to not include the namespace (while correct)

```
    tls:
      credentialName: star-customer-ocs-nu-crt
      mode: SIMPLE
```

At least shows the certificate as valid (pod name change due to a restart to make sure it picks it up):

```
# istioctl pc secret istio-gateway-76676d4954-8hhjl.istio-ingress
RESOURCE NAME                                   TYPE           STATUS     VALID CERT     SERIAL NUMBER                           NOT AFTER                NOT BEFORE
default                                         Cert Chain     ACTIVE     true           8101cda2556b2fd7c31872f9d013d72f        2025-02-01T12:31:02Z     2025-01-31T12:29:02Z
kubernetes://star-customer-ocs-nu-crt           Cert Chain     ACTIVE     true           4c8a2f7ccab5ff0c7aa61dd2a46aa9bef0b     2025-04-30T11:56:26Z     2025-01-30T11:56:27Z
ROOTCA                                          CA             ACTIVE     true           c6b587095c06abdabc53c84b1af924d3        2035-01-18T12:59:47Z     2025-01-20T12:59:47Z
```","kubernetes, istio, mtls, istio-gateway",79403088.0,"It was caused by an incorrect DestinationRule I wasn't thinking of:

```
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  labels:
    app: application
  name: application
  namespace: application-customer
spec:
  host: application
  subsets:
  - labels:
      app: application
    name: default
```

(the host should be `application.customer.ocs.nu`, not just `application`).",2025-01-31T14:45:00,2025-01-31T12:01:22
79402469,Ejabberd different pods produce different output on API call,"I have 3 pods of ejabberd running on GCP, the configuration file regarding which database should it use looks like this:

```
{%- if env[""DEFAULT_DB""] is defined %}
default_db: {{ env[""DEFAULT_DB""] }}
{%- endif %}
```

The problem is that I connect to the pods and only one of the pods is returning correct result when running `get_user_rooms` endpoint. All the others ones return empty array.

I tried to reload the config, restart the pod, delete the pod, but in all cases I see that the configuration was loaded successfully and no errors during the startup but for some reason it still produces incorrect result.

```
2025-01-31 14:28:07.432 GET
2025-01-31 10:28:07.431631+00:00 [info] Loading configuration from /home/ejabberd/conf/ejabberd.yml
2025-01-31 14:28:07.437 GET
2025-01-31 10:28:07.435907+00:00 [warning] Option 'commands_admin_access' is deprecated and has no effect anymore. Use option 'api_permissions' instead.
2025-01-31 14:28:07.613 GET
2025-01-31 10:28:07.612765+00:00 [info] Configuration loaded successfully
...
2025-01-31 14:28:11.378 GET
[entrypoint_script] ejabberd did join cluster successfully
```","kubernetes, google-cloud-platform, ejabberd",79411344.0,"I'll give you several ideas to investigate. Hopefully one of them will lead you to the problem.

### Are the three nodes really configured to use the same database?

Go to each different pod, get what configuration options each one is really using, and compare ALL the configuration files. Maybe they aren't really using the same database:

```
$ ejabberdctl dump_config /tmp/aaa.yml
$ cat /tmp/aaa.yml
```

Is there any difference between the node that shows the rooms in get_user_rooms ?

### Do the nodes correctly use the same database?

Register an account in the database, then check in the three nodes that they really get that account:

```
$ ejabberdctl registered_users localhost
admin
```

### Maybe mod_muc and get_user_rooms doesn't behave as you expect

An account is registered in the cluster, and the user can login using those credentials in any node of the cluster. When the client logins to that account in a node, the session exists only in that node.

Similarly, the configuration of the rooms is stored in the cluster, and a room can be created in any node, and will be accessible transparently from all the other nodes.

The muc room in fact is alive in one specific node, and the other nodes will just point to that room in that node:

> Rooms are distributed at creation time on all available MUC module
> instances. The multi-user chat module is clustered but the rooms
> themselves are not clustered nor fault-tolerant: if the node managing
> a set of rooms goes down, the rooms disappear and they will be
> recreated on an available node on first connection attempt.

So, maybe the ejabberd nodes connect correctly to the same database, but get_user_rooms doesn't show correct values, or the problem is only in the MUC service?",2025-02-04T10:30:15,2025-01-31T10:37:51
79402349,How to enable Client Certificate Validation for specific paths in Nginx Ingress Controller?,"I have applications deployed in Kubernetes using the Nginx Ingress Controller. I need to implement path-based Client Certificate Validation where:

- 'app.example.com/**auth**' -> path should **require** client certificates
- 'app.example.com/**tool**' -> path should **not require** client certificates

Currently, I'm using this annotation to enable/disable Client Certificate Validation (Authentication):
*nginx.ingress.kubernetes.io/auth-tls-verify-client: ""**on**""*

What I understand is client cert auth is a global configuration and it can not be configured for specific path.
for referece, see first few lines of the doc:

*[https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#client-certificate-authentication](https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#client-certificate-authentication)*

We also thought to use 2 ingress controller but both URLs have same domain so domain can only be resolved to any one Load Balancer IP of ingress controller service.

Please advise how can We enabled client cert validation on specific path?  We are also flexible to switch to some other Ingress controller.","kubernetes, nginx, kubernetes-ingress, nginx-ingress, client-certificates",79462733.0,"Simply use two separate ingress resources for two different paths:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auth-ingress
  annotations:
    nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
    nginx.ingress.kubernetes.io/auth-tls-secret: ""default/auth-secret""
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: ""1""
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tool-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /tool
        pathType: Prefix
        backend:
          service:
            name: tool-service
            port:
              number: 80
```

/auth path with have a block for cert validation and /tool path will bypass the validation.",2025-02-24T07:38:07,2025-01-31T09:53:20
79401340,random.sample() generating same sequence every time it is run,"I run this code on two machines:

```
from apscheduler.schedulers.asyncio import AsyncIOScheduler

# this part is simplified. It is only here to show how scheduler is basically initialized (for context)
scheduler = AsyncIOScheduler(timezone=utc)
scheduler.start()

# This is real code (with exception of the list)
@scheduler.scheduled_job('interval', minutes=1, misfire_grace_time=None)
async def do_dada_news():
    pages = [...] # shortened for better readability. It is longer than 20 elements
    print(""---"")
    for page in random.sample(pages, min(len(pages), 20)):
        print(page)
```

On both machines I get different outputs which are strange:

- Local docker container: I get 20 different lines every time `do_dada_news()` runs.
- Kubernetes cluster: I get the exact same 20 lines every time it is run.

I expect both machines to have the same behavior. How can this be such a different behavior?

To temporarily fix the problem, I now do `random.seed(time.time()*10000)` inside `do_dada_news()`. But that does not feel right.","python, kubernetes, random",79443922.0,"If no seed is provided for pythons built-in random then it will use [os.urandom()](https://docs.python.org/3/library/os.html#os.urandom) to set the seed. Crucially, if the operating system (Linux and Windows both do this) has a built in source of randomness it will default to using that instead of just using the system time.

While you *could* mess with the Linux configuration settings, it would be much easier just to initialize a random seed with random.seed(int(time.time())**20%999979).

Linux in particular uses an entropy pool as the source of randomness, and there's a suggestion [here](https://techdocs.broadcom.com/us/en/symantec-security-software/identity-security/vip-authentication-hub/2-1/Installing/modes-of-deployment/checking-entropy-level.html) that the issue might be ameliorable with an upgrade to 5.6. In general though the entropy pool will require a short delay in order to generate the randomness needed.

If I was very concerned about not having this issue in future, I would set up a queue and create a function that when called returns the top number from the queue, deques it, and then adds a new random number to the bottom of the queue based on the mod-product of the numbers still in it. That way you shouldn't should be at least guaranteed a source of randomness that you control.",2025-02-16T20:59:14,2025-01-30T22:04:41
79400282,NVIDIA GPU Not Detected in Kubernetes (WSL2) After Installing GPU Operator &amp; NVIDIA Device Plugin,"I am running Kubernetes on **Docker Desktop with WSL2** and trying to set up **GPU monitoring** using the **NVIDIA GPU Operator** and **NVIDIA Device Plugin**.

**What I Have Done:**

**GPU Confirmed Working in WSL2**

- `nvidia-smi` **works correctly** and detects my NVIDIA RTX 4070 GPU.

Running a **CUDA container works fine**:
- `docker run --rm --gpus all nvidia/cuda:12.6.2-base-ubuntu22.04 nvidia-smi`
- ✅ Shows correct **CUDA version** and **GPU details**.

### **Issue: GPU Not Detected in Kubernetes**

- `kubectl get nodes -o=jsonpath='{.items[*].status.allocatable}'` **does not show GPU**.
- `kubectl logs -n gpu-operator -l app=nvidia-device-plugin-daemonset` **shows NVML not found**.
- **NVIDIA GPU Operator and Device Plugin are running but not detecting GPU**.

### **What I Have Tried:**

**Ensured `nvidia-container-runtime` is set correctly**

- Edited `/etc/docker/daemon.json`

```
{
  ""default-runtime"": ""nvidia"",
  ""runtimes"": {
    ""nvidia"": {
      ""path"": ""/usr/bin/nvidia-container-runtime"",
      ""runtimeArgs"": []
    }
  }
}
```

### **What I Need Help With:**

1. **Why is Kubernetes not detecting the GPU ?**
2. **Why does `nvidia-device-plugin` fail with `could not load NVML library`?**
3. **Is there a special configuration needed for WSL2 to work with Kubernetes GPU Operator?**
4. **Are there any alternative debugging steps to confirm NVML is correctly installed?**

### **System Information:**

- **OS:** Ubuntu 24.04 LTS (WSL2)
- **Kubernetes:** Docker Desktop with WSL2
- **NVIDIA Driver:** 566.36
- **CUDA Version:** 12.7 (Confirmed in `nvidia-smi`)
- **NVIDIA Container Toolkit:** Installed (`nvidia-container-toolkit` latest version)
- **NVIDIA GPU:** RTX 4070 Laptop GPU
- **Docker Runtime:**

`docker info | grep -i runtime`

**Output:**

`Runtimes: io.containerd.runc.v2 `

`nvidia runc Default Runtime: runc`

### **Any Help is Appreciated!**

If anyone has successfully set up **NVIDIA GPU Operator in WSL2 with Kubernetes**, please share insights!","docker, kubernetes, gpu, windows-subsystem-for-linux, nvidia",,,,2025-01-30T14:51:53
79400204,How can I apply tolerations to EKS&#39;s Add-Ons when using Terraform,"I'm trying to install Cilium in my EKS cluster, to acomplish that I need to create my cluster's node groups with the following taint:

```
taints:
   - key: ""node.cilium.io/agent-not-ready""
     value: ""true""
     effect: ""NoExecute""
```

I also need to create my Add-Ons (VPC CNI, EBS CSI, Kube Proxy and CoreDNS), but because of the taint on the nodes, the Add-Ons are installed with error.
I'm using Terraform to create everything.

Update: I was able to create the VPC CNI, Kube Proxy and CoreDNS using the tolerations, as described on the aws eks describe-addon-configuration command.
The problem I'm having now is with the AWS EBS CSI Driver. It doesn't support the tolereations scheme I'm trying.

After I run the following command:

```
aws eks describe-addon-configuration --addon-name aws-ebs-csi-driver --addon-version v1.38.1-eksbuild.2 | jq "".configurationSchema"" | jq ""fromjson"" | jq "".properties.node.properties.tolerations""
```

The return is:

```
{
  ""default"": [
    {
      ""effect"": ""NoExecute"",
      ""operator"": ""Exists"",
      ""tolerationSeconds"": 300
    }
  ],
  ""description"": ""Tolerations of the node pod"",
  ""items"": {
    ""type"": ""object""
  },
  ""type"": ""array""
}
```

My configuration is the following:

```
configuration_values = jsonencode({
tolerations = [{
  ""key"" : ""node.cilium.io/agent-not-ready"",
  ""operator"": ""Equal"",
  ""value"": ""true"",
  ""effect"" : ""NoExecute""
}]
```

})

And yet I'm still receiving the following error: InvalidParameterException: ConfigurationValue provided in request is not supported: Json schema validation failed with error: [$.tolerations: is not defined in the schema and the schema does not allow additional properties]","amazon-web-services, kubernetes, terraform, amazon-eks, cilium",79564469.0,"This works for the latest versions of EBS CSI driver addons:

- aws-ebs-csi-driver: v1.41.0-eksbuild.1

Passing below code block as input for the [https://github.com/terraform-aws-modules/terraform-aws-eks](https://github.com/terraform-aws-modules/terraform-aws-eks) module.

```
    aws-ebs-csi-driver = {
      addon_version = ""v1.41.0-eksbuild.1""
      configuration_values = jsonencode({
        node : {
          tolerations : [
            {
              effect : ""NoSchedule"",
              key : ""<CustomTaintKey>"",
              operator : ""Equal"",
              value : ""<CustomTaintValue>""
            }
          ]
        },
        controller : {
          tolerations : [
            {
              effect : ""NoSchedule"",
              key : ""<CustomTaintKey>"",
              operator : ""Equal"",
              value : ""<CustomTaintValue>""
            }
          ]
        }
      })
```",2025-04-09T13:31:04,2025-01-30T14:20:09
79400204,How can I apply tolerations to EKS&#39;s Add-Ons when using Terraform,"I'm trying to install Cilium in my EKS cluster, to acomplish that I need to create my cluster's node groups with the following taint:

```
taints:
   - key: ""node.cilium.io/agent-not-ready""
     value: ""true""
     effect: ""NoExecute""
```

I also need to create my Add-Ons (VPC CNI, EBS CSI, Kube Proxy and CoreDNS), but because of the taint on the nodes, the Add-Ons are installed with error.
I'm using Terraform to create everything.

Update: I was able to create the VPC CNI, Kube Proxy and CoreDNS using the tolerations, as described on the aws eks describe-addon-configuration command.
The problem I'm having now is with the AWS EBS CSI Driver. It doesn't support the tolereations scheme I'm trying.

After I run the following command:

```
aws eks describe-addon-configuration --addon-name aws-ebs-csi-driver --addon-version v1.38.1-eksbuild.2 | jq "".configurationSchema"" | jq ""fromjson"" | jq "".properties.node.properties.tolerations""
```

The return is:

```
{
  ""default"": [
    {
      ""effect"": ""NoExecute"",
      ""operator"": ""Exists"",
      ""tolerationSeconds"": 300
    }
  ],
  ""description"": ""Tolerations of the node pod"",
  ""items"": {
    ""type"": ""object""
  },
  ""type"": ""array""
}
```

My configuration is the following:

```
configuration_values = jsonencode({
tolerations = [{
  ""key"" : ""node.cilium.io/agent-not-ready"",
  ""operator"": ""Equal"",
  ""value"": ""true"",
  ""effect"" : ""NoExecute""
}]
```

})

And yet I'm still receiving the following error: InvalidParameterException: ConfigurationValue provided in request is not supported: Json schema validation failed with error: [$.tolerations: is not defined in the schema and the schema does not allow additional properties]","amazon-web-services, kubernetes, terraform, amazon-eks, cilium",79418250.0,"For the latest versions of these addons:

- aws-ebs-csi-driver: v1.38.1-eksbuild.2
- aws-mountpoint-s3-csi-driver: v1.11.0-eksbuild.1

I had to add this in the Terraform scripts for addon configuration:

```
configuration_values = jsonencode({
node: {
  tolerations = [
    {
      operator = ""Exists""
      effect    = ""NoSchedule""
    }
  ]
}
```

})",2025-02-06T14:09:39,2025-01-30T14:20:09
79400204,How can I apply tolerations to EKS&#39;s Add-Ons when using Terraform,"I'm trying to install Cilium in my EKS cluster, to acomplish that I need to create my cluster's node groups with the following taint:

```
taints:
   - key: ""node.cilium.io/agent-not-ready""
     value: ""true""
     effect: ""NoExecute""
```

I also need to create my Add-Ons (VPC CNI, EBS CSI, Kube Proxy and CoreDNS), but because of the taint on the nodes, the Add-Ons are installed with error.
I'm using Terraform to create everything.

Update: I was able to create the VPC CNI, Kube Proxy and CoreDNS using the tolerations, as described on the aws eks describe-addon-configuration command.
The problem I'm having now is with the AWS EBS CSI Driver. It doesn't support the tolereations scheme I'm trying.

After I run the following command:

```
aws eks describe-addon-configuration --addon-name aws-ebs-csi-driver --addon-version v1.38.1-eksbuild.2 | jq "".configurationSchema"" | jq ""fromjson"" | jq "".properties.node.properties.tolerations""
```

The return is:

```
{
  ""default"": [
    {
      ""effect"": ""NoExecute"",
      ""operator"": ""Exists"",
      ""tolerationSeconds"": 300
    }
  ],
  ""description"": ""Tolerations of the node pod"",
  ""items"": {
    ""type"": ""object""
  },
  ""type"": ""array""
}
```

My configuration is the following:

```
configuration_values = jsonencode({
tolerations = [{
  ""key"" : ""node.cilium.io/agent-not-ready"",
  ""operator"": ""Equal"",
  ""value"": ""true"",
  ""effect"" : ""NoExecute""
}]
```

})

And yet I'm still receiving the following error: InvalidParameterException: ConfigurationValue provided in request is not supported: Json schema validation failed with error: [$.tolerations: is not defined in the schema and the schema does not allow additional properties]","amazon-web-services, kubernetes, terraform, amazon-eks, cilium",79402279.0,"The [terraform documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eks_addon#example-add-on-usage-with-custom-configuration_values) has an example on how you can achieve this. In the example the addon is `coredns` version `v1.10.1-eksbuild.1`.

Calling the api:

```
 aws eks describe-addon-configuration \
 --addon-name coredns \
 --addon-version v1.10.1-eksbuild.1 \
 --query ""configurationSchema"" | jq '. | fromjson'
```

you can see that `tolerations` is a property. So to specify the toleration in this example:

```
resource ""aws_eks_addon"" ""example"" {
  cluster_name  = ""mycluster""
  addon_name    = ""coredns""
  addon_version = ""v1.10.1-eksbuild.1""

  configuration_values = jsonencode({
    tolerations = [{
      ""key"" : ""node.cilium.io/agent-not-ready"",
      ""operator"" : ""NoExecute""
    }]
  })
}
```",2025-01-31T09:26:22,2025-01-30T14:20:09
79399232,Forbidden: uniqueItems cannot be set to true since the runtime complexity becomes quadratic,"My Kubernetes CRD:

```
type FooSpec struct {
    // +kubebuilder:validation:UniqueItems=true
    MyItems []string `json:""myItems""`
```

Fails:

> Forbidden: uniqueItems cannot be set to true since the runtime complexity becomes quadratic

This is documented: [kubernetes.io validation docs](https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#validation)

How can I ensure that the slice `MyItems` contains no duplicates without writing a webhook?","go, validation, kubernetes, kubernetes-custom-resources",79399280.0,"This works: `listType=set`

```
type FooSpec struct {
    // +listType=set
    MyItems []string `json:""myItems""`
```",2025-01-30T08:51:13,2025-01-30T08:33:14
79398379,"Tilt.dev Kubernetes Minikube Local Dev env, Live update shows Update Stopped but still updates","**Background**

So I am currently setting up [tilt](https://tilt.dev/) for my team's local dev environment. Everything actually works as expected, so I am not sure if the following is an UI issue (then maybe I should also post this onto its github issues page).

Currently we have one docker image that will build a service that will need live update during development. We have our devOps scripts/resources inside a separate repository. And the team would like to initialize the dev environment in the parent directory, such as:

```
-- org
|
|-- deployment (own repo)
|--|-- local
|--|--|-- images/api_server.dockerfile
|--|--|-- Tiltfile
|
|-- api_server (own repo)
|-- ui (own repo)
```

So our tilefile is as follows:

```
docker_build(
    'org/api-server',
    '../../',
    build_args={
        ""UID"" : UID,
        ""GID"" : GID,
    },
    dockerfile='./images/api_server.dockerfile',
    live_update=[sync('../../api_server', '/var/www/html')],
    ignore=['../../deployment', '../../ui']
)
```

However on the actual UI it says the following during any change inside the deployment repo/directory:

`LiveUpdate ""api-server-deployment:api-server"" UpdateStopped: Found file(s) not matching any sync (files: [deployment/dev/cluster.yaml])`

Then it goes on and updates/rebuilds image:

```
1 File Changed: [deployment/dev/cluster.yaml]
STEP 1/3 — Building Dockerfile: [org/api-server]
Building Dockerfile for platform linux/amd64:
```

Not sure if this is the right place to post this, and it could be my own misunderstanding of how it works, but I couldn't find the relevant documentations for this. Is this behaving correctly, shouldn't tilt not update since the files changed is inside the deployment directory (specified inside Tiltfile and also in the tilt UI)

Tilt version: v0.33.22, built 2025-01-03
minikube version: v1.34.0
Docker version 27.3.1, build ce12230","docker, kubernetes, devops, minikube",,,,2025-01-29T22:51:06
79398004,kube-api pod is in kube-system namespace while kubernetes service is in default namespace,"why the service for api server is in default namespace instead of kube-system namespace? since kube-api pod run on kube-system namespace, why we have Kubernetes API service in default namespace?",kubernetes,79401358.0,"Because when creating Kubernetes' new cluster, it has [initial namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/#initial-namespaces) that help different projects, teams, or customers to share its Kubernetes cluster. The namespace for objects created by the Kubernetes system is called *‘kube-system’* which part of this core component is *‘kube-apiserver’* that exposes the Kubernetes HTTP API. Also, [subdividing cluster namespaces](https://kubernetes.io/docs/tasks/administer-cluster/namespaces/#subdividing-your-cluster-using-kubernetes-namespaces), by default it will instantiate a *‘default’* namespace when provisioning the cluster to hold the default set of Pods, Services, and Deployments used by the cluster.",2025-01-30T22:17:13,2025-01-29T19:41:43
79397526,Access denied on pvc mount after Kubernetes cluster worker node reboot,"Thanks so much in advance,

After a graceful restart of nodes, I'm experiencing an unusual access denied error on the pvc used for llm model cache stored on a local-nfs storage class.

```
  Warning  FailedMount       16m                  kubelet            MountVolume.SetUp failed for volume ""pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69"" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o retrans=2,timeo=30,vers=3 10.101.156.22:/export/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69 /var/lib/kubelet/pods/70e3e22b-dd08-4945-a039-a9ce107e525d/volumes/kubernetes.io~nfs/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69
Output: Created symlink /run/systemd/system/remote-fs.target.wants/rpc-statd.service → /lib/systemd/system/rpc-statd.service.
mount.nfs: Operation not permitted
  Warning  FailedMount  16m  kubelet  MountVolume.SetUp failed for volume ""pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69"" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o retrans=2,timeo=30,vers=3 10.101.156.22:/export/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69 /var/lib/kubelet/pods/70e3e22b-dd08-4945-a039-a9ce107e525d/volumes/kubernetes.io~nfs/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69
Output: mount.nfs: Operation not permitted
  Warning  FailedMount  15s (x14 over 16m)  kubelet  MountVolume.SetUp failed for volume ""pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69"" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o retrans=2,timeo=30,vers=3 10.101.156.22:/export/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69 /var/lib/kubelet/pods/70e3e22b-dd08-4945-a039-a9ce107e525d/volumes/kubernetes.io~nfs/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69
Output: mount.nfs: access denied by server while mounting 10.101.156.22:/export/pvc-8d73fc95-b785-4e12-b47a-c8d1c3d12f69
```

This is causing pods to be stuck in ContainerCreating status.

```
videosearch        vss-blueprint-0                                                   0/1     ContainerCreating   0              20h    <none>            worker-1    <none>
videosearch        vss-vss-deployment-5f758bc5df-fbm66                               0/1     Init:0/3            0              21h    <none>            worker-1    <none>
vllm               llama3-70b-bc4788446-9q8c2                                        0/1     ContainerCreating   0              21h    <none>            worker-2    <none>
```

The pv and pvc are both healthy, it seems just the mount command that the pods are issuing is failing.

My previous solution was to delete the pv and pvc and then redeploy the entire helm chart, but this is not ideal to have to redeploy a major workload after restart.

Would anyone happen to have a suggestion for something like this?","kubernetes, large-language-model, nfs",,,,2025-01-29T16:51:00
79396885,How Microsoft HybridCache works in a Microservice Setup with Multiple Pods,"The Microsoft.Extensions.Caching.Hybrid package (available in .NET 9) provides a hybrid caching mechanism that allows you to combine both in-memory caching (for fast local access) and distributed caching (such as Redis) for microservices running in multiple pods.

How It Works in a Microservices Setup with Multiple Pods? If data is stored in-memory also, each pod uses its own memory, so if data is refreshed or updated in one pod, how to invalidate the in-memory cache in other pods so it do not return the stale data?

As far as I know, when we use in-memory for data storage in multiple pods application, there is no way the data changed in one pod would get changed in other pods also without any custom implementation.
But, want to see if there is anything handled for this usecase within the HybridCache library itself which could handle the multiple pods scenario well and data gets refreshed in each pod?",".net, asp.net-core, kubernetes, microservices, hybridcache",79401180.0,"***But, want to see if there is anything handled for this usecase within the HybridCache library itself which could handle the multiple pods scenario well and data gets refreshed in each pod?***

No, currently there is nothing of the sorts. This is covered in the docs in [Cache storage](https://learn.microsoft.com/en-us/aspnet/core/performance/caching/hybrid?view=aspnetcore-9.0#cache-storage) section:

> When invalidating cache entries by key or by tags, they are invalidated in the current server and in the secondary out-of-process storage. However, the in-memory cache in other servers isn't affected.

It would be even hard to workaround at the moment since the is no publicily exposed APIs to invalidate (remove) only local cache data since `HybridCache.RemoveAsync` will clear both local and distributed cache. But if you are OK with rewriting local one via `HybridCache.SetAsync` then you can create some pub/sub (for example on Redis) which will handle ""remote"" cache invalidation.

This issue is discussed in [[API Proposal]: Cache Synchronization for Hybrid Caching in Multi-Node Environments](https://github.com/dotnet/extensions/issues/5517) issue at github.

Also you can look into [`FusionCache`](https://github.com/ZiggyCreatures/FusionCache) which AFAIK already has such feature.",2025-01-30T20:36:31,2025-01-29T13:23:59
79396576,Payara Micro in Kubernetes does not open port 8080 randomly,"We are running payara/micro:6.2024.7-jdk21 with our application in a Kubernetes cluster managed by DigitalOcean.

**Problem**:

Randomly, port 8080 does not open inside the pod. As a result, the readiness probe fails, and the container restarts. After the restart, everything works as expected. This happens completely randomly. It occurs at a stage when no service is yet routing traffic to the pod. The service switches between blue/green only after both pods are functional.

HPA is set to a minimum of 2 replicas.

We deploy using a blue/green deployment strategy.

**Tests performed:**
• Immediately after starting the container, we run netstat -tuln. It shows no open ports.
• Once Payara Micro starts up, we can see ports 8080 and 6900. Occasionally, 8080 is missing.
• No problematic logs, except for:

```
SEVERE: Error adding HttpProbes. NetworkListener http-listeners GrizzlyProxy is NULL.
```

```
FINE: Failed to clear ResourceBundle references for web application [unknown]
java.lang.NoSuchFieldException: loaderRef
    at java.base/java.lang.Class.getDeclaredField(Class.java:2782)
    at org.glassfish.web.loader.WebappClassLoader.clearReferencesResourceBundles(WebappClassLoader.java:2773)
    at org.glassfish.web.loader.WebappClassLoader.clearReferences(WebappClassLoader.java:2192)
    at org.glassfish.web.loader.WebappClassLoader.stop(WebappClassLoader.java:2044)
    at org.glassfish.web.loader.WebappClassLoader.preDestroy(WebappClassLoader.java:2017)
    at org.glassfish.deployment.common.DeploymentContextImpl.getClassLoader(DeploymentContextImpl.java:293)
    at org.glassfish.deployment.common.DeploymentContextImpl.getClassLoader(DeploymentContextImpl.java:235)
    at com.sun.enterprise.v3.server.ApplicationLifecycle.prepare(ApplicationLifecycle.java:571)
    at org.glassfish.deployment.admin.DeployCommand.execute(DeployCommand.java:570)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2$1.run(CommandRunnerImpl.java:556)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2$1.run(CommandRunnerImpl.java:552)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:400)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:453)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2.execute(CommandRunnerImpl.java:551)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$3.run(CommandRunnerImpl.java:582)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$3.run(CommandRunnerImpl.java:574)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:400)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:453)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl.doCommand(CommandRunnerImpl.java:573)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl.doCommand(CommandRunnerImpl.java:1497)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$ExecutionContext.execute(CommandRunnerImpl.java:1879)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$ExecutionContext.execute(CommandRunnerImpl.java:1755)
    at com.sun.enterprise.admin.cli.embeddable.DeployerImpl.deploy(DeployerImpl.java:131)
    at com.sun.enterprise.admin.cli.embeddable.DeployerImpl.deploy(DeployerImpl.java:104)
    at fish.payara.micro.impl.PayaraMicroImpl.deployAll(PayaraMicroImpl.java:1739)
    at fish.payara.micro.impl.PayaraMicroImpl.bootStrap(PayaraMicroImpl.java:1092)
    at fish.payara.micro.impl.PayaraMicroImpl.create(PayaraMicroImpl.java:236)
    at fish.payara.micro.impl.PayaraMicroImpl.main(PayaraMicroImpl.java:223)
    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
    at java.base/java.lang.reflect.Method.invoke(Method.java:580)
    at fish.payara.micro.boot.loader.MainMethodRunner.run(MainMethodRunner.java:50)
    at fish.payara.micro.boot.loader.Launcher.launch(Launcher.java:114)
    at fish.payara.micro.boot.loader.Launcher.launch(Launcher.java:73)
    at fish.payara.micro.boot.PayaraMicroLauncher.create(PayaraMicroLauncher.java:88)
    at fish.payara.micro.boot.PayaraMicroLauncher.main(PayaraMicroLauncher.java:72)
    at fish.payara.micro.PayaraMicro.main(PayaraMicro.java:467)

FINE: Method not found: clearProperties
java.lang.NoSuchMethodException: jakarta.el.BeanELResolver.clearProperties(java.lang.ClassLoader)
    at java.base/java.lang.Class.getMethod(Class.java:2395)
    at org.glassfish.web.loader.CachingReflectionUtil.lambda$getMethodFromCache$1(CachingReflectionUtil.java:76)
    at java.base/java.util.concurrent.ConcurrentHashMap.computeIfAbsent(ConcurrentHashMap.java:1708)
    at org.glassfish.web.loader.CachingReflectionUtil.getMethodFromCache(CachingReflectionUtil.java:69)
    at org.glassfish.web.loader.WebappClassLoader.clearBeanResolver(WebappClassLoader.java:2681)
    at org.glassfish.web.loader.WebappClassLoader.clearBeanELResolverCache(WebappClassLoader.java:2666)
    at org.glassfish.web.loader.WebappClassLoader.stop(WebappClassLoader.java:2054)
    at org.glassfish.web.loader.WebappClassLoader.preDestroy(WebappClassLoader.java:2017)
    at org.glassfish.deployment.common.DeploymentContextImpl.getClassLoader(DeploymentContextImpl.java:293)
    at org.glassfish.deployment.common.DeploymentContextImpl.getClassLoader(DeploymentContextImpl.java:235)
    at com.sun.enterprise.v3.server.ApplicationLifecycle.prepare(ApplicationLifecycle.java:571)
    at org.glassfish.deployment.admin.DeployCommand.execute(DeployCommand.java:570)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2$1.run(CommandRunnerImpl.java:556)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2$1.run(CommandRunnerImpl.java:552)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:400)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:453)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$2.execute(CommandRunnerImpl.java:551)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$3.run(CommandRunnerImpl.java:582)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$3.run(CommandRunnerImpl.java:574)
    at java.base/java.security.AccessController.doPrivileged(AccessController.java:400)
    at java.base/javax.security.auth.Subject.doAs(Subject.java:453)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl.doCommand(CommandRunnerImpl.java:573)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl.doCommand(CommandRunnerImpl.java:1497)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$ExecutionContext.execute(CommandRunnerImpl.java:1879)
    at com.sun.enterprise.v3.admin.CommandRunnerImpl$ExecutionContext.execute(CommandRunnerImpl.java:1755)
    at com.sun.enterprise.admin.cli.embeddable.DeployerImpl.deploy(DeployerImpl.java:131)
    at com.sun.enterprise.admin.cli.embeddable.DeployerImpl.deploy(DeployerImpl.java:104)
    at fish.payara.micro.impl.PayaraMicroImpl.deployAll(PayaraMicroImpl.java:1739)
    at fish.payara.micro.impl.PayaraMicroImpl.bootStrap(PayaraMicroImpl.java:1092)
    at fish.payara.micro.impl.PayaraMicroImpl.create(PayaraMicroImpl.java:236)
    at fish.payara.micro.impl.PayaraMicroImpl.main(PayaraMicroImpl.java:223)
    at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)
    at java.base/java.lang.reflect.Method.invoke(Method.java:580)
    at fish.payara.micro.boot.loader.MainMethodRunner.run(MainMethodRunner.java:50)
    at fish.payara.micro.boot.loader.Launcher.launch(Launcher.java:114)
    at fish.payara.micro.boot.loader.Launcher.launch(Launcher.java:73)
    at fish.payara.micro.boot.PayaraMicroLauncher.create(PayaraMicroLauncher.java:88)
    at fish.payara.micro.boot.PayaraMicroLauncher.main(PayaraMicroLauncher.java:72)
    at fish.payara.micro.PayaraMicro.main(PayaraMicro.java:467)
```

Even when the port does not open, the application deploys correctly:

```
INFO: {
    ""Instance Configuration"": {
        ""Host"": ""some-app-green-5768f94d4b-ct2n2"",
        ""Http Port(s)"": ""8080"",
        ""Https Port(s)"": """",
        ""Instance Name"": ""Poor-Flyingfish"",
        ""Instance Group"": ""MicroShoal"",
        ""Hazelcast Member UUID"": ""e4a1bcea-1da1-4952-92af-4d282b93dcb1"",
        ""Deployed"": [
            {
                ""Name"": ""someapp"",
                ""Type"": ""war"",
                ""Context Root"": ""/app""
            }
        ]
    }
}
```

Running kubectl get networkpolicy -A returns nothing.

The only event logs are about readiness probe failures.

Dockerfile excerpt:

```
FROM --platform=linux/amd64 payara/micro:6.2024.7-jdk21

COPY dist/ /opt/payara/deployments/
COPY conf/mysql-connector-j-8.2.0.jar /opt/payara/deployments/mysql-connector-j-8.2.0.jar
COPY conf/configure-datasource.sh /opt/payara/scripts/configure-datasource.sh

ENV JVM_ARGS=""-XX:MaxHeapFreeRatio=50""

CMD [""--enableRequestTracing"", ""2"", ""--addjars"", ""/opt/payara/deployments/mysql-connector-j-8.2.0.jar"", ""--postbootcommandfile"", ""/opt/payara/scripts/configure-datasource.sh"", ""--deploy"", ""/opt/payara/deployments/someapp.war"", ""--contextroot"", ""/app"", ""--port"", ""8080"", ""--clustermode"", ""kubernetes:namespace-ns,backend-service""]
```

Post-boot script excerpt:

```
set configs.config.server-config.thread-pools.thread-pool.http-thread-pool.min-thread-pool-size=5
set configs.config.server-config.thread-pools.thread-pool.http-thread-pool.max-thread-pool-size=350
set configs.config.server-config.thread-pools.thread-pool.thread-pool-1.min-thread-pool-size=3
set configs.config.server-config.thread-pools.thread-pool.thread-pool-1.max-thread-pool-size=250

set configs.config.server-config.web-container.session-config.session-properties.timeout-in-seconds=432000

set-metrics-configuration --enabled=false

set configs.config.server-config.network-config.protocols.protocol.http-listener.http.cookie-same-site-enabled=true
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.cookie-same-site-value=None
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.compressable-mime-type=text/plain,text/html,text/xml,text/css,application/xml,application/xhtml+xml,application/rss+xml,application/javascript,image/svg+xml,application/json
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.compression=on
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.compression-level=6
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.file-cache.enabled=true
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.file-cache.max-age-seconds=3600
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.file-cache.max-cache-size-bytes=52857600
set configs.config.server-config.network-config.protocols.protocol.http-listener.http.file-cache.max-files-count=5000
```

**Question:**

Has anyone encountered a similar issue where Payara Micro randomly does not open port 8080 on startup? What could be the possible causes and solutions?","kubernetes, payara",79430902.0,"The exceptions you posted are ugly but can be ignored. They happen during cleanup phase after deployment, because the internal JDK field `loaderRef` was removed in Java 11, and the internal method clearProperties also doesn't exist on `BeanELResolver` anymore. Payara should fix this, but it doesn't do any harm, Payara ignores the exception and continues as normal.

I think the message `SEVERE: Error adding HttpProbes. NetworkListener http-listeners GrizzlyProxy is NULL.` is also harmless. It's probably logged because the HTTPS listener is not enabled and therefore is NULL.

None of the errors explain why the Payara Micro doesn't binds to the 8080 port.",2025-02-11T17:46:14,2025-01-29T11:30:13
79395938,How to Set Dynamic execution_timeout for a Task in Airflow Using GKEPodOperator?,"I have an Airflow DAG with six tasks, all using GKEPodOperator. My DAG has an execution_timeout of 1 hour, but sometimes Task 5 takes longer to execute, causing failures.

I want to set a dynamic execution_timeout for Task 5 using the following logic:
•   DAG Start Time + 60 minutes (total execution window)
•   Subtract the end time of Task 4 (the task prior to Task 5)
•   Leave a 2-minute buffer for Task 6 to complete

I tried retrieving the DAG start time and Task 4’s end time using XCom and dag_run.get_task_instance(), but it is failing inside GKEPodOperator.

Is there a way to dynamically compute and apply execution_timeout within Task 5 itself, without creating an extra task?

Any suggestions on achieving this in Airflow?","python, kubernetes, airflow, jinja2, kubernetespodoperator",,,,2025-01-29T07:04:50
79395870,User-specified executor pod templates are not working in Spark 3.3.1,"We are attempting to use a pod template for the executor to expose a JMX port for exporting metrics. While the pod template works for the driver — with the JMX port being created and the template successfully mounted at /opt/spark/pod-template — the same template is not being applied to the executor.

For more details, you can refer to the Jira ticket I raised,
[https://issues.apache.org/jira/browse/SPARK-51020](https://issues.apache.org/jira/browse/SPARK-51020)

The following Spark properties are also being passed during job submission:

spark.kubernetes.driver.podTemplateFile=s3a://mybucket/code/driver-pod-template.yaml
spark.kubernetes.executor.podTemplateFile=s3a://mybucket/code/exec-pod-template.yaml
Despite these configurations, the executor pod template does not appear to take effect.","apache-spark, kubernetes",,,,2025-01-29T06:25:38
79394610,How to share compiled subgraph between supervisors deployed in different containers?,"I am trying to implement Langgraph (Python) hierarchical agent architecture within a microservices environment. I would like some help in understanding how to transmit compiled subgraphs to parent agents located in separate containers. Is there a feasible method for sharing these compiled subgraphs across different microservice containers?

I attempted to serialize the compiled graph using pickle, but encountered an error related to nested functions.","kubernetes, microservices, langgraph",,,,2025-01-28T17:00:31
79394276,How to limit the memory usage of a Python process?,"I'm trying to limit the memory usage of a Python service running in Kubernetes. I'm currently testing with Python 3.10 running in WSL2. I want the service to be aware of limitations set by Kubernetes, so it can throw a MemoryError when it's trying to allocate too much memory and handle that error in the code. I'm trying to set RLIMIT_AS as described [here](https://carlosbecker.com/posts/python-docker-limits/). Kubernetes has a limit of 500 MB for the process. But when I set the RLIMIT_AS to 500 MB, the service does not even start. I then wrote a very simple script and checked how low I can set the RLIMIT_AS.

Script:

```
import resource

limit = 1000 * 1024 * 1024  # 1000 MB
resource.setrlimit(resource.RLIMIT_AS, (limit, limit))
print(f""Setting memory limit to {limit} bytes."")
bytearray(1 * 1024 * 1024)  # Allocate 1 MB
print(""Successfully allocated memory."")
```

Output:

```
Setting memory limit to 1048576000 bytes.
Traceback (most recent call last):
  File ""/mnt/c/Users/xxx/foobar.py"", line 6, in <module>
    bytearray(1 * 1024 * 1024)  # Allocate 1 MB
MemoryError

Process finished with exit code 1
```

I have to set the limit to 1048 MB for the script to be successful. ""htop"" in WSL2 is showing me a VIRT of 1047 MB for the Python script, so it seems RLIMIT_AS has to be greater than VIRT, which is already more than 1 GB for the most simple script.

```
PID   USER PRI NI VIRT  RES   SHR   S CPU%▽ MEM% TIME+   Command
56232 xxx  20  0  1046M 81112 26156 S 0.0   0.2  0:00.10 /home/xxx/.virtualenvs/xxx/bin/python3 /mnt/c/Users/xxx/foobar.py
```

Why is VIRT so high? How can I limit the real memory usage of the process?","python, kubernetes, memory, windows-subsystem-for-linux, setrlimit",79394367.0,"### Why is VIRT usage so high?

VIRT includes all the memory that the process can access, not just the memory physically allocated (RES). It also includes:

- Mapped shared libraries.
- Reserved but unused memory regions.
- Memory-mapped files.
- The Python interpreter itself allocates a significant amount of memory during startup for internal data structures, libraries, and the garbage collector. This adds to the high VIRT.

As for How can I limit the real memory usage of the process?:

1. Use RLIMIT_RSS which is exactly physical memory usage, but it may not be available in modern system, it depends on platform.
2. Use RLIMIT_AS with additional space meant for libraries and other extras
3. Since you are in kubernetes you can use pod configuration:

```
resources:
  limits:
    memory: ""500Mi""
  requests:
    memory: ""500Mi""
```

this limits both swap,physical memory

4. Last, you can do a loop in async checking memory like:

```
process = psutil.Process(os.getpid())
mem_info = process.memory_info()
rss = mem_info.rss  # Resident Set Size (physical memory used)
```",2025-01-28T15:34:40,2025-01-28T15:04:51
79393702,generate core dump when native library crashes in java application running in kubernetes,"I have a java service which is running in k8s,uses a native library (written in c++). I have set the core dump path to persistent mount NFS (with this command: `echo ""/nfs/core_dumps/core.%t.%p"" | tee /proc/sys/kernel/core_pattern`). The core dump was not generating when the native library crashes, the last logs are as follows,

```
> > #
> # A fatal error has been detected by the Java Runtime Environment:
> #
> #  SIGSEGV (0xb) at pc=0x00007efde450e688, pid=1, tid=103
> #
> # JRE version: OpenJDK Runtime Environment Temurin-17.0.13+11 (17.0.13+11) (build 17.0.13+11)
> # Java VM: OpenJDK 64-Bit Server VM Temurin-17.0.13+11 (17.0.13+11, mixed mode, tiered, compressed oops, compressed class ptrs, g1 gc,
> linux-amd64)
> # Problematic frame:
> # C  [lib.Converter.so.1.0+0xee688]  SetConverterMode+0xc8
> #
> # Core dump will be written. Default location: /nfs/core_dumps/core.%t.1
> #
> # JFR recording file will be written. Location: //hs_err_pid1.jfr
> #
> # An error report file with more information is saved as:
> # /nfs/my-folder/core_dump_%t.log
> #
> # If you would like to submit a bug report, please visit:
> #   https://github.com/adoptium/adoptium-support/issues
> # The crash happened outside the Java Virtual Machine in native code.
> # See problematic frame for where to report the bug.
> #
>
> [error occurred during error reporting (), id 0xb, SIGSEGV (0xb) at
> pc=0x00007eff17d249a2]
```

I was calling a native method (that crashes with SIGSEGV signal), in this scenario the
Core dump is expected to be generated (the logs says, core dump will be written at `/nfs/core_dumps`) but actually it is not generating (i could confirm this, as after the crash, pod restarted and I logged into the container shell and could see no core file under the nfs(persistent) folder)","java, kubernetes, shared-libraries, native, coredump",79729440.0,"In order to generate the coredump, you need to mount the

```
/nfs/core_dumps/
```

folder to the host VM. you need to edit you pod deployment file to mount the host path to the

```
/nfs/core_dumps/
```

as once the pod restarts, the pod in which the core-dump happened does not exists anymore so please mount the directory to host VM",2025-08-08T07:16:13,2025-01-28T11:53:48
79393643,Retrieve programmatically JSON Schema of a Resource in a Live Kubernetes Cluster,"I want to programmatically retrieve the JSON Schema of the NetworkPolicy resource from a live Kubernetes cluster.

Although I found methods to convert a struct into a JSON Schema, my application will run on multiple clusters, and the schema may vary between clusters (e.g., due to different Kubernetes versions).

Is there a way to dynamically fetch the JSON Schema for the NetworkPolicy resource directly from the cluster at runtime?","go, kubernetes, kubernetes-networkpolicy",,,,2025-01-28T11:36:57
79393586,kubectl patch deployment to new configmap not working,"I have two kube configmaps, one of them is being used in a deployment. The config maps, and the pod deployment are below

```
File : configmap.yml

apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-url-config
data:
    base-url: ""http://api.infra.com:80/Test""

---

apiVersion: v1
kind: ConfigMap
metadata:
  name: actual-url-config
data:
    base-url: ""http://api.infra.com:80""

---

File : deployment.yml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-api
  annotations:
    kubernetes.twilio.com/service-name: test-api
spec:
  replicas: 1
  template:
    ......
    ......
    spec:
      serviceAccountName: test-sa-account
      containers:
        - name: test-api
          env:
            - name: API_BASE_URL
              valueFrom:
                configMapKeyRef: // USING THE CONFIGMAP VALUE
                  name: mock-url-config
                  key: base-url
          image: ......
    ..........
```

When I run this deployment, the container `test-api` is deployed with the `mock-url-config` configmap. After my tests are run, I want to update the container to use a different configMap (`actual-url-config`). For that, Im using a kube job to do a kube patch and restart the pods so that the restarted containers use the new configmap. The job is defined as follows:

```
File : configmap.yml

apiVersion: batch/v1
kind: Job
metadata:
  name: apply-actual-url
  annotations:
    argocd.argoproj.io/hook: PostSync
    argocd.argoproj.io/hook-delete-policy: BeforeHookCreation
    argocd.argoproj.io/sync-wave: ""1""
spec:
  template:
    spec:
      serviceAccountName: test-sa-account
      containers:
        - name: kubectl
          image: <image>
          command: [""/bin/sh"", ""-c""]
          args:
            - |
              kubectl patch deployment test-api -n test-ns \
                --patch '{""spec"": {""template"": {""spec"": {""containers"": [{""name"": ""test-api"", ""env"": [{""name"": ""BASE_URL"", ""valueFrom"": {""configMapKeyRef"": {""name"": ""actual-url-config"", ""key"": ""base-url""}}}]}]}}}}' && \
              sleep 5 && \
              kubectl rollout restart deployment test-api -n test-ns
          volumeMounts:
            - name: config-volume
              mountPath: /config
          ......
      volumes:
        - name: config-volume
          configMap:
            name: actual-url-config
```

When this job is run, I see the patch being applied and the test-api pods being restarted. But I noticed the pods are again restarted with the old configmap(`mock-url-config`) instead of the new configmap(`actual-url-config`). Can I get some help with figuring out why the pod is being restarted with the old configmap even after applying the kubectl patch.","kubernetes, kubectl, configmap",,,,2025-01-28T11:13:35
79392767,helm remove double quote from arithmetic expression while templating to JSON,"1. helm configmap:

```
  apiVersion: v1
  kind: ConfigMap
  metadata:
      name: {{ .Release.Name }}-config
      {{- include ""commonMeta"" . | nindent 2 }}
  data:
      config.play.ts: |
        const config = {{ tpl (.Values.app.play | mustToPrettyJson) $ | indent 6 }}
        export default config;
```

1. values.yaml

```
   play:
      PORT: 1114
      PLAY_EXPIRES_IN_MS: 10 * 60 * 1000
      API_URL: ""https://stag.com/api""
```

1. helm template output:

```
  play:
      ""PORT"": 1114
      ""PLAY_EXPIRES_IN_MS"": ""10 * 60 * 1000""
      ""API_URL"": ""https://stag.com/api""
```

1. Expected output:

```
  play:
      ""PORT"": 1114
      ""PLAY_EXPIRES_IN_MS"": 10 * 60 * 1000
      ""API_URL"": ""https://stag.com/api""
```

While templating to JSON, it adds double quotes and displays as PLAY_EXPIRES_IN_MS: ""10 * 60 * 1000"". I need to pass the PLAY_EXPIRES_IN_MS value without double quote.

I tried with:

```
    {{ tpl (.Values.app.play | mustToPrettyJson) $ | indent 6 | replace ""\""10 * 60 * 1000\"""" ""10 * 60 * 1000"" }}
```

Problem is, value could be different in future so i need to have a dynamic logic remove double quote from any number as well as from any arithmetic expression.

This value ""PLAY_EXPIRES_IN_MS"": ""10 * 60 * 1000"", may get change to ""10 * 60 * 80 * 1000"".","kubernetes, kubernetes-helm",79393791.0,"I don't think this particular combination of automatic quoting and unquoting is possible.

JSON doesn't allow expressions.  `10 * 60 * 1000` is a valid Javascript expression, but if you were transporting that value in a JSON document, you'd have to first evaluate the expression and then include the result in the JSON; `""PLAY_EXPIRES_IN_MS"": 60000`.  In a Helm context, the thing this means is that `toJson` and its variants don't expect to generate expressions that shouldn't be quoted.

Both JSON and YAML have a basic notion of typing.  In YAML's standard rules, if a value has an unquoted value, it's a number if it can be parsed as a number and a string if not.  This means that, in your Helm values, `PLAY_EXPIRES_IN_MS` has a string value.  `toJson` will therefore serialize it as a string, including double quotes.

This setup doesn't have any way to automatically recognize that something isn't a number, but it is a Javascript expression that would produce a number.  If you can use any Javascript expression this gets even harder – is `Math.PI/2` a URL or an expression (both have dots and slashes)?

If you can't preëvaluate the millisecond value in your settings, I might just directly embed the Javascript fragment in the Helm values.  If you use YAML block-scalar syntax, you can embed a multi-line string in the values.

```
# values.yaml

app:
  # play holds a Javascript object that is the configuration.
  play: |
    {
      ""PORT"": 1114,
      ""PLAY_EXPIRES_IN_MS"": 10 * 60 * 1000,
      ""API_URL"": ""https://stag.com/api""
    }
```

```
# configmap.yaml
  data:
      config.play.ts: |
        const config = {{ tpl .Values.app.play $ | indent 8 | trim }};
        export default config;
```

(The ConfigMap is basically the same except it removes the `mustToPrettyJson call; I've also tweaked the indentation and added a cosmetic `trim`.)

If this is a fixed combination of settings, another is to handle each value separately.  This would let you manually handle the quoting for the option that needs it.

```
# values.yaml

app:
  play:
    port: 1114,
    playExpiresInMs: 10 * 60 * 1000,
    apiUrl: https://stag.com/api
```

```
# configmap.yaml
  data:
      config.play.ts: |
        const config = {
          ""PORT"": {{ .Values.app.play.port }},
          ""PLAY_EXPIRES_IN_MS"": {{ .Values.app.play.playExpiresInMs }},
          ""API_URL"": ""{{ .Values.app.play.apiUrl }}""
        };
        export default config;
```

Note here that I've explicitly quoted the last URL value (`{{ ...apiUrl | toJson }}` would have the same effect and be more robust), and I *haven't* quoted the preceding value even though it's internally a string type.",2025-01-28T12:22:22,2025-01-28T04:48:17
79392445,Spark 3.1.2: Kubernetes Client Closed Warning Leading to Executor Task Hanging – How to Fix or Work Around?,"I’m using Spark version 3.1.2 in Kubernetes. Occasionally, I encounter an issue where logs show a warning ""WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed,"" followed by the application hanging. One executor task keeps spinning indefinitely, and this seems similar to [SPARK-33349](https://issues.apache.org/jira/browse/SPARK-33349).

Later Spark releases mention an upgrade to the Kubernetes client, but it’s unclear whether this fixes my issue. Upgrading Spark is a complex process, and I’d like to know:

1. Which version specifically fixes this issue?
2. Is there any workaround with settings to solve this issue without upgrading?","apache-spark, kubernetes",79438898.0,"I solved the hanging issue by applying persist(MEMORY_AND_DISK) to the DataFrames retrieved from the database. It seems that the data was not being cached or was getting lost in memory, which resulted in additional queries to the database—one of which would hang. It’s unclear whether the hanging is related to the Kubernetes client eventually failing with the error 'too old resource version'. I hope this information is helpful to someone.",2025-02-14T09:27:55,2025-01-27T23:39:54
79392070,ScalingModifiers not working in KEDA ScaledObject,"I am using KEDA scaledObject for scaling my pods based on the triggers. But I would like interrupt resources scaled by the triggers using ScalingModifiers if the Utilization is not enough. For example I have following two triggers for my scaled

```
triggers:
    - metadata:
        value: '75'
      metricType: Utilization
      name: ""one""
      type: cpu
    - metadata:
        desiredReplicas: '5'
        end: 20 8 * * *
        start: 10 8 * * *
      type: cron
```

So in above example the desiredReplicas will be 5 during 8.10am to 8.20am. But I would like to make sure if CPU Utilization is less than 75 during 8.10am to 8.20am then I would like to set desiredReplicas to 3. So I am trying to use following scalingModifier solution.

```
scalingModifiers:
        formula: ""one < 75 ? 1 : 0""
        target: ""3""
        activationTarget: ""1""
        metricType: ""Utilization""
```

But I get an error `error validating formula in ScalingModifiers invalid argument for float(one)`
I am not sure why it is giving error on my trigger name `one`. Even if I change the name of the trigger still it gives the same error.

Also if you have another solution for above use case feel free to suggest one. Your response is greatly appreciated.","kubernetes, keda, keda-scaledobject",79395161.0,"Based on the [Pull Request](https://github.com/kedacore/keda-docs/pull/1246) in Github community from KEDA releases v.2.13.0 concepts [casting 'float'](https://github.com/kedacore/keda-docs/commit/754943c60bcf0f90a20463d9805d2a299b0c12c2#diff-30ce2091937fb966578e23f7cbf86e44f757337047bda64841b891f43300428f) before returning the result is a must if a ternary operator result is ‘any’ as per [experimental scaling modifier](https://keda.sh/docs/2.12/concepts/scaling-deployments/#scaling-modifiers-experimental).

```
scalingModifiers:
    formula: ""float(one < 75 ? 1 : 0)""
    target: ""3""
    activationTarget: ""1""
    metricType: ""Utilization""
```

You may check your KEDA version using [kubectl command](https://kubernetes.io/docs/reference/kubectl/quick-reference/):

```
kubectl get deployment keda-operator -n keda -o=jsonpath='{.spec.template.spec.containers[0].image}'
```",2025-01-28T20:53:05,2025-01-27T20:11:25
79391422,How do I automatically retry a request in traefik when the downstream service isn&#39;t yet ready,"I've configured Traefik within a Kubernetes (k8s) cluster as the ingress. However, I have some legacy containers that are being exposed that don't behave as well as one would want from a modern containerised application. I would like to be able to configure a Traefik middleware such that it will retry when the downstream service isn't yet ready.","kubernetes, traefik, traefik-ingress, traefik-middleware, traefik-routers",79391841.0,You can probably use the Docker container health check. Traefik will only forward requests to healthy containers. Healthchecks can be defined as addition in a compose file.,2025-01-27T18:41:13,2025-01-27T16:08:49
79391422,How do I automatically retry a request in traefik when the downstream service isn&#39;t yet ready,"I've configured Traefik within a Kubernetes (k8s) cluster as the ingress. However, I have some legacy containers that are being exposed that don't behave as well as one would want from a modern containerised application. I would like to be able to configure a Traefik middleware such that it will retry when the downstream service isn't yet ready.","kubernetes, traefik, traefik-ingress, traefik-middleware, traefik-routers",79391423.0,"One would naively have expected the [retry middleware](https://doc.traefik.io/traefik/middlewares/http/retry/) to satisfy this requirement. Unfortunately (and by design) this does not work as it appears to receive a 503 status code from the backend service and, as is clearly stated in the documentation, treats any response whatsoever from downstream services as a non-retryable event.

To navigate around this, I used the [error middleware](https://doc.traefik.io/traefik/middlewares/http/errorpages/) instead. With this, I also provided a deployment (with associated service) in my cluster/namespace that was capable of serving a static html page that automatically refreshed e.g. some html that contained:

```
<meta http-equiv=""refresh"" content=""5"">
```

My middleware configuration looked like:

```
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: retry-on-503
spec:
  errors:
    status:
      - ""503""
    query: /retry.html
    service:
      name: staticsite
      port: 80
```

Whilst not ideal as the retry is exposed to the client, this works for my specific needs.",2025-01-27T16:08:49,2025-01-27T16:08:49
79391154,Why do i need keepalived in kubernetes?,"I have got a Kubernetes cluster with nginx installed.
Nginx makes K8s fault tolerant. In case of a K8s component failure, the standby one will be taken.

So what is the point of keepalived?","kubernetes, high-availability, keepalived",79394378.0,"One thing you might need to consider is that they both are performing similar functions. Both NGINX and Keepalived provide similar functionality in terms of failover, but at different layers.

While NGINX handles application-level failover and load balancing, Keepalived manages network-level failover with a Virtual IP (VIP).

In a setup where both are used, they might overlap, but Keepalived is more focused on the availability of the IP address, while NGINX ensures smooth traffic routing at the application layer. If you're already using NGINX effectively for fault tolerance, Keepalived might be redundant unless you specifically need the network-level failover.

Together, I believe they provide both network and application-level fault tolerance.",2025-01-28T15:37:35,2025-01-27T14:34:39
79391154,Why do i need keepalived in kubernetes?,"I have got a Kubernetes cluster with nginx installed.
Nginx makes K8s fault tolerant. In case of a K8s component failure, the standby one will be taken.

So what is the point of keepalived?","kubernetes, high-availability, keepalived",79391896.0,"Keepalived has [three components](https://docs.nginx.com/nginx/admin-guide/high-availability/ha-keepalived/#:%7E:text=Deployment%20Guides.-,High%20Availability%20Support%20Based%20on%20keepalived,-NGINX%20Plus%20R6) that supports active-passive high-availability setup which are:

- The daemon for Linux servers.
- Ensuring services remain online even in the event of server failures by implementing Virtual Router Redundancy Protocol (VRRP) wherein backup node listens for VRRP advertisement packets from the primary node, if it does not receive, the backup node takes over as primary and assigns the configured VIPs to itself.
- Configured number of health-checks for primary node failures keepalived reassigns virtual IP address from primary node to passive node.

The [main goal of this project](https://www.keepalived.org/index.html) is to provide simple and robust facilities for load balancing and high-availability Linux based infrastructures.",2025-01-27T19:02:06,2025-01-27T14:34:39
79389782,Failing to connect to https://registry.k8s.io/ from inside the minikube VM,"I'm getting this error while running ""minikube start"". I came across the below link while searching for a solution. I don't know how to set up a proxy, where do I look for the values to be used in the command to set up the proxy? I'm using a Windows machine.

[https://minikube.sigs.k8s.io/docs/handbook/vpn_and_proxy/](https://minikube.sigs.k8s.io/docs/handbook/vpn_and_proxy/)","docker, kubernetes, minikube, http-proxy",,,,2025-01-27T05:09:56
79388115,Invalid CSRF Nodejs React,"I have a Nodejs app with React in front. I implemented my csrf like:

```
// initial-csrf.js

var csrf = require(""csurf"");

const csrfProtection = csrf({});

module.exports = {
 csrfProtection,
};

//Server.js
 app.use(csrfProtection); // It goes after app.use(appSession);

// Session controller

 const csrfToken = (req, res) => {
 res.send(req.csrfToken());
};

// Initial express session

 var appSession = session({
  store: new postgresSession({
  pool: sessionDBaccess,
  tableName: ""sessions"",
 }),
 name: ""mCookie"",
 secret: ""my-secret""
 resave: false,
 saveUninitialized: false,
 cookie: {
  maxAge: 1000 * 60 * 60 * 24 * 7,
  sameSite: true,
  secure: false,
 },
});
}
```

In React side :
whenever I send a request I do like :

```
 handleCreate() {
  const csrf_data = {};

    const csrf_response = callAxios(
    ""/api/sessions/csrfToken"",
    ""GET"",
    csrf_data,
    );
   csrf_response.then((csrf_res) => {
   const data = {
    // some data
  };
  const response = callAxios(""/api/sessions"", ""POST"", data, csrf_res.data);
  response.then((res) => {
    if (res) {
      if (res.data) {
        // Some functions
      } else {
        if (res.response) {
          // Some functions
        }
      }
     }
    });
   });
  }
```

This configuration works very well if there is one instance of the app (one pod). If I create multiple replicas from the app in my kubernetes cluster. I get CSRF invalid error. In fact I double check and my csrftoken is in session table in the database. Any ideas?","node.js, kubernetes, csrf",,,,2025-01-26T07:00:04
79388100,local knative endpoint to function routing,"So I've started to play around with knative. Installed it locally, just on my laptop. Just the bare minimum hello world project: k3s, knative-serving with kourier.

```
curl -sfL https://get.k3s.io | sh -s - --disable traefik
export KUBECONFIG=""$XDG_CONFIG_HOME/kube/config""
sudo k3s kubectl config view --raw > ""$KUBECONFIG""
export KNATIVE_VERSION=v1.16.0
kubectl apply -f https://github.com/knative/serving/releases/download/knative-$KNATIVE_VERSION/serving-crds.yaml
kubectl apply -f https://github.com/knative/serving/releases/download/knative-$KNATIVE_VERSION/serving-core.yaml
kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-$KNATIVE_VERSION/kourier.yaml
kubectl patch configmap/config-network \
  --namespace knative-serving \
  --type merge \
  --patch '{""data"":{""ingress-class"":""kourier.ingress.networking.knative.dev""}}'
```

Registry is just in a docker: `docker run -d -p 5000:5000 --restart always --name my-registry registry:2`

Then created a hello world function, deployed it and I can invoke it with `func invoke`.

I'm new to the k8s things. These networking things are hard. Also I can't setup the DNS on my router at home. Although the cluster has an external IP and port, I can't even call it locally with `curl -H ""Host: hello-world.default.svc.cluster.local"" http://localhost:32198`.

But skipping this and I would like to go straight to serving the functions on endpoints like `/hello-world` would go straight to the hello-world function.

The ideal state would be just `curl http://localhost:32198/hello-world`

(even better would be `curl https://localhost/api/v1/hello-world`, but let's keep things simple)","kubernetes, knative",,,,2025-01-26T06:45:25
79387049,Kafka Bitnami (KRaft) problem with Kubernetes deployment,"I have problem with **deployment of Kafka Bitnami type KRaft on Kubernetes**. Deployment file:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: kafka
  name: kafka
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - env:
            - name: ALLOW_PLAINTEXT_LISTENER
              value: ""yes""
            - name: KAFKA_BROKER_ID
              value: ""1""
            - name: KAFKA_CFG_ADVERTISED_LISTENERS
              value: PLAINTEXT://kafka:9092
            - name: KAFKA_CFG_CONTROLLER_LISTENER_NAMES
              value: CONTROLLER
            - name: KAFKA_CFG_CONTROLLER_QUORUM_VOTERS
              value: 1@kafka:9093
            - name: KAFKA_CFG_LISTENERS
              value: PLAINTEXT://:9092,CONTROLLER://:9093
            - name: KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP
              value: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
            - name: KAFKA_CFG_NODE_ID
              value: ""1""
            - name: KAFKA_CFG_PROCESS_ROLES
              value: broker,controller
            - name: KAFKA_ENABLE_KRAFT
              value: ""yes""
            - name: KAFKA_KRAFT_CLUSTER_ID
              value: MkU3OEVBNTcwNTJENDM2Qk
          image: bitnami/kafka:3.9.0
          name: kafka
          ports:
            - containerPort: 9092
              protocol: TCP
      restartPolicy: Always

---

apiVersion: v1
kind: Service
metadata:
  labels:
    app: kafka
  name: kafka
spec:
  ports:
    - name: ""9092""
      port: 9092
      targetPort: 9092
  selector:
    app: kafka
```

As a result **Kafka is restarted** many times. Logs from Kubernetes:

```
[2025-01-25 08:04:58,857] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-01-25 08:04:58,857] INFO [BrokerLifecycleManager id=1] Unable to register the broker because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-01-25 08:04:58,963] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-01-25 08:04:58,965] ERROR [ControllerRegistrationManager id=1 incarnation=zHQ1Oie1TRW4qje9o_Pmow] RegistrationResponseHandler: channel manager timed out before sending the request. (kafka.server.ControllerRegistrationManager)
[2025-01-25 08:04:59,066] INFO [ControllerRegistrationManager id=1 incarnation=zHQ1Oie1TRW4qje9o_Pmow] maybeSendControllerRegistration: waiting for the previous RPC to complete. (kafka.server.ControllerRegistrationManager)
[2025-01-25 08:05:03,460] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-01-25 08:05:03,460] INFO [BrokerLifecycleManager id=1] Unable to register the broker because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-01-25 08:05:08,169] INFO [NodeToControllerChannelManager id=1 name=heartbeat] Disconnecting from node 1 due to socket connection setup timeout. The timeout value is 9499 ms. (org.apache.kafka.clients.NetworkClient)
[2025-01-25 08:05:08,172] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-01-25 08:05:08,172] INFO [BrokerLifecycleManager id=1] Unable to register the broker because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
[2025-01-25 08:05:08,977] INFO [NodeToControllerChannelManager id=1 name=registration] Disconnecting from node 1 due to socket connection setup timeout. The timeout value is 11048 ms. (org.apache.kafka.clients.NetworkClient)
[2025-01-25 08:05:13,079] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
[2025-01-25 08:05:13,079] INFO [BrokerLifecycleManager id=1] Unable to register the broker because the RPC got timed out before it could be sent. (kafka.server.BrokerLifecycleManager)
```

As you can see there is one error with message: **channel manager timed out before sending the request**.

The same configuration as **Docker Compose works fine**:

```
  kafka:
    image: bitnami/kafka:3.9.0
    container_name: kafka
    ports:
      - 9092:9092
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092
      - KAFKA_BROKER_ID=1
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      - ALLOW_PLAINTEXT_LISTENER=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_KRAFT_CLUSTER_ID=MkU3OEVBNTcwNTJENDM2Qk
    networks:
      - network
```

Tested on:

- Windows 11 OS
- Docker 27.4.0
- Kubernetes type Kind 0.26.0

Source Code:

- Github: [https://github.com/wisniewskikr/chrisblog-it-java-springboot/blob/main/arichitectures/event-driven/stream/kafka/kraft/springboot3-eventdriven-kafka-bitnami/k8s/kafka-bitnami/infrastructure/kafka.yaml](https://github.com/wisniewskikr/chrisblog-it-java-springboot/blob/main/arichitectures/event-driven/stream/kafka/kraft/springboot3-eventdriven-kafka-bitnami/k8s/kafka-bitnami/infrastructure/kafka.yaml)

Any idea **how can I resovle this issue**? Why **docker compose** configuratin works file but **kubernetes** configuration doesn't?","docker, kubernetes, apache-kafka, bitnami-kafka",,,,2025-01-25T15:59:27
79386606,"NiFi auto creating the Execute Processors with additional wget, run commands","At my clients infra, I had deployed NiFi 1.9, it has custom processors along with Kakfa producer to push few logs to logs stash. Which is currently stopped.

While monitoring the Nifi, I found randomly hundreds of Execute Processors got created with  commands like wget, run etc. These processors are not created by any of the NiFi admin users and are getting auto generated.

Check below images of the auto generated processors (40+ are auto created) and the setting > command of one of the processor.

[![enter image description here](https://i.sstatic.net/9QnSv3FK.png)](https://i.sstatic.net/9QnSv3FK.png)

[![enter image description here](https://i.sstatic.net/M6yjEX1p.png)](https://i.sstatic.net/M6yjEX1p.png)

Does anyone seen similar issue on NiFi ? Any help will be grateful.","kubernetes, apache-nifi",,,,2025-01-25T11:16:04
79385754,Deploying using Spinnaker other than default,"I'm trying to deploy on the k8s using Spinnaker where I've the setting nameSpaceOverride as enabled. This is now becoming a problem as for a resource (just 1) I've a specific namespace, which is mentioned in the yaml file and Spinnaker is overriding this to default. While rest should go to the default namespace.

What is the ideal way to do it without lot of changes? Helm3 is used for chart generation by Spinnaker.","kubernetes, kubernetes-helm, cicd, spinnaker",,,,2025-01-24T22:05:54
79385062,How to fix CSR denial by csr-auto-approver?,"In my k8s csr-auto-approver is used.
After restarting kubelet, it started to deny CSR requests with the following reason:

> Denying kubelet-serving CSR. Regex/IP checks failed. Reason:One of the SAN IP addresses, , is not contained in the set of resolved IP addresses, denying the CSR

How approver performs this check? And how to fix it?",kubernetes,79385503.0,"The reason for denials is missing IP address based on the error you’ve mentioned. The approver checks if the IP addresses specified in CSR match the IP addresses of the node, if some requests are made without IP or there’s a mismatch it denies the request.

You need to check if your node IP address matches the CSR SAN field and you can update it with the correct one.

You can also approve the request manually. See this [documentation](https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/#approval-rejection-kubectl) for further information :

```
kubectl certificate approve <csr-name>
```

For additional Information see this documentation :

[https://github.com/postfinance/kubelet-csr-approver?tab=readme-ov-file#which-verifications-do-we-put-in-place-](https://github.com/postfinance/kubelet-csr-approver?tab=readme-ov-file#which-verifications-do-we-put-in-place-)",2025-01-24T20:03:26,2025-01-24T16:52:01
79384091,result.ReconcileAfter and envTest,"I develop a Kubernetes controller in Go.

I just learned that `Result.Requeue` will be deprecated [[1]](https://kubernetes.slack.com/archives/C02MRBMN00Z/p1737642883339899?thread_ts=1737630584.060499&cid=C02MRBMN00Z).

Now I want to use `Result.RequeueAfter`. But now my envTests fail.

In the envTest I use `Eventually()` with a timeout of two seconds. This worked fine up to now.

`Result.Requeue` had the benefit, that there was exponential back-off.

Now I need to return a value in `Result.RequeueAfter`.

When I take a short value, then this might create useless load (it can take up to 10 minutes for my external system to be in the desired state).

When I take a long value, then the time the user needs to wait might be too long.

In my case I think a value of 10 seconds is fine. But I do not want to wait 10 seconds in a test.

How would you solve that?","go, kubernetes, controller-runtime",,,,2025-01-24T10:59:04
79384015,How to Configure Horizontal Pod Autoscaler (HPA) on EKS Fargate with Metrics Server - Getting 403 Forbidden Error,"I am trying to configure the Horizontal Pod Autoscaler (HPA) on my EKS Fargate setup, but I am running into an issue where the Metrics Server is returning a 403 Forbidden error when it tries to scrape metrics from my Fargate pods.

What I've done so far:
I have a working EKS Fargate cluster.
I have installed the Metrics Server on EKS.
I have configured the Horizontal Pod Autoscaler (HPA) based on CPU utilization.
I ensured that the Metrics Server has the correct arguments to scrape pod-level metrics.
Metrics Server Configuration:
I have updated the Metrics Server deployment with the following arguments:

yaml
Copy
Edit
args:

- --cert-dir=/tmp
- --secure-port=10250
- --metric-resolution=15s
- --kubelet-insecure-tls
- --kubelet-preferred-address-types=InternalIP
- The Issue:
Despite the above configuration, I am getting a 403 Forbidden error in the Metrics Server logs when trying to scrape metrics:

Troubleshooting Steps Taken:
I checked the Metrics Server logs and verified that the server is running securely on port 4443.
I verified that the HPA is correctly set up to scale based on CPU utilization.
Error Details:
The key error I see is the 403 Forbidden:

This indicates that the Metrics Server is trying to scrape the Kubelet, but the request is being blocked.","amazon-web-services, kubernetes, amazon-eks, aws-fargate, hpa",,,,2025-01-24T10:35:40
79383327,I want to run a command in host from inside the pod in a kubernetes cluster,"The pod base image doesn't have tools like netstat or ss in that so I need the output of that command from inside the pod by running the command in the host is there a way to do that
Can the answer be given with some examples so

Can the answer be given with some examples","kubernetes, netstat",,,,2025-01-24T05:36:43
79380167,How to fix disk-pressure on k8s node?,"K8s 1.24.17
Containerd as a container runtime(instead of docker)
How to fix disk-pressure taint on node?
I tried:

> sudo crictl rmi --prune && sudo crictl rm -a

but still nothing happened, only a little bit of space emptied.
Then i tried:

> kubectl taint node foo-node1 node.kubernetes.io/disk-pressure-

After several seconds running **kubectl describe node foo-node1** but my node is still has disk-pressure taint

How to fix it?

Result of **df -h**
[![enter image description here](https://i.sstatic.net/Z4fVvFBm.png)](https://i.sstatic.net/Z4fVvFBm.png)","kubernetes, containerd",79381108.0,"If `crictl` command doesn’t resolve your issue try the below steps to fix disk pressure taint  issue:

First, Detect node pressure by running ` $ kubectl get nodes` command , Any nodes whose status includes `DiskPressure=True` are under disk pressure .

Run the command `$kubectl describe pod <pod-name\>` to get more information about each of the Pods running on the node.

In the Volume section, check which PVCs (if any) the Pod is using & then check storage resources mapped to the PVC to figure out which data actually exists in them.

Verify if log files are excessively large, which can trigger disk pressure because K8s is not supporting log rotation and they can grow to hundreds of Gigs.

Use `lsof` to show the list of used files and sort them at [https://unix.stackexchange.com/a/382696/380398](https://unix.stackexchange.com/a/382696/380398)

In addition to the disk space consumed by individual Pods, Kubernetes components can also consume additional storage space on a node.

The exact storage paths vary between distributions and container runtimes. Kubernetes typically uses  **directory /var** and in subdirectories like **/var/lib** and **/var/lib/containers** locations to store data like container images, which could also be sucking up disk space and causing node disk pressure issues.

Adding storage capacity to a node is also the one way to resolve node disk pressure.

After cleanup try running the below command to remove the taint for disk pressure.

```
$ kubectl taint nodes <nodename> node.kubernetes.io/disk-pressure-
```",2025-01-23T12:48:06,2025-01-23T07:27:34
79379986,Overriding Service Account: default and enforcing GOOGLE_APPLICATION_CREDENTIALS,"I have env set in my pod as
`GOOGLE_APPLICATION_CREDENTIALS:  /tmp/sa.json`
This is the same pod that has my Java/Spring Boot app that runs. To verify certain things, I kept this file as empty. It does not exist actually. Just the env is pointing to it. However, I know that a default SA is injected into the POD. I do see this in describe pod
`Service Account:  default`

The issue that when I make a rest api call that goes to this POD it is honoring it and fetching data from BQ. The default SA account which has the permissions seems to be kicking in. I had hoped to get access denied.

How do I enforce the env to take precedence?","spring-boot, kubernetes, google-cloud-platform",,,,2025-01-23T06:06:24
79379779,Nifi 1.9 FileSystemRepository Unable to write to container default due to archive file size constraints; waiting for archive cleanup,"I am using Nifi 1.9 from past 3 years on Docker instance, in this NiFi workflow, I have custom processors too. Recently we moved to Kubernetes instance. On Kubernetes infra after 1 month of testing I started getting below issue.

```
o.a.n.c.repository.FileSystemRepository Unable to write to container default due to archive file size constraints; waiting for archive cleanup
```

Based on this error, I check the nifi config of previous working Docker instance with the new Kubernetes instance, but both were similar. So I started making few alteration in nifi.properties on the Kubernetes instance, below are 2 changes I did and check the flow.

```
nifi.flow.configuration.archive.max.storage=2048 MB
```

So first I increased the archive storage from default 500MB to 2 GB. On restarting the Nifi it worked for initial 30 mins later again started giving the FileSystemRepository issue. So I made second change of disabling the archive by changing below property

```
nifi.flow.configuration.archive.enabled=false
```

This too had same effect of running initial 30 mins and later started giving the FileSystemRepository issue.

Is there any server level issue or any issue with the Nifi configuration, or should is monitor any processor on the NiFi Ui?","kubernetes, apache-nifi",79385160.0,"[Update] After struggling for 2 days, I figured out the kubernetes has it own Memory and CPU allocation, which initially was 2 core and 5GB.
After increasing it to 4 core 8GB, the NIFI is working without any issue.

If anyone find any other answer to this issue, kindly share it as this was my try error approach, which is working for now but may fail later.",2025-01-24T17:28:33,2025-01-23T03:43:50
79378839,Sample/Reference application using spring boot stack similar to eshopOnContainers application,"Is there a Sample/Reference application using spring boot stack similar to eshopOnContainers application?

eshopOnContaines application from microsoft is a complete suite of different services implemented with polyglot.","spring-boot, kubernetes, docker-compose, microservices, observability",,,,2025-01-22T18:13:10
79378530,Airflow KubernetesPodOperator cannot start Dask client because multiprocess,"In an Airflow pipeline I want to run a KubernetesPodOperator using the `@task.kubernetes` operator calling a custom method.

```
@task.kubernetes(
    task_id=""parse-legislation-nl"",
    name=""legislation-nl-parse-job"",
    **default_pod_args,
)
def parse_legislation_task(**kwargs):
    from my_module import my_dask_method

    my_dask_method.start(**kwargs)
```

The method starts a Dask client:

```
dask_client = Client(address=None)
```

But that returns the following log error:

```
2025-01-22 16:08:14,001 - ERROR - Failed to start process
Traceback (most recent call last):
  File ""/usr/local/lib/python3.12/site-packages/distributed/nanny.py"", line 452, in instantiate
    result = await self.process.start()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/distributed/nanny.py"", line 750, in start
    await self.process.start()
  File ""/usr/local/lib/python3.12/site-packages/distributed/process.py"", line 55, in _call_and_set_future
    res = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/site-packages/distributed/process.py"", line 215, in _start
    process.start()
  File ""/usr/local/lib/python3.12/multiprocessing/process.py"", line 121, in start
    self._popen = self._Popen(self)
                  ^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/multiprocessing/context.py"", line 289, in _Popen
    return Popen(process_obj)
           ^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/multiprocessing/popen_spawn_posix.py"", line 32, in __init__
    super().__init__(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_fork.py"", line 19, in __init__
    self._launch(process_obj)
  File ""/usr/local/lib/python3.12/multiprocessing/popen_spawn_posix.py"", line 42, in _launch
    prep_data = spawn.get_preparation_data(process_obj._name)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/usr/local/lib/python3.12/multiprocessing/spawn.py"", line 164, in get_preparation_data
    _check_not_importing_main()
  File ""/usr/local/lib/python3.12/multiprocessing/spawn.py"", line 140, in _check_not_importing_main
    raise RuntimeError('''
RuntimeError:
        An attempt has been made to start a new process before the
        current process has finished its bootstrapping phase.

        This probably means that you are not using fork to start your
        child processes and you have forgotten to use the proper idiom
        in the main module:

            if __name__ == '__main__':
                freeze_support()
                ...

        The ""freeze_support()"" line can be omitted if the program
        is not going to be frozen to produce an executable.

        To fix this issue, refer to the ""Safe importing of main module""
        section in https://docs.python.org/3/library/multiprocessing.html
```","kubernetes, airflow, dask",,,,2025-01-22T16:34:36
79377914,Difference between PodDisruptionBudget and minimum replicas in HPA,"Kubernetes has introduced a [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb), which prevents the functionality of our applications from being disrupted through manual and automatic vertical (node) scaling.

Let's assume we're using a [HozirontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/). What would be the value of using a PDB additionally? What is the difference between [PodDisruptionBudget](https://kubernetes.io/docs/tasks/run-application/configure-pdb) `minAvailable` and [HozirontalPodAutoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) `minReplicas`?","kubernetes, horizontal-pod-autoscaling",79378089.0,"They do different things.  The HorizontalPodAutoscaler controls the Deployment's `replicas:` count based on some metric.  The PodDisruptionBudget principally affects whether or not a node can be taken offline.

Let's imagine that you have a cloud-hosted cluster, and your cluster administrator wants to upgrade the base operating system on the cluster nodes.  To do this they need to ""cordon"" each node – prevent new Pods from being scheduled there – and ""drain"" it – relocate all of the Pods that are located there – before they can restart it.

Meanwhile, let's also imagine that your application Deployment has `replicas: 3`, a PodDisruptionBudget that specifies `minReplicas: 2`, and it happens that two of the replicas are on the same node that's being terminated.  The drain action will delete these replicas and recreate them on a different node.  The PDB will keep one of the replicas running until the other has been deleted and recreated.

```
+--------------+   +--------------+
| Healthy node |   | Drained node |
+--------------+   +--------------+
| pod-1        |   | pod-2        | <-- PDB keeps this replica alive
|              |   |~pod-3~~~~~~~~|     while this Pod is deleted and recreated
+--------------+   +--------------+
```

You ""should"" have a PodDisruptionBudget, but outside of scenarios like this where a cluster administrator is intentionally deleting nodes, you won't notice it.

The PDB is independent of the HPA.  The one interaction I'd note is that it doesn't make sense to set the PDB `minReplicas:` higher than the HPA `minReplicas:` (if you could tolerate autoscaling down to 1 replica, you could support being on 1 replica because of an outage too).  If the PDB `minReplicas:` is a percentage, it is a percentage of the Deployment's `replicas:`, which means it will track changes the HPA makes.",2025-01-22T14:23:12,2025-01-22T13:40:27
79376974,Argo Image Updater: filter-tag not working,"We are using ArgoCD and Argo Image Updater on AKS cluster and everything looks perfect except the fact that I'm unable to force Argo Image Updater to filter only specific tags (In my case only tags that have SNAPSHOT in it's name)

These are annotations that were added in Application configuration:

```
""argocd-image-updater.argoproj.io/git-branch"" =  ""main""
""argocd-image-updater.argoproj.io/image-list""  = ""rtm=<redacted>.azurecr.io/rtm""
""argocd-image-updater.argoproj.io/rtm.filter-tag""       = ""^.*SNAPSHOT.*$""
""argocd-image-updater.argoproj.io/rtm.pull-secret""  = ""pullsecret:argo/acr-credentials""
""argocd-image-updater.argoproj.io/write-back-method""      = ""git""
```

I even tried to use regexp: ^.*SNAPSHOT.*$ as a value of argocd-image-updater.argoproj.io/rtm.filter-tag but no luck, Argo Image Updater is always pulling the latest tag whatever I set. I also tried with annotation argocd-image-updater.argoproj.io/rtm.allow-tags but I'm getting the same results.

When checking Argo Image Updater pod logs it just says that it detected new image and it's updating SNAPSHOT version of image with the latest one.

Based on documentation I'm not missing anything but again maybe someone had the same issue and there is some additional annotation that needs to be added. I just wanna note that ArgoCD and Argo Image updater does work , the only thing that doesn't work (but it's crucial for project) is ability to make Argo Image Updater filter only specific tags.","azure, kubernetes, devops, argocd, argocd-image-updater",79495180.0,"From official documentation [https://argocd-image-updater.readthedocs.io/en/stable/basics/update-strategies/](https://argocd-image-updater.readthedocs.io/en/stable/basics/update-strategies/)

```
argocd-image-updater.argoproj.io/myimage.allow-tags: regexp:^[0-9a-f]{7}$
```

That means it must start with `regexp:…`

Also take a look at regexp syntax, cause `^` means input beginning, and `$` input ending, with this changes, your annotation would be:

```
argocd-image-updater.argoproj.io/rtm.allow-tags: regexp:.*SNAPSHOT.*
```",2025-03-08T23:26:55,2025-01-22T08:29:11
79376575,client-go batch/v1 apply vs update vs patch,"What are the differences between patch, apply, and update in [https://pkg.go.dev/k8s.io/client-go/kubernetes/typed/batch/v1](https://pkg.go.dev/k8s.io/client-go/kubernetes/typed/batch/v1)

I am working with Kubernetes client-go and managing Job objects.

I have come across three methods for updating resources: patch(), apply(), and update().

However, I am not entirely clear on their differences and best use cases.

I understand that patch and apply differ in whether they perform in-place modifications.

How does update behave compared to these?  Could someone explain when to use patch, apply, or update, ideally with practical examples?

Additionally, are there any caveats or best practices I should be aware of when working with Job resources in this context?","kubernetes, kubernetes-go-client",79377609.0,"There's a section [Updates to existing resources](https://kubernetes.io/docs/reference/using-api/api-concepts/#patch-and-apply) in the generic Kubernetes API documentation.  This notes that there are three ways to modify an existing object, which corresponds to these verbs:

1. **Update** takes an entire new object and stores it; but this can be bulky and can conflict if the object revision number is wrong
2. **Patch** takes only a diff and attempts to apply it; but you need to construct the diff and you may not notice concurrent changes
3. **Server-side apply** takes a partial object and updates those specific fields, tracking on the server side which controllers own which fields; but you can't update fields other processes or controllers own

Those three `client-go` methods correspond to these three actions.  You can see this in their method signatures:

```
Update(ctx context.Context, job *batchv1.Job, opts metav1.UpdateOptions)
//                          ^^^^^^^^^^^^^^^^ entire object

Patch(ctx context.Context, name string, pt types.PatchType,
      data []byte, opts metav1.PatchOptions, subresources ...string)
//    ^^^^^^^^^^^ arbitrary patch

Apply(ctx context.Context, job *applyconfigurationsbatchv1.JobApplyConfiguration, opts metav1.ApplyOptions)
//                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
//                         like batchv1.Job but every field is a pointer
```

Of these three, `Update()` is easiest to use.  You can `Get()` an object, modify it in your controller code, and `Update()` it with the changed object.  If there are multiple actors in your system then you can hit version conflicts.  [`retry.RetryOnConflict()`](https://pkg.go.dev/k8s.io/client-go@v0.32.1/util/retry#RetryOnConflict) might help, repeating the main loop if these change.

`Apply()` is a newer Kubernetes feature (requires Kubernetes 1.22; as of this writing current is 1.32, and all modern clusters should support it) and requires slightly more work to use.  In the [`metav1.ApplyOptions`](https://pkg.go.dev/k8s.io/apimachinery/pkg/apis/meta/v1#ApplyOptions) you must provide a `FieldManager` saying who you are, and you must fill in the `JobApplyConfiguration` as a separate object rather than mutating the object you have.

`Patch()` requires you to construct the diff by hand as a byte slice.

If you are using the [`controller-runtime`](https://pkg.go.dev/sigs.k8s.io/controller-runtime) library (maybe via Kubebuilder) then its operation is somewhat different.  In the [`client.Writer`](https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/client#Writer) interface, the `Patch()` method handles both ""patch"" and ""apply"" verbs.  You need to construct a [`client.Patch`](https://pkg.go.dev/sigs.k8s.io/controller-runtime@v0.20.0/pkg/client#Patch) object, but there are helper methods to do this.  For server-side apply, there does not appear to be an option to manage individual fields.

```
var job batchv1.Job
err := client.Get(ctx, ..., &job)

// Client-side apply: mutate the object, create a diff with the original,
// and Patch()
newJob := job.deepCopy()
modifyJob(newJob)
err = client.Patch(ctx, newJob, client.MergeFrom(job))

// Server-side apply: you don't control the field list
err = client.Patch(ctx, newJob, client.Apply, client.FieldOwner(""my-controller""))
```

In the controller I've written most recently, I use only `Update()`.  It works fine, and controller-runtime retries on error, but that means I do see frequent unsightly conflict errors in logs.  I should perhaps update this to use one of the `Patch()` mechanisms, server-side apply if you care about other controllers not modifying your fields and a plan patch if that doesn't matter to you.

> Are there any caveats or best practices I should be aware of when working with Job resources in this context?

Since a Job by default only runs once, you can't practically modify most of the fields in a Job.  Only a couple fields are specifically flagged in [the API docs](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/job-v1/) as immutable, but if you've created a Job and it's created its Pod, you can't usefully change the Pod template.",2025-01-22T11:54:41,2025-01-22T05:06:36
79375379,Cannot send HTTP request to pods from outside of cluster on minikube,"## SOLVED
I solved problem using minikube service k8s-go-rest Problem was tunneling

I am using WSL with Ubuntu on Windows 11. I am trying to send HTTP request using curl to pods from another shell within Ubuntu. My WSL version is at down.

```
wsl --version

WSL version: 2.3.26.0
Kernel version: 5.15.167.4-1
WSLg version: 1.0.65
MSRDC version: 1.2.5620
Direct3D version: 1.611.1-81528511
DXCore version: 10.0.26100.1-240331-1435.ge-release
Windows version: 10.0.26100.2894
```

I followed those steps:

```
kubectl create deployment k8s-go-rest-deployment --image=alptht/k8s-go-rest:multistage
```

then

```
kubectl expose deployment k8s-go-rest-deployment --port=8080 --type=NodePort
```

Checked it with

```
kubectl get deploy
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
k8s-go-rest-deployment   1/1     1            1           5m5s
```

```
kubectl get svc
NAME                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
k8s-go-rest-deployment   NodePort    10.96.131.39   <none>        8080:30387/TCP   22s
```

```
kubectl get pods
NAME                                      READY   STATUS    RESTARTS   AGE
k8s-go-rest-deployment-6d5456b464-69m8w   1/1     Running   0          112s
```

```
minikube ip
192.168.49.2
```

When I try to send request to
`curl 192.168.49.2:30387` request gets timeout.

I tried to send request to pods within minikube using ssh:

```
minikube ssh
```

then in shell

```
curl 10.96.131.39:8080
```

I get correct response as:
`Hostname : k8s-go-rest-deployment-6d5456b464-69m8w`

How can I find or fix problem? I can add more information if you need. Thanks","docker, kubernetes, curl, windows-subsystem-for-linux, minikube",79385277.0,I solved the problem using `minikube service k8s-go-rest`. Problem was tunneling.,2025-01-24T18:13:06,2025-01-21T17:26:24
79375047,How to test Kubernetes validation webhook with curl?,"I want to test a validating webhook with `curl`.

There is a port-forwarding to that service via `kubectl`.

I created `capi-cluster.yaml`.

But this fails:

```
curl --insecure -X POST -H ""Content-Type: application/json"" \
    --data-binary @capi-cluster.yaml \
    https://127.0.0.1:9443/validate-cluster-x-k8s-io-v1beta1-cluster
```

```
{""kind"":""Cluster"",""apiVersion"":""cluster.x-k8s.io/v1beta1"",""response"":{""uid"":"""",""allowed"":false,""status"":{""metadata"":{},""message"":""unknown operation \""\"""",""code"":400}}}
```

What needs to be changed to get it working?","kubernetes, curl, webhooks",79375067.0,"I found the answer:

I need to create a json file like this:

```
{
  ""kind"": ""AdmissionReview"",
  ""apiVersion"": ""admission.k8s.io/v1"",
  ""request"": {
    ""uid"": ""test-uid"",
    ""kind"": {
      ""group"": """",
      ""version"": ""v1"",
      ""kind"": ""Pod""
    },
    ""resource"": {
      ""group"": """",
      ""version"": ""v1"",
      ""resource"": ""pods""
    },
    ""namespace"": ""default"",
    ""operation"": ""CREATE"",
    ""object"": <RESOURCE_JSON>,
    ""oldObject"": null,
    ""dryRun"": false,
    ""options"": {
      ""apiVersion"": ""meta.k8s.io/v1"",
      ""kind"": ""CreateOptions""
    }
  }
}
```

Then convert my yaml to json with `yq -oj`, and insert it in above snippet.

Then it works:

```
curl --insecure -X POST -H ""Content-Type: application/json"" \
    --data-binary @t.json
    https://127.0.0.1:9443/validate-cluster-x-k8s-io-v1beta1-cluster
```

> {""kind"":""AdmissionReview"",""apiVersion"":""admission.k8s.io/v1"",""response"":{""uid"":""test-uid"",""allowed"":true,""status"":{""metadata"":{},""code"":200}}}",2025-01-21T15:55:30,2025-01-21T15:48:00
79371979,Property spring.config.import from application.properties is not overwritten by environement variables in Spring Boot 3 microservice application,"I have the project with following microservices:

- **FE**: Spring Boot 3 application which displays information to the user
- **BE**: Spring Boot 3 application which connects with DB and sends information to FE
- **MySql**: database which stores data
- **Config Sever**: Spring Cloud application which stores services configurations on Github

In FE and BE services I have configured property **spring.config.import** which connects these services with Config Server. For instance in BE project (springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config_BE) I have following **application.properties** file:

```
# Port
server.port=8081

# Service Name
spring.application.name=be

# Config Server
spring.config.import=configserver:http://localhost:8888
```

I try to overwrite this property in **docker-compose.yaml** file (**spring.config.import: configserver:http://config:8888**):

```
be:
    image: be-image:0.0.1
    container_name: be-container
    build:
      context: ./springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config_BE
      dockerfile: Dockerfile
    depends_on:
      config:
        condition: service_healthy
    ports:
      - 8081:8081
    environment:
      spring.config.import: configserver:http://config:8888
    healthcheck:
      test: [""CMD"", ""curl"", ""-f"", ""http://localhost:8081/actuator/health""]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - helloworld-network
```

I execute docker compose using following command:

```
docker-compose up -d --build
```

I get following error in docker desktop:

```
2025-01-20 17:02:36 Caused by: org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:8888/be/default"": Connection refused
2025-01-20 17:02:36     at org.springframework.web.client.RestTemplate.createResourceAccessException(RestTemplate.java:926)
2025-01-20 17:02:36     at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:906)
```

It seems that BE service tries to connect to **localhost** instead of **config** sevice. The same happens wit **FE** service and **Docker** and **Kubernetes** environment properties.

Tested on:

- Java: 23
- Spring Boot: 3.4.1
- Spring Cloud: 2024.0.0
- Mvn: 3.9.6

Link to Github repository: [https://github.com/wisniewskikr/chrisblog-it-cloud/tree/main/spring-cloud/config/springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config](https://github.com/wisniewskikr/chrisblog-it-cloud/tree/main/spring-cloud/config/springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config)

Any idea why property **spring.config.import** is not overwritten by environment variables from Docker, Docker Compose or Kubernetes?","java, spring-boot, docker, kubernetes, docker-compose",79372019.0,"The '.' in an environment variable that should overwrite a matching application.properties value should be replaced by an '_' and convert to uppercase, ie SPRING_CONFIG_IMPORT - see [here](https://docs.spring.io/spring-boot/docs/2.1.x/reference/html/boot-features-external-config.html#boot-features-external-config-relaxed-binding-from-environment-variables)",2025-01-20T16:28:01,2025-01-20T16:16:28
79371979,Property spring.config.import from application.properties is not overwritten by environement variables in Spring Boot 3 microservice application,"I have the project with following microservices:

- **FE**: Spring Boot 3 application which displays information to the user
- **BE**: Spring Boot 3 application which connects with DB and sends information to FE
- **MySql**: database which stores data
- **Config Sever**: Spring Cloud application which stores services configurations on Github

In FE and BE services I have configured property **spring.config.import** which connects these services with Config Server. For instance in BE project (springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config_BE) I have following **application.properties** file:

```
# Port
server.port=8081

# Service Name
spring.application.name=be

# Config Server
spring.config.import=configserver:http://localhost:8888
```

I try to overwrite this property in **docker-compose.yaml** file (**spring.config.import: configserver:http://config:8888**):

```
be:
    image: be-image:0.0.1
    container_name: be-container
    build:
      context: ./springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config_BE
      dockerfile: Dockerfile
    depends_on:
      config:
        condition: service_healthy
    ports:
      - 8081:8081
    environment:
      spring.config.import: configserver:http://config:8888
    healthcheck:
      test: [""CMD"", ""curl"", ""-f"", ""http://localhost:8081/actuator/health""]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 10s
    networks:
      - helloworld-network
```

I execute docker compose using following command:

```
docker-compose up -d --build
```

I get following error in docker desktop:

```
2025-01-20 17:02:36 Caused by: org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://localhost:8888/be/default"": Connection refused
2025-01-20 17:02:36     at org.springframework.web.client.RestTemplate.createResourceAccessException(RestTemplate.java:926)
2025-01-20 17:02:36     at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:906)
```

It seems that BE service tries to connect to **localhost** instead of **config** sevice. The same happens wit **FE** service and **Docker** and **Kubernetes** environment properties.

Tested on:

- Java: 23
- Spring Boot: 3.4.1
- Spring Cloud: 2024.0.0
- Mvn: 3.9.6

Link to Github repository: [https://github.com/wisniewskikr/chrisblog-it-cloud/tree/main/spring-cloud/config/springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config](https://github.com/wisniewskikr/chrisblog-it-cloud/tree/main/spring-cloud/config/springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config)

Any idea why property **spring.config.import** is not overwritten by environment variables from Docker, Docker Compose or Kubernetes?","java, spring-boot, docker, kubernetes, docker-compose",79372002.0,"I found workaround for this problem. If there is **NO** property **spring.config.import** in **application.properties** file then environment variables from **Docker**, **Docker Compose** and **Kubernetes** work file.

Problem is when I try to run this service manually from command line tool. I have to use command with parameter **spring-boot.run.arguments** then:

```
mvn -f ./springcloud-fe-thymeleaf-be-springboot-db-sql-mysql-config_BE spring-boot:run -Dspring-boot.run.arguments=""--spring.config.import=configserver:http://localhost:8888""
```",2025-01-20T16:21:31,2025-01-20T16:16:28
79371532,Why throws the POD an InvocationTargetException,"I am new in Kubernetes and I want to run a basic Spring-Boot-application inside a namespace.

The Pod alsways tries to start and throws this Error:

> Exception in thread ""main"" java.lang.reflect.InvocationTargetException
> at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown
> Source)
> at java.base/java.lang.reflect.Method.invoke(Unknown Source)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64)
> at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40)
> Caused by: java.lang.reflect.InvocationTargetException

and this Error:

> > Caused by: java.lang.StackOverflowError
> > at java.base/java.lang.ThreadLocal.getCarrierThreadLocal(Unknown Source)
> > at java.base/java.lang.System$2.getCarrierThreadLocal(Unknown Source)
> > at java.base/jdk.internal.misc.CarrierThreadLocal.get(Unknown Source)
> > at java.base/sun.nio.fs.NativeBuffers.getNativeBufferFromCache(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.copyToNativeBuffer(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.stat(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributes.get(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/java.nio.file.Files.readAttributes(Unknown Source)
> > at java.base/java.util.zip.ZipFile$Source.get(Unknown Source)
> > at java.base/java.util.zip.ZipFile$CleanableResource.(Unknown
> > Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:141)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:124)
> > at org.springframework.boot.loader.net.protocol.jar.UrlNestedJarFile.(UrlNestedJarFile.java:42)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFileForNested(UrlJarFileFactory.java:86)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFile(UrlJarFileFactory.java:55)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFiles.getOrCreate(UrlJarFiles.java:72)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.connect(JarUrlConnection.java:289)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.getJarFile(JarUrlConnection.java:99)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.getJarFile(JarUrlClassLoader.java:188)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackage(JarUrlClassLoader.java:146)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackageIfNecessary(JarUrlClassLoader.java:129)

I am using Helm as well, but with the helm files is everything fine.

My Dockerfile looks like that:

```
FROM /ubi8/minimum/java-21:8.10-1088-1-java21.0.5_11

COPY /target/*-spring-boot.jar app.jar

ENTRYPOINT [""java"", ""-Xms2G"", ""-Xmx2G"", ""-XX:+UseG1GC"", ""-XX:+ExitOnOutOfMemoryError"", ""-jar"", ""app.jar""]

LABEL COMMIT_ID=${COMMIT_ID}
```

And my POM looks like that:

```
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.4.1</version>
        <relativePath/>
    </parent>

    <properties>
        <java.version>21</java.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

        <maven-compiler-plugin.version>3.13.0</maven-compiler-plugin.version>
        <maven-clean-plugin.version>3.4.0</maven-clean-plugin.version>
        <maven-surefire-plugin-version>3.5.2</maven-surefire-plugin-version>
        <maven-dependency-plugin.version>3.8.1</maven-dependency-plugin.version>
        <maven-spring-boot-plugin.version>3.4.0</maven-spring-boot-plugin.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven-compiler-plugin.version}</version>
                <configuration>
                    <release>${java.version}</release>
                    <parameters>true</parameters>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
                <version>${maven-spring-boot-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>repackage</goal>
                        </goals>
                        <configuration>
                            <classifier>spring-boot</classifier>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <version>${maven-dependency-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>properties</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${maven-surefire-plugin-version}</version>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-clean-plugin</artifactId>
                <version>${maven-clean-plugin.version}</version>
            </plugin>
        </plugins>
    </build>

    <profiles>
        <profile>
            <id>OWASP</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.owasp</groupId>
                        <artifactId>dependency-check-maven</artifactId>
                    </plugin>
                </plugins>
            </build>
        </profile>
    </profiles>
```

Thanks in advance!
Maybe somebody can help me :)","java, spring, spring-boot, docker, kubernetes",79436081.0,"In your Dockerfile you have set `-Xms2G` which mean your application needs to allocate '2G' of heap space to begin with at the time of initialization.

Accordingly, you must also configure equivalent or more `request memory` for your pod where your container will be deployed. Hence, in your `deployment.yaml` you need to configure the following:

```
....
resources:
      request:
        cpu: 0.1
        memory: 2G
....
```",2025-02-13T12:03:18,2025-01-20T13:49:57
79371532,Why throws the POD an InvocationTargetException,"I am new in Kubernetes and I want to run a basic Spring-Boot-application inside a namespace.

The Pod alsways tries to start and throws this Error:

> Exception in thread ""main"" java.lang.reflect.InvocationTargetException
> at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown
> Source)
> at java.base/java.lang.reflect.Method.invoke(Unknown Source)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64)
> at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40)
> Caused by: java.lang.reflect.InvocationTargetException

and this Error:

> > Caused by: java.lang.StackOverflowError
> > at java.base/java.lang.ThreadLocal.getCarrierThreadLocal(Unknown Source)
> > at java.base/java.lang.System$2.getCarrierThreadLocal(Unknown Source)
> > at java.base/jdk.internal.misc.CarrierThreadLocal.get(Unknown Source)
> > at java.base/sun.nio.fs.NativeBuffers.getNativeBufferFromCache(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.copyToNativeBuffer(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.stat(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributes.get(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/java.nio.file.Files.readAttributes(Unknown Source)
> > at java.base/java.util.zip.ZipFile$Source.get(Unknown Source)
> > at java.base/java.util.zip.ZipFile$CleanableResource.(Unknown
> > Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:141)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:124)
> > at org.springframework.boot.loader.net.protocol.jar.UrlNestedJarFile.(UrlNestedJarFile.java:42)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFileForNested(UrlJarFileFactory.java:86)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFile(UrlJarFileFactory.java:55)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFiles.getOrCreate(UrlJarFiles.java:72)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.connect(JarUrlConnection.java:289)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.getJarFile(JarUrlConnection.java:99)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.getJarFile(JarUrlClassLoader.java:188)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackage(JarUrlClassLoader.java:146)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackageIfNecessary(JarUrlClassLoader.java:129)

I am using Helm as well, but with the helm files is everything fine.

My Dockerfile looks like that:

```
FROM /ubi8/minimum/java-21:8.10-1088-1-java21.0.5_11

COPY /target/*-spring-boot.jar app.jar

ENTRYPOINT [""java"", ""-Xms2G"", ""-Xmx2G"", ""-XX:+UseG1GC"", ""-XX:+ExitOnOutOfMemoryError"", ""-jar"", ""app.jar""]

LABEL COMMIT_ID=${COMMIT_ID}
```

And my POM looks like that:

```
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.4.1</version>
        <relativePath/>
    </parent>

    <properties>
        <java.version>21</java.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

        <maven-compiler-plugin.version>3.13.0</maven-compiler-plugin.version>
        <maven-clean-plugin.version>3.4.0</maven-clean-plugin.version>
        <maven-surefire-plugin-version>3.5.2</maven-surefire-plugin-version>
        <maven-dependency-plugin.version>3.8.1</maven-dependency-plugin.version>
        <maven-spring-boot-plugin.version>3.4.0</maven-spring-boot-plugin.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven-compiler-plugin.version}</version>
                <configuration>
                    <release>${java.version}</release>
                    <parameters>true</parameters>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
                <version>${maven-spring-boot-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>repackage</goal>
                        </goals>
                        <configuration>
                            <classifier>spring-boot</classifier>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <version>${maven-dependency-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>properties</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${maven-surefire-plugin-version}</version>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-clean-plugin</artifactId>
                <version>${maven-clean-plugin.version}</version>
            </plugin>
        </plugins>
    </build>

    <profiles>
        <profile>
            <id>OWASP</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.owasp</groupId>
                        <artifactId>dependency-check-maven</artifactId>
                    </plugin>
                </plugins>
            </build>
        </profile>
    </profiles>
```

Thanks in advance!
Maybe somebody can help me :)","java, spring, spring-boot, docker, kubernetes",79433517.0,"From what I see, you forgot to request memory in your helm chart.",2025-02-12T14:57:30,2025-01-20T13:49:57
79371491,Quarkus smallrye jwt sign method crashes when reading key from kubernetes secret,"I want to sign and validate jwt tokens in my quarkus rest app. The private- and public keys should be stored as kubernetes secrets.

What I did so far:

- Create keys

```
openssl genrsa -out keypair.pem 2048
openssl rsa -in keypair.pem -pubout -out publickey.crt
openssl pkcs8 -topk8 -inform PEM -outform PEM -nocrypt -in keypair.pem -out pkcs8.key
```

- Create kubernetes secret

```
kubectl create secret generic jwt-keys -n my-namespace --from-file=privkey=pkcs8.key --from-file=pubkey=publickey.crt
```

- Edit application.properties

```
quarkus.kubernetes-config.secrets=jwt-keys
mp.jwt.sign.key=jwt-keys.privkey
mp.jwt.verify.publickey=jwt-keys.pubkey
smallrye.jwt.sign.key=jwt-keys.privkey
```

- Try to sign a token

```
    private String createToken(Role role) {
        long now = Instant.now().getEpochSecond();
        long exp = now + TimeUnit.MINUTES.toSeconds(60);
        return Jwt.issuer(issuer)
                .claim(Claims.groups, role)
                .issuedAt(Instant.ofEpochSecond(now))
                .expiresAt(Instant.ofEpochSecond(exp))
                .sign();
    }
```

Behaviour when I call the createToken method:

```
io.smallrye.jwt.build.JwtSignatureException: SRJWT05009:
        at io.smallrye.jwt.build.impl.JwtSignatureImpl.sign(JwtSignatureImpl.java:109)
        at de.infinityq.auth.TokenManager.createToken(TokenManager.java:21)
        at de.infinityq.auth.TokenManager.createUserToken(TokenManager.java:29)ethod:
.
.
.
.
.
Caused by: java.lang.IllegalArgumentException: SRJWT05028: Signing key can not be created from the loaded content
        at io.smallrye.jwt.build.impl.JwtSignatureImpl.sign(JwtSignatureImpl.java:102)
```

So the sign function crashes when trying to load the key I defined in the kubernetes secret, but I do not understand why.

I verified that the keys are valid by passing them as files directly, as shown in the example here: [https://quarkus.io/guides/security-jwt#token-decryption](https://quarkus.io/guides/security-jwt#token-decryption)

I verified that my approach should work by reading the smallrye documentation: [https://smallrye.io/docs/smallrye-jwt/generate-jwt.html](https://smallrye.io/docs/smallrye-jwt/generate-jwt.html) which clearly states that I can pass the key as value

I don't know what else to do, hope someone can help.

EDIT:

As requested, here is the content of the jwt-keys secret (it's just for local testing, so sharing them here is fine. Will create new keys anyway).

```
    ""apiVersion"": ""v1"",
    ""data"": {
        ""privkey"": ""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"",
        ""pubkey"": ""LS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS0KTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUFuWjBQbFdPMlVXTHhseE5zdXZMYQo1WjhLQUcyWDNRcDJvdzQ2d0FOS0llRmllTW5Qelprbm1CYzY1b1lpSUloWnJaQ2RUNnVaMmcvWWRXRWhPZENWCmlrTFhxYml3R1NTVXVqTWdGL3FtMFA1UFhTUzRaY1dNbTF0QkRJbUNlejlQamZZTlB6V3kzY0ZXYUlMaXJZamQKMnFzM2R1ZE45ek9TYkwvRkFGSVBkS2dCYm5ucW50ZlBBcGFCNDRiMEhXU05CeTMwcVEvMG95b0NuYnNqS3VITQoxYVZuMDM2Mm1SUXFuSHVUcnlFYkRSeXRPT3FFd25FUHY1MHI0ZDgzZUw4NTVYWW0vZnBHNVBLS2h5UVB0Q0NNCkdybHFuZGxDZ1c0YUxhTG9Tdnp4YXE0ekpwbkZLcWM3MjdMbnY3SXBQNVRScVBnSDE0SE1Cbml6SFF6akdJNTQKUFFJREFRQUIKLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tCg==""
    },
    ""kind"": ""Secret"",
    ""metadata"": {
        ""creationTimestamp"": ""2025-01-20T12:34:19Z"",
        ""name"": ""jwt-keys"",
        ""namespace"": ""my-namespace"",
        ""resourceVersion"": ""6222080"",
        ""uid"": ""f3e1cb39-4e95-4af0-bef2-4eabf87e4777""
    },
    ""type"": ""Opaque""
}
```","java, kubernetes, jwt, quarkus, smallrye",,,,2025-01-20T13:33:15
79371403,Prometheus not showing ServiceMonitors in Targets,"I am trying to load ArgoCD Dashboard in Grafana.
I have installed Grafana-Loki and Prometheus helm charts.

I have already set `serviceMonitorSelectorNilUsesHelmValues` to `false`.

Running `kubectl get prometheuses.monitoring.coreos.com --all-namespaces -o jsonpath=""{.items[*].spec.serviceMonitorSelector}""` returns an empty object `{}` which earlier used to return `prometheus`.

So it seems that the flag is set correctly.
Yet I cannot see ServiceMonitor in the list of targets on Prometheus

[![enter image description here](https://i.sstatic.net/TpPltoJj.png)](https://i.sstatic.net/TpPltoJj.png)

I have already enabled Metrics and ServiceMonitors on ArgoCD.

What am I missing here? It seems that because ServiceMonitors are not loaded on Prometheus, nothing is loaded on the ArgoCD Grafana Dashboard.

How can I resolve this?
[![enter image description here](https://i.sstatic.net/27m0n4M6.png)](https://i.sstatic.net/27m0n4M6.png)

**EDIT** List of pods in monitoring namespace

[![enter image description here](https://i.sstatic.net/z1k1EYz5.png)](https://i.sstatic.net/z1k1EYz5.png)","kubernetes, prometheus, grafana, argocd, prometheus-operator",,,,2025-01-20T13:04:13
79371178,Loss of x-forwarded-for header when using service=ClusterIP,"We use k8s to deploy our applications and encountered unexpected behavior. We have a microservice written in java 21 using the spring framework. This microservice provides an HTTP API for external consumers.

The microservice is deployed in k8s using helm chart, in the configuration it is specified that it will use a Service with the `ClusterIP` type. Everything works fine, but we encountered a problem: when sending the `X-Forwarded-For` header with the client's IP address to the microservice, this header is ""automatically"" cut off.
That is, we go to the deployed pod of the microservice and execute the request:

```
wget --no-check-certificate -S \
  --timeout=0 \
  --header 'X-Forwarded-For: 10.100.10.100' \
  --header 'X-Forwarded-Host: example.com' \
  --header 'X-CustomHeader: custom-header' \
  --header 'Content-Type: application/json' \
   -q 'http://127.0.0.1:8080/api/v1/test'
```

In the microservice logs, we see:

```
POST ""/api/v1/test"", parameters={}, headers={host:[127.0.0.1:8080], user-agent:[Wget], accept:[*/*], connection:[close], x-forwarded-host:[example.com], x-customheader:[custom-header], content-type:[application/json], content-length:[102]} in DispatcherServlet 'dispatcherServlet'
```

As you can see, the manually transmitted `X-Forwarded-For` header did not reach the microservice.
We found [this answer](https://stackoverflow.com/a/57080315/14330235) on SO, but the question itself is quite old and there is no explicit information that the header will be removed.
Can someone tell me why this is happening?","spring, kubernetes, microservices, clientip",,,,2025-01-20T11:36:23
79370130,How to regex in semantic versioning in flux image policy for &quot;v2.2.9a&quot;,"I want the flux to be able to read and triggers with versioning such as v2.2.9, v2.2.9a, v2.2.9b

Currently it was working for v2.2.9, v2.3.0 but not when appending alphabet at the end.

```
kind: ImagePolicy
metadata:
  name: node-deployment
  namespace: flux-system
spec:
  imageRepositoryRef:
    name: node-deployment
  filterTags:
    # pattern: '^v2\.\d+\.\d+[a-zA-Z0-9]*$'
    pattern: '^2\\.2\\.9[a-z]?$'
  policy:
    semver:
      range: '^2.x'
```","kubernetes, charts, flux, semantic-versioning",79372026.0,You can find the 2 officially suggested regular expressions on the Semantic Versioning homepage: [https://semver.org/#is-there-a-suggested-regular-expression-regex-to-check-a-semver-string](https://semver.org/#is-there-a-suggested-regular-expression-regex-to-check-a-semver-string),2025-01-20T16:31:43,2025-01-20T01:54:41
79369211,OpenTelemetry export to Prometheus – Unsupported compression: snappy (prometheusremotewrite),"The .NET OpenTelemetry.AutoInstrumentation package fails to export metrics to Prometheus, via an OpenTelemetry Collector (otel/opentelemetry-collector-contrib) due to snappy compression.

Prometheus OTLP endpoint `/api/v1/otlp/v1/metrics` throws `400 Bad Request`

```
unsupported compression: snappy. Only ""gzip"" or no compression supported
```

Full logs of OpenTelemetry Collector metrics requests:

1. INFO **debug**
2. ERROR **prometheusremotewrite**

```
2025-01-19T15:16:36.519Z    info    Metrics {""kind"": ""exporter"", ""data_type"": ""metrics"", ""name"": ""debug"", ""resource metrics"": 1, ""metrics"": 36, ""data points"": 150}
2025-01-19T15:16:36.526Z    error   internal/queue_sender.go:103    Exporting failed. Dropping data.    {""kind"": ""exporter"", ""data_type"": ""metrics"", ""name"": ""prometheusremotewrite"", ""error"": ""Permanent error: Permanent error: Permanent error: remote write returned HTTP status 400 Bad Request; err = %!w(<nil>): unsupported compression: snappy. Only \""gzip\"" or no compression supported\n"", ""dropped_items"": 150}
go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
    go.opentelemetry.io/collector/exporter@v0.116.0/exporterhelper/internal/queue_sender.go:103
go.opentelemetry.io/collector/exporter/internal/queue.(*Consumers[...]).Start.func1
    go.opentelemetry.io/collector/exporter@v0.116.0/internal/queue/consumers.go:43
```

`kube-prometheus-stack` includes configuration to open the OTLP endpoint: `/api/v1/otlp/v1/metrics`

```
prometheus:
  prometheusSpec:
    additionalArgs:
      - name: web.enable-otlp-receiver
        value: """"
```

OpenTelemetry Collector configuration:

```
# https://opentelemetry.io/docs/languages/js/exporters/#prometheus
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: ${NAME}
  namespace: ${NAMESPACE}
spec:
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    # https://github.com/open-telemetry/opentelemetry-helm-charts/issues/23#issuecomment-910885716
    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25

    exporters:
      debug:
        verbosity: basic
      prometheusremotewrite:
        endpoint: http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/otlp/v1/metrics
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug, prometheusremotewrite]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug]
```

OpenTelemetry Instrumentation to automatically consume .NET metrics:

```
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: ${NAME}
  namespace: ${NAMESPACE}
spec:
  exporter:
    endpoint: http://${COLLECTOR_SERVICE}.${COLLECTOR_NAMESPACE}.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: ""1""
```

Kubernetes deployment of .NET container has an template metadata annotation:

```
instrumentation.opentelemetry.io/inject-dotnet: true
```","kubernetes, prometheus, open-telemetry, prometheus-operator, open-telemetry-collector",79369355.0,"In my Collector configuration, I am using the Prometheus RemoteWrite exporter, which pushes metrics via PRW, to the Prometheus OTLP endpoint. What I want to do, is pick one of those protocols and forget about the other.

1. If I want to push metrics via PRW, I update the endpoint to `http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write`
2. If I want to push metrics via OTLP, I replace my current exporter with the [OTLP exporter](https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter)

✅  PRW solution:

```
prometheus:
  prometheusSpec:
    enableRemoteWriteReceiver: true
    enableFeatures:
      - remote-write-receiver
```

and

```
prometheusremotewrite:
  endpoint: http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write
  tls:
    insecure: true
```",2025-01-19T16:50:33,2025-01-19T15:27:22
79368275,Injecting secret using fabric8 Java SDK for k8,"I have the following in the yaml file's Deployment manifest that works

```
          envFrom:
          - secretRef:
              name: pocsecrets
```

I have a secret on my K8, with above name which has few `key, value` items and they are injected just fine, when I use `kubectl`

When using `fabric8`, how do I inject it?
I am trying below snippet but not sure, if this is right

```
        var sec = new SecretEnvSource();
        sec.setName(""pocsecrets"");

.....other Deployment code with builder and all and then below:
         .addNewEnvFrom().withSecretRef(sec).endEnvFrom()
```","kubernetes, fabric8-kubernetes-client",,,,2025-01-19T01:46:21
79366201,Flink Kubernetes S3 state support,"Been looking at the documentation with Flink Kubernetes Operator v1.10, is there a way to preconfigure the cluster so that all submitted jobs will be using rocksdb state with some predefined s3 path? What would be required for that to work? I've been trying to set the jobs up with S3 backend but it's saying that the s3 backend is not supported and I would need to enable the s3 plugin, but I'm unsure how to go about that.","kubernetes, apache-flink, flink-streaming",,,,2025-01-17T22:01:54
79365249,Why each pods in a StatefulSet has seprate PVC,"I have a statefulset which has 3 replicas of mysql pods. Why it is not recommended to share the same backend storage/PV among these 3 replicas. Why each pod has its own pvc and pv.

- [This approach is recommended](https://i.sstatic.net/Wx8T4gWw.jpg)
- [What is the problem with this approach](https://i.sstatic.net/Y7sy9Wx7.jpg)

I searched a lot for this, but didn't get any appropriate answer. Can anyone explain this to me.","kubernetes, kubernetes-statefulset",79365570.0,"Sharing files is more challenging than it sounds.  This isn't anything specific to MySQL, or even to containers: if you have multiple processes trying to read or write the same file, there's a risk that one process will overwrite the data that another's trying to write.  Databases tend to use complex binary file formats, and so if one process is trying to write data while another is updating an index, you'll get a corrupted file.  There are also some hidden bad things that can happen if the data is crossing a network, and packets get lost or delayed or corrupted in flight.

One typical protection against this – again, not specific to containers – is to write a *lock file* into the data store.  Assuming all of the processes cooperate, this ensures that only one copy of the application can use a given file store.  This is particularly common among databases: if you have two copies of MySQL that are trying to use the same physical storage, one of them just won't start up.

Let's say you're using a database like MongoDB or Elasticsearch that's conscious of this, and is able to run multiple copies of itself.  You might configure Elasticsearch to have five replicas.  Any given datum will be stored in two of them, so if a single replica fails the data can be replicated back to a node that's still alive.  But this setup means that **each replica needs its own independent data store**.

This is the setup that a StatefulSet is aiming towards: you have multiple replicas, each needs its own *independent* data volume, but the in-process logic knows how to route requests to the right place to find the data.

Assuming your cluster can create a ReadWriteMany volume (these can be a little tricky to come by) and you can tolerate both concurrent writes and network hiccups, you could independently create a volume and mount it into multiple replicas of a Deployment.  A StatefulSet wouldn't manage these, though.  If you needed other properties of a StatefulSet like ordered replicas with consistent names, you could mount the RWX volume into StatefulSet replicas too, but it wouldn't be part of the `volumeClaimTemplates:`.",2025-01-17T17:10:56,2025-01-17T15:24:48
79362256,Where and how to find the right Ambassador CRD versions,"I am trying to install Ambassador Edge Stack on my cluster with terraform, and getting the following error:

```
authservices.getambassador.io failed to create kubernetes rest client for update of resource: resource [apiextensions.k8s.io/v1beta1/CustomResourceDefinition] isn't valid for cluster, check the APIVersion and Kind fields are valid
```

If I understand correctly, the reason for that is a mismatched version of applied CRDs for my K8s cluster.

How do I find the right versions of CRDs. Even the URLs that I have in the code (below) was an educated guess.

Here is my terraform:

```
data ""http"" ""aes_crds_yaml"" {
  url = ""https://app.getambassador.io/yaml/edge-stack/3.12.2/aes-crds.yaml""
}

resource ""kubectl_manifest"" ""aes_crds"" {
  yaml_body = data.http.aes_crds_yaml.response_body
}

data ""http"" ""ambassador_crds_yaml"" {
  url = ""https://app.getambassador.io/yaml/ambassador/1.14.4/ambassador-crds.yaml""
}

resource ""kubectl_manifest"" ""ambassador_crds"" {
  yaml_body = data.http.ambassador_crds_yaml.response_body
}

resource ""helm_release"" ""ambassador"" {
  count             = 1
  name              = ""edge-stack""
  namespace         = ""ambassador""
  repository        = ""https://s3.amazonaws.com/datawire-static-files/charts""
  chart             = ""edge-stack""
  version           = ""8.12.2""
  timeout           = 300
  create_namespace  = true
  cleanup_on_fail   = true
  dependency_update = true
  verify            = true

  depends_on = [kubectl_manifest.aes_crds, kubectl_manifest.ambassador_crds]
}
```

Cluster versions:

```
$ kubectl version
Client Version: v1.32.0
Kustomize Version: v5.5.0
Server Version: v1.31.1
```

Ambassadors helms:

```
$ helm search repo datawire
NAME                            CHART VERSION   APP VERSION         DESCRIPTION
datawire/ambassador             6.9.5           1.14.4              A Helm chart for Datawire Ambassador
datawire/ambassador-agent       1.0.22          1.0.22              The Ambassador Agent populates the Developer Co...
datawire/ambassador-operator    0.3.0           v1.3.0              A Helm chart for Kubernetes
datawire/edge-stack             8.12.2          3.12.2              A Helm chart for Ambassador Edge Stack
datawire/eg-edge-stack          0.0.1           v4.0.0-preview.1    Ambassador Labs Edge Stack API Gateway built on...
datawire/eg-edge-stack-crds     0.0.1           v4.0.0-preview.1    Custom Resource Definitions for Ambassador Labs...
datawire/emissary-ingress       8.12.2          3.12.2              A Helm chart for Emissary Ingress
datawire/telepresence           2.20.1          2.20.1              A chart for deploying the server-side component...
datawire/telepresence-crds      2.20.1          2.20.1              A Helm chart containing the CRDs for telepresence.
```","kubernetes, terraform, ambassador",79365850.0,"Just wanted to share my solution here.
The issue was with the wrong repo URL. I ended up with the following manifest:

```
data ""http"" ""cert_manager_crds_yaml"" {
  url = ""https://github.com/cert-manager/cert-manager/releases/download/v1.16.3/cert-manager.crds.yaml""
}

resource ""kubectl_manifest"" ""cert_manager_crds"" {
  yaml_body = data.http.cert_manager_crds_yaml.response_body
}

resource ""helm_release"" ""cert_manager"" {
  count             = 1
  name              = ""cert-manager""
  namespace         = ""cert-manager""
  repository        = ""https://charts.jetstack.io""
  chart             = ""cert-manager""
  version           = ""v1.16.3""
  timeout           = 600
  create_namespace  = true
  cleanup_on_fail   = true
  # dependency_update = true
  # verify            = true

  values = [ file(""values/cert-manager.yaml"") ]

  depends_on = [kubectl_manifest.cert_manager_crds]
}
```",2025-01-17T18:58:02,2025-01-16T15:54:09
79361520,How to create pods with their own non-sharable folders,"I want to create a headless service with a stateful set with 3 replicas.
The application I want to run is a simple calculator web app. My application's docker image has the following lines

```
ARG UID=10001
RUN adduser \
    --disabled-password \
    --gecos """" \
    --home ""/data"" \
    --shell ""/sbin/nologin"" \
    --no-create-home \
    --uid ""${UID}"" \
    appuser
USER appuser
```

that means the application inside a container runs as a non-root (as `appuser` with uid=10001).

In particular, I want create three replicas/pods so that **each pod has its own (exclusive) folder and pods could have read/write access to the folder**. I create my `yaml` file as following:

```
apiVersion: v1
kind: Namespace
metadata:
  name: myns
---

apiVersion: v1
kind: Service
metadata:
  namespace: myns
  name: calc-headless
spec:
  clusterIP: None # headless
  ports:
    - port: 3000
  selector:
    app: calc

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  namespace: myns
  labels:
    app: calc
  name: calc
spec:
  serviceName: calc-headless
  replicas: 3
  minReadySeconds: 5
  selector:
    matchLabels:
      app: calc
  template:
    metadata:
      labels:
        app: calc
    spec:
      terminationGracePeriodSeconds: 3
      securityContext:
        runAsUser: 10001
        runAsGroup: 10001
        fsGroup: 10001
        fsGroupChangePolicy: ""Always""
      containers:
      - image: path/to/my/image
        name: calc
        ports:
          - containerPort: 3000
        volumeMounts:
          - name: node-data
            mountPath: /my-data
        securityContext:
          runAsUser: 10001
          runAsGroup: 10001
  volumeClaimTemplates:
    - metadata:
        name: node-data
      spec:
        accessModes: [ ""ReadWriteOncePod"" ]
        resources:
          requests:
            storage: 1Gi
```

When I run `kubectl apply -f headless.yml`, k8s successfully creates service and pods. Then I check each pod using `kubectl exec -it calc-0 -- /bin/sh` and list folder `ls -al` and I see that the folder `/my-data` belongs to `root:root` and hence my pod has no permission to write to it. What do I miss?

**UPDATE**:

I use minikube. The output of `kubectl get storageclasses` is

```
NAME                 PROVISIONER                RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
standard (default)   k8s.io/minikube-hostpath   Delete          Immediate           false                  45d
```

And output of `kubectl get pv` is

```
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                      STORAGECLASS   VOLUMEATTRIBUTESCLASS   REASON   AGE
pvc-170ff701-8b9e-4df1-b88f-78f4758a19f4   1Gi        RWOP           Delete           Bound    ckad/node-data-calc-0      standard       <unset>                          6m22s
pvc-21d2bca0-be99-4765-8ad1-7c8b28583d8b   1Gi        RWOP           Delete           Bound    ckad/node-data-calc-1      standard       <unset>                          6m12s
pvc-a85807e8-8f48-474e-9ca6-eaf91a8f70a4   1Gi        RWOP           Delete           Bound    ckad/node-data-calc-2      standard       <unset>                          6m2s
```",kubernetes,,,,2025-01-16T12:07:48
79360402,Permission denied @ rb_sysopen - /home/oxidized/.config/oxidized/pid,"I'm trying to implement Oxidized and have chosen to use Kubernetes. I created a storage account in Azure where there are the main files that oxidized needs, which are config, router.db and the logs and crash folder that are created after execution. The problem is that I am getting the error *Permission denied @rb_sysopen - /home/oxidized/.config/oxidized/pid*. I'm building my deploy inside a Terraform file using the deploy_kubernetes resource.

I am using initContainers and even so, the folder permissions are not changed. Accessing the pod using kubectl exec -it  -- bash and using the commands to change permissions doesn't work either.

Here's my code...

```
resource ""kubernetes_deployment"" ""poc-oxidized"" {
  metadata {
    name      = ""poc-oxidized""
    namespace = kubernetes_namespace.namespace_poc-oxidized.metadata.0.name
  }
  spec {
    replicas = 1
    selector {
      match_labels = {
        app = ""poc-oxidized""
      }
    }
    template {
      metadata {
        labels = {
          app = ""poc-oxidized""
        }
      }
      spec {
        init_container {
          name = ""set-permissions""
          image = ""busybox""
          command = [""sh"", ""-c"", ""chmod -R 775 /home/oxidized/.config/oxidized""]
          security_context {
            privileged = true
          }
          volume_mount {
            name       = ""poc-oxidized-pvc""
            mount_path = ""/home/oxidized/.config/oxidized""
          }
        }
        container {
          name  = ""poc-oxidized""
          image = ""oxidized/oxidized:latest""

          env {
            name = ""HOME""
            value = ""/home/oxidized""
          }

          port {
            name           = ""http""
            container_port = __port__
          }

          volume_mount {
            name       = ""poc-oxidized-pvc""
            mount_path = ""/home/oxidized/.config/oxidized""
          }
        }
        volume {
          name = ""poc-oxidized-pvc""
          persistent_volume_claim {
            claim_name = kubernetes_persistent_volume_claim.claim.metadata.0.name
          }
        }
      }
    }
  }
```","kubernetes, poc",79366258.0,"In this [topic of Dockers by the Github community](https://github.com/ytti/oxidized/issues/2664), it is a must to have file location of your *pid file, router.db, logs, crash folder etc.* into your home directory, you may change the file location of your *pid file* by [kubectl plugins](https://kubernetes.io/docs/reference/kubectl/#examples-creating-and-using-plugins:%7E:text=kubectl%20diff%20%2Df%20%2D-,Examples%3A%20Creating%20and%20using%20plugins,-Use%20the%20following) to avoid denied permission.",2025-01-17T22:40:12,2025-01-16T04:53:46
79360239,Does kubelet erase ephemeral storage data physically before release?,"Say we have a pod that uses emptyDir for a volume. When the pod terminates, the storage consumed by the emptyDir would be released. However, if the same disk/SSD space is assigned to another pod's emptyDir, the new pod could read the stale data from the physical medium in principle. Or, forensic tools may be able to recover the previous pod's data. Does the kubelet erase the data physically, such as by writing zeroes to the disk, before releasing the emptyDir storage?

The [Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/volumes/#:%7E:text=When%20a%20Pod%20is%20removed%20from%20a%20node%20for%20any%20reason%2C%20the%20data%20in%20the%20emptyDir%20is%20deleted%20permanently.) says: ""When a Pod is removed from a node for any reason, the data in the emptyDir is deleted permanently."" This sounds like the data is erased on the storage medium. But it will be good to have some confirmation.","kubernetes, ephemeral-storage",79360690.0,"In Kubernetes, EmptyDir is a type of volume that is created when a pod is assigned to Node. It remains as long as the pod is running on that node. The data in an EmptyDir is ephemeral and is deleted permanently when the pod is removed from the node. This makes EmptyDir volumes for temporary storage that shares the data between the containers.

**As per this [Article](https://devcodef1.com/news/1460467/kubelet-emptydir-volume-data-release) by DevCodeF1 Editors :**

> **Termination of a Pod using EmptyDIR Volume :** when a Pod is using an EmptyDir volume terminates, the directory and the contents are deleted from the node. However, the disk space used by EmptyDir is not immediately released. The space is released when the node is deleted or when the node's available storage drops down the certain threshold, at which point the node’s CSI driver may reclaim the unused space.
>
>
> **Reuse of EmptyDir space by other Pod :** Since the EmptyDir is created on the node not on a separate storage system, the space used by the terminated Pod’s EmptyDir can be reused by other pods. This can lead to data conflict if multiple pods write to the same EmptyDir without proper synchronization mechanism in place.

This means that while the EmptyDir volume is cleared from the pods perspective, there is a possibility that the data can still be recoverable using the forensic tools if the same disk space is reassigned to another pod.

Refer to this Decisive DevOps [article](https://decisivedevops.com/understanding-kubernetes-emptydir-with-3-practical-use-cases-960f550e0e34/) and also check this [blog](https://www.devopsschool.com/blog/kubernetes-volume-emptydir-explained-with-examples/) by Rajesh Kumar for more information which might be helpful for you.",2025-01-16T07:27:51,2025-01-16T02:44:35
79359638,How to count how many pods were started by namespace each day in PromQL?,"I'm trying to answer a question of *how many pods were started/scheduled/whatever per namespace per day*. I have not found any useful counter-type metric that would be counting that, just related gauges like `kube_pod_status_scheduled_time`.

So far best thing I was able to come up with is this:

```
topk(3,
  sum_over_time((
    delta((
      count by (namespace) (
        count by (pod, namespace) (kube_pod_status_scheduled_time{namespace=~"".*-tenant""})
      )
    )[5m:]) > 0
  )[1d:])
)
```

But first this seems to be too heavy/inefficient to finish before request timeout and second I'm very skeptical it is actually correct.

Would you have any ideas please?","kubernetes, openshift, prometheus, promql",,,,2025-01-15T20:37:10
79359177,Docker Install causing malware warning,"**Problem:**
Malware warning (attached) keep popping-up on every restart and login

**Question:**
How do I stop the pop-ups?

**Current Setup:**
Installed Docker Desktop v4.37.2 and seems to start successfully.
MacOS 15.2 - Apple M3 Chip.

**What Happened:**

- I tried to install Docker Desktop
- Got the the attached warnings
- Searched the issue on the web found the below links
- Docker Desktop seems to be running successfully but
- Pop-ups still appear on restart + login

**Instructions Followed:**

[https://forums.docker.com/t/malware-blocked-com-docker-vmnetd-was-not-opened-because-it-contains-malware/145930/6](https://forums.docker.com/t/malware-blocked-com-docker-vmnetd-was-not-opened-because-it-contains-malware/145930/6)

[https://github.com/docker/for-mac/issues/7527](https://github.com/docker/for-mac/issues/7527)

[How to resolve the ""'Docker.app' will damage your computer"" warning on MacOS?](https://stackoverflow.com/questions/79344101/how-to-resolve-the-docker-app-will-damage-your-computer-warning-on-macos/79351570#79351570)

[https://github.com/docker/for-mac/issues/7520](https://github.com/docker/for-mac/issues/7520)

[![enter image description here](https://i.sstatic.net/Jplk8QN2.png)](https://i.sstatic.net/Jplk8QN2.png)
[![enter image description here](https://i.sstatic.net/OndvcF18.png)](https://i.sstatic.net/OndvcF18.png)","docker, macos, kubernetes, docker-desktop",,,,2025-01-15T17:23:22
79355222,Is containerID unique inside the kubernetes cluster?,"I have a use case, where I am trying to track containers in kubernetes cluster and store it in table.
For this I am planning to use containerId as primary key.

```
kubectl get pod java-hello-world-multiple-docker-7995bdf9fc-98sc8 -o jsonpath='{.status.containerStatuses[*].containerID}'

output:
cri-o://f8afd894dbacb78ccee734d5f0f323aa559a36832939bad8e6e6ad52e8a18c28
cri-o://46c2223e25a8c63cc6505414449164ccbdc2b2f6533143ff429e531ddbb0f2f1
```

In kubernetes documentation it is mentioned, ID is assigned by container runtime:
[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podstatus-v1-core](https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.27/#podstatus-v1-core)

```
ContainerID is the ID of the container in the format '<type>://<container_id>'.
Where type is a container runtime identifier, returned from Version call of CRI API (for example ""containerd"").
```

I couldn't find any concrete documentation stating the containerIds will be unique inside the kubernetes cluster.

Can anyone confirm whether the containerIds are actually unique by sharing documentation, so I can decide on whether to choose containerId as primary key or not for a cluster?","kubernetes, containers",79356396.0,"Unfortunately the API doc does not offer to much info:

[![enter image description here](https://i.sstatic.net/WZHmu4wX.png)](https://i.sstatic.net/WZHmu4wX.png)

But `containerId` is a unique SHA-256 hash generated by the container runtime, cri-o truncates it to the first 60ish chars.
[![enter image description here](https://i.sstatic.net/Fy2g1IRV.png)](https://i.sstatic.net/Fy2g1IRV.png)

Also can check how is generated in the codebase:

[https://github.com/cri-o/cri-o/blob/c956013c220998c7736e4bc6a98fccd797e47abd/server/naming.go#L30](https://github.com/cri-o/cri-o/blob/c956013c220998c7736e4bc6a98fccd797e47abd/server/naming.go#L30)

[https://github.com/cri-o/cri-o/blob/c956013c220998c7736e4bc6a98fccd797e47abd/internal/factory/container/container.go#L351](https://github.com/cri-o/cri-o/blob/c956013c220998c7736e4bc6a98fccd797e47abd/internal/factory/container/container.go#L351)",2025-01-14T21:14:06,2025-01-14T13:50:44
79354922,Kubernetes role to create limited roles,"Is it possible to create a role that gives the right to create roles which can only give the right to create and delete pods, deployments, secrets

For example the role A permit to create roles X,Y,Z so that roles X,Y,Z are limited to the resources pods, deployments, secrets

Regards","kubernetes, roles, rbac",79355397.0,"**Yes it is possible to create a role that allows creating roles with limited permissions:**

**Create a Role Creator role:**

This role has the all  permissions to create other roles  within your kubernetes Cluster

It allows creating, updating and deleting roles and RoleBinding objects.

**Create  Limited role:**

These roles have restricted permissions such as only allowing actions on pods, deployments and secrets.

By using role creator roles you can create and distribute the limited roles to other entities within your cluster, granting them controlled access to specific resources.

To enforce  the restrictions in a limited role . you need to use [admission controllers](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/)

[RBAC](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) Bindings  are used to grant the Role Creator role  and limited role to the specific user or service accounts.

Refer to this [Medium Blog](https://harsh05.medium.com/mastering-kubernetes-rbac-user-creation-role-based-access-control-75713e63059a#:%7E:text=Creating%20Roles%20and%20Role%20Bindings%3A&text=Subsequently%2C%20we%27ll%20bind%20these,and%20do%20the%20following%20things.)  by Harsh for more information about Mastering Kubernetes RBAC: User Creation & Role-Based Access Control.",2025-01-14T14:46:43,2025-01-14T12:01:18
79354804,jenkins pipeline failed: Exec Failure: HTTP:401. Message:Unauthorized,"I ran jenkins in k8s cluster, and I installed the kubernetes plugin in the jenkins.

The pipeline failed with below jenkins log:

```
2025-01-14 06:48:38.371+0000 [id=2218]  INFO    o.internal.platform.Platform#log: ALPN callback dropped: HTTP/2 is disabled. Is alpn-boot on the boot class path?
2025-01-14 06:48:38.439+0000 [id=2174]  INFO    o.c.j.p.k.p.ContainerExecDecorator$1#doLaunch: Created process inside pod: [maven-r266l], container: [maven][90 ms]
2025-01-14 06:48:38.766+0000 [id=2218]  INFO    o.internal.platform.Platform#log: ALPN callback dropped: HTTP/2 is disabled. Is alpn-boot on the boot class path?
2025-01-14 06:48:38.793+0000 [id=2218]  SEVERE  i.f.k.c.d.i.ExecWebSocketListener#onFailure: Exec Failure: HTTP:401. Message:Unauthorized
java.net.ProtocolException: Expected HTTP 101 response but was '401 Unauthorized'
        at okhttp3.internal.ws.RealWebSocket.checkResponse(RealWebSocket.java:229)
        at okhttp3.internal.ws.RealWebSocket$2.onResponse(RealWebSocket.java:196)
        at okhttp3.RealCall$AsyncCall.execute(RealCall.java:203)
        at okhttp3.internal.NamedRunnable.run(NamedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
2025-01-14 06:48:39.153+0000 [id=2174]  INFO    o.c.j.p.k.KubernetesSlave#_terminate: Terminating Kubernetes instance for agent maven-r266l
2025-01-14 06:48:39.177+0000 [id=2174]  INFO    o.internal.platform.Platform#log: ALPN callback dropped: HTTP/2 is disabled. Is alpn-boot on the boot class path?
2025-01-14 06:48:39.335+0000 [id=2256]  INFO    o.j.p.workflow.job.WorkflowRun#finish: appjkzql/pm-mom/ks-hz #73 completed: FAILURE
2025-01-14 06:48:39.445+0000 [id=2174]  INFO    o.c.j.p.k.KubernetesSlave#deleteSlavePod: Terminated Kubernetes instance for agent kubesphere-devops-worker/maven-r266l
2025-01-14 06:48:39.446+0000 [id=2174]  INFO    o.c.j.p.k.KubernetesSlave#_terminate: Disconnected computer maven-r266l
Terminated Kubernetes instance for agent kubesphere-devops-worker/maven-r266l
Disconnected computer maven-r266l
2025-01-14 06:48:39.449+0000 [id=2256]  INFO    j.s.DefaultJnlpSlaveReceiver#channelClosed: Computer.threadPoolForRemoting [#230] for maven-r266l terminated: java.nio.channels.ClosedChannelException
2025-01-14 06:48:39.451+0000 [id=2217]  INFO    hudson.remoting.Request$2#run: Failed to send back a reply to the request hudson.remoting.Request$2@74b67b7b: hudson.remoting.ChannelClosedException: Channel ""hudson.remoting.Channel@3c9dbe07:JNLP4-connect connection from 10.233.70.92/10.233.70.92:55624"": channel is already closed
```

the jenkinsfile is very simple, the core piece:

```
    stages {
        stage('docker-login') {

            steps {
                container('maven') {
                  echo 'hello'
                  sh 'docker login $REGISTRY -u xxx -p xxx'
                }

            }
        }
    }
```

the weird thing is, I ran the pipeline many times, most times it failed (at above stage), sometimes it succeeded.

The 401 status code seems indicate something like token used by jenkins is not correct, but I don't know which one.

Anyone knows how to resolve this? thanks very much!","kubernetes, jenkins, http-status-code-401",79360145.0,"Turned out it's due to one master of 5 in the k8s has expired kube-apiserver certificates.

1/5 possibility the jenkins connected to this master thus get 401 Unauthorized.

Ran `kubeadm certs renew all` then restarted the k8s components, everything went okay now.",2025-01-16T01:36:56,2025-01-14T11:05:33
79353364,gRPC Unary Calls- shutdown state and unaryClientInterceptor,"I am creating an application library for managing gRPC clients and connections for my microservices on a Kubernetes cluster. I am still getting my arms around gRPC in general but I have a fairly solid intermediate understanding of it. My idea is this...

I created a unaryClientInterceptor I am using to add metadata about the calling service and the particular call. I also want to check the connection status and if it is in a shutdown state - reset the connection.

here is my code...

```
func (cm *InternalClientSet) unaryClientInterceptor() grpc.UnaryClientInterceptor {

    return func(
        ctx context.Context,
        method string,
        req, reply interface{},
        cc *grpc.ClientConn,
        invoker grpc.UnaryInvoker,
        opts ...grpc.CallOption,
    ) error {

        target := cc.Target()
        log.Debug().Msgf(""Intercepting unary call to host: %s"", target)

        // Extract the Kubernetes service name from the host name
        hostParts := strings.Split(target, ""."")
        var kubeServiceName, namespace string
        if len(hostParts) >= 2 {
            kubeServiceName = hostParts[0] // The service name is the first part of the host name
            namespace = hostParts[1]       // The namespace is the second part of the host name
            log.Debug().Msgf(""Kubernetes service name: %s, namespace: %s"", kubeServiceName, namespace)
        }
        var reconnectError error
        state := cc.GetState()
        if state == connectivity.Shutdown || cc == nil {
            reconnectError = cm.attemptReconnect(namespace, kubeServiceName) //this code will seek out the service in the cluster and reconnect if it exists.
            if reconnectError != nil {
                log.Debug().Err(reconnectError).Msgf(""error reconnecting grpc client on connection fail %s"", kubeServiceName)
                return reconnectError
            }
        }

        md := metadata.Pairs(
            ""caller-service"", cm.settings.clientName,
            ""caller-tenant-id"", cm.settings.clientTenantConfig.TenantUUID.String(),
            ""caller-tenant-type"", cm.settings.clientTenantConfig.TenantType,
            ""caller-tenant-name"", cm.settings.clientTenantConfig.TenantName,
            ""caller-tenant-short-name"", cm.settings.clientTenantConfig.TenantShortName,
            ""caller-tenant-sub-domain"", cm.settings.clientTenantConfig.TenantSubDomain,
            ""caller-tenant-liege-id"", cm.settings.clientTenantConfig.TenantLiegeUUID.String(),
        )
        if cm.settings.clientTenantConfig.TenantType != ""lord"" {
            md.Append(""caller-liege-tenant-id"", cm.settings.clientTenantConfig.TenantLiegeUUID.String())
        }

        ctx = metadata.NewOutgoingContext(ctx, md)
        return invoker(ctx, method, req, reply, cc, opts...)
    }
}
```

The problem I am having is this.

I am running in Kubernetes - I have tried simulating a shutdown state by disabling the deployment of the called service, deleting the Kubernetes service - implementing a network policy to block traffic to the called service,,, all sorts of things - and I only ever get a TransientFailure state. never a Shutdown state. I also have set the grpc server keep alive settings to less than a minute for the calling service. It does not seem to work.

Am i wasting my time - do I need to worry about connection cutoffs like this for unary requests?
Am i thinking about this wrongly? why should i worry about this?

Need others with some wisdom to chime in.","kubernetes, grpc, grpc-go",,,,2025-01-13T20:26:16
79353181,How do I get a certificate (public and private key) into a windows container in AKS?,Given a **windows** container running inside Azure Kubernetes Service (AKS). How do I get a certificate (PFX) that I've stored in Azure Key Vault (AKV) stored in the local certificate store of the container?,"powershell, kubernetes, cryptography, azure-aks, azure-keyvault",79353182.0,"N.B. This assumes you've already successfully gotten AKS wired up and talking to AKV. Pause and start [elsewhere](https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver) if you've not successfully brought simple passwords across into the environment of your windows container yet.

The trick is to recognise that when you install a certificate (PFX) into keyvault this is accessed as two separate objects and you can get these pulled into the environment as a combined PEM if you setup your k8s secret provider appropriately.

First you must setup your k8s secrets to request it as an objecttype of 'secret' (not key or cert) e.g. :

```
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: sc-demo-keyvault-csi
spec:
  provider: azure
  parameters:
    usePodIdentity: ""false""
    useVMManagedIdentity: ""true""                                   # Set to true for using managed identity
    userAssignedIdentityID: <redacted>   # Set the clientID of the user-assigned managed identity to use
    keyvaultName: <redacted>                                     # Set to the name of your key vault
    objects:  |
      array:
        - |
          objectName: testcert           # keyvault secret name
          objectType: secret             # getting a cert as a secret returns the public & private key pair as a pem, a type of cert just returns the public key (https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/getting-certs-and-keys/)
    tenantId: <REDACTED>                # The tenant ID of the key vault
  secretObjects:
  - data:
    - key: secretcert
      objectName: testcert
    secretName: foosecret
    type: Opaque
```

Once this is done and you've mapped the secret through to your container as an environment variable in your deployment/pod description e.g.

```
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
        - name: test
          env:
            - name: SIGNING_KEYPAIR
              valueFrom:
                secretKeyRef:
                  name: foosecret
                  key: secretcert
          volumeMounts:
            - name : secrets-store01-inline
              mountPath: ""/mnt/secrets-store""
              readOnly: true
      volumes:
        - name: secrets-store01-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: 'sc-demo-keyvault-csi'
```

If you were to fire up your windows container at this point you'd find your environment contains a PEM file (I assume there's a potential issue here around the size of the certificates but not something I@ve run into.)

So then we just need to take that PEM, reconstruct into into a PFX file, load it into the certificate store in the container and apply the appropriate permissions.

Something like this works in powershell:

```
# Extract the keys from the environment variable
$matches = [regex]::match($Env:SIGNING_KEYPAIR,'(?smi)-----BEGIN PRIVATE KEY-----\s*(.+)-----END PRIVATE KEY-----\s*-----BEGIN CERTIFICATE-----\s*(.+)-----END CERTIFICATE-----')
$PRIVATE_KEY= $matches.Groups[1].Value
$PUBLIC_KEY= $matches.Groups[2].Value

# Write them out to a random file pair
$RANDOM_FILE= New-Guid
Out-File -FilePath ""$RANDOM_FILE.key"" -InputObject $PRIVATE_KEY
Out-File -FilePath ""$RANDOM_FILE.cer"" -InputObject $PUBLIC_KEY

# Create the PFX (the .key file will be attached as it shares the same filename)
& certutil -p ""ignored,$RANDOM_FILE"" -MergePFX ""$RANDOM_FILE.cer"" ""$RANDOM_FILE.pfx""  | Out-Null
$c= Import-PfxCertificate -Password (ConvertTo-SecureString -String ""$RANDOM_FILE"" -AsPlainText -Force) -FilePath ""$RANDOM_FILE.pfx"" -CertStoreLocation ""Cert:\LocalMachine\My""

# Cleanup the environment
# (doesn't really improve the security position, but I'd rather not have secrets in two places)
Remove-Item ""$RANDOM_FILE.*""
```

At this point you should have everything you need ($c.Thumbprint) to setup appropriate access to the private key as you would normally do.

The approach described here definitely works on containers based on mcr.microsoft.com/dotnet/framework/aspnet:4.8-windowsservercore-ltsc2019 . YMMV for containers based on other base containers.

edit: The mounting of the secrets described above is not required to get the secrets into the environment. Further as an alternative to grabbing the pair out of the environment, you can access the mounted secrets directly from c:\mnt.",2025-01-13T18:49:40,2025-01-13T18:49:40
79353071,Route traffic with nginx.org Ingress to different Kubernetes services based on cookies with values,"I would like to route traffic via Ingress in following scenario with 2 API Services:

```
kind: Service
metadata:
  name: api-v1
  namespace: api
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 8080
    - name: http2
      protocol: TCP
      port: 81
      targetPort: 8080
    - name: https
      protocol: TCP
      port: 443
      targetPort: 8080
  selector:
    k8s-app: api-v1
  type: ClusterIP
  internalTrafficPolicy: Cluster
```

```
kind: Service
metadata:
  name: api-v2
  namespace: api
spec:
  ports:
    - name: http
      protocol: TCP
      port: 80
      targetPort: 8080
    - name: http2
      protocol: TCP
      port: 81
      targetPort: 8080
    - name: https
      protocol: TCP
      port: 443
      targetPort: 8080
  selector:
    k8s-app: api-v2
  type: ClusterIP
  internalTrafficPolicy: Cluster
```

Based on incoming requests containing cookie ***Beta=1***, I would like to route traffic to Service api-v1 and based on incoming requests containing ***Beta=0*** or none ***Beta=*** cookie, I would like to route traffic to Service api-v2.

```
kind: Ingress
metadata:
  name: api
  namespace: api
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.org/server-snippets:
    nginx.org/location-snippets:
status:
  loadBalancer:
    ingress:
      - ip: 1.2.3.4
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - api.domain.com
      secretName: api-domain-com-tls
  rules:
    - host: api.domain.com
```

How to configure **nginx.org/server-snippets** and **nginx.org/location-snippets** or other to get this working?

Ref: [https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/configmap-resource/#snippets-and-custom-templates](https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/configmap-resource/#snippets-and-custom-templates)","kubernetes, nginx-ingress",79353350.0,"If you need to use Nginx open source, the ""closest"" solutions are based on IP addresses [IP Hash](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#choosing-a-load-balancing-method) or user-defined variables [hash](https://nginx.org/en/docs/http/ngx_http_upstream_module.html#hash).

If you want to use **session persistence**, which seems to be the case, check out the [Nginx PLUS documentation](https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/#enabling-session-persistence) where some examples of ""stickiness"" with cookies are present.",2025-01-13T20:18:25,2025-01-13T17:59:50
79351984,Does all the kubectl commands executed using put params in concourse explicitly do readiness check?,"I m trying to deploy a docker container into Kubernetes using concourse ci put params, I could see after executing the kubectl command it explicitly checks for the readiness of all the others pods present in the same namespace. I don’t want to include the readiness check of all pods other than the pod I m trying to deploy to.

```
    type: kubernetes
    icon: kubernetes
    source:
      insecure_skip_tls_verify: false
      kubeconfig: {{kubernetes-config}}

  - name: deploy_capability_docker_kubernetes
    plan:
      - get: bufferautomationsourcecode
      - get: docker_hub_details_capability_development
        passed: [build_deploy_hub]
        trigger: true
      - get: version
      - get: concoursesourcecode
      - task: update_deploymentfile_git
        file: concoursesourcecode/task/kubernetes_fileupdate_task_parameterized1.yaml
        params:
          BRANCH: ""Dev""
          SOURCE_CODE: ""bufferautomationsourcecode""
      - put: kubernetes-cluster-deployment
        params:
          kubectl: config current-context
      - put: kubernetes-cluster-deployment
        params:
          kubectl: get pods -l app=video-buffer-detect-app -n videoautomationcapabilities
      - put: kubernetes-cluster-deployment
        params:
          kubectl: get pods -n videoautomationcapabilities --show-labels
      - put: kubernetes-cluster-deployment
        params:
          kubectl: apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
        ensure:
          do:
            - put: kubernetes-cluster-deployment
              params:
                kubectl: wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s || true
              ensure:
                do:
                  - put: kubernetes-cluster-deployment
                    params:
                      kubectl: rollout status deployment/video-buffer-detect-app --timeout=300s

+ kubectl config current-context
anvil-dev-01-videoautomationcapabilities
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl get pods -l app=video-buffer-detect-app -n videoautomationcapabilities
NAME                                       READY   STATUS    RESTARTS   AGE
video-buffer-detect-app-7cddd646cb-m589k   1/1     Running   0          15m
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
deployment.apps/video-buffer-detect-app configured
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s
pod/video-buffer-detect-app-7cddd646cb-m589k condition met
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
+ kubectl rollout status deployment/video-buffer-detect-app --timeout=300s
deployment ""video-buffer-detect-app"" successfully rolled out
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)
```

There are total 22 pods present in the namespace Iam targeting buffer detect app pod, and has only 1 replica, despite of targeting the specific pod. I get these logs printed for every kubectl command I execute. I want to stop these readiness probe checking for other pods readiness. This is causing a false failure though the pod I targeted got deployed successfully and bc of other pod being in unready or crashed state showing a false failure in put step.

“waiting for pods to be ready for 30s ( interval:3s , selector: ‘ ‘)
waiting for pods to be ready… (22/22)","kubernetes, kubectl, concourse, concourse-pipeline, concourse-resource-types",79647430.0,"You're using a Kubernetes ""put"" step in Concourse CI like this:

```
- put: kubernetes-cluster-deployment
  params:
    kubectl: wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s || true
```

I already had a problem with the native put of concourse-CI and I went through a task finally tried something like:

```
- task: deploy-specific-pod
  config:
    platform: linux
    image_resource:
      type: registry-image
      source:
        repository: bitnami/kubectl
    inputs:
      - name: bufferautomationsourcecode
    params:
      KUBECONFIG: ((kubeconfig))
    run:
      path: sh
      args:
        - -exc
        - |
          echo ""$KUBECONFIG"" > /root/.kube/config
          kubectl apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
          kubectl wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s
          kubectl rollout status deployment/video-buffer-detect-app --timeout=300s
```",2025-06-01T10:45:08,2025-01-13T11:09:26
79351826,Kubernetes deployment issue for Camunda workflows in Spring Boot,"We have A Camunda Workflows application implemented in Spring Boot framework. This application is deployed on Kubernetes. We are facing following issue:

The pod is going in crashloopbackoff state repeatedly.

The HPA set for this application is 1 in the artifacts.devops properties file, following is the corresponding snippet:

```
devops.application.use.k8s.deployment.replicas.prod=1
#Container Size
devops.container.limits.memory=XXL
devops.container.limits.cpu=XXL

devops.application.hpa.memory.max.replicas.prod=1

devops.application.use.base.image=openjdk:8-jdk-alpine
```

This is because if we set the HPA greater than one, then
1> The Camunda Login page doesn't work
2> Multiple Process models may get triggered parallelly upon increasing the number.

So is there a way out of this? can we have HPA of 3 without parallel execution? The workflows are quite complex and heavy.

Note: I neither developed this application nor deployed it. People who did it are gone now.","spring-boot, kubernetes, camunda, hpa",,,,2025-01-13T10:16:58
79351457,Host header causing traffic to only one pod,"There is an issue where all traffic is being routed to a single Pod when using Host headers in istio enabled services communicating via Kubernetes service IP.

- pod `A1` and pod `A2` runs in namespace `A`
- Service `A` includes pods `A1` and `A2` and internal IP is `1.1.1.1`
- pod `B` runs in namespace `B`

When calling Service A using `curl 1.1.1.1` or `curl A.A.svc.cluster.local`, it performs round-robin load balancing without any issues.

When I rewrite the record `abc.xyz` to Service `A` in CoreDNS and call it using `curl abc.xyz` or `curl -H ""Host:abc.xyz"" A.A.svc.cluster.local`, requests are consistently routed to either Pod A1 or A2, rather than being load balanced.

Any help is highly appreciated!

We've found that the issue doesn't occur when istio proxy is absent from sender or receiver. Since this problem arose during the migration of the VirtualService settings to CoreDNS, we cannot resolve it using VirtualService.","kubernetes, dns, istio",,,,2025-01-13T07:41:26
79349874,NGINX Ingress Controller auth-url doesn&#39;t forward to the authentication service,"I have set up the NGINX Ingress Controller on my GKE cluster. I am trying to validate **`example.com`** before loading the page for the user. To achieve this, I created another service using FastAPI and deployed it to a subdomain. This service loads a page where the user can provide their credentials, and after a successful login, they should be redirected to example.com.

However, the `auth-url` annotation is not working for me. When I deployed this Ingress resource, it was supposed to forward the user to the domain `https://fastapi-auth.example.com/auth`, but nothing happened. The homepage simply loads without forcing the user to validate.

What could I be missing here?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ms-ingress
  namespace: code-oss
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/add-headers: 'Cache-Control: max-age=31536000; public'
    nginx.ingress.kubernetes.io/enable-access-log: ""true""
    nginx.ingress.kubernetes.io/enable-debug: ""true""
    nginx.ingress.kubernetes.io/proxy-buffering: ""on""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/auth-url: ""https://fastapi-auth.example.com/auth""

spec:
    ingressClassName: nginx
    tls:
    - hosts:
        - example.com
      secretName: ms-app-tls
    rules:
    - host: example.com
      http:
        paths:
        - path: /vfb-pod-one(/|$)(.*)
          pathType: ImplementationSpecific
          backend:
            service:
              name: vfb-pod-one
              port:
                number: 8000
        - path: /(.*)
          pathType: ImplementationSpecific
          backend:
            serviceName:  vfb-pod-one
            servicePortNumber: 8000
```","authentication, kubernetes, google-kubernetes-engine, nginx-ingress, ingress-controller",79417059.0,"Here is my solution that worked for me. I am only posting it so that people don’t think I’m doing this out of frustration.

```
nginx.ingress.kubernetes.io/auth-url: ""https://example.com/fast-api/login/check""
nginx.ingress.kubernetes.io/auth-signin: ""https://example.com/fast-api/auth?rd=$request_uri""
```",2025-02-06T07:20:07,2025-01-12T12:18:20
79349816,How to do pagination when using LabelSelector list pods,"I have a controller to list pod with LabelSelector. But it would get too many pods sometimes. I want to do pagination to get podlist.

I need to list pods without using cache, because for some reason I can't watch&list pod. I tried to use the `Limit `and `Continue `parameters in `ListOptions`. When I did not add the `LabelSelector`, I could get the pods by paging normally. Once the `LabelSelector `was added, all pods matching the `LabelSelector `were returned and the paging was invalid. I wonder if my usage was wrong or the client does not support this feature yet?

```
err = r.APIReader.List(ctx, podList, &client.ListOptions{
    LabelSelector: podSelector,
    FieldSelector: fields.OneTermEqualSelector(""status.phase"", ""Running""),
    Limit:         1,
    Continue:      continueToken,
})
```","go, kubernetes, kubernetes-go-client",,,,2025-01-12T11:33:27
79349279,Restrict external access to pod,"I have a helm chart configured with this service account:

```
apiVersion: v1
kind: Service
metadata:
  name: {{ include ""router.fullname"" . }}
  labels:
    {{- include ""router.labels"" . | nindent 4 }}
spec:a
  type: {{ .Values.service.type }}
  ports:
    - name: http
      nodePort: 30079 # Public port to access router resources. For example, http://<Kubernetes node IP>:30079
      protocol: TCP
      port: 80 # Will expose the kubernetes service within the cluster so communication between multiple different pods can happen and will redirect the request to TargetPort
      targetPort: 8180 # Microservice port. For router it's port 8180
  selector:
    {{- include ""router.selectorLabels"" . | nindent 4 }}
```

I need to access the pod only from internal pods. I would like to disable the public access. How I can implement this into the above configuration?","kubernetes, kubernetes-helm",79349830.0,"The `service: { type: }` controls this.  There are three [Service types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types), and `ClusterIP` is the one that's unreachable from outside the cluster.

With this setup, it should almost be enough to deploy with a Helm value setting that changes that setting

```
# deploy.yaml
service:
  type: ClusterIP
```

```
helm upgrade --install -f deploy.yaml ...
```

The one trick is that `nodePort:` isn't a valid setting for ClusterIP-type Services, so you also need to update your chart code to not deploy it.  (I'd also make the actual port number both optional and configurable.)

```
spec:
  type: {{ .Values.service.type }}
  ports:
    - name: http
{{- if and (ne .Values.service.type ""ClusterIP"") .Values.service.nodePort }}
      nodePort: {{ .Values.service.nodePort }}
{{- end }}
```",2025-01-12T11:42:02,2025-01-12T03:13:15
79346457,How to skip helm tests deployment in Grafana Tanka,"I am trying to deploy helm chart via tanka and as I have understood - tanka does helm template and then works with the generated outputs.

The issue with this is that all the chart tests are being rendered as well and then `tk diff/apply` fails.

Is there a way to ignore them?

Example:

```
local tanka = import 'github.com/grafana/jsonnet-libs/tanka-util/main.libsonnet';
local helm = tanka.helm.new(std.thisFile);

{
  myChart:  helm.template('chart', '../../charts/mychart', {
     namespace: 'mychart',
     values: {
       ...
     },
   }),
}
```","kubernetes, kubernetes-helm, jsonnet",79346469.0,"You can filter them out in your Jsonnet code by applying a filter to exclude any resource with this annotation.
Here's how you can modify your Jsonnet code:

```
local tanka = import 'github.com/grafana/jsonnet-libs/tanka-util/main.libsonnet';
local helm = tanka.helm.new(std.thisFile);

{
  myChart: helm.template('chart', '../../charts/mychart', {
    namespace: 'mychart',
    values: {
      ...
    },
  }).filter(function(resource)
    resource.metadata.annotations['helm.sh/hook'] != 'test'
  ),
}
```",2025-01-10T17:05:32,2025-01-10T16:59:52
79346457,How to skip helm tests deployment in Grafana Tanka,"I am trying to deploy helm chart via tanka and as I have understood - tanka does helm template and then works with the generated outputs.

The issue with this is that all the chart tests are being rendered as well and then `tk diff/apply` fails.

Is there a way to ignore them?

Example:

```
local tanka = import 'github.com/grafana/jsonnet-libs/tanka-util/main.libsonnet';
local helm = tanka.helm.new(std.thisFile);

{
  myChart:  helm.template('chart', '../../charts/mychart', {
     namespace: 'mychart',
     values: {
       ...
     },
   }),
}
```","kubernetes, kubernetes-helm, jsonnet",79346465.0,"It turns out tanka has a flag to handle this:

Usage example:

```
local tanka = import 'github.com/grafana/jsonnet-libs/tanka-util/main.libsonnet';
local helm = tanka.helm.new(std.thisFile);

{
  myChart:  helm.template('chart', '../../charts/mychart', {
     namespace: 'mychart',
     values: {
       ...
     },
    skipTests: true,
   }),
}
```",2025-01-10T17:03:18,2025-01-10T16:59:52
79344586,Dependent Vars not resolving on kubectl,"This is just a contrived example from [https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/](https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/) I'm just trying to create this behavior using kubectl command (not thru yaml file).

`k run hello --image=bash --env=""GREETING=\""Hello\"""" --env=""NAME=\""World\"""" --env=""MESSAGE=\""$(GREETING) $(NAME)\"""" --command -- echo $(MESSAGE)`

While the pod gets created, $MESSAGE does not resolve to ""Hello World"" like i expect. The yaml file generated:

```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cni.projectcalico.org/containerID: 33ecc343be235582afd9c0ae518ca437f3252512f813a71e803fa0dd13d634bd
    cni.projectcalico.org/podIP: 10.168.1.8/32
    cni.projectcalico.org/podIPs: 10.168.1.8/32
  creationTimestamp: ""2025-01-10T03:51:22Z""
  labels:
    run: hello
  name: hello
  namespace: default
  resourceVersion: ""9320""
  uid: f65f0819-f045-42b0-a031-991d3186df30
spec:
  containers:
  - command:
    - echo
    env:
    - name: GREETING
      value: '""Hello""'
    - name: NAME
      value: '""World""'
    - name: MESSAGE
      value: '"" ""'
    image: bash
    imagePullPolicy: Always
    name: hello
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
....
```

Can someone tell me what's wrong with my kubectl command?","kubernetes, kubectl",79344931.0,"**$MESSAGE** does not resolve to ""Hello World""  because MESSAGE is being set to **literal string** $(GREETING) $(NAME) . Such type of substitution does not work when using `kubectl run`.

You have to use the expanded values of GREETING and NAME.

As mentioned in the [document](https://spacelift.io/blog/kubernetes-environment-variables):

> Kubernetes environment variables are defined at the container level within your Kubernetes Pod manifests. Different containers within the Pod can, therefore, use their own sets of environment variables.
>
>
> You can set environment variables in the env and envFrom manifest fields. env is used to write key-value pairs with hardcoded values, whereas envFrom lets you populate environment variables from the contents of ConfigMaps, Secrets, and runtime Pod properties.

Try running the below command to resolve your issue:

```
$kubectl run hello --image=bash --env=""GREETING=Hello"" --env=""NAME=World"" --env=""MESSAGE=Hello World” --command -- echo $MESSAGE
```",2025-01-10T07:44:14,2025-01-10T04:09:33
79342925,AWS EKS External DNS keeps deleting and recreating records,"I have an EKS cluster that uses external-dns controller to create DNS records in Route53 for ingresses. this has been working seamlessly until recently it started deleting and recreating sets of records causing the apps to go off and back online every minute.

here's an example of my ingress manifest:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: test
  annotations:
    external-dns.alpha.kubernetes.io/hostname: stg.test.domain.com
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.name: ""staging-external""
    alb.ingress.kubernetes.io/listen-ports: '[{""HTTP"": 80}, {""HTTPS"": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
spec:
  ingressClassName: alb
  rules:
  - host: ""stg.test.domain.com""
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: test-service. ##service name
            port:
              number: 80
```

*Edit*
External-dns pod logs

```
time=""2025-01-10T08:51:45Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:45Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzonename> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/service-name""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/notification-controller""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/source-controller""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/metrics-server""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/aws-load-balancer-webhook-service""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service external-secrets/external-secrets-webhook""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/webhook-receiver""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service default/external-dns""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service default/kubernetes""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/eks-extension-metrics-api""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/kube-dns""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/service-name-ingress: [app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com [] app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app4.domain.com 300 IN A alb-FQDN.amazonaws.com [{alias true}], setting ttl=300""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzonename> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=info msg=""Applying provider record filter for domains: [domain.com. .domain.com.]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzoneId> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app3.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app3.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app2.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app2.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app3.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app2.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app1-backend.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app1.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app4.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app5.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1-backend.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1-backend.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app4.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app4.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app5.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app5.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""18 record(s) were successfully updated"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
```

Just keeps repeating these actions","amazon-web-services, kubernetes, amazon-eks, amazon-route53, external-dns",79345499.0,"I figured out what was causing the problem.

So I have two almost identical clusters(Staging and Production), they both use the same hosted zone on Route53 in their external-dns controller so they both have access to all the records there. So the logs I wasn't checking were the logs on the external-dns controller on the production cluster which actually logged the DELETE events causing the staging cluster to continue recreating them.

This was fixed by adding the following argument to the external-dns deployment manifest to make sure each external-dns instance only has access to manage the records it created.

```
containers:
        - name: external-dns
          ## other config ...
          args:
            - --txt-owner-id=unique.staging.cluster.string.id
            ## other args ...
```

The *--txt-owner-id* argument gives each record a unique string Id with which it will be managed without conflict.

Thanks to everyone for their time and suggestions",2025-01-10T11:11:15,2025-01-09T14:13:32
79342320,Environment variables with a comma,"In my k8s infrastructure I have the following environment variable declared: `password,role,enabled`, as a string.

When I check the envs, everything is fine and I can see :

`USER` : `password,role,enabled`

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-conf
  namespace: {{ .Release.Namespace }}
data:
  application-users.properties: |
    username: ${USER}
```

The problem is that those commas are giving me hard times, I don't know why, but they are not injected correctly in the configMap. If I edit the configMap manually and I insert directly `password,role,enabled`, everything will work.

Already tried with:

```
sa-mktboss: ""${USER}""
```

and:

```
sa-mktboss : password\,role\,enabled
```","kubernetes, environment-variables",,,,2025-01-09T10:50:37
79342084,GKE container kept crashing when deployed with GitHub Actions workflow,"I'm trying to deploy a pod of three containers (Vue, Express and MongoDB) on GKE using GitHub Actions. When deployed manually using the following commands:

```
kubectl apply -f deployment-sit.yaml
kubectl apply -f vue-service-sit.yaml
kubectl apply -f express-service-sit.yaml
kubectl apply -f mongodb-service-sit.yaml
kubectl apply -f sit-ingress.yaml
```

everthing worked out fine.

But when I tried to run the CI pipline that used exactly the same docker-compose file and Dockerfile for image build, the image seems to be built in away that the Express container just kept crashing. When the container crashed, not much warning or error messages was available other than:

```
stream closed EOF for default/seg-dashboard-sit-6c9d6d5798-lscd5 (seg-dashboard-sit-express)
```

and

```
 Warning  BackOff 3m30s (x24 over 8m26s)  kubelet Back-off restarting failed container seg-dashboard-sit-express in pod seg-dashboard-sit-6c9d6d5798-lscd5_default(7042eb1d-c7bc-455f-bfc5-4159733aa00e)
```

I'm pretty sure the problem happened in the build stage rather than the deploy stage since the built image wouldn't run properly when i tried to deploy the images built from the CI pipeline.

Here are my YAML files for reference. What can I try next?

express docker-compose.yaml

```
version: '3.8'
services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: seg-dashboard-vue
    env_file:
      - ./.env.production
    environment:
      - NODE_ENV=production
      - HOSTNAME=0.0.0.0
      - BASE_URL=/seg-dashboard/api
      - OPSGENIE_APIKEY=4075da9d-07b2-4db3-9388-2a1b7c851c78
      - OPSGENIE_APIURL=https://api.opsgenie.com/v2
    image: asia-east1-docker.pkg.dev/visitor-access-system/tools/seg-dashboard-express:latest
    platform: linux/amd64
```

express Dockerfile

```
FROM node:lts-jod AS build
LABEL maintainer=""jackylai327@gmail.com"" version=""1.0""
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
EXPOSE 11200
ENV NODE_ENV=production
ENV HOSTNAME=0.0.0.0
ENV BASE_URL=/seg-dashboard/api
ENV OPSGENIE_APIKEY=4075da9d-07b2-4db3-9388-2a1b7c851c78
ENV OPSGENIE_APIURL=https://api.opsgenie.com/v2
CMD [""npm"", ""start""]
```

ci-pipeline

```
name: SEG Dashboard CI/CD Pipeline

on:
  push:
    branches:
      # - main
      - develop

jobs:
  build:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '21'

    - name: Install Docker Compose
      run: |
        sudo apt-get update
        sudo apt-get install -y docker-compose

    - name: Build Docker images
      run: |
        docker-compose -f vue-app/docker-compose.yaml build --no-cache
        docker-compose -f express-app/docker-compose.yaml build --no-cache

    - name: Log in to Google Container Registry
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Configure Docker to use the gcloud command-line tool as a credential helper
      run: gcloud auth configure-docker asia-east1-docker.pkg.dev

    - name: Push Docker images to Google Container Registry
      run: |
        docker push asia-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/tools/seg-dashboard-vue:latest
        docker push asia-east1-docker.pkg.dev/${{ secrets.GCP_PROJECT_ID }}/tools/seg-dashboard-express:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Log in to Google Container Registry
      uses: google-github-actions/auth@v2
      with:
        credentials_json: ${{ secrets.GCP_SA_KEY }}

    - name: Set up gcloud Cloud SDK environment
      uses: google-github-actions/setup-gcloud@v2.1.2
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        install_components: 'kubectl,gke-gcloud-auth-plugin'

    - name: Configure gcloud and kubectl
      run: |
        gcloud container clusters get-credentials ${{ secrets.GKE_CLUSTER_NAME }} --zone ${{ secrets.GKE_CLUSTER_LOCATION }} --project ${{ secrets.GCP_PROJECT_ID }}
        gcloud config set project ${{ secrets.GCP_PROJECT_ID }}
        kubectl config current-context

    - name: Deploy to Kubernetes
      run: |
        kubectl apply -f deployment-sit.yaml
        kubectl apply -f vue-service-sit.yaml
        kubectl apply -f express-service-sit.yaml
        kubectl apply -f mongodb-service-sit.yaml
        kubectl apply -f sit-ingress.yaml
        kubectl rollout restart deployment seg-dashboard-sit
```

I have tried giving enough permissions on GCP AMI, checking the env variables, checking the healthcheck path, including the livenessProbe check and removing it.

I am expecting the containers run without crashing.","kubernetes, google-cloud-platform, github-actions, google-kubernetes-engine",,,,2025-01-09T09:33:51
79341797,Deploy .NET Aspire Orleans app to Kubernetes using Aspirate,"I have a simple .NET Aspire Orleans application that I'm trying to deploy to Azure using Aspirate.

I see that Aspirate deploys my containers to my AKS cluster in Azure but it doesn't deploy the Azure resources (storage account). But it does create a bicep file with the storage account to be deployed.

However, I cannot discover a way to configure the Aspire application to use the connection string from the deployed bicep file's storage account.

Here's the error output:

```
Unhandled exception. System.InvalidOperationException: Connection string doesn't have value for keyword '{storage.outputs.tableEndpoint}'.
   at Azure.Core.ConnectionString.Validate(String connectionString, String segmentSeparator, String keywordValueSeparator, Boolean allowEmptyValues)
   at Azure.Core.ConnectionString.Parse(String connectionString, String segmentSeparator, String keywordValueSeparator, Boolean allowEmptyValues)
   at Azure.Data.Tables.TableConnectionString.ParseInternal(String connectionString, TableConnectionString& accountInformation, String& error)
   at Azure.Data.Tables.TableConnectionString.Parse(String connectionString)
   at Azure.Data.Tables.TableServiceClient..ctor(String connectionString, TableClientOptions options)
   at Microsoft.Extensions.Hosting.AspireTablesExtensions.TableServiceComponent.<>c__DisplayClass0_0.<AddClient>b__0(TableClientOptions options, TokenCredential cred) in /_/src/Components/Aspire.Azure.Data.Tables/AspireTablesExtensions.cs:line 78
   at Microsoft.Extensions.Azure.AzureClientFactoryBuilder.<>c__DisplayClass8_0`2.<Azure.Core.Extensions.IAzureClientFactoryBuilderWithCredential.RegisterClientFactory>b__0(TOptions options, TokenCredential credential, IServiceProvider _)
   at Microsoft.Extensions.Azure.AzureClientFactoryBuilder.<>c__DisplayClass15_0`2.<RegisterClientFactory>b__0(IServiceProvider provider, Object options, TokenCredential credential)
   at Microsoft.Extensions.Azure.ClientRegistration`1.GetClient(IServiceProvider serviceProvider, Object options, TokenCredential tokenCredential)
   at Microsoft.Extensions.Azure.AzureClientFactory`2.CreateClient(String name)
   at Aspire.Azure.Common.AzureComponent`3.<>c.<AddClient>b__12_4(IServiceProvider serviceProvider, Object serviceKey) in /_/src/Components/Common/AzureComponent.cs:line 110
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier)
   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetRequiredKeyedService(Type serviceType, Object serviceKey, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderKeyedServiceExtensions.GetRequiredKeyedService[T](IServiceProvider provider, Object serviceKey)
   at Orleans.Hosting.AzureTableStorageClusteringProviderBuilder.<>c__DisplayClass0_0.<Configure>b__1(AzureStorageClusteringOptions options, IServiceProvider services) in /_/src/Azure/Orleans.Clustering.AzureStorage/AzureTableStorageClusteringProviderBuilder.cs:line 34
   at Microsoft.Extensions.Options.OptionsFactory`1.Create(String name)
   at Microsoft.Extensions.Options.UnnamedOptionsManager`1.get_Value()
   at Orleans.Runtime.MembershipService.AzureBasedMembershipTable..ctor(ILoggerFactory loggerFactory, IOptions`1 clusteringOptions, IOptions`1 clusterOptions) in /_/src/Azure/Orleans.Clustering.AzureStorage/AzureBasedMembershipTable.cs:line 33
   at System.RuntimeMethodHandle.InvokeMethod(Object target, Void** arguments, Signature sig, Boolean isConstructor)
   at System.Reflection.MethodBaseInvoker.InvokeDirectByRefWithFewArgs(Object obj, Span`1 copyOfArgs, BindingFlags invokeAttr)
   at System.Reflection.MethodBaseInvoker.InvokeWithFewArgs(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)
   at System.Reflection.RuntimeConstructorInfo.Invoke(BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier)
   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService(Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService(IServiceProvider provider, Type serviceType)
   at Orleans.Configuration.Internal.ServiceCollectionExtensions.<>c__DisplayClass1_0.<AddFromExisting>b__0(IServiceProvider sp) in /_/src/Orleans.Core/Configuration/ServiceCollectionExtensions.cs:line 48
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier)
   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService(Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService(IServiceProvider provider, Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService[T](IServiceProvider provider)
   at Orleans.Hosting.DefaultSiloServices.<>c.<AddDefaultServices>b__1_8(IServiceProvider sp) in /_/src/Orleans.Runtime/Hosting/DefaultSiloServices.cs:line 272
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier)
   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngineScope.GetService(Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService(IServiceProvider provider, Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService[T](IServiceProvider provider)
   at Orleans.Runtime.Silo..ctor(ILocalSiloDetails siloDetails, IServiceProvider services) in /_/src/Orleans.Runtime/Silo/Silo.cs:line 73
   at System.RuntimeMethodHandle.InvokeMethod(Object target, Void** arguments, Signature sig, Boolean isConstructor)
   at System.Reflection.MethodBaseInvoker.InvokeDirectByRefWithFewArgs(Object obj, Span`1 copyOfArgs, BindingFlags invokeAttr)
   at System.Reflection.MethodBaseInvoker.InvokeWithFewArgs(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)
   at System.Reflection.RuntimeConstructorInfo.Invoke(BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitIEnumerable(IEnumerableCallSite enumerableCallSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitRootCache(ServiceCallSite callSite, RuntimeResolverContext context)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor`2.VisitCallSite(ServiceCallSite callSite, TArgument argument)
   at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.CreateServiceAccessor(ServiceIdentifier serviceIdentifier)
   at System.Collections.Concurrent.ConcurrentDictionary`2.GetOrAdd(TKey key, Func`2 valueFactory)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(ServiceIdentifier serviceIdentifier, ServiceProviderEngineScope serviceProviderEngineScope)
   at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService(IServiceProvider provider, Type serviceType)
   at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetRequiredService[T](IServiceProvider provider)
   at Microsoft.Extensions.Hosting.Internal.Host.StartAsync(CancellationToken cancellationToken)
   at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token)
   at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.RunAsync(IHost host, CancellationToken token)
   at Microsoft.Extensions.Hosting.HostingAbstractionsHostExtensions.Run(IHost host)
```

When attempting to deploy via azd, it appears that my only option is to deploy to a Container Apps instance and I need this deployed to AKS.

The storage account's name is a unique id generated from the resource group name. It is not clear how to pass the storage account's resource group name into the configuration settings for my Aspire AppHost so that it understands where to get the connection string from.

Because I'm using Orleans there are way more environment variables that must be assigned instead of just the connection string in order for the Orleans app to even startup and recognize its providers properly. So, checking if the ExecutionContext is running vs publishing to override the connection string doesn't generate the details necessary for the Aspire Orleans package to pickup the configuration settings as far as which providers to use, the CluserId, etc.

How would I go about informing the AppHost about the connection string I want to use for the storage account with minimal modifications to the code example below?

Here are the steps I follow to deploy my application:

1. Execute `aspirate init`
2. Execute 'aspirate build'
3. Execute 'aspirate generate'
4. Execute 'aspirate apply'

Here's the AppHost Program.cs code:

```
var builder = DistributedApplication.CreateBuilder(args);

var storage = builder.AddAzureStorage(""storage"");
var clusteringTable = storage.AddTables(""clustering"");
var grainStorage = storage.AddBlobs(""grain-state"");

var orleans = builder.AddOrleans(""default"")
    .WithClustering(clusteringTable)
    .WithGrainStorage(""Default"", grainStorage);

builder.AddProject<Projects.IdentityBackend>(""identitybackend"")
    .WithReference(orleans)
    .WithReplicas(3);

builder.AddProject<Projects.Identity>(""identityfrontend"")
    .WithReference(orleans.AsClient())
    .WithExternalHttpEndpoints()
    .WithReplicas(3);

builder.Build().Run();
```",".net, kubernetes, orleans, dotnet-aspire",,,,2025-01-09T08:02:23
