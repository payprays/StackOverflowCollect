Question ID,Question Title,Question Body,Question Tags,Answer ID,Answer Body,Answer Creation Date,Question Creation Date,newAnswer Body
79839554,How to add a tty on pod?,"I can exec a bash shell using this command

```
kubectl exec --stdin --tty ftp1-7686766766-8v5s2 -- /bin/bash
bash-4.2#
```

but I want to know why kubectl attach don't work

```
kubectl attach -it ftp1-7686766766-8v5s2
error: Unable to use a TTY - container ftp1 did not allocate one
All commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.
If you don't see a command prompt, try pressing enter.
```

I have tried this way (yaml fails too)

```
kubectl get po ftp1-7686766766-8v5s2 -o json  > ftp.json
```

then edit json

```
    ""tty"": ""true"",
```

but...

```
kubectl apply  -f ftp.json
Warning: resource pods/ftp1-7686766766-8v5s2 is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.

another error

The Pod ""ftp1-7686766766-8v5s2"" is invalid: spec: Forbidden: pod updates may not change fields other than `spec.containers[*].image`,`spec.initContainers[*].image`,`spec.activeDeadlineSeconds`,`spec.tolerations` (only additions to existing tolerations),`spec.terminationGracePeriodSeconds` (allow it to be set to 1 if it was previously negative)
@@ -127,9 +127,9 @@
    ""TerminationMessagePolicy"": ""File"",
    ""ImagePullPolicy"": ""Never"",
    ""SecurityContext"": null,
-   ""Stdin"": false,
-   ""StdinOnce"": false,
-   ""TTY"": false
+   ""Stdin"": true,
+   ""StdinOnce"": true,
+   ""TTY"": true
   }
  ],
```

If I remove

```
    ""tty"": ""true"",
```

works (but no tty! so attach don't work)

```
kubectl apply  -f ftp.json
pod/ftp1-7686766766-8v5s2 configured
```

any idea? My target is create a pod with tty so attach can work.

EDIT: I solve the first error, I had to delete pod and recreate with kubectl apply -f

but...it create without tty!

```
kubectl attach -it ftp1-7686766766-pslz7
error: Unable to use a TTY - container ftp1 did not allocate one
All commands and output from this session will be recorded in container logs, including credentials and sensitive information passed through the command prompt.
If you don't see a command prompt, try pressing enter.
```

This command confirm tty are missing

```
kubectl get po ftp1-7686766766-pslz7 -o yaml|egrep -i 'tty|stdin'
```",kubernetes,79839576.0,"The issue is that **tty, stdin**, and **stdinOnce** are immutable fields. They can only be set when the Pod is created, not modified afterward. You need to modify the **Deployment/StatefulSet** that creates the Pod.

Edit the Deployment (not the Pod):

`kubectl edit deployment ftp1`

and add these under the containers:

```
spec:
  template:
    spec:
      containers:
      - name: ftp1
        image: your-image
        stdin: true
        stdinOnce: true
        tty: true
```",2025-12-06T08:52:21,2025-12-06T08:10:35,"```
kubectl edit deployment ftp1
```

The issue is that **tty, stdin**, and **stdinOnce** are immutable fields. They can only be set when the Pod is created, not modified afterward. You need to modify the **Deployment/StatefulSet** that creates the Pod.

Edit the Deployment (not the Pod):

---

```yaml
spec:
  template:
    spec:
      containers:
      - name: ftp1
        image: your-image
        stdin: true
        stdinOnce: true
        tty: true
```

and add these under the containers:"
79838905,Turning off buffering in k8s nginx ingress greatly increases client side latency and streaming in downstream,"I have the following architecture:

`Browser App (React)` -> `Nginx K8S Ingress` -> `Streaming Service A (Kotlin, POD)` -> `Streaming Service B (Java,POD)`

From the browser I upload a json array of 500 MB. `Service A`  proxies the request using streaming, `Service B` processes that `InputStream` in chunks of 500 documents.

When request buffering is `ON` in the Nginx ingress, it takes **~10ms** for `Service B` to fetch a chunk of data (500 docs, ~32 megabytes) from the InputStream. Uploading and processing the JSON from the client's perspective takes only a **couple of seconds**.

When request buffering is `OFF` in the Nginx ingress, it takes ~**4.5 seconds** for `Service B` to fetch a chunk of data from the `InputStream` and the entire request takes **minutes** to complete from the client's perspective.

Can you help me understand why there is such a huge slowdown when buffering is off?","kubernetes, nginx, network-programming, streaming, kubernetes-ingress",79840841.0,"I double checked if the services are bottlenecks but I managed to rule them out.

I deployed a netcat with nginx ingress to the cluster, called it from my machine and I see the same behaviour.

With request buffering on, the 300 mb request finishes in 2-3 seconds and I can see the whole request in the log file.

With buffering off it takes at least a minute.

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netcat-service
  namespace: staging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netcat-service
  template:
    metadata:
      labels:
        app: netcat-service
    spec:
      containers:
        - name: netcat
          image: ubuntu:26.04
          command: [""/bin/sh"", ""-c""]
          args:
            - apt update && apt install netcat-openbsd && apt install less && nc -lk -p 8080 > /tmp/requests.log
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: netcat-service
  namespace: staging
spec:
  selector:
    app: netcat-service
  ports:
    - protocol: TCP
      name: http
      port: 80
      targetPort: 8080
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: netcat-ingress
  namespace: staging
  annotations:
    cert-manager.io/cluster-issuer: ""letsencrypt""
    nginx.ingress.kubernetes.io/limit-rps: ""10""
    nginx.ingress.kubernetes.io/proxy-body-size: ""600m""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
spec:
  ingressClassName: nginx
  rules:
    - host: mypublicdns
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: netcat-service
                port:
                  number: 80
  tls:
    - hosts:
        - mypublicdns
      secretName: netcat-service-tls
```",2025-12-08T10:35:24,2025-12-05T13:00:07,"```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: netcat-service
  namespace: staging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: netcat-service
  template:
    metadata:
      labels:
        app: netcat-service
    spec:
      containers:
        - name: netcat
          image: ubuntu:26.04
          command: [""/bin/sh"", ""-c""]
          args:
            - apt update && apt install netcat-openbsd && apt install less && nc -lk -p 8080 > /tmp/requests.log
          ports:
            - containerPort: 8080
---
apiVersion: v1
kind: Service
metadata:
  name: netcat-service
  namespace: staging
spec:
  selector:
    app: netcat-service
  ports:
    - protocol: TCP
      name: http
      port: 80
      targetPort: 8080
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: netcat-ingress
  namespace: staging
  annotations:
    cert-manager.io/cluster-issuer: ""letsencrypt""
    nginx.ingress.kubernetes.io/limit-rps: ""10""
    nginx.ingress.kubernetes.io/proxy-body-size: ""600m""
    nginx.ingress.kubernetes.io/proxy-request-buffering: ""off""
spec:
  ingressClassName: nginx
  rules:
    - host: mypublicdns
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: netcat-service
                port:
                  number: 80
  tls:
    - hosts:
        - mypublicdns
      secretName: netcat-service-tls
```

I double checked if the services are bottlenecks but I managed to rule them out.

I deployed a netcat with nginx ingress to the cluster, called it from my machine and I see the same behaviour.

With request buffering on, the 300 mb request finishes in 2-3 seconds and I can see the whole request in the log file.

With buffering off it takes at least a minute."
79815411,YQ: load a text file as array and use it for an operation,"I have a yaml file with an arbitrary amount of documents, and I'm trying to replace all missing namespaces for namespaceable resources with an arbitrary input one.

Getting the non-namespaceable resources is easy:

```
kubectl api-resources --namespaced=false --no-headers | awk '{print $NF}' > /tmp/bad_resources.yaml
```

The problem is using this list in YQ (mike farah's).

This code works for hardcoded resources:

```
      NAMESPACE=""$NAMESPACE"" yq  '
        select(.kind != ""Namespace"" and .kind != ""CustomResourceDefinition"") |
        .metadata.namespace = (.metadata.namespace // strenv(NAMESPACE))
      ' ""$INPUT"" > ""$OUTPUT""
```

How can I replace this hardcoded list with the list I generated via `kubectl`?

I'm kind of going crazy with this, even LLMs utterly fail at this and keep mistaking `yq` versions and suggesting input arguments that don't exist.

Sample yaml:

```
---
kind: Namespace
metadata:
  name: test
---
kind: ConfigMap
metadata:
  name: test1
  namespace: asd
---
kind: ConfigMap
metadata:
  name: test2
```

In this example, it should be able to add the namespace to the Configmap `test2`, but not change `test1`, nor add it to `Namespace`, because `Namespace` is not a namespaceable resource. The output should be the same, except for the added namespace, so the last resource should have a new `metadata.namespace` field with the input namespace.

The `kubectl` list of resources looks like this:

```
Namespace
Node
PersistentVolume
```

Given that I'm generating it with the command I posted above, I can manipulate this, so it could also be a yaml array.","bash, kubernetes, yq",79815454.0,"Use `load_str` to load a text file, `/` to split by lines, and `all_c` to check against all items:

```
NAMESPACE=""nsp"" goyq '
  (load_str(""list.txt"") / ""\n"") as $list
  | select(.kind as $kind | $list | all_c(. != $kind))
    .metadata.namespace |= . // strenv(NAMESPACE)
' sample.yaml
```

```
---
kind: Namespace
metadata:
  name: test
---
kind: ConfigMap
metadata:
  name: test1
  namespace: asd
---
kind: ConfigMap
metadata:
  name: test2
  namespace: nsp
```

using [mikefarah/yq](https://github.com/mikefarah/yq) v4.32+

(Replacing `/ ""\n""` with `| split(""\n"")` will make it work with v4.18+)",2025-11-10T10:01:52,2025-11-10T09:37:46,"```bash
NAMESPACE=""nsp"" goyq '
  (load_str(""list.txt"") / ""\n"") as $list
  | select(.kind as $kind | $list | all_c(. != $kind))
    .metadata.namespace |= . // strenv(NAMESPACE)
' sample.yaml
```

Use `load_str` to load a text file, `/` to split by lines, and `all_c` to check against all items:

using [mikefarah/yq](https://github.com/mikefarah/yq) v4.32+

(Replacing `/ ""\n""` with `| split(""\n"")` will make it work with v4.18+)

```yaml
---
kind: Namespace
metadata:
  name: test
---
kind: ConfigMap
metadata:
  name: test1
  namespace: asd
---
kind: ConfigMap
metadata:
  name: test2
  namespace: nsp
```"
79804562,How to translate caddy to ingress nginx controller,"I'm having this config from Caddy and I want to migrate it to ingress nginx controller

```
    @restrictAccess {
        path /path1/loc1/*
        path /path2/loc3/*
    }
    route @restrictAccess {
        forward_auth check-auth:1221 {
            uri /review/request
            copy_headers Cookie
            @deniedAccess status 403
            handle_response @deniedAccess {
                respond ""Access denied!"" 403
            }
        }

        @pathOrigin header Origin *
        header @pathOrigin {
            +Vary ""Origin""
            +Access-Control-Allow-Credentials ""true""
            +Access-Control-Allow-Origin ""{http.request.header.Origin}""
        }
    }
```

What I'm having right now for ingress is:
(LE with the solution maybe will help someone else)

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://check-auth.default.svc.cluster.local:1221//review/request
    nginx.ingress.kubernetes.io/auth-snippet: |
       if ( $request_uri !~ ^/path1/loc1/ ) {
         return 200;
       }
    nginx.ingress.kubernetes.io/configuration-snippet: |
       if ( $request_uri ~ ^/path1/loc1/ ) {
         more_set_headers ""Access-Control-Allow-Origin: $http_origin"";
         more_set_headers ""Access-Control-Allow-Credentials: true"";
         more_set_headers ""Vary: Origin"";
         more_set_headers ""Cookie: $http_cookie"";
       }
  name: ingress-1
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: page
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
```

but don't know how to actually finish this.

Any help is more than welcome.","kubernetes, nginx, nginx-ingress, caddy, caddyfile",79805658.0,"Founded the solution.

Use of `auth-url` and `auth-snippet` will do the trick

The end result will look like:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://check-auth.default.svc.cluster.local:1221//review/request
    nginx.ingress.kubernetes.io/auth-snippet: |
       if ( $request_uri !~ ^/path1/loc1/ ) {
         return 200;
       }
    nginx.ingress.kubernetes.io/configuration-snippet: |
       if ( $request_uri ~ ^/path1/loc1/ ) {
         more_set_headers ""Access-Control-Allow-Origin: $http_origin"";
         more_set_headers ""Access-Control-Allow-Credentials: true"";
         more_set_headers ""Vary: Origin"";
         more_set_headers ""Cookie: $http_cookie"";
       }
  name: ingress-1
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: page
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
```",2025-10-31T10:24:37,2025-10-30T09:06:22,"```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/auth-url: http://check-auth.default.svc.cluster.local:1221//review/request
    nginx.ingress.kubernetes.io/auth-snippet: |
       if ( $request_uri !~ ^/path1/loc1/ ) {
         return 200;
       }
    nginx.ingress.kubernetes.io/configuration-snippet: |
       if ( $request_uri ~ ^/path1/loc1/ ) {
         more_set_headers ""Access-Control-Allow-Origin: $http_origin"";
         more_set_headers ""Access-Control-Allow-Credentials: true"";
         more_set_headers ""Vary: Origin"";
         more_set_headers ""Cookie: $http_cookie"";
       }
  name: ingress-1
  namespace: default
spec:
  ingressClassName: nginx
  rules:
  - host: example.com
    http:
      paths:
      - backend:
          service:
            name: page
            port:
              number: 80
        path: /
        pathType: ImplementationSpecific
```

Founded the solution.

Use of `auth-url` and `auth-snippet` will do the trick

The end result will look like:"
79801711,How to overwrite an env value with Helm,"I want to deploy an app on Kubernetes with Helm. This app is composed of multiple parts, 2 of them are a Spring backend and a Mongo database.

I want to deploy theme in 2 pods and have them talk with each other, so I set up a service to allow my DB and my backend to talk.

Here is my service:

```
apiVersion: v1
kind: Service
metadata:
  name: mongo-{{ .Values.global.branch }}
  namespace: {{ default .Release.Namespace .Values.global.namespace }}
spec:
  selector:
    app: mongo-{{ .Values.global.branch }}
  ports:
    - port: {{ .Values.mongo.port }}
      targetPort: {{ .Values.mongo.port }}
```

Here is some of my `values.yaml` file

```
global:
  namespace: """"
  branch: ""poc-cicd""

backend:
  repository: backcicd
  tag: ""{{ .Values.global.branch }}""
  replicas: 1
  port: 8080

mongo:
  repository: mongocicd
  tag: ""{{ .Values.global.branch }}""
  port: 27017
  pullPolicy: IfNotPresent
```

Here is where I override the URL of my Mongo pod in the `backend-deployment.yaml` file:

```
          env:
            - name: MONGO_URL
              value: ""mongodb://root:pass@mongo-{{ .Values.global.branch }}:{{ .Values.mongo.port }}""
```

Everything works when I try it in Minikube but when I push to test on the real cluster I get this error:

```
Caused by: com.mongodb.MongoTimeoutException: Timed out after 30000 ms while waiting to connect. Client view of cluster state is {type=UNKNOWN, servers=[{address=mongo:27017, type=UNKNOWN, state=CONNECTING, exception={com.mongodb.MongoSocketException: mongo}, caused by {java.net.UnknownHostException: mongo}}]
```

here is the top of the error

```
org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'app': Unsatisfied dependency expressed through field 'sched': Error creating bean with name 'schedulerFactoryBean' defined in class path resource [org/poc/backend/app/scheduler/QuartzConfig.class]: Error while initializing the indexes
```","mongodb, kubernetes, kubernetes-helm",79836288.0,"You shold use the FQDN and PORT of the service for the Mongo URL, which is composed of `<service-name>.<namespace>.svc.cluster.local:<port>`

Considering you are deploying on the `default` namespace it should be:

```
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-{{ .Values.global.branch }}.default.svc.cluster.local:{{ .Values.mongo.port }}""
```

More information here [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services)

Also make sure to verify your variables are getting correctly replaced by doing a helm template:

`helm template <release-name> <chart-path-or-name> -f values.yaml`

Or test harcoding the MONGO_URL for now to discard any issue with variable replacement:

```
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-poc-cicd.default.svc.cluster.local:27017""
```",2025-12-02T20:03:09,2025-10-27T10:37:26,"You shold use the FQDN and PORT of the service for the Mongo URL, which is composed of `<service-name>.<namespace>.svc.cluster.local:<port>`

---

```yaml
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-{{ .Values.global.branch }}.default.svc.cluster.local:{{ .Values.mongo.port }}""
```

Considering you are deploying on the `default` namespace it should be:

More information here [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#namespaces-of-services)

---

`helm template <release-name> <chart-path-or-name> -f values.yaml`

Also make sure to verify your variables are getting correctly replaced by doing a helm template:

Or test harcoding the MONGO_URL for now to discard any issue with variable replacement:

---

```yaml
env:
- name: MONGO_URL
  value: ""mongodb://root:pass@mongo-poc-cicd.default.svc.cluster.local:27017""
```"
79797988,Helm subchart uses baseline values.yaml instead of merged values.yaml + values-dev.yaml when deployed via parent chart,"Here is my helm chart structure:

```
app/
 ├── Chart.yaml
 ├── values.yaml
 ├── values-dev.yaml
 └── templates/

app-test/
 ├── Chart.yaml
 ├── values.yaml
 ├── values-dev.yaml
 └── charts/
      └── app-1.0.0.tgz
```

I want the subchart app to use a combination of `values.yaml` + `values-dev.yaml` for certain environments when deploying app-test. Values in app-test are symbolic links to values in app.
I'm running the app with:

```
helm upgrade app-test ./app-test -f values.yaml -f values-dev.yaml
```

If I install only app, the combination of `values.yaml` + `values-dev.yaml` works correctly.
So the behavior of the subchart changes depending on whether it’s deployed standalone or as a dependency.

How can I make the subchart app use the dev values (merged with baseline `values.yaml`) when deploying the parent chart (app-test)?
Do I need to merge values manually when packaging the subchart, or is there a recommended Helm way to do this?","kubernetes, kubernetes-helm",79798056.0,"In fact, the Helm values are processed differently if a chart is deployed as an independent chart *vs.* if it is a dependency of another chart.  There is some discussion of this in the Helm documentation in [the general description of Helm values](https://docs.helm.sh/docs/topics/charts/#scope-dependencies-and-values), with a further example in [Subcharts and Global Values](https://docs.helm.sh/docs/chart_template_guide/subcharts_and_globals/#overriding-values-from-a-parent-chart).

If the chart is a top-level chart, then its settings are at the top level of the Helm values

```
appSpecificValue: something
```

But if it is a dependency of another chart, then its settings are under a key with the chart's name.

```
app:
  appSpecificValue: something
```

In both cases, the chart code sees `.Values` as the settings for this chart specifically, so if it is as a subchart, `.Values.appSpecificValue` sees the value under `app: { appSpecificValue: }`.  You can create a [`global:` top-level key](https://docs.helm.sh/docs/topics/charts/#global-values) that will be visible to all subcharts, but this probably doesn't fit your use case.

You don't describe how `app` and `app-test` are related.  If `app-test` just provides some extra Kubernetes artifacts to the application (a data-loading Job; a debugging Deployment/Service; an in-cluster database StatefulSet/Service) then the easiest approach will be to use two separate Helm releases for them.

```
helm upgrade app ./app -f values.yaml -f values-dev.yaml
helm upgrade app-test ./app-test -f values.yaml -f values-dev.yaml
```

With this setup `app-test` would not directly have `app` as a dependency.  You might need to pass `app`'s Helm release name as a value to `app-test`.

It also could make sense to move the `app-test` content directly into the `app` chart and have it controlled by Helm values.

```
{{-/* This was in app-test, but we can make it conditional in the main chart */-}}
{{- if .Values.debugService.enabled -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include ""chart.fullname"" . }}-debug
...
{{- end -}}
```

```
debugService:
  enabled: false
```

But there isn't a way for a chart to be used as both a top-level chart and a subchart, and also for identical `helm install -f extras.yaml` files to have the same effects on both; the YAML layout is different for subcharts.",2025-10-23T17:50:51,2025-10-23T15:56:05,"```yaml
appSpecificValue: something
```

This is a Helm values snippet for a top-level chart, where the settings are at the top level.

```yaml
app:
  appSpecificValue: something
```

This is a Helm values snippet for when the chart is used as a dependency; its settings are nested under a key with the chart's name.

```bash
helm upgrade app ./app -f values.yaml -f values-dev.yaml
helm upgrade app-test ./app-test -f values.yaml -f values-dev.yaml
```

These are shell commands showing how to deploy `app` and `app-test` as two separate Helm releases.

```yaml
{{-/* This was in app-test, but we can make it conditional in the main chart */-}}
{{- if .Values.debugService.enabled -}}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include ""chart.fullname"" . }}-debug
...
{{- end -}}
```

This is a Helm template/YAML snippet that conditionally creates a Deployment based on `.Values.debugService.enabled`.

```yaml
debugService:
  enabled: false
```

This is a values.yaml snippet defining the `debugService.enabled` flag used by the template above."
79779172,My django API “next” link uses a stale hostname only when receiving requests from GKE services (Like Cloud Run),"**What are the details of your problem?**

I have a application in Django that is deployed using GKE. It uses an ingress to deploy it...
Those `manifests.yml` that are used for deploying applications on Google Cloud...

The application deploys it successfully, I'm able to login, navigate, and do most of my tasks.
The problem is, whenever I try to request a API route that returns a list, I get the correct results but the `next` pagination link is built with a 'stale hostname'...

It should appear at like something like this: [https://www.my-api-url.com/api/stores/?page=1&active=true](https://www.my-api-url.com/api/stores/?page=1&active=true)
But the hostname is being built like this: [https://api-back.my-api-url.com/api/stores/?active=true&page=2](https://api-back.my-api-url.com/api/stores/?active=true&page=2)
This 'api-back' is actually the hostname of my API Container from Google Cloud... The Docker Container on Kubernetes.

But the thing is: I already configured in Django Settings the correct hosts, and I checked the Environment Variables... They all point to the correct URLs.
I also searched the entire codebase, Kubernetes manifests, and ingress configs and I can’t find this 'api-back.my-api-url.com' anywhere.

I found this related question stating DRF uses the request hostname for the paginator, but I still can’t figure out where the stale hostname is coming from: [How to change the host in next key in a paginated URL in django rest framework?](https://stackoverflow.com/questions/62421753/how-to-change-the-host-in-next-key-in-a-paginated-url-in-django-rest-framework)

**What did you try and what were you expecting?**

I expected my DRF and Django Settings to build next with the API URL defind in my settings and my enviroment variables.
Checklist of what I saw to see if it's OK or not:

- `ALLOWED_HOSTS` variable from `django.settings` contains the correct host.
- Tested with both `USE_X_FORWARDED_HOST` settings (True and False).
- I als configured `SECURE_PROXY_SSL_HEADER to ('HTTP_X_FORWARDED_PROTO', 'https')`.
- I double-checked my ingress configurations to see that it uses the correct public host... It did...
- Saw no hard-coded references to `api-back.my-api-url.com` in the repo or K8s manifests.

Despite that, the **`next`** link still shows the stale hostname.

What could be causing this behaviour?","django, kubernetes, django-rest-framework, pagination, nginx-ingress",79809027.0,"DRF builds pagination links using `request.build_absolute_uri()`, which depends on the `Host` header it receives.

If your app is behind a GKE Ingress or Load Balancer, it’s likely not forwarding the original host —

so Django sees your internal service name like `api-back.my-api-url.com`.

1. In your **Django settings.py**:

```
USE_X_FORWARDED_HOST = True
SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
```

2.In your **Ingress annotations**, preserve the original host:

```
nginx.ingress.kubernetes.io/use-forwarded-headers: ""true""
nginx.ingress.kubernetes.io/configuration-snippet: |
  proxy_set_header Host $host;
  proxy_set_header X-Forwarded-Host $host;
```

3.If needed, override DRF’s pagination link generator:

```
from rest_framework.pagination import PageNumberPagination

class FixedHostPagination(PageNumberPagination):
    def get_next_link(self):
        url = super().get_next_link()
        if url:
            return url.replace('api-back.my-api-url.com', 'www.my-api-url.com')
        return None
```

Why it happens:

Your ingress or proxy rewrites the `Host` header to the internal service name.

DRF uses that to build links, so you end up with stale internal URLs.",2025-11-04T14:30:03,2025-09-30T13:47:22,"```text
DRF builds pagination links using `request.build_absolute_uri()`, which depends on the `Host` header it receives.

If your app is behind a GKE Ingress or Load Balancer, it’s likely not forwarding the original host —

so Django sees your internal service name like `api-back.my-api-url.com`.

1. In your **Django settings.py**:
```

```yaml
USE_X_FORWARDED_HOST = True
SECURE_PROXY_SSL_HEADER = ('HTTP_X_FORWARDED_PROTO', 'https')
```

```text
2.In your **Ingress annotations**, preserve the original host:
```

```yaml
nginx.ingress.kubernetes.io/use-forwarded-headers: ""true""
nginx.ingress.kubernetes.io/configuration-snippet: |
  proxy_set_header Host $host;
  proxy_set_header X-Forwarded-Host $host;
```

```text
3.If needed, override DRF’s pagination link generator:
```

```python
from rest_framework.pagination import PageNumberPagination

class FixedHostPagination(PageNumberPagination):
    def get_next_link(self):
        url = super().get_next_link()
        if url:
            return url.replace('api-back.my-api-url.com', 'www.my-api-url.com')
        return None
```

```text
Why it happens:

Your ingress or proxy rewrites the `Host` header to the internal service name.

DRF uses that to build links, so you end up with stale internal URLs.
```"
79767130,Kubernetes HPA algorithm,"i have 2 questions:

1 - i'd like to know if i can set my hpa to compare the limits resources instead of the requests resources with the target utilization i choose for memory and cpu(i'm using normal metrics, not external or custom ones).

2 - i encountered the problem where i created a hpa that should scale up my deployment if memory or cpu hit 80%, now the app itself baseline usage is around 65% avg memory. so at peak times the hpa create another pod which is fine. the problem starts when scale down cant occur because of the calculation the hpa use to determine the desired replicas(it turns 1.1 >= to 2 pods) so for scale down to occur the avg memory should be around 50% which cant be. i tried to change the resources a bit and couldnt get fine results.

i'd be happy to get some advices","kubernetes, resources, openshift, scale, hpa",79767284.0,"1. No, you can't. HPA's built-in resource metrics always use requests, not limits.

If you need limits-based scaling, use custom metrics instead.

2. Quick fix - Increase resource requests:

Alternative approaches:

Lower HPA target from 80% to 70%

Tune scale-down behavior:

```
spec:
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

Your requests should be set so baseline usage = ~50-60% of requests, giving HPA room to scale down properly.

if you want to scale at 80% but need scale-down at 50%, set requests = baseline_usage / 0.5. So 650MB baseline needs ~1.3GB requests.",2025-09-17T11:47:24,2025-09-17T09:53:08,"```text
1. No, you can't. HPA's built-in resource metrics always use requests, not limits.

If you need limits-based scaling, use custom metrics instead.

2. Quick fix - Increase resource requests:

Alternative approaches:

Lower HPA target from 80% to 70%

Tune scale-down behavior:
```

```yaml
spec:
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
```

```text
Your requests should be set so baseline usage = ~50-60% of requests, giving HPA room to scale down properly.

if you want to scale at 80% but need scale-down at 50%, set requests = baseline_usage / 0.5. So 650MB baseline needs ~1.3GB requests.
```"
79759113,Conditional Argo Workflow Execution,"i have a simple workflow with dag, it runs the first job and depending on the output of that job, it will run either one or both of the following jobs named `optional-job-one` or `optional-job-two`. here is the part of my yaml file that does this:

```
workflowSpec:
  serviceAccountName: ""{{ .Values.serviceAccountName }}""
  entrypoint: mother
  templates:
    - name: mother
      dag:
        tasks:
          - name: main-job
            template: main-job-step

          - name: optional-job-one
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION1 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-one-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""

          - name: optional-job-two
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION2 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-two-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""
```

Now, i want to add a new step. This new step will run if either one or both of the `optional-job-one` or `optional-job-two` have ran and finished successfully. how do i do it? i asked the AI chatbots for help and i got this but it doesnt work and im lost (im super new to k8 and argo)!

```
- name: optional-job-three
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: false
    error: false
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```

Thank you in advance for your help!","kubernetes, argo-workflows",79767555.0,"Thanks to the previous response i came across this solution that works perfectly, its a bit ugly but it does the job:

```
- name: optional-job-three
  depends: ""(optional-job-one.Succeeded && optional-job-two.Skipped) || (optional-job-one.Skipped && optional-job-two.Succeeded) || (optional-job-one.Succeeded && optional-job-two.Succeeded)""
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```",2025-09-17T15:52:31,2025-09-08T15:52:17,"```yaml
- name: optional-job-three
  depends: ""(optional-job-one.Succeeded && optional-job-two.Skipped) || (optional-job-one.Skipped && optional-job-two.Succeeded) || (optional-job-one.Succeeded && optional-job-two.Succeeded)""
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```

Thanks to the previous response i came across this solution that works perfectly, its a bit ugly but it does the job:"
79759113,Conditional Argo Workflow Execution,"i have a simple workflow with dag, it runs the first job and depending on the output of that job, it will run either one or both of the following jobs named `optional-job-one` or `optional-job-two`. here is the part of my yaml file that does this:

```
workflowSpec:
  serviceAccountName: ""{{ .Values.serviceAccountName }}""
  entrypoint: mother
  templates:
    - name: mother
      dag:
        tasks:
          - name: main-job
            template: main-job-step

          - name: optional-job-one
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION1 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-one-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""

          - name: optional-job-two
            dependencies: [main-job]
            when: ""{{`{{tasks.main-job.outputs.parameters.command}} == OPTION2 || {{tasks.main-job.outputs.parameters.command}} == BothOptions`}}""
            templateRef:
              name: master-templater
              template: option-two-template
            arguments:
              parameters:
                - name: argument-one
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-one}}`}}""
                - name: argument-two
                  value: ""{{`{{tasks.main-job.outputs.parameters.argument-two}}`}}""
```

Now, i want to add a new step. This new step will run if either one or both of the `optional-job-one` or `optional-job-two` have ran and finished successfully. how do i do it? i asked the AI chatbots for help and i got this but it doesnt work and im lost (im super new to k8 and argo)!

```
- name: optional-job-three
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: false
    error: false
  templateRef:
    name: master-templater
    template: option-three-template
  arguments:
    parameters:
      - name: argument-one
        value: ""{{`{{tasks.scraper.outputs.parameters.argument-one}}`}}""
```

Thank you in advance for your help!","kubernetes, argo-workflows",79764130.0,"instead of doing .status directly in `when`, you should combine it with `dependencies`  because conditions don’t support checking `.status` directly. also you should use `continueOn.failed: true` to avoid aborting the task when one dependent fails :

```
  dependencies: [optional-job-one, optional-job-two]
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: true
    error: true
```",2025-09-14T06:45:47,2025-09-08T15:52:17,"```yaml
  dependencies: [optional-job-one, optional-job-two]
  when: ""{{tasks.optional-job-one.status}} == Succeeded || {{tasks.optional-job-two.status}} == Succeeded""
  continueOn:
    failed: true
    error: true
```

instead of doing .status directly in `when`, you should combine it with `dependencies`  because conditions don’t support checking `.status` directly. also you should use `continueOn.failed: true` to avoid aborting the task when one dependent fails :"
79738705,How to access keys with a period in the name?,"I'm working on a Helm deployment of an app that I've written. I thought a neat way of managing the configuration was to specify it in `values.yaml` in the following format:

```
configFiles:
  file.yaml:
    example:
      foo: bar
```

This makes the file name and content very clear, but I'm struggling to use it in the config map template:

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include ""example.fullname"" . }}
  labels:
    {{- include ""example.labels"" . | nindent 4 }}
data:
  file.yaml: |-
    {{- .Values.configFiles.file.yaml | toYaml | toString | nindent 4 }}
```

It seems like it doesn't resolve `file.yaml` as a key, but `yaml` as a subkey of `file`, which is obviously empty and I get the following error:

```
Error: template: ...: executing ""..."" at <.Values.configFiles.file.yaml>: nil pointer evaluating interface {}.yaml
```

I've tried escaping the `.` with backslash (`\.`) and putting quotes around the key `.Values.configFiles.""file.yaml""`, but that just makes it complain about bad characters.

How do I use the value of a key that has a period in it, in a Helm template?","kubernetes, kubernetes-helm",79738727.0,"There's a get method that does this:

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include ""example.fullname"" . }}
  labels:
    {{- include ""example.labels"" . | nindent 4 }}
data:
  file.yaml: |-
    {{- get .Values.configFiles ""file.yaml"" | toYaml | toString | nindent 4 }}
```",2025-08-18T12:43:46,2025-08-18T12:29:46,"```yaml
kind: ConfigMap
apiVersion: v1
metadata:
  name: {{ include ""example.fullname"" . }}
  labels:
    {{- include ""example.labels"" . | nindent 4 }}
data:
  file.yaml: |-
    {{- get .Values.configFiles ""file.yaml"" | toYaml | toString | nindent 4 }}
```

There's a get method that does this:"
79699411,ArgoCD ApplicationSet not deploying manifests from nested folder structure,"I'm trying to use ArgoCD ApplicationSet to deploy all manifests stored in my output/ folder in a Git repository.
Here is my folder structure:

```
output/
├── app1/
│   ├── deployment/
│   │   └── manifest1.yml
│   │   └── manifest2.yml
│   ├── service/
│   │   └── manifest.yml
│   └── serviceaccount/
│       └── manifest1.yml
│       └── manifest2.yml
├── app2/
│   ├── deployment/
│   │   └── manifest.yml
│   └── service/
│       └── manifest1.yml
│       └── manifest2.yml
└── app3/
    └── deployment/
        └── manifest.yml
```

And here the code of the appset:

```
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: output-appset
  namespace: my-namespace
spec:
  generators:
  - git:
      repoURL: https://gitlab.com/gitlab/repo.git
      revision: HEAD
      directories:
      - path: output/*
  template:
    metadata:
      name: '{{path.basenameNormalized}}'
      namespace: my-namespace
    spec:
      project: my-project
      source:
        repoURL: https://gitlab.com/gitlab/repo.git
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: my-namespace
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

The ApplicationSet creates successfully an Application for each app folder (app1, app2, app3), but each of them returns error:

```
'Lua returned an invalid health status'
```

None of my manifests gets deployed and when I try to open the Application in Argo I get error:

```
'Unable to load data: permission denied'
```

I want ArgoCD to automatically discover and deploy all manifests from the nested folder structure, ideally creating one Application per app folder (app1, app2, app3), but not necessarily. Ultimately I only need to deploy all the manifests found in the app's subfolders, I've already validated them by successfully running:

```
kubectl apply -f *.yaml -n my-namespace
```

How should I configure the ApplicationSet generator to handle this nested folder structure where manifests are located in subfolders within each application directory? I'd be also okay to use an Application instead.","kubernetes, argocd",79709423.0,"The solution was using a different ArgoCD project.

```
    spec:
      project: my-project
```

My-project was not allowing the creation of Application and ApplicationSet objects, so I had to use a different AppProject which doesn't explicitly deny the creation of these resources.",2025-07-21T17:16:12,2025-07-12T16:06:38,"```yaml
    spec:
      project: my-project
```

The solution was using a different ArgoCD project.

My-project was not allowing the creation of Application and ApplicationSet objects, so I had to use a different AppProject which doesn't explicitly deny the creation of these resources."
79699411,ArgoCD ApplicationSet not deploying manifests from nested folder structure,"I'm trying to use ArgoCD ApplicationSet to deploy all manifests stored in my output/ folder in a Git repository.
Here is my folder structure:

```
output/
├── app1/
│   ├── deployment/
│   │   └── manifest1.yml
│   │   └── manifest2.yml
│   ├── service/
│   │   └── manifest.yml
│   └── serviceaccount/
│       └── manifest1.yml
│       └── manifest2.yml
├── app2/
│   ├── deployment/
│   │   └── manifest.yml
│   └── service/
│       └── manifest1.yml
│       └── manifest2.yml
└── app3/
    └── deployment/
        └── manifest.yml
```

And here the code of the appset:

```
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: output-appset
  namespace: my-namespace
spec:
  generators:
  - git:
      repoURL: https://gitlab.com/gitlab/repo.git
      revision: HEAD
      directories:
      - path: output/*
  template:
    metadata:
      name: '{{path.basenameNormalized}}'
      namespace: my-namespace
    spec:
      project: my-project
      source:
        repoURL: https://gitlab.com/gitlab/repo.git
        targetRevision: HEAD
        path: '{{path}}'
      destination:
        server: https://kubernetes.default.svc
        namespace: my-namespace
      syncPolicy:
        automated:
          prune: true
          selfHeal: true
```

The ApplicationSet creates successfully an Application for each app folder (app1, app2, app3), but each of them returns error:

```
'Lua returned an invalid health status'
```

None of my manifests gets deployed and when I try to open the Application in Argo I get error:

```
'Unable to load data: permission denied'
```

I want ArgoCD to automatically discover and deploy all manifests from the nested folder structure, ideally creating one Application per app folder (app1, app2, app3), but not necessarily. Ultimately I only need to deploy all the manifests found in the app's subfolders, I've already validated them by successfully running:

```
kubectl apply -f *.yaml -n my-namespace
```

How should I configure the ApplicationSet generator to handle this nested folder structure where manifests are located in subfolders within each application directory? I'd be also okay to use an Application instead.","kubernetes, argocd",79705144.0,"You can set directory recursive = true for application/applicationset

Refer - [https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection](https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection)

```
apiVersion: argoproj.io/v1alpha1
kind: Application
spec:
  source:
    directory:
      recurse: true
```",2025-07-17T16:39:38,2025-07-12T16:06:38,"```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
spec:
  source:
    directory:
      recurse: true
```

You can set directory recursive = true for application/applicationset

Refer - [https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection](https://argo-cd.readthedocs.io/en/stable/user-guide/directory/#enabling-recursive-resource-detection)"
79699234,why Argoworkflows multi-app-docker-build image with kaniko fails,"new to argworkflows ** I am trying to create a workflow that takes the name of the application that we are going to build the docker image for and push it to an ECR .**

this is my workflow.yaml:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env3
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env3
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]
          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: path_inside_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.path_inside_repo}}""
            dependencies: [clone-and-check]
    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/application-$APP
            cd /workspace/application-$APP/$PATH_INSIDE_REPO
            ls -l
            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            cat /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo
            cat /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace
    - name: debug-list-files
      inputs:
        parameters:
          - name: name
          - name: path_inside_repo
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - ls -l /workspace
      volumeMounts:
        - name: workspace
          mountPath: /workspace
    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
          - name: path_inside_repo
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/application-{{inputs.parameters.name}}/{{inputs.parameters.path_inside_repo}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
        volumeMounts:
          - name: workspace
            mountPath: /workspace

  volumes:
    - name: workspace
      emptyDir: {}
```

I my kaniko step fails with this error  :
[![kaniko pod error](https://i.sstatic.net/itMYRCaj.png)](https://i.sstatic.net/itMYRCaj.png)

although i did the cat in the previous step and i did add the dockerfile to the shared volume i can't understand why it can't find the dockerfile .
this is the previous step of logs :
[![git-checkout](https://i.sstatic.net/BOQgjDrz.png)](https://i.sstatic.net/BOQgjDrz.png)","docker, kubernetes, amazon-ecr, argo-workflows, argo",79726984.0,"As @[Thomas Delrue](https://stackoverflow.com/users/4958265/thomas-delrue) pointed out, the issue was caused by using an `emptyDir` volume. However, instead of switching to a PersistentVolume (PV), I initially intended to use artifacts .

Here's my updated Argo Workflow file:

```
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]

          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            dependencies: [clone-and-check]

    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            # Clone to the artifact path
            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/source
            cd /workspace/source/$PATH_INSIDE_REPO

            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""

    - name: debug-list-files
      inputs:
        parameters:
          - name: name
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - |
            echo ""=== Listing /workspace/source ===""
            ls -la /workspace/source
            echo ""=== Listing application directory ===""
            ls -la /workspace/source/*/
            echo ""=== Finding Dockerfiles ===""
            find /workspace/source -name ""Dockerfile"" -type f

    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/source/{{inputs.parameters.name}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_SESSION_TOKEN
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SESSION_TOKEN
          - name: AWS_SDK_LOAD_CONFIG
            value: ""true""
```",2025-08-06T08:00:40,2025-07-12T11:34:10,"As @[Thomas Delrue](https://stackoverflow.com/users/4958265/thomas-delrue) pointed out, the issue was caused by using an `emptyDir` volume. However, instead of switching to a PersistentVolume (PV), I initially intended to use artifacts .

Here's my updated Argo Workflow file:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: build-image
  namespace: argo-workflows
spec:
  serviceAccountName: argo-workflow
  entrypoint: build-and-deploy-env
  arguments:
    parameters:
      - name: env_name
        value: test
      - name: aws_region
        value: eu-west-1
      - name: expiration_date
        value: ""2024-12-31T23:59:59Z""
      - name: values_path
        value: ./demo-app/helm/values.yaml
      - name: configurations
      - name: configurations
        value: '[{""keyPath"": ""global.app.main.name"", ""value"": ""updated-app""}, {""keyPath"": ""global.service.backend.port"", ""value"": 8080}]'
      - name: application_list
        value: '[{""name"": ""backend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/backend"", ""path_inside_repo"": ""backend""}, {""name"": ""frontend"", ""repo_url"": ""org/project-demo-app.git"", ""branch"": ""demo-app"", ""ecr_repo"": ""demo-app/frontend"", ""path_inside_repo"": ""frontend""}]'

  templates:
    - name: build-and-deploy-env
      dag:
        tasks:
          - name: build-push-app
            template: build-push-template
            arguments:
              parameters:
                - name: app
                  value: ""{{item}}""
            withParam: ""{{workflow.parameters.application_list}}""

    - name: build-push-template
      inputs:
        parameters:
          - name: app
      dag:
        tasks:
          - name: clone-and-check
            template: clone-and-check-template
            arguments:
              parameters:
                - name: app
                  value: ""{{inputs.parameters.app}}""

          - name: build-and-push
            template: kaniko-build-template
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
                - name: image_tag
                  value: ""{{tasks.clone-and-check.outputs.parameters.image_tag}}""
                - name: ecr_url
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_url}}""
                - name: ecr_repo
                  value: ""{{tasks.clone-and-check.outputs.parameters.ecr_repo}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            when: ""{{tasks.clone-and-check.outputs.parameters.build_needed}} == true""
            dependencies: [clone-and-check]

          - name: debug-list-files
            template: debug-list-files
            arguments:
              parameters:
                - name: name
                  value: ""{{tasks.clone-and-check.outputs.parameters.name}}""
              artifacts:
                - name: source-code
                  from: ""{{tasks.clone-and-check.outputs.artifacts.source-code}}""
            dependencies: [clone-and-check]

    - name: clone-and-check-template
      inputs:
        parameters:
          - name: app
      outputs:
        parameters:
          - name: name
            valueFrom:
              path: /tmp/name
          - name: image_tag
            valueFrom:
              path: /tmp/image_tag
          - name: ecr_url
            valueFrom:
              path: /tmp/ecr_url
          - name: ecr_repo
            valueFrom:
              path: /tmp/ecr_repo
          - name: path_inside_repo
            valueFrom:
              path: /tmp/path_inside_repo
          - name: build_needed
            valueFrom:
              path: /tmp/build_needed
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: bitnami/git:latest
        command: [bash, -c]
        args:
          - |
            set -e
            apt-get update && apt-get install -y jq awscli

            APP=$(echo '{{inputs.parameters.app}}' | jq -r '.name')
            REPO_URL=$(echo '{{inputs.parameters.app}}' | jq -r '.repo_url')
            BRANCH=$(echo '{{inputs.parameters.app}}' | jq -r '.branch')
            ECR_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.ecr_repo')
            PATH_INSIDE_REPO=$(echo '{{inputs.parameters.app}}' | jq -r '.path_inside_repo')

            # Clone to the artifact path
            git clone --branch $BRANCH https://x-access-token:$ALL_REPO_ORG_ACCESS@github.com/$REPO_URL /workspace/source
            cd /workspace/source/$PATH_INSIDE_REPO

            if [[ ! -f ""Dockerfile"" ]]; then
              echo ""Dockerfile not found in $PATH_INSIDE_REPO""
              exit 1
            fi

            COMMIT_HASH=$(git rev-parse --short HEAD)
            IMAGE_TAG=""${APP}-${BRANCH}-${COMMIT_HASH}-{{workflow.parameters.env_name}}""

            ECR_URL=""$AWS_ACCOUNT_ID.dkr.ecr.{{workflow.parameters.aws_region}}.amazonaws.com""
            EXISTS=$(aws ecr describe-images --repository-name $ECR_REPO --image-ids imageTag=$IMAGE_TAG 2>/dev/null || echo ""not-found"")

            if [[ ""$EXISTS"" != ""not-found"" ]]; then
              echo ""false"" > /tmp/build_needed
            else
              echo ""true"" > /tmp/build_needed
            fi

            echo ""$APP"" > /tmp/name
            echo ""$IMAGE_TAG"" > /tmp/image_tag
            echo ""$ECR_URL"" > /tmp/ecr_url
            echo ""$ECR_REPO"" > /tmp/ecr_repo
            echo ""$PATH_INSIDE_REPO"" > /tmp/path_inside_repo

        env:
          - name: ALL_REPO_ORG_ACCESS
            valueFrom:
              secretKeyRef:
                name: github-creds
                key: ALL_REPO_ORG_ACCESS
          - name: AWS_ACCOUNT_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCOUNT_ID
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""

    - name: debug-list-files
      inputs:
        parameters:
          - name: name
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: alpine:latest
        command: [sh, -c]
        args:
          - |
            echo ""=== Listing /workspace/source ===""
            ls -la /workspace/source
            echo ""=== Listing application directory ===""
            ls -la /workspace/source/*/
            echo ""=== Finding Dockerfiles ===""
            find /workspace/source -name ""Dockerfile"" -type f

    - name: kaniko-build-template
      inputs:
        parameters:
          - name: name
          - name: image_tag
          - name: ecr_url
          - name: ecr_repo
        artifacts:
          - name: source-code
            path: /workspace/source
      container:
        image: gcr.io/kaniko-project/executor:latest
        command:
          - /kaniko/executor
        args:
          - --context=dir:///workspace/source/{{inputs.parameters.name}}
          - --dockerfile=Dockerfile
          - --destination={{inputs.parameters.ecr_url}}/{{inputs.parameters.ecr_repo}}:{{inputs.parameters.image_tag}}
          - --cache=true
          - --verbosity=debug
        env:
          - name: AWS_REGION
            value: ""{{workflow.parameters.aws_region}}""
          - name: AWS_ACCESS_KEY_ID
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_ACCESS_KEY_ID
          - name: AWS_SECRET_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SECRET_ACCESS_KEY
          - name: AWS_SESSION_TOKEN
            valueFrom:
              secretKeyRef:
                name: registry-creds
                key: AWS_SESSION_TOKEN
          - name: AWS_SDK_LOAD_CONFIG
            value: ""true""
```"
79697905,The ocp application service cannot be connected correctly,"The route is set and the pod is started.

But when you enter [https://www.test.gov.tw/my-apps](https://www.test.gov.tw/my-apps)
an error page will appear.
[ocp error page](https://i.sstatic.net/xyN3CniI.png)

The following issues have already been checked:

The Service is not correctly mapped to the corresponding Pod: It's possible that the Service's selector or target port is misconfigured, preventing the request from being properly forwarded to the application running inside the Pod.

The Pod is running, but the application has not started correctly or is not bound to the expected port: Please ensure that the application is listening on the correct port and has completed its startup process.

Route configuration issues: Although the Route exists, it may be pointing to an incorrect Service name or path. Additionally, the Route path might not match the actual context path of the application service, leading to routing errors.

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app: my-apps
  name: my-apps
  namespace: test-systems
  resourceVersion: ""48338578""
  uid: 36692e79-0f08-4416-8242-cdb0087900da
spec:
  clusterIP: 172.26.110.30
  clusterIPs:
  - 172.26.110.30
  internalTrafficPolicy: Cluster
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 8080
    protocol: TCP
    targetPort: http
  - name: actuator
    port: 5678
    protocol: TCP
    targetPort: actuator
  selector:
    app: my-apps
  sessionAffinity: None
  type: ClusterIP
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    app: my-apps
  name: my-apps-qbrjf
  namespace: test-systems
  resourceVersion: ""48406718""
  uid: 47e6b206-a15a-4b51-9853-a11d4d1243d4
spec:
  host: www.test.gov.tw
  path: /my-apps
  port:
    targetPort: http
  to:
    kind: Service
    name: my-apps
    weight: 100
  wildcardPolicy: None
```","kubernetes, routes, openshift",79697914.0,"The YAML snippet you provided is the TLS configuration section of an OpenShift Route. It defines how HTTPS/TLS connections are handled.

Client → HTTPS → Router (TLS termination) → HTTP → Pod

If the client tries to access via HTTP, the router sends a 302 redirect to the HTTPS URL.

```
tls:
  insecureEdgeTerminationPolicy: Redirect
  termination: edge
```

[https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes](https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes)",2025-07-11T06:25:18,2025-07-11T06:13:28,"```yaml
tls:
  insecureEdgeTerminationPolicy: Redirect
  termination: edge
```

The YAML snippet you provided is the TLS configuration section of an OpenShift Route. It defines how HTTPS/TLS connections are handled.

Client → HTTPS → Router (TLS termination) → HTTP → Pod

If the client tries to access via HTTP, the router sends a 302 redirect to the HTTPS URL.

[https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes](https://docs.redhat.com/en/documentation/openshift_container_platform/4.8/html/networking/configuring-routes)"
79692778,How to host a simple REST service as a Kubernetes pod,"I have been working in Kubernetes. I am well aware of all the components like pod, deployment, service etc.

But I am not able to get the whole picture and hence need help.

Suppose I want to create a new pod only for this small Java program:

```
package com.test;

import javax.ws.rs.GET;
import javax.ws.rs.Path;
import javax.ws.rs.Produces;
import javax.ws.rs.core.MediaType;

@Path(""/hello"")
public class HelloWorldRestService {

    @GET
    @Produces(MediaType.TEXT_PLAIN)
    public String getMessage(){
        return ""Hello World"";
    }
}
```

In that case what all do I need to launch this pod in the Kubernetes cluster? REST clients outside the cluster should be able to access this URL.

So I need:

1. The image and the image should be hosted in some registry
2. The pod yaml file
3. The deployment yaml file
4. Route table
5. Service
6. Gateway
7. Load balancer

But what should be the configuration in each of them and how they will be related to each other?

I have followed many tutorials etc, but everyone is just explaining the concepts, not sharing the YAML code. How can I achieve the same through coding?","kubernetes, cloud",79692889.0,"Here are the rough steps (code snippets are untested, but hopefully give you the basic idea):

1. **Build your app**. First, you need to build your app into some sort of deployable artifact. In the Java world, the most common way to do this is to create a `.jar` file. One option is to do this manually (this assumes your Java code is in the `src` folder):

```
javac -d ./build src/*.java
jar cvf app.jar ./build/*
```

That said, a more realistic option would be to use a build system such as [Gradle](https://gradle.org/) or [Maven](https://maven.apache.org/) to manage your build and dependencies.
2. **Package your app as a Docker image**. Next, you need to package your app artifact (the `.jar` file) as a Docker image.

```
# Use OpenJDK 17 as base image
FROM openjdk:17-jdk-slim

# Set working directory
WORKDIR /app

# Copy source code
COPY src/ ./src/

# Create directory for compiled classes
RUN mkdir -p build

# Create jar file. If you use Gradle or Maven, run those here instead.
RUN javac -d ./build src/*.java
RUN jar cvf app.jar ./build/*

# Set default command to run the JAR file
CMD [""java"", ""-jar"", ""app.jar""]
```

To build the Docker image:

```
docker build -t my-app:v1 .
```
3. **Push to a Docker registry**. You now have a Docker image, but it's only on your own computer. Your Kubernetes cluster won't be able to access it there, so you need to push the image to a registry that is accessible to the cluster. For example, you might use [Docker Hub](https://hub.docker.com/) as a registry. You can use the web UI in Docker Hub to create a user for yourself named `username` and a new repository under that user named `my-app`. You can then login to Docker hub:

```
docker login
```

This will allow you to login via your web browser. Once authenticated, tag your Docker image with your Docker Hub username and repo name, and push the image:

```
docker tag my-app:v1 username/my-app:v1
docker push username/my-app:v1
```
4. **Create a `Deployment`**. There are many ways to deploy apps in Kubernetes. One option is to create a `Deployment`, which is a declarative way to manage an application in Kubernetes. The Deployment allows you to declare which Docker images to run, how many copies of them to run (replicas), a variety of settings for those images (e.g., CPU, memory, port numbers, environment variables), and so on, and the Deployment will then work to ensure that the requirements you declared are always met. Here's the YAML for a basic `Deployment`:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: sample-app-pods
    spec:
      containers:
        - name: sample-app
          # Specify the Docker image to deploy from your Docker registry
          image: username/my-app:v1
          ports:
            # Specify the port your app listens on for HTTP requests
            - containerPort: 8080
  selector:
    matchLabels:
      app: sample-app-pods
```

Note that if your app is in a private Docker registry, you'll have to [give your Kubernetes cluster a way to authenticate to that registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/).

You can use [kubectl](https://kubernetes.io/docs/reference/kubectl/) to create this `Deployment`. First, you need to [authenticate to your Kubernetes cluster](https://kubernetes.io/docs/reference/access-authn-authz/authentication/). How you do this depends on the cluster. For example, if you're using the local [Kubernetes cluster built into Docker Desktop](https://docs.docker.com/desktop/features/kubernetes/), you can authenticate to it as follows:

```
kubectl config use-context docker-desktop
```

If the YAML for the `Deployment` is in a file called `deployment.yml`, you can create it as follows:

```
kubectl apply -f deployment.yml
```
5. **Create a `Service`**. A `Deployment` will get your app running in the cluster, but it won't make it available to other services over the network. To expose your app to the outside world, you can create a `Service`:

```
apiVersion: v1
kind: Service
metadata:
  name: sample-app-loadbalancer
spec:
  type: LoadBalancer
  selector:
    app: sample-app-pods
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

If the YAML for this `Service` is in `service.yml`, you can create it as follows:

```
kubectl apply -f service.yml
```
6. **Test**. It'll take a minute or two for everything to deploy. To see the status of your `Deployment`:

```
kubectl describe deployment sample-app-deployment
```

To see the status of your `Service`:

```
kubectl describe service sample-app-loadbalancer
```

If everything is working, that last command should output a `LoadBalancer Ingress` field, which shows you the URL to use for the load balancer. You can then test that URL:

```
curl http://<<URL>
```

If everything is working, you should see ""Hello, World.""

This is a minimal deployment just for learning. I've glossed over many details. For a lot more info, including working & tested code examples, check out the Container Orchestration section of [How to manage your apps using orchestration tools](https://books.gruntwork.io/books/fundamentals-of-devops/deploying-apps-orchestration-vms-containers-serverless) (an article I wrote).",2025-07-07T12:57:43,2025-07-07T11:23:56,"```bash
javac -d ./build src/*.java
jar cvf app.jar ./build/*
```

First, you need to build your app into some sort of deployable artifact. In the Java world, the most common way to do this is to create a `.jar` file. One option is to do this manually (this assumes your Java code is in the `src` folder).

---

```dockerfile
# Use OpenJDK 17 as base image
FROM openjdk:17-jdk-slim

# Set working directory
WORKDIR /app

# Copy source code
COPY src/ ./src/

# Create directory for compiled classes
RUN mkdir -p build

# Create jar file. If you use Gradle or Maven, run those here instead.
RUN javac -d ./build src/*.java
RUN jar cvf app.jar ./build/*

# Set default command to run the JAR file
CMD [""java"", ""-jar"", ""app.jar""]
```

Next, you need to package your app artifact (the `.jar` file) as a Docker image.

---

```bash
docker build -t my-app:v1 .
```

To build the Docker image.

---

```bash
docker login
```

You can then login to Docker hub. This will allow you to login via your web browser.

---

```bash
docker tag my-app:v1 username/my-app:v1
docker push username/my-app:v1
```

Once authenticated, tag your Docker image with your Docker Hub username and repo name, and push the image.

---

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: sample-app-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: sample-app-pods
    spec:
      containers:
        - name: sample-app
          # Specify the Docker image to deploy from your Docker registry
          image: username/my-app:v1
          ports:
            # Specify the port your app listens on for HTTP requests
            - containerPort: 8080
  selector:
    matchLabels:
      app: sample-app-pods
```

Here's the YAML for a basic `Deployment`.

---

```bash
kubectl config use-context docker-desktop
```

For example, if you're using the local Kubernetes cluster built into Docker Desktop, you can authenticate to it as above.

---

```bash
kubectl apply -f deployment.yml
```

If the YAML for the `Deployment` is in a file called `deployment.yml`, you can create it as above.

---

```yaml
apiVersion: v1
kind: Service
metadata:
  name: sample-app-loadbalancer
spec:
  type: LoadBalancer
  selector:
    app: sample-app-pods
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```

To expose your app to the outside world, you can create a `Service` with YAML as above.

---

```bash
kubectl apply -f service.yml
```

If the YAML for this `Service` is in `service.yml`, you can create it as above.

---

```bash
kubectl describe deployment sample-app-deployment
```

To see the status of your `Deployment`.

---

```bash
kubectl describe service sample-app-loadbalancer
```

To see the status of your `Service`.

---

```bash
curl http://<<URL>
```

You can then test that URL."
79690486,Correct Url to contact selenium in a Kubernetes cluster,"I am working on trying to use selenium as a sidecar container for an application. In the application code I have tried the following:

```
 URL remoteUrl = new URL(""http://localhost:4444/wd/hub"");

 ChromeOptions options = new ChromeOptions();
 options.addArguments(""--headless"", ""--disable-gpu"", ""--no-sandbox"", ""--disable-dev-shm-usage"");
 log.info(""starting web driver"");
 WebDriver driver = new RemoteWebDriver(remoteUrl, options);
```

I have also tried

```
 URL remoteUrl = new URL(""http://remote-chrome-webdriver.default.svc.cluster.local:4444/wd/hub"");
```

and

```
URL remoteUrl = new URL(""http://remote-chrome-webdriver:4444/wd/hub"");
```

And all of these options have yielded a ConnectException.

My yaml defines the selenium container like this:

```
      initContainers:
        - name: wait-for-chrome
          image: busybox:latest
          imagePullPolicy: IfNotPresent
          command: ['sh', '-c', 'until curl -f http://remote-chrome-webdriver:4444/wd/hub/status; do echo ""Waiting fro remote-chrome-webdriver...""; sleep 5; done;']
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
        - name: remote-chrome-webdriver
          image: xxxxxxxxxx/selenium/standalone-chrome:4.23.1
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 4444
              protocol: TCP
          restartPolicy: Always
          env:
            - name: ""xxxxx_APPLICATION_CREDENTIALS""
              value: {{ .Values.env.xxxxxxxxxxx}}
          envFrom:
            - secretRef:
                name: xxxxxx
          volumeMounts:
            - name: xxxxxx
              readOnly: true
              mountPath: ""/etc/xxxxx""
          resources:
            {{- toYaml .Values.resources | nindent 12}}
```

Where I have redacted some sensitive information.

What is the proper syntax to use in java code to allow the application to connect to the selenium sidecar?","java, kubernetes, selenium-webdriver",79696072.0,"The correct syntax in the code to reach the selenium standalone running in the kubernetes cluster was

[http://remote-chrome-webdriver.default.svc:80/wd/hub](http://remote-chrome-webdriver.default.svc:80/wd/hub)

Also we had to create a Kubernetes service to expose this selenium to other workloads in the cluster

```
apiVersion: v1
kind: Service
metadata:
  name: remote-chrome-webdriver
  labels:
    app: remote-chrome-webdriver
spec:
  selector:
    app: remote-chrome-webdriver
  ports:
    - protocol: TCP
      port: 80
      targetPort: 4444
  type: LoadBalancer
```",2025-07-09T18:06:17,2025-07-04T16:46:52,"```text
[http://remote-chrome-webdriver.default.svc:80/wd/hub](http://remote-chrome-webdriver.default.svc:80/wd/hub)
```

The correct syntax in the code to reach the selenium standalone running in the kubernetes cluster was

---

```yaml
apiVersion: v1
kind: Service
metadata:
  name: remote-chrome-webdriver
  labels:
    app: remote-chrome-webdriver
spec:
  selector:
    app: remote-chrome-webdriver
  ports:
    - protocol: TCP
      port: 80
      targetPort: 4444
  type: LoadBalancer
```

Also we had to create a Kubernetes service to expose this selenium to other workloads in the cluster"
79688743,kubectl RBAC roles for scaling down the deployment,"I've a requirement to provide RBAC roles for a cluster role so that they can scale up or scale down the replicas. The role already has 'get' 'list' & 'watch' verbs and I've added the below code to the yaml for the additional access.

```
  - apiGroups:
    - ""apps""
    resources:
      - deployments/scale
    verbs:
      - update
      - patch
```

I'm able to edit the deployment file using `kubectl edit deployment deplName` and adjust the replica count but I'm getting denied with permission error when I try to use the `kubectl scale deploy deplName --replicas` command. Below is the error when I tried to scale the replicas using `kubectl scale`

```
Error from server (Forbidden): deployments.apps ""Name"" is forbidden: User ""system:serviceaccount:xyz"" cannot patch resource ""deployments/scale"" in API group ""apps"" in the namespace ""namespace""
```

Am I missing anything here or do I need to provide any other roles?

Below is the output from ""kubectl describe clusterrole RoleName""

```
deployments.extensions                []                 []              [get list watch]
  deployments.apps                      []                 []              [list watch get patch]
  deployments.apps/status               []                 []              [list watch get]
  deployments.apps                      []                 []              [list watch get]
  deployments                           []                 []              [list watch get]
  deployments.apps.apps/status          []                 []              [list watch get]
  deployments.apps.apps                 []                 []              [list watch get]
  deployments.apps.batch/status         []                 []              [list watch get]
  deployments.apps.batch                []                 []              [list watch get]
  deployments.batch                     []                 []              [list watch get]
  deployments.apps/scale                []                 []              [update patch]
```","kubernetes, k8s-cluster-role",79693196.0,"The error you're seeing indicating that the patch action is forbidden suggests that the RBAC  settings may not be correctly configured. The `ClusterRole` must explicitly allow the `update` or `patch` verbs on the `deployments/scale` subresource in the apps API group.

Here is an example of a corrected [ClusterRole YAML configuration](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#clusterrole-example):

```
rules:
- apiGroups: [""apps""]
  resources: [""deployments"", ""deployments/scale""]
  verbs: [""get"", ""list"", ""watch"", ""patch"", ""update""]
```

- `get, list, watch,` and `patch` permissions on deployments for general operations and editing (such as kubectl edit deployment).
- `get, update,` and `patch` permissions on `deployments/scale` for scaling actions (such as kubectl scale).

After applying these changes, verify the `ClusterRole` . And confirm that the `deployments.apps/scale` resource has both update and patch verbs.

```
kubectl describe clusterrole RoleName
```

You can also test the permission using [kubectl auth can-i](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_auth/kubectl_auth_can-i/). The output should be `yes` otherwise `no`:

```
kubectl auth can-i patch deployments/scale --as=system:serviceaccount:xyz -n <namespace>
```

If the issue persists after applying changes above. Verify that the `RoleBinding `or `ClusterRoleBinding` properly associates the service account `xyz` in the specified namespace with the `ClusterRole`:

```
kubectl describe clusterrolebinding role-name-binding
```",2025-07-07T17:01:51,2025-07-03T11:12:59,"```yaml
rules:
- apiGroups: [""apps""]
  resources: [""deployments"", ""deployments/scale""]
  verbs: [""get"", ""list"", ""watch"", ""patch"", ""update""]
```

The error you're seeing indicating that the patch action is forbidden suggests that the RBAC  settings may not be correctly configured. The `ClusterRole` must explicitly allow the `update` or `patch` verbs on the `deployments/scale` subresource in the apps API group.

Here is an example of a corrected [ClusterRole YAML configuration](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#clusterrole-example):

- `get, list, watch,` and `patch` permissions on deployments for general operations and editing (such as kubectl edit deployment).
- `get, update,` and `patch` permissions on `deployments/scale` for scaling actions (such as kubectl scale).

After applying these changes, verify the `ClusterRole` . And confirm that the `deployments.apps/scale` resource has both update and patch verbs.

```bash
kubectl describe clusterrole RoleName
```

You can also test the permission using [kubectl auth can-i](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_auth/kubectl_auth_can-i/). The output should be `yes` otherwise `no`:

```bash
kubectl auth can-i patch deployments/scale --as=system:serviceaccount:xyz -n <namespace>
```

If the issue persists after applying changes above. Verify that the `RoleBinding `or `ClusterRoleBinding` properly associates the service account `xyz` in the specified namespace with the `ClusterRole`:

```bash
kubectl describe clusterrolebinding role-name-binding
```"
79687835,"Spring Boot application running in Kubernetes does not receive X-Forwarded-For header, but works with docker run","I'm facing an issue where my Spring Boot application does not receive the `X-Forwarded-For:` header when running inside a Kubernetes cluster, even though it works correctly when running the same Docker image locally with `docker run`.

I'm using Spring Boot version 3.5.3 with its embedded Tomcat server.  I'm calling the Pod directly from inside the Kubernetes cluster, either using the Pod's IP address or using `kubectl exec` to `curl localhost` from inside the Pod directly.

Code of `HeaderController.java`:

```
package com.example.headerdemo.controller;

import org.springframework.web.bind.annotation.*;

import jakarta.servlet.http.HttpServletRequest;
import java.util.*;

@RestController
@RequestMapping(""/headers"")
public class HeaderController {

    @GetMapping
    public Map<String, String> getAllHeaders(HttpServletRequest request) {
        Map<String, String> headers = new HashMap<>();

        Enumeration<String> headerNames = request.getHeaderNames();
        if (headerNames != null) {
            while (headerNames.hasMoreElements()) {
                String headerName = headerNames.nextElement();
                String headerValue = request.getHeader(headerName);
                headers.put(headerName, headerValue);
                System.out.printf(""Header: %s = %s%n"", headerName, headerValue);
            }
        }

        return headers;
    }
}
```

`Dockerfile`:

```
# Stage 1: Build the application
FROM maven:3.9.6-eclipse-temurin-17 as builder
WORKDIR /app
COPY pom.xml .
COPY src ./src
RUN mvn clean package -DskipTests

# Stage 2: Create a minimal runtime image
FROM eclipse-temurin:17-jdk-alpine
WORKDIR /app
COPY --from=builder /app/target/*.jar app.jar

# Expose port
EXPOSE 8080

# Run the Spring Boot app
ENTRYPOINT [""java"", ""-jar"", ""app.jar""]
```

When running with **docker run** locally:

```
docker run -p 8080:8080 myapp
curl -H ""X-Forwarded-For: 1.1.1.1"" http://localhost:8080/headers
```

The header `X-Forwarded-For` is received and printed in the controller.

When running in Kubernetes:

```
kubectl exec -it <my-pod> -- curl -H ""X-Forwarded-For: 1.1.1.1"" http://localhost:8080/headers
```

or `curl` to IP of `my-pod` from another pod in cluster, the application does not receive the `X-Forwarded-For` header (it’s missing in `HttpServletRequest#getHeaderNames()`).

I call the pod IP directly, not through a Kubernetes Service or Ingress.  No proxy or sidecar (e.g., Istio, Linkerd) is involved.  No custom filters in my Spring Boot app.

Other custom headers are received just fine — only `X-Forwarded-For` is dropped.","spring-boot, kubernetes, x-forwarded-for",79689389.0,"I was able to reproduce this issue with a pod inside a microk8s cluster.

I solved it by following the [Spring docs](https://docs.spring.io/spring-boot/how-to/webserver.html#howto.webserver.use-behind-a-proxy-server):

> If the proxy adds the commonly used `X-Forwarded-For` and `X-Forwarded-Proto` headers, setting `server.forward-headers-strategy` to `NATIVE` is enough to support those. With this option, the Web servers themselves natively support this feature; you can check their specific documentation to learn about specific behavior.
>
>
> If this is not enough, Spring Framework provides a [ForwardedHeaderFilter](https://docs.spring.io/spring-framework/reference/6.2/web/webmvc/filters.html#filters-forwarded-headers) for the servlet stack and a [ForwardedHeaderTransformer](https://docs.spring.io/spring-framework/reference/6.2/web/webflux/reactive-spring.html#webflux-forwarded-headers) for the reactive stack. You can use them in your application by setting `server.forward-headers-strategy` to `FRAMEWORK`.

I chose the latter strategy and added this to *application.yml*:

```
server:
  forward-headers-strategy: framework
```

and used `HttpServletRequest#remoteAddr` in controller.

**Example controller (Kotlin)**

```
import jakarta.servlet.http.HttpServletRequest
import org.springframework.web.bind.annotation.GetMapping
import org.springframework.web.bind.annotation.RequestMapping
import org.springframework.web.bind.annotation.RestController

@RestController
@RequestMapping(""/hello"")
class HelloWorldController {
    @GetMapping
    fun hello(request: HttpServletRequest): String {
        val clientIp = request.remoteAddr
        return ""Hello World, $clientIp!""
    }
}
```

I also tested this with a Ktor-app, and the `X-Forwarded-For` header came through:

```
call.request.header(""X-Forwarded-For"")
```",2025-07-03T19:37:35,2025-07-02T17:15:59,"```text
I was able to reproduce this issue with a pod inside a microk8s cluster.

I solved it by following the [Spring docs](https://docs.spring.io/spring-boot/how-to/webserver.html#howto.webserver.use-behind-a-proxy-server):

> If the proxy adds the commonly used `X-Forwarded-For` and `X-Forwarded-Proto` headers, setting `server.forward-headers-strategy` to `NATIVE` is enough to support those. With this option, the Web servers themselves natively support this feature; you can check their specific documentation to learn about specific behavior.
>
>
> If this is not enough, Spring Framework provides a [ForwardedHeaderFilter](https://docs.spring.io/spring-framework/reference/6.2/web/webmvc/filters.html#filters-forwarded-headers) for the servlet stack and a [ForwardedHeaderTransformer](https://docs.spring.io/spring-framework/reference/6.2/web/webflux/reactive-spring.html#webflux-forwarded-headers) for the reactive stack. You can use them in your application by setting `server.forward-headers-strategy` to `FRAMEWORK`.

I chose the latter strategy and added this to *application.yml*:
```

```yaml
server:
  forward-headers-strategy: framework
```

```text
and used `HttpServletRequest#remoteAddr` in controller.

**Example controller (Kotlin)**
```

```kotlin
import jakarta.servlet.http.HttpServletRequest
import org.springframework.web.bind.annotation.GetMapping
import org.springframework.web.bind.annotation.RequestMapping
import org.springframework.web.bind.annotation.RestController

@RestController
@RequestMapping(""/hello"")
class HelloWorldController {
    @GetMapping
    fun hello(request: HttpServletRequest): String {
        val clientIp = request.remoteAddr
        return ""Hello World, $clientIp!""
    }
}
```

```text
I also tested this with a Ktor-app, and the `X-Forwarded-For` header came through:
```

```kotlin
call.request.header(""X-Forwarded-For"")
```"
79672356,cp Command Fails to Copy JAR File to Target Folder,"I am trying to copy a file (jar file) so that I can run the jar (`java -jar`) in my pod. But the copy command just doesn't work. The pod logs don't throw any error also.

My `deployment.yaml` looks like (in brief):

```
- name: glowroot-jar-init-container
image: ""{{ .Values.images.repository }}/{{ .Values.config.aptm.image }}""
securityContext:
  runAsUser: 1000
  runAsGroup: 1000
  runAsNonRoot: true
  readOnlyRootFilesystem: true
imagePullPolicy: {{ .Values.images.pullPolicy }}
command: [""cp"",""/opt/tools/aptm/glowroot.jar"",""/aptm""]
volumeMounts:
  - name: aptm-data-glowroot
    mountPath: /aptm
.
.
.
.
.
.
containers:
- name: {{ template ""name"" . }}
  image: ""{{ .Values.images.repository }}/com.gtt.ecomp.vod.dev/vod:{{ .Values.images.vodTag }}""
  imagePullPolicy: {{ .Values.images.pullPolicy }}
  securityContext:
    runAsUser: 1000
    runAsGroup: 1000
    runAsNonRoot: true
    readOnlyRootFilesystem: true
  command:
    - sh
    - -c
    - -x
    - >
      .
      .
      .
      .

      echo ""Copying aptm JAR.""
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;

      .
      .
      .

      bash /mounted-config/start_tomcat.sh;
  args:
  - ""30000""
.
.
.
.
- name: aptm-data
  mountPath: /opt/tools/aptm
- name: aptm-data-glowroot
  mountPath: /aptm
.
.
.
.
- name: aptm-data
  emptyDir: {}
- name: aptm-data-glowroot
  emptyDir: {}
 .
 .
 .
```

The file `hram-agent-0.13.jar` is present in the `WEB-INF/lib/` folder. But when I do a bash and get into the pod to check the if the jar file was copied or not I do not see it.

```
vodadmin@vod-58867c5dc6-lg8ch:/usr/local/tomcat/webapps/vod/WEB-INF/lib$ ls -lrt hr*
-rw-r--r-- 1 vodadmin vodadmin 13864793 Apr 25 12:56 hram-agent-0.13.jar
vodadmin@vod-58867c5dc6-lg8ch:/usr/local/tomcat/webapps/vod/WEB-INF/lib$
```

But when `cd` to the target folder:

```
vodadmin@vod-58867c5dc6-lg8ch:/opt/tools/aptm$ ls -lrt
total 0
```

All the trouble started when I changed everything to read only root file system in my pod.","kubernetes, kubernetes-helm",79672475.0,"Just a quick guess -

When you use `>` in a yaml, it stacks the lines of its data together into one line.

```
   - >
      echo ""Copying aptm JAR.""
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

becomes

```
echo ""Copying aptm JAR."" cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

which I bet outputs

```
Copying aptm JAR. cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm
```

So add a semicolon after the `echo` statement. Then

```
   - >
      echo ""Copying aptm JAR."";
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

becomes

```
echo ""Copying aptm JAR.""; cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

and might work.

Or use a `|` instead, which preserves the internal newlines.

I'm still suspicious of how the syntax gets delivered to the parser, though.
Maybe write a script that wraps all that in a simpler call, and rebuild it into your image?",2025-06-19T18:05:50,2025-06-19T16:15:04,"```text
Just a quick guess -

When you use `>` in a yaml, it stacks the lines of its data together into one line.
```

```yaml
   - >
      echo ""Copying aptm JAR.""
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

```text
becomes
```

```bash
echo ""Copying aptm JAR."" cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

```text
which I bet outputs
```

```text
Copying aptm JAR. cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm
```

```text
So add a semicolon after the `echo` statement. Then
```

```yaml
   - >
      echo ""Copying aptm JAR."";
      cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
      . . .
```

```text
becomes
```

```bash
echo ""Copying aptm JAR.""; cp /usr/local/tomcat/webapps/vod/WEB-INF/lib/hram-agent-0.13.jar /opt/tools/aptm;
```

```text
and might work.

Or use a `|` instead, which preserves the internal newlines.

I'm still suspicious of how the syntax gets delivered to the parser, though.
Maybe write a script that wraps all that in a simpler call, and rebuild it into your image?
```"
79670263,How to call a env variable in React VITE without the need of a .env,"I'm currently trying to deploy a frontend app in a container in a kubernetes cluster, I have made a env variable named `VITE_SERVER_ADDRESSS` following the vite naming convention for environment variables. I have also checked if the pod containing this container has indeed the env variable. This environment variable is loaded from a config map.

Nonetheless, I am still not able to call this env variable in my frontend code which works locally but makes use of a .env file.

Due to the fact that this variables changes according to the environment, I need to use a configmap. Which with helm, allows me to change the `VITE_SERVER_ADDRESS` according to the environment.

This is the code for the frontend that calls the env variable

```
const serverAddress: string = import.meta.env.VITE_SERVER_ADDRESS;
console.log(import.meta.env.VITE_SERVER_ADDRESS);
const url: string = `http://${serverAddress}:8000/run-main`; //in the cluster I now have a undefined
```","reactjs, kubernetes, vite",79670605.0,"Ok, so I found a fix to the problem,

The issue was with the fact that I needed to get the env variables *before *`npm run build` i.e build time. Me using a config map meant that I was getting the env variable in runtime and not build time. Instead with my original Dockerfile I was running `npm run build` which didn't have the environment variables due to the configmap.

Instead in my helm template manifest files, I overrode the CMD in the Dockerfile with CMD and ARGS, and also utilized helms variables (e.g. `{{.Values.env}}`)that changes according to the values.yml file with helm as shown bellow:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: project-ui-deployment
  namespace: {{.Values.namespace}}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: project-ui-app
  template:
    metadata:
      labels:
        app: project-ui-app
    spec:
      containers:
        - name: project-ui
          image: {{.Values.ui_image}}
          ports:
            - containerPort: 3000
              protocol: TCP
          env:
            - name: VITE_SERVER_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: project-configmap
                  key: VITE_SERVER_ADDRESS

          command: [""/bin/sh""]
          args:
            - ""-c""
            - |
              echo $VITE_SERVER_ADDRESS
              export VITE_SERVER_ADDRESS=project-server-service{{.Values.env}}.project{{.Values.env}}.svc.cluster.local
              npm run build
              node server.js
```",2025-06-18T12:06:14,2025-06-18T08:23:42,"```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: project-ui-deployment
  namespace: {{.Values.namespace}}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: project-ui-app
  template:
    metadata:
      labels:
        app: project-ui-app
    spec:
      containers:
        - name: project-ui
          image: {{.Values.ui_image}}
          ports:
            - containerPort: 3000
              protocol: TCP
          env:
            - name: VITE_SERVER_ADDRESS
              valueFrom:
                configMapKeyRef:
                  name: project-configmap
                  key: VITE_SERVER_ADDRESS

          command: [""/bin/sh""]
          args:
            - ""-c""
            - |
              echo $VITE_SERVER_ADDRESS
              export VITE_SERVER_ADDRESS=project-server-service{{.Values.env}}.project{{.Values.env}}.svc.cluster.local
              npm run build
              node server.js
```

Ok, so I found a fix to the problem,

The issue was with the fact that I needed to get the env variables *before *`npm run build` i.e build time. Me using a config map meant that I was getting the env variable in runtime and not build time. Instead with my original Dockerfile I was running `npm run build` which didn't have the environment variables due to the configmap.

Instead in my helm template manifest files, I overrode the CMD in the Dockerfile with CMD and ARGS, and also utilized helms variables (e.g. `{{.Values.env}}`)that changes according to the values.yml file with helm as shown bellow:"
79663394,Helm Templates dockerconfigjson secret - Cannot unmarshal string into Go struct field Secret.data,"I have defined a helm template like the one below to get a predefined set of private registries in values and create a dockerconfigjson type secret in the namespace if needed by copying the templates into the helm chart.

**value.yaml**

```
privateregistries:
  - registry: internal1.local
    username: ""sxs""
    token: ""sxs""
  - registry: internal2.local
    username: ""sxs""
    token: ""sxs""
```

**template file**

```
{{- $auths := dict }}
{{ range $key, $value := .Values.privateregistries }}
   {{- $auth := printf ""%s:%s"" $value.username $value.token | b64enc }}
   {{- $_ := set $auths $value.registry (dict ""auth"" $auth) }}
{{ end }}
{{- $json := dict ""auths"" $auths | toJson }}

apiVersion: v1
kind: Secret
metadata:
  name: regcred
type: kubernetes.io/dockerconfigjson
data:
  .dockerconfigjson: {{- quote (b64enc $json) }}
```

But when trying to apply, I see the error below. What could be the error?

cannot unmarshal string into Go struct field Secret.data of type map[string][]uint8","kubernetes, kubernetes-helm, go-templates",79663419.0,"The hyphen in the final line makes the YAML structure invalid.

```
data:
  .dockerconfigjson: {{- quote (b64enc $json) }}
  #                    ^ this one
```

You can just remove it.  You don't specifically need to `quote` the value either.  (YAML doesn't require the quotes and a base64 string won't have punctuation that potentially confuses YAML; if you do need to quote something, `toJson` will be more robust.)

```
data:
  .dockerconfigjson: {{ b64enc $json }}
```

The hyphen inside the curly braces causes the Go templating engine to remove all of the whitespace outside the curly braces.  That puts the value directly up against the key, but the YAML syntax requires at least one space after the colon.

```
# original form, doesn't parse:
.dockerconfigjson:""e30=""

# final form (without `quote`), works:
.dockerconfigjson: e30=
```

Running `helm template --debug` will dump out the output of the template even if it's not valid YAML, which can occasionally help you to find problems like this.  It tends to be more obvious with extra or missing hyphens at the start or end of whole lines where you can get lines joined together or missing indentation.",2025-06-12T11:11:39,2025-06-12T10:51:30,"```text
The hyphen in the final line makes the YAML structure invalid.
```

```yaml
data:
  .dockerconfigjson: {{- quote (b64enc $json) }}
  #                    ^ this one
```

```text
You can just remove it.  You don't specifically need to `quote` the value either.  (YAML doesn't require the quotes and a base64 string won't have punctuation that potentially confuses YAML; if you do need to quote something, `toJson` will be more robust.)
```

```yaml
data:
  .dockerconfigjson: {{ b64enc $json }}
```

```text
The hyphen inside the curly braces causes the Go templating engine to remove all of the whitespace outside the curly braces.  That puts the value directly up against the key, but the YAML syntax requires at least one space after the colon.
```

```text
# original form, doesn't parse:
.dockerconfigjson:""e30=""

# final form (without `quote`), works:
.dockerconfigjson: e30=
```

```text
Running `helm template --debug` will dump out the output of the template even if it's not valid YAML, which can occasionally help you to find problems like this.  It tends to be more obvious with extra or missing hyphens at the start or end of whole lines where you can get lines joined together or missing indentation.
```"
79658864,How to persist ConfigMap values on deployment upgrade?,"I'm using ConfigMap to switch on/off some functionality of the application in the pod. I have mounted it in the deployment like that:

```
volumes:
  - name: {{ .Chart.Name }}-config-volume
    projected:
      sources:
      - configMap:
          name: {{ .Chart.Name }}-content-config
```

then I have some configuration data in ConfigMap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Chart.Name }}-content-config
data:
  content.properties: |
    {
      ""Enabled"": false,
      ""ApiEndpoint"": ""...""
    }
```

When the functionality is configured and ready to be enabled, I run`kubectl edit cm` and set ""Enabled"" to true. Application is reading file every 2 minutes and refreshing configuration respectively without restarting the pod. Ok, it's working, it's persisting through pod restarts.

But, if I'm doing `helm upgrade` to the next version - everything is reset and again has default values, e.g. ""Enabled: false"". Is there any way to make ConfigMap persistent no matter the upgrades?","kubernetes, kubernetes-helm, configmap",79658954.0,"Don't try to use two separate tools to manage your Kubernetes manifests.  You should be able to manage this entirely in Helm.

For example, you can put the API endpoint value in deploy-time configuration

```
# values.yaml
apiEndpoint: https://...
```

Then when your Helm chart produces the ConfigMap, it can insert the values from your Helm-level configuration.  Helm includes a [`toJson` extension function](https://docs.helm.sh/docs/chart_template_guide/function_list/#type-conversion-functions) that can encode an arbitrary value as JSON.

```
# templates/configmap.yaml
data:
  content.properties: |
    {
      ""Enabled"": {{ toJson .Values.contentEnabled }},
      ""ApiEndpoint"": {{ toJson .Values.apiEndpoint }}
    }
```

Then you can keep a reference set of override values (probably in source control, maybe managed in your CD system).  If you need to change these values then you can use `helm upgrade`, and it will consistently redeploy everything from the rendered templates.

```
# deploy/values-dev.yaml
apiEndpoint: https://internal.example.com/api/
```

```
helm upgrade --install -f deploy/values-dev.yaml -n dev my-app .
```

Once Helm has deployed it, don't try to `kubectl edit` any of the resources (except maybe in very-short-term debugging scenarios, but if you do, make sure you put things back the way you found them).

(Some values of Helm have included a ""3-way merge"" that attempts to do what you describe.  IME that has been more a source of confusion than anything helpful: if a deploy fails then Helm tries to do a merge between the previous version, the failed deploy, and the corrected version, and you inevitably wind up with something that's plainly right there in your template file not showing up in the cluster.  A previous deploy pipeline went out of its way to explicitly uninstall the previous version specifically to get around the problems that 3-way merged introduced.)",2025-06-09T13:27:46,2025-06-09T12:20:48,"```text
Don't try to use two separate tools to manage your Kubernetes manifests.  You should be able to manage this entirely in Helm.

For example, you can put the API endpoint value in deploy-time configuration
```

```yaml
# values.yaml
apiEndpoint: https://...
```

```text
Then when your Helm chart produces the ConfigMap, it can insert the values from your Helm-level configuration.  Helm includes a [`toJson` extension function](https://docs.helm.sh/docs/chart_template_guide/function_list/#type-conversion-functions) that can encode an arbitrary value as JSON.
```

```yaml
# templates/configmap.yaml
data:
  content.properties: |
    {
      ""Enabled"": {{ toJson .Values.contentEnabled }},
      ""ApiEndpoint"": {{ toJson .Values.apiEndpoint }}
    }
```

```text
Then you can keep a reference set of override values (probably in source control, maybe managed in your CD system).  If you need to change these values then you can use `helm upgrade`, and it will consistently redeploy everything from the rendered templates.
```

```yaml
# deploy/values-dev.yaml
apiEndpoint: https://internal.example.com/api/
```

```bash
helm upgrade --install -f deploy/values-dev.yaml -n dev my-app .
```

```text
Once Helm has deployed it, don't try to `kubectl edit` any of the resources (except maybe in very-short-term debugging scenarios, but if you do, make sure you put things back the way you found them).

(Some values of Helm have included a ""3-way merge"" that attempts to do what you describe.  IME that has been more a source of confusion than anything helpful: if a deploy fails then Helm tries to do a merge between the previous version, the failed deploy, and the corrected version, and you inevitably wind up with something that's plainly right there in your template file not showing up in the cluster.  A previous deploy pipeline went out of its way to explicitly uninstall the previous version specifically to get around the problems that 3-way merged introduced.)
```"
79652287,ReportPortal analyzer service unable to connect to RabbitMQ,"We have ReportPortal deployed with k8s. It works well overall but the issue auto-analysis doesn't work due to analyzer service not being able to start. Here are logs from the API pod:

```
2025-06-03 15:32:47.705 2025-06-03 13:32:47,609 - [un8qThWMTjKjDSy8SF4-4Q] - INFO - analyzerApp.amqp - Trying to connect to amqp://reportportal-rabbitmq.reportportal.svc.cluster.local:5672/analyzer?heartbeat=30
2025-06-03 15:32:47.706 2025-06-03 13:32:47,609 - [xy61hSzYS9-D13MzhYNnNA] - INFO - analyzerApp.amqp - AMQP connection established.
2025-06-03 15:32:47.706 2025-06-03 13:32:47,604 - [ofVyUEV-QzapYgMIoePE7w] - ERROR - analyzerApp.amqp - Connection/channel lost. Reconnecting. Exchange: 'analyzer-default'. Queue: 'index_suggest_info'.
2025-06-03 15:32:47.706 Traceback (most recent call last):
2025-06-03 15:32:47.706   File ""/backend/app/amqp/amqp.py"", line 187, in receive
2025-06-03 15:32:47.706     self._bind_queue(channel, queue, self._config.amqpExchangeName)
2025-06-03 15:32:47.706   File ""/backend/app/amqp/amqp.py"", line 147, in _bind_queue
2025-06-03 15:32:47.706     channel.queue_bind(exchange=exchange_name, queue=name, routing_key=name)
2025-06-03 15:32:47.706   File ""/venv/lib64/python3.11/site-packages/pika/adapters/blocking_connection.py"", line 2570, in queue_bind
2025-06-03 15:32:47.706     self._flush_output(bind_ok_result.is_ready)
2025-06-03 15:32:47.706   File ""/venv/lib64/python3.11/site-packages/pika/adapters/blocking_connection.py"", line 1339, in _flush_output
2025-06-03 15:32:47.706     raise self._closing_reason  # pylint: disable=E0702
2025-06-03 15:32:47.706     ^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-06-03 15:32:47.706 pika.exceptions.ChannelClosedByBroker: (404, ""NOT_FOUND - no exchange 'analyzer-default' in vhost 'analyzer'"")
```

RabbitMQ pod logs:

```
2025-06-03 16:00:36.600 2025-06-03 14:00:36.117476+00:00 [error] <0.8690128.0> Channel error on connection <0.8690983.0> (10.19.172.218:43084 -> 10.19.227.194:5672, vhost: 'analyzer', user: 'rabbitmq'), channel 1:
2025-06-03 16:00:36.600 2025-06-03 14:00:36.117476+00:00 [error] <0.8690128.0> operation queue.bind caused a channel exception not_found: no exchange 'analyzer-default' in vhost 'analyzer'
```

Apparently the `analyzer-default` exchange is not being created. I didn't change any default settings for the analyzer service.

Helm chart version: 25.5.30","kubernetes, reportportal",79652288.0,"The problem is solved by setting the RabbitMQ exchange name explicitly in `values.yaml`:

```
    msgbroker:
      analyzerExchangeName: analyzer
```",2025-06-04T07:17:40,2025-06-04T07:17:40,"```yaml
    msgbroker:
      analyzerExchangeName: analyzer
```

The problem is solved by setting the RabbitMQ exchange name explicitly in `values.yaml`:"
79645483,Can&#39;t see my virtuals ports in my service&#39;s endpoint,"I've created an agent in order to list virtuals and physics ports.create virtuals ports etc.

But when i call the agent in order to display virtuals ports, it shows nothing.

I'm using socat to create virtual ports on my Alpine.

Here's my Kubernetes deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: socat-api
  namespace: camera
  labels:
    app: socat-api
spec:
  replicas: 1
  selector:
    matchLabels:
      app: socat-api
  template:
    metadata:
      labels:
        app: socat-api
    spec:
      nodeSelector:
        kubernetes.io/hostname: cs12
      containers:
        - name: socat-agent
          image: xxx.xx.com:xxx/cam/rs232:0.0.4
          ports:
            - containerPort: 3001
              hostPort: 3001
          securityContext:
            privileged: true
            runAsUser: 0
            runAsGroup: 0
          volumeMounts:
            - name: dev-volume
              mountPath: /host-dev
      volumes:
        - name: dev-volume
          hostPath:
            path: /dev
            type: Directory
```

And here is the Node.js API endpoint I'm using to list the virtual ports:

```
app.get('/serial-ports', (req, res) => {
  console.log(""API pour les ports virtuels appelée..."");

  execFile('ls /dev/ttyV* 2>/dev/null', async (error, stdout, stderr) => {
    if (error || stderr) {
      return res.status(500).json({ error: 'Erreur lors de la détection des ports virtuels.' });
    }

    const ports = stdout.split('\n').filter(line => line.trim() !== '');

    const portInfos = await Promise.all(
      ports.map(portPath => new Promise(resolve => {
        execFile(`stty -a -F ${portPath}`, (err, sttyOutput) => {
          if (err) {
            return resolve({ port: portPath, error: 'Erreur récupération infos' });
          }

          const baud = sttyOutput.match(/speed (\d+) baud/)?.[1] || 'unknown';
          const dataBits = sttyOutput.match(/\bcs(5|6|7|8)\b/)?.[0] || 'unknown';
          const parity = sttyOutput.includes('parenb')
            ? (sttyOutput.includes('parodd') ? 'odd' : 'even')
            : 'none';
          const stopBits = sttyOutput.includes('cstopb') ? '2' : '1';
          const flowControl =
            sttyOutput.includes('crtscts') ? 'hardware' :
            (sttyOutput.includes('ixon') || sttyOutput.includes('ixoff')) ? 'software' :
            'none';
          const mode = sttyOutput.includes('icanon') ? 'canonical' : 'non-canonical';
          const echo = sttyOutput.includes('echo');

          resolve({
            port: portPath.replace('/dev/', ''),
            path: portPath,
            baudRate: baud,
            dataBits,
            parity,
            stopBits,
            flowControl,
            mode,
            echo
          });
        });
      }))
    );
    res.json(portInfos);
  });
});
```

When I call `/serial-ports`, I get an empty list even though `/dev/ttyV0` and `/dev/ttyV1` exist on the host. Why is that?","kubernetes, socat",79645621.0,"the reason is that even if the /dev/ttyV0 and /dev/ttyV1 exist on your host you are not mounting at that path in your container:

This line is the path where you are mounting the host path thus you are mounting it to `/host-dev` in the container section of your deployment and then looking at `/dev` .

```
volumeMounts:
- name: dev-volume
  mountPath: /host-dev
```

change those lines to:

```
volumeMounts:
- name: dev-volume
  mountPath: /dev
```

Also note that you are mounting a host-path thus you need to make sure that path exist in all the nodes of your cluster...",2025-05-30T14:23:30,2025-05-30T12:45:37,"```yaml
volumeMounts:
- name: dev-volume
  mountPath: /host-dev
```

This line is the path where you are mounting the host path thus you are mounting it to `/host-dev` in the container section of your deployment and then looking at `/dev` .

```yaml
volumeMounts:
- name: dev-volume
  mountPath: /dev
```

the reason is that even if the /dev/ttyV0 and /dev/ttyV1 exist on your host you are not mounting at that path in your container:

change those lines to:

Also note that you are mounting a host-path thus you need to make sure that path exist in all the nodes of your cluster..."
79642916,I can&#39;t figure out the command/args settings for acronjob in kubernetes,"I am trying to create a cronJob in kubernetes which generates a report about an Auth0 tenant.  The cronJob can run for any of our Auth0 tenants, so it needs to take arguments to specify which tenant and which database in that tenant to run for.

My deployment file contains the following:

```
              command: [""/bin/sh"", ""-c"", "". /home/cronrun/start.sh""]
              args: [""tenant1"", ""legacy-db""]
```

My Dockerfile only copies the scripts into the image, sets up a non-root user, and sets the WORKDIR properly.  It does not contain CMD or ENTRYPOINT directives.

The script start.sh contains a debug line at the top of the script:

```
               echo ""DEBUG: in start.sh  1:  $1   2:  $2""
```

When the cron runs, I can see this in the logs:

```
               DEBUG: in start.sh  1: legacy-db  2:
```

So obviously I am mishandling the arguments position variables.

What am I doing wrong here?",kubernetes,79643190.0,"You don't need the `sh -c` wrapper, and it's causing the positional-parameter issue you're having.

```
command: [""/home/cronrun/start.sh""]  # no `sh -c` or `.`
args: [""tenant1"", ""legacy-db""]
```

Your shell script needs to be executable and begin with a correct ""shebang"" line, usually `#!/bin/sh`; these should be correct in your source tree, and you shouldn't need to do any special Docker-level setup for these.

If you run [**sh**(1)](https://pubs.opengroup.org/onlinepubs/9799919799/utilities/sh.html) with a `-c` option, its syntax is

```
sh -c command_string command_name argument ...
```

where only the `command_string` parameter is required.  In your original form, combining the `command:` and `args:` yielded

```
sh -c "". /home/cronrun/start.sh"" tenant1      legacy-db
#     command_string............ command_name argument
```

and matching these up you see `tenant1` assigned to the `command_name` parameter.  In your debugging script you'd see this as the positional parameter `$0`, which is typically the script name in normal use.

If you really did want to use `sh -c` here then you could supply an artifical parameter to be the script name

```
command:
  - /bin/sh
  - -c
  - "". /home/cronrun/start.sh""  # command_string
  - start.sh                    # command_name, $0
args:
  - tenant1                     # $1
  - legacy-db                   # $2
```",2025-05-29T01:51:12,2025-05-28T19:51:51,"```text
command: [""/home/cronrun/start.sh""]  # no `sh -c` or `.`
args: [""tenant1"", ""legacy-db""]
```

You don't need the `sh -c` wrapper, and it's causing the positional-parameter issue you're having.

Your shell script needs to be executable and begin with a correct ""shebang"" line, usually `#!/bin/sh`; these should be correct in your source tree, and you shouldn't need to do any special Docker-level setup for these.

If you run [**sh**(1)](https://pubs.opengroup.org/onlinepubs/9799919799/utilities/sh.html) with a `-c` option, its syntax is

```bash
sh -c command_string command_name argument ...
```

where only the `command_string` parameter is required.  In your original form, combining the `command:` and `args:` yielded

```bash
sh -c "". /home/cronrun/start.sh"" tenant1      legacy-db
#     command_string............ command_name argument
```

and matching these up you see `tenant1` assigned to the `command_name` parameter.  In your debugging script you'd see this as the positional parameter `$0`, which is typically the script name in normal use.

```yaml
command:
  - /bin/sh
  - -c
  - "". /home/cronrun/start.sh""  # command_string
  - start.sh                    # command_name, $0
args:
  - tenant1                     # $1
  - legacy-db                   # $2
```

If you really did want to use `sh -c` here then you could supply an artifical parameter to be the script name"
79640662,How can I use ASP.NET Core development certificate inside Docker Desktop Kubernetes?,"When Visual Studio creates the docker compose file, it adds the necessary bindings so that the container has access to the user secrets and the dev certificate:

```
volumes:
  - ${APPDATA}/Microsoft/UserSecrets:/home/app/.microsoft/usersecrets:ro
  - ${APPDATA}/Microsoft/UserSecrets:/root/.microsoft/usersecrets:ro
  - ${APPDATA}/ASP.NET/Https:/home/app/.aspnet/https:ro
  - ${APPDATA}/ASP.NET/Https:/root/.aspnet/https:ro
```

How can I achieve the same using Docker Desktop Kubernetes?","docker, asp.net-core, kubernetes, docker-desktop",79651910.0,"In case anyone needs this, there are several ways you can achieve this. The one I went with was the following:

You need a `hostPath` volume, in Docker Desktop Kubernetes you can access the host machine with this prefix: `/run/desktop/mnt/host/`

So the `${APPDATA}/ASP.NET/Https` folder becomes `/run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https`.

And then use environment variables to configure Kestrel's certificate (in VS this is done by the `launchSettings.json` file).

This is the full deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      pod: gateway-pod
  template:
    metadata:
      labels:
        pod: gateway-pod
    spec:
      containers:
        - name: gateway-container
          image: localhost:5500/gateway
          env:
            - name: Kestrel__Endpoints__Https__Url
              value: ""https://+:8081""
            - name: Kestrel__Endpoints__Https__Certificate__Path
              value: ""/home/app/.aspnet/https/Gateway.pfx""
            - name: Kestrel__Endpoints__Https__Certificate__Password
              value: ""...""
            - name: ASPNETCORE_ENVIRONMENT
              value: ""Development""
            - name: ASPNETCORE_HTTP_PORTS
              value: ""8080""
            - name: ASPNETCORE_HTTPS_PORTS
              value: ""8081""
          volumeMounts:
            - name: https-certs
              mountPath: /home/app/.aspnet/https
              readOnly: true
      volumes:
        - name: https-certs
          hostPath:
            path: /run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https
            type: Directory
```

This is quick and simple to setup a local dev Kubernetes, but needless to say, for production YAML files used with GitOps, you should use Secrets and ConfigMaps.",2025-06-03T21:27:25,2025-05-27T14:17:39,"```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gateway
spec:
  replicas: 2
  selector:
    matchLabels:
      pod: gateway-pod
  template:
    metadata:
      labels:
        pod: gateway-pod
    spec:
      containers:
        - name: gateway-container
          image: localhost:5500/gateway
          env:
            - name: Kestrel__Endpoints__Https__Url
              value: ""https://+:8081""
            - name: Kestrel__Endpoints__Https__Certificate__Path
              value: ""/home/app/.aspnet/https/Gateway.pfx""
            - name: Kestrel__Endpoints__Https__Certificate__Password
              value: ""...""
            - name: ASPNETCORE_ENVIRONMENT
              value: ""Development""
            - name: ASPNETCORE_HTTP_PORTS
              value: ""8080""
            - name: ASPNETCORE_HTTPS_PORTS
              value: ""8081""
          volumeMounts:
            - name: https-certs
              mountPath: /home/app/.aspnet/https
              readOnly: true
      volumes:
        - name: https-certs
          hostPath:
            path: /run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https
            type: Directory
```

In case anyone needs this, there are several ways you can achieve this. The one I went with was the following:

You need a `hostPath` volume, in Docker Desktop Kubernetes you can access the host machine with this prefix: `/run/desktop/mnt/host/`

So the `${APPDATA}/ASP.NET/Https` folder becomes `/run/desktop/mnt/host/c/Users/<your-username>/AppData/Roaming/ASP.NET/Https`.

And then use environment variables to configure Kestrel's certificate (in VS this is done by the `launchSettings.json` file).

This is the full deployment:

This is quick and simple to setup a local dev Kubernetes, but needless to say, for production YAML files used with GitOps, you should use Secrets and ConfigMaps."
79633671,Kafka client attempt to connect only one node from advertised listeners,"I am running kafka in kubernetes using this configuration:

```
  KAFKA_ADVERTISED_LISTENERS: ""INTERNAL://localhost:9090,INSIDE_PLAINTEXT://proxy:19097""
  KAFKA_LISTENERS: ""INTERNAL://0.0.0.0:9090,INTERNAL_FAILOVER://0.0.0.0:9092,INSIDE_PLAINTEXT://0.0.0.0:9094""
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ""INTERNAL:PLAINTEXT,INTERNAL_FAILOVER:PLAINTEXT,INSIDE_PLAINTEXT:PLAINTEXT""
  KAFKA_INTER_BROKER_LISTENER_NAME: ""INTERNAL""
  KAFKA_BOOTSTRAP_SERVERS: ""kafka-mock:9090, kafka-mock:9092""
```

I am attempting to connect to this kafka from my client-app service, running in the same namespace as my kafka.

However my app connects to boostrap server, which should return list of nodes defined in `KAFKA_ADVERTISED_LISTENERS`, connecting to `localhost` node should fail since its not running in same pod, so it should proceed and attempt to conncet to `proxy:19097`, however this does not happen. It attempts to connect to `localhost` and thats it.

IS my configuration wrong for kafka? Did i missplace listener names ? Why isnt it connecting?

If i add another node in `ADVERTISED_LISTENERS` for example `'INTERNAL_PLAINTEXT:kafka-mock:9095'` and also add node that listens on port 9095 to kafka_listeners ( and also mapped 9095:9095), it works. The localhost connection fails but it sends data trough this node, but it always ignores proxy node.

Thanks for help","docker, kubernetes, apache-kafka",79634108.0,"your problem is here
` ""INTERNAL://localhost:9090`

Why kafka-mock:9095 Works ? Kafka broker returns a resolvable DNS name (kafka-mock) to your client.

Use resolvable service names for internal communication.

a config such this will work :

```
KAFKA_ADVERTISED_LISTENERS: ""INTERNAL://kafka-mock:9090,INSIDE_PLAINTEXT://kafka-service:19097""
KAFKA_LISTENERS: ""INTERNAL://0.0.0.0:9090,INSIDE_PLAINTEXT://0.0.0.0:9094""
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ""INTERNAL:PLAINTEXT,INSIDE_PLAINTEXT:PLAINTEXT""
KAFKA_INTER_BROKER_LISTENER_NAME: ""INTERNAL""
```",2025-05-22T15:48:47,2025-05-22T11:44:17,"your problem is here  
` ""INTERNAL://localhost:9090`

Why kafka-mock:9095 Works ? Kafka broker returns a resolvable DNS name (kafka-mock) to your client.

Use resolvable service names for internal communication.

---

```yaml
KAFKA_ADVERTISED_LISTENERS: ""INTERNAL://kafka-mock:9090,INSIDE_PLAINTEXT://kafka-service:19097""
KAFKA_LISTENERS: ""INTERNAL://0.0.0.0:9090,INSIDE_PLAINTEXT://0.0.0.0:9094""
KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: ""INTERNAL:PLAINTEXT,INSIDE_PLAINTEXT:PLAINTEXT""
KAFKA_INTER_BROKER_LISTENER_NAME: ""INTERNAL""
```

a config such this will work :"
79627889,How to setup k8s Pod readiness probe only for the initial phase,"Configuring a Kubernetes pod container `readinessProbe` hot to achieve the probes to be produces only on the initiation phase and once the container is ready just to consider the container is ready all the further way. So that it doesn't spam the traffic as the ready condition is not of a matter any more.

P.S. Found in the [docs](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-readiness-probes)

```
Note:
Readiness probes runs on the container during its whole lifecycle.
```

So still is there maybe any work around to achieve the intention.

Or at least is it possible to configure different check intervals for the two phases separately?

Is it possible to share the state among the checks and once we hit `ready` just go within the check process for internal sleep?","kubernetes, readinessprobe",79627903.0,"It's called [startupProbe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes)

Example from docs:

```
startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30
  periodSeconds: 10
```

Then you can either omit or create completely separate readinessProbe",2025-05-18T22:36:30,2025-05-18T22:17:20,"```yaml
startupProbe:
  httpGet:
    path: /healthz
    port: liveness-port
  failureThreshold: 30
  periodSeconds: 10
```

It's called [startupProbe](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#define-startup-probes)

Example from docs:

Then you can either omit or create completely separate readinessProbe"
79626543,How to override the health check port and endpoint for Kubernetes Service using OCI cloud controller manager,"I have a self-managed cluster consisting of a control plane node, and two worker nodes, all of which are hosted as VMs on OCI. Additionally, I also have configured the [OCI Cloud Controller Manager](https://github.com/oracle/oci-cloud-controller-manager) properly, in order to use OCI load balancers.

I can confirm that the OCI Cloud Controller Manager is configured properly because I have tried deploying a K8s service of type LoadBalancer, and it successfully provisions the LoadBalancer, and the K8s service also gets an external IP. However, when I inspect the LoadBalancer from the dashboard, it says the health is critical because the backendset by default sends HTTP pings on port 10256 at the endpoint ""/healthz"". To make things worse, all my nodes and control plane are returning `healthy: false` because IPv6 is not configured properly on them, and I don't want to debug why. I figured, the easiest solution would be to point the health checks to port 80 at the endpoint ""/"".

Here's the curl response from my control plane:

```
ubuntu@kubemaster:~$ curl -I http://localhost:10256/healthz
HTTP/1.1 503 Service Unavailable
Content-Type: application/json
X-Content-Type-Options: nosniff
Date: Sat, 17 May 2025 12:44:19 GMT
Content-Length: 284

ubuntu@kubemaster:~$ curl -s http://localhost:10256/healthz | jq
{
  ""lastUpdated"": ""2025-05-17T12:40:10.153177379Z"",
  ""currentTime"": ""2025-05-17T12:44:26.894449614Z"",
  ""nodeEligible"": true,
  ""healthy"": false,
  ""status"": {
    ""IPv4"": {
      ""lastUpdated"": ""2025-05-17T12:40:10.153177379Z"",
      ""healthy"": true
    },
    ""IPv6"": {
      ""lastUpdated"": ""2025-05-15T17:18:25.70543555Z"",
      ""healthy"": false
    }
  }
}
```

Here's my `frontend-service.yaml` file:

```
# frontend-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-service
  annotations:
    service.beta.kubernetes.io/oci-load-balancer-shape: ""flexible""
    service.beta.kubernetes.io/oci-load-balancer-internal: ""false""
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-min: ""2""
    service.beta.kubernetes.io/oci-load-balancer-shape-flex-max: ""8""
    # Add these health check annotations:
    service.beta.kubernetes.io/oci-load-balancer-health-check-protocol: ""HTTP""
    service.beta.kubernetes.io/oci-load-balancer-health-check-port: ""80""
    service.beta.kubernetes.io/oci-load-balancer-health-check-path: ""/""
  labels:
    app: checklister-frontend
spec:
  type: LoadBalancer
  selector:
    app: checklister-frontend
  ports:
  - protocol: TCP
    port: 80
    targetPort: 80
```","kubernetes, cloud, oracle-cloud-infrastructure, kubernetes-service",79627314.0,"Try to specify loadbalancer like this

```
apiVersion: v1
kind: Service
metadata:
  name: frontend-lb
  namespace: yournm
spec:
  type: LoadBalancer
  selector:
    app: checklister-frontend  # Selects pods with this label
  ports:
  - port: 80             # Port exposed by the load balancer
    targetPort: 80       # Port the container is listening on
    protocol: TCP
    name: http
  - port: 443            # HTTPS port
    targetPort: 80
    protocol: TCP
    name: https
  sessionAffinity: None
```",2025-05-18T09:36:14,2025-05-17T12:46:41,"```yaml
apiVersion: v1
kind: Service
metadata:
  name: frontend-lb
  namespace: yournm
spec:
  type: LoadBalancer
  selector:
    app: checklister-frontend  # Selects pods with this label
  ports:
  - port: 80             # Port exposed by the load balancer
    targetPort: 80       # Port the container is listening on
    protocol: TCP
    name: http
  - port: 443            # HTTPS port
    targetPort: 80
    protocol: TCP
    name: https
  sessionAffinity: None
```

Try to specify loadbalancer like this"
79624987,Websockets on GKE with Nginx Ingress,"I am trying to get websockets to work on GKE. Seems very trivial, but I am failing to get this to work, I just continuously keep getting 400 at Nginx Ingress.

The manifest is like this:

```
apiVersion: v1
kind: Namespace
metadata:
  name: my-test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-ws-backend
  namespace: my-test
  labels:
    app: my-ws-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-ws-backend
  template:
    metadata:
      labels:
        app: my-ws-backend
    spec:
      containers:
        - name: backend
          image: ksdn117/web-socket-test
          imagePullPolicy: Always
          ports:
            - containerPort: 8010
          env:
            - name: NODE_ENV
              value: production
            - name: DEBUG
              value: socket*
---
apiVersion: v1
kind: Service
metadata:
  name: my-ws-backend
  namespace: my-test
spec:
  selector:
    app: my-ws-backend
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8010
  sessionAffinity: ClientIP
  type: ClusterIP
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: my-ws-ingress
  namespace: my-test
  annotations:
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
    nginx.ingress.kubernetes.io/upgrade-insecure-requests: ""true""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_set_header Upgrade $http_upgrade;
      proxy_set_header Connection $connection_upgrade;
      proxy_set_header Host $host;
    nginx.ingress.kubernetes.io/server-snippet: |
      error_log /var/log/nginx/error.log debug;
    cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
spec:
  ingressClassName: nginx
  rules:
    - host: ws-my-test.myhost.com
      http:
        paths:
          - path: /socket.io
            pathType: Prefix
            backend:
              service:
                name: my-ws-backend
                port:
                  number: 80
  tls:
    - hosts:
        - ws-my-test.myhost.com
      secretName: ws-my-test-cert
```

I tried hitting the endpoint with wscat and a simplistic Node.js script shown below to test. What am I missing?

```
const { io } = require('socket.io-client');

const socket = io('wss://ws-my-test.myhost.com', {
  transports: ['websocket'],
  reconnection: false,
});

socket.on('connect', () => {
  console.log('Connected!');
  socket.disconnect();
});

socket.on('connect_error', (err) => {
  console.error('Connection error:', err);
});
```","kubernetes, websocket, google-kubernetes-engine, nginx-ingress",79628162.0,"Got this working in the end, these are the annotations in my Ingress

```
cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
nginx.ingress.kubernetes.io/proxy-buffering: ""off""
nginx.ingress.kubernetes.io/proxy-connect-timeout: ""10""
nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
```

I think the problem was including duplicates as my annotations below:

```
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection $connection_upgrade;
proxy_set_header Host $host;
```

which were not required, and caused the header to have duplicate values set in the header, that caused rejection of the request with status 400.

Ingress-NGINX controller already comes preconfigured with the required Upgrade/Connection headers set, so not needed to set them again.",2025-05-19T06:16:53,2025-05-16T10:30:02,"```text
Got this working in the end, these are the annotations in my Ingress
```

```yaml
cert-manager.io/cluster-issuer: letsencrypt-prod-nginx
nginx.ingress.kubernetes.io/proxy-http-version: ""1.1""
nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
nginx.ingress.kubernetes.io/proxy-buffering: ""off""
nginx.ingress.kubernetes.io/proxy-connect-timeout: ""10""
nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""
nginx.ingress.kubernetes.io/proxy-send-timeout: ""3600""
```

```text
I think the problem was including duplicates as my annotations below:
```

```nginx
proxy_set_header Upgrade $http_upgrade;
proxy_set_header Connection $connection_upgrade;
proxy_set_header Host $host;
```

```text
which were not required, and caused the header to have duplicate values set in the header, that caused rejection of the request with status 400.

Ingress-NGINX controller already comes preconfigured with the required Upgrade/Connection headers set, so not needed to set them again.
```"
79617217,clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410),"I am trying to deploy `penpot` on a local `minikube` cluster using the following `yaml` file:

```
apiVersion: v1
kind: Namespace
metadata:
  name: penpot
---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: penpot
spec:
  ports:
    - port: 5432
  selector:
    app: postgres
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: postgres
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:latest
          env:
            - name: POSTGRES_DB
              value: penpot
            - name: POSTGRES_USER
              value: penpot
            - name: POSTGRES_PASSWORD
              value: penpot
          ports:
            - containerPort: 5432
          volumeMounts:
            - mountPath: /var/lib/postgresql/data
              name: postgres-storage
      volumes:
        - name: postgres-storage
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: penpot
spec:
  ports:
    - port: 6379
  selector:
    app: redis
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
        - name: redis
          image: redis:7
          ports:
            - containerPort: 6379
---
apiVersion: v1
kind: Service
metadata:
  name: penpot-backend
  namespace: penpot
spec:
  ports:
    - port: 6060
  selector:
    app: penpot-backend
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-backend
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-backend
  template:
    metadata:
      labels:
        app: penpot-backend
    spec:
      containers:
        - name: penpot-backend
          image: penpotapp/backend:latest
          env:
            - name: PENPOT_PUBLIC_URI
              value: http://penpot-frontend
            - name: PENPOT_DATABASE_URI
              value: postgresql://penpot:penpot@postgres:5432/penpot
            - name: PENPOT_REDIS_URI
              value: redis://redis:6379
          ports:
            - containerPort: 6060

---
apiVersion: v1
kind: Service
metadata:
  name: penpot-frontend
  namespace: penpot
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30090
  selector:
    app: penpot-frontend

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-frontend
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-frontend
  template:
    metadata:
      labels:
        app: penpot-frontend
    spec:
      containers:
        - name: penpot-frontend
          image: penpotapp/frontend:latest
          env:
            - name: PENPOT_BACKEND_URI
              value: http://penpot-backend:6060
          ports:
            - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: penpot-exporter
  namespace: penpot
spec:
  ports:
    - port: 6061
  selector:
    app: penpot-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: penpot-exporter
  namespace: penpot
spec:
  replicas: 1
  selector:
    matchLabels:
      app: penpot-exporter
  template:
    metadata:
      labels:
        app: penpot-exporter
    spec:
      containers:
        - name: penpot-exporter
          image: penpotapp/exporter:latest
          env:
            - name: PENPOT_PUBLIC_URI
              value: http://penpot-frontend
          ports:
            - containerPort: 6061
```

But I do have following problem with `penpot-backend` pod:

```
kubectl logs penpot-backend-58ff898db9-p5bz2 -n penpot

+ exec /opt/jdk/bin/java -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Dlog4j2.configurationFile=log4j2.xml -XX:-OmitStackTraceInFastThrow --enable-preview -jar penpot.jar -m app.main
[2025-05-12 04:59:18.265] I app.metrics - action=""initialize metrics""
[2025-05-12 04:59:18.290] I app.db - hint=""initialize connection pool"", name=""main"", uri=""postgresql://penpot:penpot@postgres:5432/penpot"", read-only=false, credentials=true, min-size=0, max-size=60
[2025-05-12 04:59:18.320] I app.migrations - hint=""running migrations"", module=:app.migrations/migrations
SUMMARY:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10005ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
 →  java.net.UnknownHostException: penpot:penpot@postgres (NioSocketImpl.java:567)
DETAIL:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
    at: integrant.core$build_exception.invokeStatic(core.cljc:410)
        integrant.core$build_exception.invoke(core.cljc:409)
        integrant.core$try_build_action.invokeStatic(core.cljc:421)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)

 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10005ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
    at: com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:710)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:189)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:147)
        com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:99)
        next.jdbc.connection$make_connection.invokeStatic(connection.clj:455)
        next.jdbc.connection$make_connection.invoke(connection.clj:439)
        next.jdbc.connection$eval18499$fn__18500.invoke(connection.clj:484)
        next.jdbc.protocols$eval16853$fn__16854$G__16844__16861.invoke(protocols.clj:25)
        next.jdbc$get_connection.invokeStatic(jdbc.clj:169)
        next.jdbc$get_connection.invoke(jdbc.clj:148)
        app.db$open.invokeStatic(db.clj:230)
        app.db$open.invoke(db.clj:227)
        app.migrations$apply_migrations_BANG_.invokeStatic(migrations.clj:445)
        app.migrations$apply_migrations_BANG_.invoke(migrations.clj:443)
        app.migrations$eval33608$fn__33610.invoke(migrations.clj:457)
        clojure.lang.MultiFn.invoke(MultiFn.java:234)
        integrant.core$try_build_action.invokeStatic(core.cljc:419)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
```

java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
java.lang.Thread.run(Thread.java:1583)

```
 →  java.net.UnknownHostException: penpot:penpot@postgres (NioSocketImpl.java:567)
    at: sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)
        java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        java.net.Socket.connect(Socket.java:751)
        org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        org.postgresql.core.PGStream.<init>(PGStream.java:121)
        org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)
```

I also tried to use the `Helm-Chart` but had the same issue.

```
kubectl describe pod penpot-backend-58ff898db9-p5bz2 -n penpot

Name:             penpot-backend-58ff898db9-p5bz2
Namespace:        penpot
Priority:         0
Service Account:  default
Node:             minikube-m03/192.168.49.4
Start Time:       Sun, 11 May 2025 21:52:23 -0700
Labels:           app=penpot-backend
                  pod-template-hash=58ff898db9
                  skaffold.dev/run-id=29a9d1cc-d97f-4e49-9fd7-9ed7a4e32b99
Annotations:      <none>
Status:           Running
IP:               10.244.2.54
IPs:
  IP:           10.244.2.54
Controlled By:  ReplicaSet/penpot-backend-58ff898db9
Containers:
  penpot-backend:
    Container ID:   docker://03bf7598b9510734458239b8bcd3b7a73168d91d985492915b3ec0b324c38914
    Image:          penpotapp/backend:latest
    Image ID:       docker-pullable://penpotapp/backend@sha256:e82c0a7ce65920e4b21fb20d644ec15dd245182a09982c2be23806ef65f1f00c
    Port:           6060/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    255
      Started:      Sun, 11 May 2025 21:58:48 -0700
      Finished:     Sun, 11 May 2025 21:59:29 -0700
    Ready:          False
    Restart Count:  5
    Environment:
      PENPOT_PUBLIC_URI:    http://penpot-frontend
      PENPOT_DATABASE_URI:  postgresql://penpot:penpot@postgres:5432/penpot
      PENPOT_REDIS_URI:     redis://redis:6379
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-66wrb (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  kube-api-access-66wrb:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  8m19s                 default-scheduler  Successfully assigned penpot/penpot-backend-58ff898db9-p5bz2 to minikube-m03
  Normal   Pulled     8m18s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 814ms (814ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     7m37s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 737ms (737ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     6m43s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 807ms (807ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     5m33s                 kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 850ms (850ms including waiting). Image size: 1210864079 bytes.
  Normal   Pulled     4m9s                  kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 1.654s (1.654s including waiting). Image size: 1210864079 bytes.
  Normal   Pulling    114s (x6 over 8m18s)  kubelet            Pulling image ""penpotapp/backend:latest""
  Normal   Created    114s (x6 over 8m17s)  kubelet            Created container: penpot-backend
  Normal   Pulled     114s                  kubelet            Successfully pulled image ""penpotapp/backend:latest"" in 828ms (828ms including waiting). Image size: 1210864079 bytes.
  Normal   Started    113s (x6 over 8m17s)  kubelet            Started container penpot-backend
  Warning  BackOff    11s (x19 over 6m55s)  kubelet            Back-off restarting failed container penpot-backend in pod penpot-backend-58ff898db9-p5bz2_penpot(79338548-cb82-49e0-99e8-71b2a354dd14)
```

`EDIT`:
I tried to modify the following part:

```
# PENPOT_DATABASE_URI: ""postgresql://penpot-postgresql:5432/penpot""
PENPOT_DATABASE_URI: postgresql://postgres/penpot
PENPOT_DATABASE_USERNAME: ""penpot""
PENPOT_DATABASE_PASSWORD: ""penpot""
```

But still get this error:

```
+ exec /opt/jdk/bin/java -Djava.util.logging.manager=org.apache.logging.log4j.jul.LogManager -Dlog4j2.configurationFile=log4j2.xml -XX:-OmitStackTraceInFastThrow --enable-preview -jar penpot.jar -m app.main
[2025-05-14 00:44:28.488] I app.metrics - action=""initialize metrics""
[2025-05-14 00:44:28.507] I app.db - hint=""initialize connection pool"", name=""main"", uri=""postgresql://penpot-postgresql:5432/penpot"", read-only=false, credentials=true, min-size=0, max-size=60
[2025-05-14 00:44:28.532] I app.migrations - hint=""running migrations"", module=:app.migrations/migrations
SUMMARY:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10001ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
 →  java.net.UnknownHostException: penpot-postgresql (NioSocketImpl.java:567)
DETAIL:
 →  clojure.lang.ExceptionInfo: Error on key :app.migrations/migrations when building system (core.cljc:410)
    at: integrant.core$build_exception.invokeStatic(core.cljc:410)
        integrant.core$build_exception.invoke(core.cljc:409)
        integrant.core$try_build_action.invokeStatic(core.cljc:421)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
        clojure.core$alter_var_root.doInvoke(core.clj:5558)
        clojure.lang.RestFn.invoke(RestFn.java:428)
        app.main$start.invokeStatic(main.clj:544)
        app.main$start.invoke(main.clj:540)
        app.main$_main.invokeStatic(main.clj:610)
        app.main$_main.doInvoke(main.clj:602)
        clojure.lang.RestFn.invoke(RestFn.java:400)
        clojure.lang.AFn.applyToHelper(AFn.java:152)
        clojure.lang.RestFn.applyTo(RestFn.java:135)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.core$apply.invokeStatic(core.clj:667)
        clojure.main$main_opt.invokeStatic(main.clj:515)
        clojure.main$main_opt.invoke(main.clj:511)
        clojure.main$main.invokeStatic(main.clj:665)
        clojure.main$main.doInvoke(main.clj:617)
        clojure.lang.RestFn.applyTo(RestFn.java:140)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.main.main(main.java:40)
    dt: {:reason :integrant.core/build-threw-exception,
         :system
         {:app.auth.oidc.providers/github nil,
          :app.db/pool #object[com.zaxxer.hikari.HikariDataSource 0x1faa9581 ""HikariDataSource (main)""],
          :app.auth.oidc.providers/gitlab nil,
          :app.http.client/client
          #object[jdk.internal.net.http.HttpClientFacade 0x129b3801 ""jdk.internal.net.http.HttpClientImpl@127cfcd2(1)""],
          :app.email/blacklist nil,
          :app.auth.oidc.providers/generic nil,
          :app.email/whitelist nil,
          :app.auth.oidc.providers/google nil,
          ...},
         :function #multifn[init-key 0x598cddca],
         :key :app.migrations/migrations,
         :value #:app.db{:pool #object[com.zaxxer.hikari.HikariDataSource 0x1faa9581 ""HikariDataSource (main)""]}}

 →  java.sql.SQLTransientConnectionException: main - Connection is not available, request timed out after 10001ms (total=0, active=0, idle=0, waiting=0) (HikariPool.java:710)
    at: com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:710)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:189)
        com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:147)
        com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:99)
        next.jdbc.connection$make_connection.invokeStatic(connection.clj:455)
        next.jdbc.connection$make_connection.invoke(connection.clj:439)
        next.jdbc.connection$eval18499$fn__18500.invoke(connection.clj:484)
        next.jdbc.protocols$eval16853$fn__16854$G__16844__16861.invoke(protocols.clj:25)
        next.jdbc$get_connection.invokeStatic(jdbc.clj:169)
        next.jdbc$get_connection.invoke(jdbc.clj:148)
        app.db$open.invokeStatic(db.clj:230)
        app.db$open.invoke(db.clj:227)
        app.migrations$apply_migrations_BANG_.invokeStatic(migrations.clj:445)
        app.migrations$apply_migrations_BANG_.invoke(migrations.clj:443)
        app.migrations$eval33608$fn__33610.invoke(migrations.clj:457)
        clojure.lang.MultiFn.invoke(MultiFn.java:234)
        integrant.core$try_build_action.invokeStatic(core.cljc:419)
        integrant.core$try_build_action.invoke(core.cljc:418)
        integrant.core$build_key.invokeStatic(core.cljc:427)
        integrant.core$build_key.invoke(core.cljc:423)
        clojure.core$partial$fn__5931.invoke(core.clj:2656)
        clojure.core.protocols$fn__8275.invokeStatic(protocols.clj:167)
        clojure.core.protocols/fn(protocols.clj:123)
        clojure.core.protocols$fn__8229$G__8224__8238.invoke(protocols.clj:19)
        clojure.core.protocols$seq_reduce.invokeStatic(protocols.clj:31)
        clojure.core.protocols$fn__8262.invokeStatic(protocols.clj:74)
        clojure.core.protocols/fn(protocols.clj:74)
        clojure.core.protocols$fn__8203$G__8198__8216.invoke(protocols.clj:13)
        clojure.core$reduce.invokeStatic(core.clj:6965)
        clojure.core$reduce.invoke(core.clj:6947)
        integrant.core$build.invokeStatic(core.cljc:453)
        integrant.core$build.invoke(core.cljc:430)
        integrant.core$init.invokeStatic(core.cljc:675)
        integrant.core$init.invoke(core.cljc:667)
        integrant.core$init.invokeStatic(core.cljc:672)
        integrant.core$init.invoke(core.cljc:667)
        app.main$start$fn__31972.invoke(main.clj:550)
        clojure.lang.AFn.applyToHelper(AFn.java:154)
        clojure.lang.AFn.applyTo(AFn.java:144)
        clojure.lang.Var.alterRoot(Var.java:310)
        clojure.core$alter_var_root.invokeStatic(core.clj:5563)
        clojure.core$alter_var_root.doInvoke(core.clj:5558)
        clojure.lang.RestFn.invoke(RestFn.java:428)
        app.main$start.invokeStatic(main.clj:544)
        app.main$start.invoke(main.clj:540)
        app.main$_main.invokeStatic(main.clj:610)
        app.main$_main.doInvoke(main.clj:602)
        clojure.lang.RestFn.invoke(RestFn.java:400)
        clojure.lang.AFn.applyToHelper(AFn.java:152)
        clojure.lang.RestFn.applyTo(RestFn.java:135)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.core$apply.invokeStatic(core.clj:667)
        clojure.main$main_opt.invokeStatic(main.clj:515)
        clojure.main$main_opt.invoke(main.clj:511)
        clojure.main$main.invokeStatic(main.clj:665)
        clojure.main$main.doInvoke(main.clj:617)
        clojure.lang.RestFn.applyTo(RestFn.java:140)
        clojure.lang.Var.applyTo(Var.java:707)
        clojure.main.main(main.java:40)

 →  org.postgresql.util.PSQLException: The connection attempt failed. (ConnectionFactoryImpl.java:364)
    at: org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:364)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)

 →  java.net.UnknownHostException: penpot-postgresql (NioSocketImpl.java:567)
    at: sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:567)
        java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)
        java.net.Socket.connect(Socket.java:751)
        org.postgresql.core.PGStream.createSocket(PGStream.java:260)
        org.postgresql.core.PGStream.<init>(PGStream.java:121)
        org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:140)
        org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:268)
        org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:54)
        org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:273)
        org.postgresql.Driver.makeConnection(Driver.java:446)
        org.postgresql.Driver.connect(Driver.java:298)
        com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:139)
        com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:367)
        com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:205)
        com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:484)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:748)
        com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:727)
        java.util.concurrent.FutureTask.run(FutureTask.java:317)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
        java.lang.Thread.run(Thread.java:1583)
```","postgresql, kubernetes",79618040.0,"Nothing in penpot documentation says that you can use

`postgresql://penpot:penpot@postgres:5432/penpot`  syntax to set both db url and user credentials in a single variable.

As per [docs](https://help.penpot.app/technical-guide/configuration/#database), you should set:

```
PENPOT_DATABASE_USERNAME: penpot
PENPOT_DATABASE_PASSWORD: penpot
PENPOT_DATABASE_URI: postgresql://postgres/penpot
```",2025-05-12T14:19:57,2025-05-12T05:05:59,"```text
`postgresql://penpot:penpot@postgres:5432/penpot`  syntax to set both db url and user credentials in a single variable.
```

Nothing in penpot documentation says that you can use

---

```yaml
PENPOT_DATABASE_USERNAME: penpot
PENPOT_DATABASE_PASSWORD: penpot
PENPOT_DATABASE_URI: postgresql://postgres/penpot
```

As per [docs](https://help.penpot.app/technical-guide/configuration/#database), you should set:"
79614460,ingress controller does not serve pages after update to 1.12.x,"I have bumped into problems after update of our [nginx ingress](https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx) from version `1.11.5` (helm chart version `4.11.5`) to `1.12.2` (helm chart version `4.12.2`).

Basically I have ingress that is working with nginx `1.11.5` and prior versions without any problems (see template bellow), but when I upgrade it, I am only getting 404s from ingress.

When I remove `configuration-snippet` annotation from ingress template entirely, the webpage is displayed, but it has wrong `ContentSecurityPolicy` header because this header is also specified globally via `controller.addHeaders`.

I know that there were signifficant changes in order to fix these CVEs: CVE-2025-1097 CVE-2025-1098 CVE-2025-1974 CVE-2025-24513 and CVE-2025-24514 ([https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1)) so this is probably related to that

In order to upgrade to `1.11.5` previously I had to enable `controller.allowSnippetAnnotations` so for `1.12.2` upgrade this value is also set up: `controller.allowSnippetAnnotations: true`

How can I enable the `*-snippet` annotations again? Is it even possible due to the security related changes? Or is there any better way how to specify headers?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers ContentSecurityPolicy ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
      more_set_headers ""Cache-Control: no-store"";
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""false""
    nginx.ingress.kubernetes.io/cors-allow-headers: Authorization, Content-Type
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, PUT, DELETE, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: https://<host>,http://localhost:3000
    nginx.ingress.kubernetes.io/cors-max-age: ""3600""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""8""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.org/proxy-pass-headers: IDAM_USER,IDAM-USER
    nginx.org/server-tokens: ""False""
  labels:
    app: <release-name>
    app.kubernetes.io/managed-by: Helm
  name: <release-name>
  namespace: <namespace>
spec:
  ingressClassName: <ingressclass-name>
  rules:
  - host: <host>
    http:
      paths:
      - backend:
          service:
            name: <release-name>
            port:
              number: 80
        path: /(.*)
        pathType: Prefix
  tls:
  - hosts:
    - ‎<host>
    secretName: <secret-name>
```","kubernetes, nginx, kubernetes-ingress",79618503.0,"Here is what we have configured in our Helm Chart `ingress-nginx-4.12.1` to enable config snippets.

```
 proxySetHeaders:
    allow-snippet-annotations: ""true""
```

```
podAnnotations:
    ingressclass.kubernetes.io/is-default-class: ""true""
    allow-snippet-annotations: ""true""
```",2025-05-12T19:02:52,2025-05-09T15:42:45,"```yaml
 proxySetHeaders:
    allow-snippet-annotations: ""true""
```

Here is what we have configured in our Helm Chart `ingress-nginx-4.12.1` to enable config snippets.

```yaml
podAnnotations:
    ingressclass.kubernetes.io/is-default-class: ""true""
    allow-snippet-annotations: ""true""
```

(No additional explanatory text present beyond what is already included.)"
79614460,ingress controller does not serve pages after update to 1.12.x,"I have bumped into problems after update of our [nginx ingress](https://github.com/kubernetes/ingress-nginx/tree/main/charts/ingress-nginx) from version `1.11.5` (helm chart version `4.11.5`) to `1.12.2` (helm chart version `4.12.2`).

Basically I have ingress that is working with nginx `1.11.5` and prior versions without any problems (see template bellow), but when I upgrade it, I am only getting 404s from ingress.

When I remove `configuration-snippet` annotation from ingress template entirely, the webpage is displayed, but it has wrong `ContentSecurityPolicy` header because this header is also specified globally via `controller.addHeaders`.

I know that there were signifficant changes in order to fix these CVEs: CVE-2025-1097 CVE-2025-1098 CVE-2025-1974 CVE-2025-24513 and CVE-2025-24514 ([https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1](https://github.com/kubernetes/ingress-nginx/releases/tag/controller-v1.12.1)) so this is probably related to that

In order to upgrade to `1.11.5` previously I had to enable `controller.allowSnippetAnnotations` so for `1.12.2` upgrade this value is also set up: `controller.allowSnippetAnnotations: true`

How can I enable the `*-snippet` annotations again? Is it even possible due to the security related changes? Or is there any better way how to specify headers?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/configuration-snippet: |
      more_set_headers ContentSecurityPolicy ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
      more_set_headers ""Cache-Control: no-store"";
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""false""
    nginx.ingress.kubernetes.io/cors-allow-headers: Authorization, Content-Type
    nginx.ingress.kubernetes.io/cors-allow-methods: GET, POST, PUT, DELETE, OPTIONS
    nginx.ingress.kubernetes.io/cors-allow-origin: https://<host>,http://localhost:3000
    nginx.ingress.kubernetes.io/cors-max-age: ""3600""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/force-ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/proxy-buffer-size: 64k
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""8""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.org/proxy-pass-headers: IDAM_USER,IDAM-USER
    nginx.org/server-tokens: ""False""
  labels:
    app: <release-name>
    app.kubernetes.io/managed-by: Helm
  name: <release-name>
  namespace: <namespace>
spec:
  ingressClassName: <ingressclass-name>
  rules:
  - host: <host>
    http:
      paths:
      - backend:
          service:
            name: <release-name>
            port:
              number: 80
        path: /(.*)
        pathType: Prefix
  tls:
  - hosts:
    - ‎<host>
    secretName: <secret-name>
```","kubernetes, nginx, kubernetes-ingress",79618035.0,"A better way to inject response headers is by using `nginx.ingress.kubernetes.io/custom-headers` annotation, as documented at [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers)

For the headers you want, define the following configmap first:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-ingress-headers
  namespace: your-namespace
data:
  ContentSecurityPolicy: ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
  Cache-Control: no-store
```

And use it in your ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/custom-headers: your-namespace/custom-ingress-headers
```

Please do note that in the linked page it is specifically stated that: `This annotation uses more_set_headers nginx directive.`",2025-05-12T14:18:19,2025-05-09T15:42:45,"```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: custom-ingress-headers
  namespace: your-namespace
data:
  ContentSecurityPolicy: ""default-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com; img-src 'self' https://*.pendo.io data:;frame-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;object-src 'self' 'unsafe-eval' 'unsafe-inline' https://*.pendo.io  https://*.storage.googleapis.com blob: data:;""
  Cache-Control: no-store
```

A better way to inject response headers is by using `nginx.ingress.kubernetes.io/custom-headers` annotation, as documented at [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/#custom-headers)

For the headers you want, define the following configmap first:

---

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/custom-headers: your-namespace/custom-ingress-headers
```

And use it in your ingress:

Please do note that in the linked page it is specifically stated that: `This annotation uses more_set_headers nginx directive.`"
79613903,K8s node.js pod setup env.js from .yaml deployment,"I have a container where i released a node.js frontend, some of my envs are stored in an env.js, how can i configure this envs from the yaml file of the deployment (using the env in deployment is not working).

In alternative i tried setupping a pvc to mount in the pod the env.js that i deposit in the pv but it is not working as the file is copied as a directory and idk why.

**env.js:**

```
window.env = { ""API_URL"": ""http://ip:port"" }
```","reactjs, node.js, kubernetes",79614136.0,"I ended up finding that what David said was on the right track, apparently the file env.js cant be in the same folder as the application, but if you set it in a subfolder for example env/env.js and configuring the ConfigMap to write the file actually works.

ConfigMap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: cfg-map
data:
  env.js: |
    window.env = {
      ""API_URL"": ""http://ip:port""
    }
```

Deployment:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment
spec:
  ...
  selector:
    spec:
      ...
      volumeMounts:
      - name: storage
        mountPath: /usr/share/nginx/html/env
    volumes:
    - name: storage
      configMap:
        name: cfg-map
        items:
        - key: ""env.js""
          path: ""env.js""
```",2025-05-09T12:42:44,2025-05-09T10:10:57,"```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cfg-map
data:
  env.js: |
    window.env = {
      ""API_URL"": ""http://ip:port""
    }
```

I ended up finding that what David said was on the right track, apparently the file env.js cant be in the same folder as the application, but if you set it in a subfolder for example env/env.js and configuring the ConfigMap to write the file actually works.

ConfigMap:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment
spec:
  ...
  selector:
    spec:
      ...
      volumeMounts:
      - name: storage
        mountPath: /usr/share/nginx/html/env
    volumes:
    - name: storage
      configMap:
        name: cfg-map
        items:
        - key: ""env.js""
          path: ""env.js""
```

Deployment:"
79609973,Trigger knative jobSink from external source,"I want to use jobSinks that can be triggered from external sources. By default, jobSinks can only be triggered from inside the kubernetes cluster (svc.cluster.local address).

For example I want to trigger the jobSink with a CURL from outside the kubernetes cluster. But so far I'm not able to expose it.

My hope with jobSink was, that I wont need a 24/7 running container that only listens for incoming requests and then triggers a job. Is it even possible at all?","kubernetes, knative",79610285.0,"[Complete example available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/knative-jobsink-79609973)

Let's remember that triggering a JobSink actually works not because you use `curl`, but because a CloudEvent is sent to a certain endpoint using http as the transport protocol.

The job-sink service is set up by knative-eventing and should look something like this (a bit cleaned for readability:

```
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: job-sink
    app.kubernetes.io/name: knative-eventing
    app.kubernetes.io/version: 1.18.1
    sinks.knative.dev/sink: job-sink
  name: job-sink
  namespace: knative-eventing
spec:
  clusterIP: 10.96.159.186
  clusterIPs:
  - 10.96.159.186
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  - name: http-metrics
    port: 9092
  selector:
    sinks.knative.dev/sink: job-sink
```

Let's now deploy a simple JobSink:

```
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: job-sink-logger
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: main
              image: docker.io/library/bash:5
              command: [ ""cat"" ]
              args:
                - ""/etc/jobsink-event/event""
```

## Use an ingress

With that out of the way, we can simply create an ingress.

Given an ingress like

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jobsink-demo
  namespace: knative-eventing
spec:
  rules:
  - host: ""jobsink-demo.192-168-1-6.sslip.io""
    http:
      paths:
      - pathType: Prefix
        path: ""/demo""
        backend:
          service:
            name: job-sink
            port:
              number: 80
```

and assuming that your cluster runs on `192.168.1.6`, with the ingress listening on port 9090, sure enough we can send our request:

```
$ curl -v \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: 123"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://jobsink-demo.192-168-1-6.sslip.io:9090/demo/job-sink-logger

* Host jobsink-demo.192-168-1-6.sslip.io:9090 was resolved.
* IPv6: (none)
* IPv4: 192.168.1.6
*   Trying 192.168.1.6:9090...
* Connected to jobsink-demo.192-168-1-6.sslip.io (192.168.1.6) port 9090
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: jobsink-demo.192-168-1-6.sslip.io:9090
> User-Agent: curl/8.12.1
> Accept: */*
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 123
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/123
< date: Wed, 07 May 2025 09:40:30 GMT
< content-length: 0
< x-envoy-upstream-service-time: 22
< server: envoy
<
* Connection #0 to host jobsink-demo.192-168-1-6.sslip.io left intact
```

and when we look at output of the created pod, we get what we expected:

```
$ kubectl get jobs.batch -n demo
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-61cceec46c111666dbac62910030fd6e   Complete   1/1           10s        29m
$ kubectl -n demo logs job-sink-logger-61cceec46c111666dbac62910030fd6e-tsw9c
{""specversion"":""1.0"",""id"":""123"",""source"":""my/curl/command"",""type"":""my.demo.event"",""datacontenttype"":""application/json"",""data"":{""details"":""JobSinkDemo""}}
```

> ***Note***
>
>
> With a plain ingress, you expose the job-sink to the outside world **without authentication**.
>
>
> *Please* ensure to secure the access!
> Allmost all ingress controllers allow to add at least BasicAuth or DigestAuth authentication.

# Alternative solution 1: use kubectl to run a one-shot pod

If the people who need to trigger the job sink can access the cluster using kubectl and are allowed to run pods, it becomes rather easy:

```
$ kubectl run -n demo submit-$(( RANDOM  )) -it --restart=Never \
  --image=alpine/curl -- \
  -iv \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://job-sink.knative-eventing.svc/demo/job-sink-logger
* Host job-sink.knative-eventing.svc:80 was resolved.
* IPv6: (none)
* IPv4: 10.96.159.186
*   Trying 10.96.159.186:80...
* Connected to job-sink.knative-eventing.svc (10.96.159.186) port 80
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: job-sink.knative-eventing.svc
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 3500
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
< Date: Wed, 07 May 2025 11:55:50 GMT
Date: Wed, 07 May 2025 11:55:50 GMT
< Content-Length: 0
Content-Length: 0
< Connection: close
Connection: close
<

* shutting down connection #0
$ kubectl -n demo get jobs
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-613109358aa56215a871ef6e4a5b06ed   Complete   1/1           4s         3m43s
```

## Alternative solution 2: use a job resource

If the users have kubectl access, they can also simply create a job triggering the job sink. Semantically, I think this is the most semantically correct:

```
apiVersion: batch/v1
kind: Job
metadata:
  name: trigger-jobsink-log
  namespace: demo
spec:
  ttlSecondsAfterFinished: 0
  template:
    spec:
      containers:
      - name: trigger
        image: alpine/curl
        command:
        - /bin/sh
        - -c
        - |
          curl -iv -H ""Connection: Close"" \
          --fail-with-body \
          -H ""content-type: application/json"" \
          -H ""ce-specversion: 1.0"" \
          -H ""ce-source: my/curl/command"" \
          -H ""ce-type: my.demo.event"" \
          -H ""ce-id:+$(( RANDOM ))"" \
          -d '{""details"":""JobSinkDemo""}' \
          http://job-sink.knative-eventing.svc/demo/job-sink-logger
      restartPolicy: Never
```

This creates a job that will vanish immediately after it was finished (`ttlSecondsAfterFinished: 0`). Note that the job will be reattempted on a curl failure or http return codes >= 400, even though `restartPolicy` is set to `Never`.

## Alternative Solution 3: use port-forwarding

```
$ kubectl port-forward -n knative-eventing svc/job-sink 8181:80 &> /dev/null &
[1] 82301
$ curl -v \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://localhost:8181/demo/job-sink-logger
* Host localhost:8181 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:8181...
* Connected to localhost (::1) port 8181
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: localhost:8181
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 28258
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/28258
< Date: Wed, 07 May 2025 12:38:37 GMT
< Content-Length: 0
< Connection: close
<
* shutting down connection #0
$ kill -TERM 82301
```",2025-05-07T10:08:30,2025-05-07T07:15:23,"```markdown
[Complete example available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/knative-jobsink-79609973)

Let's remember that triggering a JobSink actually works not because you use `curl`, but because a CloudEvent is sent to a certain endpoint using http as the transport protocol.

The job-sink service is set up by knative-eventing and should look something like this (a bit cleaned for readability:
```

```yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app.kubernetes.io/component: job-sink
    app.kubernetes.io/name: knative-eventing
    app.kubernetes.io/version: 1.18.1
    sinks.knative.dev/sink: job-sink
  name: job-sink
  namespace: knative-eventing
spec:
  clusterIP: 10.96.159.186
  clusterIPs:
  - 10.96.159.186
  ipFamilies:
  - IPv4
  ipFamilyPolicy: SingleStack
  ports:
  - name: http
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
  - name: http-metrics
    port: 9092
  selector:
    sinks.knative.dev/sink: job-sink
```

```markdown
Let's now deploy a simple JobSink:
```

```yaml
apiVersion: sinks.knative.dev/v1alpha1
kind: JobSink
metadata:
  name: job-sink-logger
spec:
  job:
    spec:
      completions: 1
      parallelism: 1
      template:
        spec:
          restartPolicy: Never
          containers:
            - name: main
              image: docker.io/library/bash:5
              command: [ ""cat"" ]
              args:
                - ""/etc/jobsink-event/event""
```

```markdown
## Use an ingress

With that out of the way, we can simply create an ingress.

Given an ingress like
```

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: jobsink-demo
  namespace: knative-eventing
spec:
  rules:
  - host: ""jobsink-demo.192-168-1-6.sslip.io""
    http:
      paths:
      - pathType: Prefix
        path: ""/demo""
        backend:
          service:
            name: job-sink
            port:
              number: 80
```

```markdown
and assuming that your cluster runs on `192.168.1.6`, with the ingress listening on port 9090, sure enough we can send our request:
```

```bash
$ curl -v \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: 123"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://jobsink-demo.192-168-1-6.sslip.io:9090/demo/job-sink-logger

* Host jobsink-demo.192-168-1-6.sslip.io:9090 was resolved.
* IPv6: (none)
* IPv4: 192.168.1.6
*   Trying 192.168.1.6:9090...
* Connected to jobsink-demo.192-168-1-6.sslip.io (192.168.1.6) port 9090
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: jobsink-demo.192-168-1-6.sslip.io:9090
> User-Agent: curl/8.12.1
> Accept: */*
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 123
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/123
< date: Wed, 07 May 2025 09:40:30 GMT
< content-length: 0
< x-envoy-upstream-service-time: 22
< server: envoy
<
* Connection #0 to host jobsink-demo.192-168-1-6.sslip.io left intact
```

```markdown
and when we look at output of the created pod, we get what we expected:
```

```bash
$ kubectl get jobs.batch -n demo
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-61cceec46c111666dbac62910030fd6e   Complete   1/1           10s        29m
$ kubectl -n demo logs job-sink-logger-61cceec46c111666dbac62910030fd6e-tsw9c
{""specversion"":""1.0"",""id"":""123"",""source"":""my/curl/command"",""type"":""my.demo.event"",""datacontenttype"":""application/json"",""data"":{""details"":""JobSinkDemo""}}
```

```markdown
> ***Note***
>
>
> With a plain ingress, you expose the job-sink to the outside world **without authentication**.
>
>
> *Please* ensure to secure the access!
> Allmost all ingress controllers allow to add at least BasicAuth or DigestAuth authentication.

# Alternative solution 1: use kubectl to run a one-shot pod

If the people who need to trigger the job sink can access the cluster using kubectl and are allowed to run pods, it becomes rather easy:
```

```bash
$ kubectl run -n demo submit-$(( RANDOM  )) -it --restart=Never \
  --image=alpine/curl -- \
  -iv \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://job-sink.knative-eventing.svc/demo/job-sink-logger
* Host job-sink.knative-eventing.svc:80 was resolved.
* IPv6: (none)
* IPv4: 10.96.159.186
*   Trying 10.96.159.186:80...
* Connected to job-sink.knative-eventing.svc (10.96.159.186) port 80
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: job-sink.knative-eventing.svc
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 3500
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/3500
< Date: Wed, 07 May 2025 11:55:50 GMT
Date: Wed, 07 May 2025 11:55:50 GMT
< Content-Length: 0
Content-Length: 0
< Connection: close
Connection: close
<

* shutting down connection #0
$ kubectl -n demo get jobs
NAME                                               STATUS     COMPLETIONS   DURATION   AGE
job-sink-logger-613109358aa56215a871ef6e4a5b06ed   Complete   1/1           4s         3m43s
```

```markdown
## Alternative solution 2: use a job resource

If the users have kubectl access, they can also simply create a job triggering the job sink. Semantically, I think this is the most semantically correct:
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: trigger-jobsink-log
  namespace: demo
spec:
  ttlSecondsAfterFinished: 0
  template:
    spec:
      containers:
      - name: trigger
        image: alpine/curl
        command:
        - /bin/sh
        - -c
        - |
          curl -iv -H ""Connection: Close"" \
          --fail-with-body \
          -H ""content-type: application/json"" \
          -H ""ce-specversion: 1.0"" \
          -H ""ce-source: my/curl/command"" \
          -H ""ce-type: my.demo.event"" \
          -H ""ce-id:+$(( RANDOM ))"" \
          -d '{""details"":""JobSinkDemo""}' \
          http://job-sink.knative-eventing.svc/demo/job-sink-logger
      restartPolicy: Never
```

```markdown
This creates a job that will vanish immediately after it was finished (`ttlSecondsAfterFinished: 0`). Note that the job will be reattempted on a curl failure or http return codes >= 400, even though `restartPolicy` is set to `Never`.

## Alternative Solution 3: use port-forwarding
```

```bash
$ kubectl port-forward -n knative-eventing svc/job-sink 8181:80 &> /dev/null &
[1] 82301
$ curl -v \
  -H ""Connection: Close"" \
  -H ""content-type: application/json"" \
  -H ""ce-specversion: 1.0"" \
  -H ""ce-source: my/curl/command"" \
  -H ""ce-type: my.demo.event"" \
  -H ""ce-id: $(( RANDOM ))"" \
  -d '{""details"":""JobSinkDemo""}' \
  http://localhost:8181/demo/job-sink-logger
* Host localhost:8181 was resolved.
* IPv6: ::1
* IPv4: 127.0.0.1
*   Trying [::1]:8181...
* Connected to localhost (::1) port 8181
* using HTTP/1.x
> POST /demo/job-sink-logger HTTP/1.1
> Host: localhost:8181
> User-Agent: curl/8.12.1
> Accept: */*
> Connection: Close
> content-type: application/json
> ce-specversion: 1.0
> ce-source: my/curl/command
> ce-type: my.demo.event
> ce-id: 28258
> Content-Length: 25
>
* upload completely sent off: 25 bytes
< HTTP/1.1 202 Accepted
< Location: /namespaces/demo/name/job-sink-logger/sources/my/curl/command/ids/28258
< Date: Wed, 07 May 2025 12:38:37 GMT
< Content-Length: 0
< Connection: close
<
* shutting down connection #0
$ kill -TERM 82301
```"
79607627,Spring Cloud Gateway routing to a service in Kubernetes environments without discovery service,"Summary:

> How can I use Spring Cloud Gateway to route requests to a given
> service, without a discovery server in the Kubernetes environment?

We have our gateway implementation based on **Zuul**, running in Kubernetes environment, using relatively old versions:

- spring-boot-starter-parent: 2.2.6.RELEASE
- spring-cloud-dependencies: Hoxton.SR4
- spring-cloud-kubernetes: 1.1.1.RELEASE

Now I try to upgrade this gateway to **Spring Cloud Gateway** and newer libraries:

- spring-boot-starter-parent :3.3.2
- spring-cloud-dependencies: 2023.0.3
- spring-cloud-kubernetes: 1.1.10.RELEASE

Most of the features of the original application is successfully refactored, and working but I am stuck, when I want to create a route to a specific Kubernetes service (using the serviceId), rather than using Kubernetes DNS resolution: `http://<serviceId>:<servicePort>`

With Zuul it was almost automatic. We created a ServiceAccount with appropriate privileges to get access to Kubernetes services. In the application I enabled Kubernetes discovery:

```
    spring:
      cloud:
        kubernetes:
          discovery:
            enabled: true
            service-labels:
              discovery: enabled
          enabled: true
        service-registry:
          auto-registration:
            enabled: false
```

We had the following dependencies:

```
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-netflix-ribbon</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-kubernetes-discovery</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-kubernetes-ribbon</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
```

and I could simply create a new ZuulRoute, passing the `serviceId` as one of the parameter.

In this solution we did not use Eureka or any other Discovery Service. My guess is that Zuul using Kubernetes API collected the service info and forwarded the requests.

I would like to implement the same functionality with Spring Cloud Gateway. Reading the documentation, the only possible way seems to be using the Loadbalacer, i.e. set the route's uri to `lb://serviceId`

I found this post: [How to set up Spring Cloud Gateway application so it can use the Service Discovery of Spring Cloud Kubernetes?](https://stackoverflow.com/questions/56170511/how-to-set-up-spring-cloud-gateway-application-so-it-can-use-the-service-discove)
Following this post, I have the following dependencies:

```
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-config</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-ribbon</artifactId>
      <version>${spring-cloud-kubernetes.version}</version>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-loadbalancer</artifactId>
      <version>1.1.10.RELEASE</version>
    </dependency>
```

This might be more than the minimum, but copied from the test application from the above link.

If I enable kubernetes discovery, like above:

```
    spring:
      cloud:
        kubernetes:
          discovery:
            enabled: true
            service-labels:
              discovery: enabled
          enabled: true
        service-registry:
          auto-registration:
            enabled: false
```

then the pod will not start, for it requires a discovery server's URL:

```
'spring.cloud.kubernetes.discovery.discovery-server-url' must be specified and be a valid URL
```

But I do not want/cannot install a discovery server in the Kubernetes environment, and cannot use Eureka any longer. This was not needed required for the Zuul-based solution.

I was hoping that Cloud Gateway can do service discovery/load balancing ""internally"" - client side - accessing Kubernets API, but if I disable kubernetes discovery, the service is not found:

```
Service unavailable: Unable to find instance for serviceId
```

I was hoping that Kubernetes' Ribbon will be used by Cloud Gateway to discover services.

Any idea, how can I achive this?

Note, that the above article discusses the possibility of auto-registering services, i.e:

```
spring:
  application.name: gateway
  cloud:
    gateway:
      discovery:
        locator:
          enabled: true
```

That is also a feature I am interested in, but I feel first I should be able to solve the explicit routing issue.","spring-boot, kubernetes, load-balancing, spring-cloud-gateway, service-discovery",79619578.0,"It seems that I have to answer my own question, maybe someone will learn from my experiences.

First of all, I am using the reactive version of Spring Cloud Gateway, i.e. it is based on WebFlux and uses Netty as the HTTP server. Mostly studying sources for:

- [Spring Cloud Kubertnetes](https://github.com/spring-cloud/spring-cloud-kubernetes)
- [Spring Cloud Gateway](https://github.com/spring-cloud/spring-cloud-gateway)

I found out, that Spring Cloud Gateway will use a *DiscoveryClient* for both load balancing and auto-registering services. And like almost everything in Spring Boot, it'll use a *DiscoveryClient* implementation bean, whichever it'll find on the classpath.

In Spring Cloud Kubernetes I found 2 implementation of *ReactiveDiscoveryClient*:

- org.springframework.cloud.kubernetes.discovery.KubernetesReactiveDiscoveryClient
- org.springframework.cloud.kubernetes.client.discovery.reactive.**KubernetesInformerReactiveDiscoveryClient**

The first implementation requires a *DiscoveryServer*, and it needs the server's URL as a configuration. The second one seems a ""native"" DiscoveryClient, i.e. it uses Kubernetes API to discover the services.

My problem was that my application's Cloud Gateway found the first on the classpath and instantiated. After realizing this, it was ""only"" trying various combination of dependencies - with some educated guesses, hints from the source - to ensure that only *KubernetesInformerReactiveDiscoveryClient* would be instatiated. For the particular case I had to have **only** the dependency:

```
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
```

For example when I've added any of the followings:

```
      <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes</artifactId>
        <version>${spring-cloud-kubernetes.version}</version>
      </dependency>

      <dependency>
          <groupId>org.springframework.cloud</groupId>
          <artifactId>spring-cloud-kubernetes-discovery</artifactId>
          <version>${spring-cloud-kubernetes.version}</version>
      </dependency>
```

although they look ""logical"" and "" innocent"", they resulted the other *DiscoveryClient* instantiated, which requires a *DiscoveryServer*.

From here it was a simple path to make my program work. I ""only""  had to figure out, what dependency should I have for load balancing - including caching-, and a couple of settings to enable/configure these implementation.

Here is the *relevant* part of my solution. Dependencies:

```
  <dependencies>
    ...
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-loadbalancer</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
    ...
  </dependencies>
```

and the settings:

```
spring:
  cloud:
    gateway:
      discovery:
        locator:
          enabled: true
    discovery:
      blocking:
        enabled: false
      reactive:
        enabled: true
      client:
        health-indicator:
          enabled: false
    kubernetes:
      discovery:
        enabled: true
        service-labels:
          discovery: enabled
      enabled: true
    service-registry:
      auto-registration:
        enabled: false
```

I don't claim that this is the best, optimal, minimal, etc. set of depenencies and settings, but my Spring Cloud Gateway based application is working, i.e:

- I can use in my route definitions service ID's, for example:

```
.uri(URI.create(""lb://"" + serviceId))
```

- routes created automatically for all of the discovered services i.e. which has a label: *discovery=enabled*. This feature can be disabled by spring.cloud.discovery.locator.enabled=false

See the [doc](https://docs.spring.io/spring-cloud-gateway/reference/spring-cloud-gateway/the-discoveryclient-route-definition-locator.html) for the Reactive Cloud Gateway for both features.",2025-05-13T11:47:52,2025-05-05T20:59:10,"```xml
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
```

For the particular case I had to have **only** the dependency:

---

```xml
      <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-kubernetes</artifactId>
        <version>${spring-cloud-kubernetes.version}</version>
      </dependency>

      <dependency>
          <groupId>org.springframework.cloud</groupId>
          <artifactId>spring-cloud-kubernetes-discovery</artifactId>
          <version>${spring-cloud-kubernetes.version}</version>
      </dependency>
```

For example when I've added any of the followings:

although they look ""logical"" and "" innocent"", they resulted the other *DiscoveryClient* instantiated, which requires a *DiscoveryServer*.

---

```xml
  <dependencies>
    ...
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.boot</groupId>
      <artifactId>spring-boot-starter-webflux</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-gateway</artifactId>
    </dependency>
    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-loadbalancer</artifactId>
    </dependency>

    <dependency>
      <groupId>org.springframework.cloud</groupId>
      <artifactId>spring-cloud-starter-kubernetes-client</artifactId>
    </dependency>
    ...
  </dependencies>
```

Here is the *relevant* part of my solution. Dependencies:

---

```yaml
spring:
  cloud:
    gateway:
      discovery:
        locator:
          enabled: true
    discovery:
      blocking:
        enabled: false
      reactive:
        enabled: true
      client:
        health-indicator:
          enabled: false
    kubernetes:
      discovery:
        enabled: true
        service-labels:
          discovery: enabled
      enabled: true
    service-registry:
      auto-registration:
        enabled: false
```

and the settings:

---

```java
.uri(URI.create(""lb://"" + serviceId))
```

- I can use in my route definitions service ID's, for example:"
79604020,How (if possible) to create a k8s object in a helm chart only if not exists,"Problem I'm trying to solve:

I'm using 1password as a secret vault and can create secrets that track those vault items just fine. I create an object OnePasswordItem using flux, which creates a secret in the namespace.

However, the process of setting up secrets to be stored in the cluster is clunky. Create the object in the flux repo, update the pipeline in the pipeline repo after. Copy the object multiple times if it's needed in multiple namespaces.

What I would like to do is include something in my chart templates:

```
{{- range .Values.onepass.items }}
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: {{ .name }}
  annotations:
    operator.1password.io/auto-restart: {{ .autorestart | default true | quote }}
spec:
  itemPath: {{ .path}}
---
{{- end }}
```

Then I can simply add to my extra values file:

```
onepass:
  items:
    - name: name
      path: ""path""
```

This works great for a single service in the namespace. However, if I want two services to use the same secret item, I get a helm error that the OnePasswordItem exists already.

Is there a flag or something that I can use in the chart that will only install that if it doesn't exist so that it can skip it and not just fail?","kubernetes, kubernetes-helm",79604036.0,"Found it. Apparently there is a lookup function. This works perfectly in my templates:

```
{{- range .Values.onepass.items }}
{{- if not (lookup ""onepassword.com/v1"" ""OnePasswordItem"" .Release.Namespace .name ) -}}
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: {{ .name }}
  annotations:
    operator.1password.io/auto-restart: {{ .autorestart | default true | quote }}
spec:
  itemPath: {{ .path}}
---
{{- end }}
{{- end }}
```",2025-05-02T19:47:51,2025-05-02T19:34:43,"```yaml
{{- range .Values.onepass.items }}
{{- if not (lookup ""onepassword.com/v1"" ""OnePasswordItem"" .Release.Namespace .name ) -}}
apiVersion: onepassword.com/v1
kind: OnePasswordItem
metadata:
  name: {{ .name }}
  annotations:
    operator.1password.io/auto-restart: {{ .autorestart | default true | quote }}
spec:
  itemPath: {{ .path}}
---
{{- end }}
{{- end }}
```

Found it. Apparently there is a lookup function. This works perfectly in my templates:"
79599829,Exposing webconsole when using Artemis operator with size &gt; 1,"I am using Artemis operator with `deploymentPlan.size=2`.

I would like to expose the Artemis management webconsole for each broker instance; I was thinking of doing it through (a kubernetes service + an ingress rule) for each instance.

The problem is that as far as I know the structure of the webconsole app requires the ingress rule to be like:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
...
  annotations:
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$1

...
          - path: /(.*)
            pathType: ImplementationSpecific
            backend:
              service:
                name: broker-webconsole-svc
                port:
                  number: 8080
```

That is, it does not tolerate an url rewriting with some prefix:

```
/instance0/(.*) --> svc0 #KO
/instance1/(.*) --> svc1 #KO
```

Am I missing something?","kubernetes, activemq-artemis, artemiscloud, arkmq",79601533.0,"The ArkMQ Operator creates an ingress for each console when the field `spec.console.expose` is `true`, i.e.

```
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  console:
    expose: true
  ingressDomain: my-domain.com
```",2025-05-01T07:28:47,2025-04-30T08:06:02,"```yaml
apiVersion: broker.amq.io/v1beta1
kind: ActiveMQArtemis
metadata:
  name: artemis-broker
spec:
  console:
    expose: true
  ingressDomain: my-domain.com
```

The ArkMQ Operator creates an ingress for each console when the field `spec.console.expose` is `true`, i.e."
79597527,Vault Agent Injector: How to render secrets to a subpath without overwriting existing files in the mount path?,"I am using HashiCorp Vault's Agent Injector to inject secrets into my Kubernetes pods using the vault.hashicorp.com/secret-volume-path annotation. I am facing an issue where the rendered secrets are being output directly to the specified path, such as /app, and this causes any existing files in the /app directory to be overwritten.

Here is the part of my configuration where I define the secret path:

```
annotations:
  vault.hashicorp.com/secret-volume-path: ""/app""
```

However, I want to render the secrets into a subdirectory under /app, such as /app/conf, while keeping the existing files in /app intact. I have checked the official documentation, but I cannot find any reference to using subPath in this context.

My goal is to preserve the contents of the /app directory and store the rendered secrets in /app/conf (or another subpath), without overriding any existing files in /app.

Has anyone encountered this issue or found a solution to render Vault secrets into a subdirectory without overwriting the contents of the original directory? Is there any way to achieve this with Vault Agent Injector in Kubernetes?","kubernetes, hashicorp-vault",79601030.0,"You should be mounting the Vault Volume Directly to the Subpath where the secrets should reside (/app/conf), rather than just the parent directory (/app).

Instead of:

```
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app"" # This mounts the VOLUME at /app
```

You set it to the desired subpath:

```
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app/conf"" # This mounts the VOLUME at /app/conf
```",2025-04-30T20:09:29,2025-04-29T02:34:25,"```yaml
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app"" # This mounts the VOLUME at /app
```

You should be mounting the Vault Volume Directly to the Subpath where the secrets should reside (/app/conf), rather than just the parent directory (/app).

Instead of:

```yaml
annotations:
   vault.hashicorp.com/secret-volume-path: ""/app/conf"" # This mounts the VOLUME at /app/conf
```

You set it to the desired subpath:"
79589597,WebSocket (WSS) to EMQX via NGINX Ingress Fails,"I'm running into a frustrating issue trying to establish a WebSocket connection (wss://ui-dev.url.com/mqtt) to an EMQX MQTT broker behind an NGINX Ingress Controller in a Kubernetes dev environment.

🔍 Problem Summary:
Trying to connect via WebSocket (wss://) from a Vue.js SPA to EMQX (/mqtt).

🧪 Setup:
NGINX Ingress with TLS termination (via tls.secretName)

Cert is self-signed (I’m okay with browser showing “not secure”)

EMQX is running as a service in the same cluster.

Domain (ui-dev.url.com) is set up in /etc/hosts for local use — DNS is not mine.

No cert-manager or Let’s Encrypt involved (don't want to manage DNS records for dev domains).

✅ What Works:
EMQX is up and running internally.

If I skip TLS and use plain ws://, things work — but obviously that’s not ideal.

❌ What Fails:
Any wss:// request hangs forever, then fails silently with status 0 after 6-7 requests then 101 succeed but takes around 60 seconds.

No relevant errors in NGINX logs.

Browser shows no handshake or TLS failure — just stalled.

🧠 What I’ve Tried:
Verified EMQX can serve WebSocket connections.

Played with Ingress annotations like:

nginx.ingress.kubernetes.io/backend-protocol: HTTPS, HTTP (HTTPS works but 60 second 6-7 attempt.)

nginx.ingress.kubernetes.io/proxy-read-timeout: ""3600""

Switched between self-signed and mkcert-generated certs — same result.

Confirmed secret is mounted and tls: block references correct domain.

Has anyone dealt with WebSocket over TLS getting stuck like this in an NGINX Ingress on Kubernetes?

Any ideas where to dig deeper — is it TLS handshake silently failing, some config I missed on the EMQX side, or Ingress not proxying WebSocket properly?

Appreciate any insight — thank you! 🙏

[![enter image description here](https://i.sstatic.net/H3no18AO.png)](https://i.sstatic.net/H3no18AO.png)","kubernetes, nginx, ssl, websocket, mqtt",79592612.0,"As per [EMQX documentation](https://www.emqx.com/en/blog/connect-to-mqtt-broker-with-websocket#:%7E:text=If%20you%27re%20using%20a%20self%2Dsigned%20certificate%20for%20the%20broker%2C%20you%20must%20manually%20add%20it%20to%20the%20browser%27s%20trust%20store), you must manually [add the self-signed certificate](https://documentation.avaya.com/bundle/AdministeringApplicationEnablementServicesForAvayaContactCenterExtendedCapacity_r102/page/Importing_a_trusted_certificate_into_the_browser_trust_store.html) to the browser’s trust store. This is likely the reason why you are getting status code 0. Unlike HTTPS, where you can manually accept the warning, WebSocket connection (wss://) fails silently if the certificate is not trusted and stored in the browser’s trust store.

Also you need to make sure that your backend-protocol is set to “HTTP” only, as EMQX websocket uses HTTP. It is also recommended to turn off the proxy-buffering to prevent NGINX from buffering WebSocket traffic, which can cause some delays.

```
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
```",2025-04-25T13:19:40,2025-04-23T22:38:27,"As per [EMQX documentation](https://www.emqx.com/en/blog/connect-to-mqtt-broker-with-websocket#:%7E:text=If%20you%27re%20using%20a%20self%2Dsigned%20certificate%20for%20the%20broker%2C%20you%20must%20manually%20add%20it%20to%20the%20browser%27s%20trust%20store), you must manually [add the self-signed certificate](https://documentation.avaya.com/bundle/AdministeringApplicationEnablementServicesForAvayaContactCenterExtendedCapacity_r102/page/Importing_a_trusted_certificate_into_the_browser_trust_store.html) to the browser’s trust store. This is likely the reason why you are getting status code 0. Unlike HTTPS, where you can manually accept the warning, WebSocket connection (wss://) fails silently if the certificate is not trusted and stored in the browser’s trust store.

Also you need to make sure that your backend-protocol is set to “HTTP” only, as EMQX websocket uses HTTP. It is also recommended to turn off the proxy-buffering to prevent NGINX from buffering WebSocket traffic, which can cause some delays.

```yaml
  annotations:
    nginx.ingress.kubernetes.io/backend-protocol: ""HTTP""
    nginx.ingress.kubernetes.io/proxy-buffering: ""off""
```"
79587626,Use Kyverno to add environment variables if configmap is present,"I am trying to use the Kyverno sample policy for injecting environment variables into a container using a configmap in the pod's namespace:

[https://kyverno.io/policies/other/add-env-vars-from-cm/add-env-vars-from-cm/](https://kyverno.io/policies/other/add-env-vars-from-cm/add-env-vars-from-cm/)

If I create a pod in a namespace without this configmap the pod will fail to create. I cannot understand how to make the Kyverno rule apply only when the configmap is present in the pod's namespace. Here's my latest attempt which fails with an error:

```
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-env-vars-from-cm
  annotations:
    policies.kyverno.io/title: Add Environment Variables from ConfigMap
    policies.kyverno.io/minversion: 1.6.0
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/category: Other
    policies.kyverno.io/description: ""Instead of defining a common set of environment variables multiple times either in manifests or separate policies, Pods can reference entire collections stored in a ConfigMap. This policy mutates all initContainers (if present) and containers in a Pod with environment variables defined in a ConfigMap named `nsenvvars` that must exist in the destination Namespace.""
spec:
  rules:
    - name: add-env-vars-from-cm
      match:
        any:
          - resources:
              kinds:
                - Pod
      context:
        - name: envVarsCmCount
          apiCall:
            urlPath: ""/api/v1/namespaces/{{ request.namespace }}/configmaps/nsenvvars""
            jmesPath: ""data | length(@)""
            default: 0
      preconditions:
        all:
          - key: envVarsCmCount
            operator: GreaterThan
            value: 0
      mutate:
        patchStrategicMerge:
          spec:
            initContainers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
            containers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
```","kubernetes, kyverno",79592666.0,"Fixed

The key was not to add a precondition to the Kyverno policy but instead make the configmap ref in the mutation optional:

```
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-env-vars-from-cm
  annotations:
    policies.kyverno.io/title: Add Environment Variables from ConfigMap
    policies.kyverno.io/minversion: 1.6.0
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/category: Other
    policies.kyverno.io/description: ""Instead of defining a common set of environment variables multiple times either in manifests or separate policies, Pods can reference entire collections stored in a ConfigMap. This policy mutates all initContainers (if present) and containers in a Pod with environment variables defined in a ConfigMap named `nsenvvars` that must exist in the destination Namespace.""
spec:
  rules:
    - name: add-env-vars-from-cm
      match:
        any:
          - resources:
              kinds:
                - Pod
      mutate:
        patchStrategicMerge:
          spec:
            initContainers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
            containers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
```",2025-04-25T13:55:35,2025-04-23T00:04:33,"```yaml
Fixed

The key was not to add a precondition to the Kyverno policy but instead make the configmap ref in the mutation optional:

apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: add-env-vars-from-cm
  annotations:
    policies.kyverno.io/title: Add Environment Variables from ConfigMap
    policies.kyverno.io/minversion: 1.6.0
    policies.kyverno.io/subject: Pod
    policies.kyverno.io/category: Other
    policies.kyverno.io/description: ""Instead of defining a common set of environment variables multiple times either in manifests or separate policies, Pods can reference entire collections stored in a ConfigMap. This policy mutates all initContainers (if present) and containers in a Pod with environment variables defined in a ConfigMap named `nsenvvars` that must exist in the destination Namespace.""
spec:
  rules:
    - name: add-env-vars-from-cm
      match:
        any:
          - resources:
              kinds:
                - Pod
      mutate:
        patchStrategicMerge:
          spec:
            initContainers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
            containers:
              - (name): ""*""
                envFrom:
                  - configMapRef:
                      name: nsenvvars
                      optional: true
```

Fixed

The key was not to add a precondition to the Kyverno policy but instead make the configmap ref in the mutation optional:"
79581979,Persistence volume node affinity,"I am facing some difficulty in implementing node affinity in persistence volume.

While create persistence volume, I am getting below error

```
PersistentVolume in version ""v1"" cannot be handled as a PersistentVolume: strict decoding error: unknown field ""spec.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution""
```

Below is my storage class, persistence volume and persistence volume claim

```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: blue-stc-cka
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

---

apiVersion: v1
kind: PersistentVolume
metadata:
  name: blue-pv-cka
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: blue-stc-cka
  local:
   path: /opt/blue-data-cka
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: node-role.kubernetes.io/control-plane
              operator: Exists

---

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: blue-pvc-cka
spec:
  accessModes:
    - ReadWriteOnce
  volumeName: blue-pv-cka
  resources:
    requests:
      storage: 50Mi
  storageClassName: blue-stc-cka
```","kubernetes, persistent-volume-claims",79582307.0,"The [API documentation for PersistentVolumes](https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/) shows that the syntax for `nodeAffinity:` is different from Pods'.  There is only one ""kind"" of affinity, `required:`, as opposed to the two-phase setup that Pods have.  You should be able to change the affinity definition to

```
nodeAffinity:
  required: # <-- change this
    nodeSelectorTerms:
      - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
```

For almost all practical uses, you should just delete the manually-created StorageClass and PersistentVolume, and delete the `storageClassName:` from the PersistentVolumeClaim (unless your cluster administrator has told you something different).  The cluster will automatically create the PersistentVolume for you using the default StorageClass.  Depending on how the application uses the storage, you often will want to use a StatefulSet, and move the PVC definitions into that object as well.",2025-04-19T11:06:56,2025-04-19T02:52:46,"```yaml
nodeAffinity:
  required: # <-- change this
    nodeSelectorTerms:
      - matchExpressions:
          - key: node-role.kubernetes.io/control-plane
            operator: Exists
```

The [API documentation for PersistentVolumes](https://kubernetes.io/docs/reference/kubernetes-api/config-and-storage-resources/persistent-volume-v1/) shows that the syntax for `nodeAffinity:` is different from Pods'.  There is only one ""kind"" of affinity, `required:`, as opposed to the two-phase setup that Pods have.  You should be able to change the affinity definition to

For almost all practical uses, you should just delete the manually-created StorageClass and PersistentVolume, and delete the `storageClassName:` from the PersistentVolumeClaim (unless your cluster administrator has told you something different).  The cluster will automatically create the PersistentVolume for you using the default StorageClass.  Depending on how the application uses the storage, you often will want to use a StatefulSet, and move the PVC definitions into that object as well."
79580793,Kubernetes startupProbe fails even though app becomes healthy within allowed threshold,"I'm running into an issue with my (GKE) Kubernetes deployment's startupProbe. My container exposes a /v1/health endpoint that returns JSON with a ""status"" field. The probe is configured as follows:

```
startupProbe:
  exec:
    command:
      - sh
      - -c
      - >
          curl --silent --fail http://localhost:8080/v1/health |
          grep --quiet -e '\""status\"":\""healthy\""'
  initialDelaySeconds: 20
  periodSeconds: 10
  timeoutSeconds: 10
  failureThreshold: 18
```

This should allow up to 3 minutes for the app to become healthy. However, the probe keeps failing and the pod restarts, even though:

The health endpoint returns ""status"":""undetermined"" for a while, then switches to ""status"":""healthy"" (usually within the 3-minute window).

If I manually exec into the pod and run the probe command, it succeeds once the app is up.

```
 k exec -ti <> -- sh -c 'curl -s http://localhost:8080/v1/health'
{""build_info"":{""app_name"":""<>"",""app_version"":""<>"",""build_timestamp"":""2025-04-16T18:08:36Z"",""built_by"":""<>"",""commit_id"":""<>""},""status"":""healthy"",""uptime"":""13m51.712240388s""}
```

Both curl and grep are present in the image.

This is the ouput when I describe the pod.

```
Warning  Unhealthy                          10m (x4 over 11m)  kubelet                  Startup probe failed:
```","kubernetes, google-kubernetes-engine",79592621.0,"Based on what you've shared, I have 2 theories about what might be wrong.

1. *(Most likely)* Since you didn't provide a full command output from inside container (i.e. `curl` vs `curl ... | grep ...`) I can assume that the `grep` version inside conatiner is working different than expected. This is usually happens with more complex commands (e.g. when using -E), but it worth checking a full piped pair.
2. *(Less likely)* Weird idea, but maybe YAML itself is not resolved correctly? Try to make it as simple as possible to 2x check:

```
startupProbe:
  exec:
    command: [""sh"", ""-c"", ""curl -s -f http://localhost:8080/v1/health | grep -q -e '\""status\"":\""healthy\""'""]
```

If this doesn't work, try to make it verbose and check the Pod logs:

```
startupProbe:
  exec:
    command:
      - echo ""PROBE DEBUG""
      - curl -v http://localhost:8080/v1/health
      - sh
      - -c
      - >
          curl http://localhost:8080/v1/health |
          grep -e '\""status\"":\""healthy\""'
      - echo ""$?""
```",2025-04-25T13:25:20,2025-04-18T09:45:19,"```text
Based on what you've shared, I have 2 theories about what might be wrong.

1. *(Most likely)* Since you didn't provide a full command output from inside container (i.e. `curl` vs `curl ... | grep ...`) I can assume that the `grep` version inside conatiner is working different than expected. This is usually happens with more complex commands (e.g. when using -E), but it worth checking a full piped pair.
2. *(Less likely)* Weird idea, but maybe YAML itself is not resolved correctly? Try to make it as simple as possible to 2x check:
```

```yaml
startupProbe:
  exec:
    command: [""sh"", ""-c"", ""curl -s -f http://localhost:8080/v1/health | grep -q -e '\""status\"":\""healthy\""'""]
```

```text
If this doesn't work, try to make it verbose and check the Pod logs:
```

```yaml
startupProbe:
  exec:
    command:
      - echo ""PROBE DEBUG""
      - curl -v http://localhost:8080/v1/health
      - sh
      - -c
      - >
          curl http://localhost:8080/v1/health |
          grep -e '\""status\"":\""healthy\""'
      - echo ""$?""
```"
79578349,APISIX ingress controller on Kind cluster not routing requests correctly,"I am trying to setup APISIX gateway ingress controller enabled application (spring-boot) on a local KIND kubernetes cluster.
Here are the steps that I followed,

I was able to succesfully install and configure the APISIX gateway (PORT: 8090) in my local kind kubernetes cluster.
I can confirm that an ingress manifest is translated to an APISIX route correctly.
The target application is configured with a service ""client-app"" and I can confirm that it does have the corresponding endpoints configured correctly.

## Setup

Ingress YAML:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: httpserver-ingress
  namespace: oidcapp
spec:
  # we use APISIX Ingress and it watches Ingress resources with ""apisix"" ingressClassName
  ingressClassName: apisix
  rules:
  - host: authclient.com
    http:
      paths:
      - backend:
          service:
            name: client-app
            port:
              number: 80
        path: /oidcapp
        pathType: Prefix
```

Target Application service:

```
$ kubectl describe svc -n oidcapp client-app
Name:                     client-app
Namespace:                oidcapp
Labels:                   app=client-app
Annotations:              <none>
Selector:                 app=client-app
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.136.11
IPs:                      10.96.136.11
Port:                     http  80/TCP
TargetPort:               8080/TCP
Endpoints:                10.244.1.23:8080
Session Affinity:         None
Internal Traffic Policy:  Cluster
```

APISIX route mapping:

```
{
    ""createdIndex"": 11332,
    ""key"": ""/apisix/routes/33d660f9"",
    ""modifiedIndex"": 11532,
    ""value"": {
        ""priority"": 0,
        ""status"": 1,
        ""uris"": [
            ""/oidcapp"",
            ""/oidcapp/*""
        ],
        ""name"": ""ing_oidcapp_httpserver-ingress_4cafc3f3"",
        ""id"": ""33d660f9"",
        ""upstream_id"": ""fdcb23fc"",
        ""host"": ""authclient.com"",
        ""create_time"": 1744820603,
        ""update_time"": 1744851162,
        ""desc"": ""Created by apisix-ingress-controller, DO NOT modify it manually"",
        ""labels"": {
            ""managed-by"": ""apisix-ingress-controller""
        }
    }
}
```

I then installed a Loadbalancer in Kind cluster, following the instructions below: [Kind Docs | LoadBalancer](https://kind.sigs.k8s.io/docs/user/loadbalancer/)

Based on these instructions, added the following Service manifest:

```
kind: Service
apiVersion: v1
metadata:
  name: apisix-gateway-service
  namespace: apisix
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: apisix
  ports:
  - port: 5678
    targetPort: 8090
```

In my etc/hosts, I have configured the following host-mapping:

```
172.18.0.2      authclient.com
```

Here are the relevant kubectl command output:

```
$ kubectl get svc -n apisix

NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
apisix-admin             ClusterIP      10.96.220.99    <none>        9180/TCP            7d18h
apisix-gateway-service   LoadBalancer   10.96.183.128   172.18.0.2    5678:31448/TCP      80m
etcd-headless            ClusterIP      None            <none>        2379/TCP,2380/TCP   7d18h
```

## Problem

When I execute the following curl command, I get a *connection reset* error:

```
$ curl -v http://authclient.com:5678/oidcapp
*   Trying 172.18.0.2:5678...
* TCP_NODELAY set
* Connected to authclient.com (172.18.0.2) port 5678 (#0)
> GET /oidcapp HTTP/1.1
> Host: authclient.com:5678
> User-Agent: curl/7.68.0
> Accept: */*
>
* Recv failure: Connection reset by peer
* Closing connection 0
curl: (56) Recv failure: Connection reset by peer
```

I expect the CURL request to be forwarded to an apisix-gateway pod and then routed to one of the service endpoints determined by the gateway's route mapping.

```
cURL ---> Kind LoadBalancer ---> APISIX ---> client-app
```

Unfortunately, the logs of load balancer and apisix-gateway-ingress-controller does not provide any further details.","kubernetes, kind, apache-apisix",79579244.0,"## TL;DR

The problem is that you are binding the Service to the wrong port: `8090`.

## Explaination

The reason why your CURL request fails, is that:

- in APISIX container, port `8090` is not mapped to anything;
- even if it were, there's nothing that is listening on that port.

In fact, by default, APISIX listens for HTTP traffic on port `9080`. Therefore, the YAML for your Service should look like this:

```
kind: Service
apiVersion: v1
metadata:
  name: apisix-gateway-service
  namespace: apisix
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: apisix
  ports:
  - port: 5678
    targetPort: 9080 # <-- APISIX proxy port
```

Some useful references:

- [APISIX Docs | FAQ: How do I configure Apache APISIX to listen on multiple ports when handling HTTP or HTTPS requests?](https://apisix.apache.org/docs/apisix/FAQ/#how-do-i-configure-apache-apisix-to-listen-on-multiple-ports-when-handling-http-or-https-requests)
- [GitHub apache/apisix | config.yaml.example](https://github.com/apache/apisix/blob/master/conf/config.yaml.example)",2025-04-17T12:26:59,2025-04-17T02:10:07,"```yaml
kind: Service
apiVersion: v1
metadata:
  name: apisix-gateway-service
  namespace: apisix
spec:
  type: LoadBalancer
  selector:
    app.kubernetes.io/name: apisix
  ports:
  - port: 5678
    targetPort: 9080 # <-- APISIX proxy port
```

The surrounding descriptive text:

## TL;DR

The problem is that you are binding the Service to the wrong port: `8090`.

## Explaination

The reason why your CURL request fails, is that:

- in APISIX container, port `8090` is not mapped to anything;
- even if it were, there's nothing that is listening on that port.

In fact, by default, APISIX listens for HTTP traffic on port `9080`. Therefore, the YAML for your Service should look like this:

Some useful references:

- [APISIX Docs | FAQ: How do I configure Apache APISIX to listen on multiple ports when handling HTTP or HTTPS requests?](https://apisix.apache.org/docs/apisix/FAQ/#how-do-i-configure-apache-apisix-to-listen-on-multiple-ports-when-handling-http-or-https-requests)
- [GitHub apache/apisix | config.yaml.example](https://github.com/apache/apisix/blob/master/conf/config.yaml.example)"
79574724,Unable to send Kafka Message after Login,"I have a single node kafka cluster set up with helm and kubernetes on [rancher desktop](https://rancherdesktop.io/). Everytime my java/spring-boot application starts, it cann log into kafka, start the producer but then fails to  send a message.

The kafka cluster is deployeyd with the help of the [bitnami helm script](https://github.com/bitnami/charts/tree/main/bitnami/kafka).

This is my value.yaml

```
controller:
  replicaCount: 1
  persistence:
    size: 1Gi
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  resources:
    limits:
      memory: 2Gi
  automountServiceAccountToken: true

externalAccess:
  enabled: true
  service:
    type: LoadBalancer
    ports:
      external: 9094
  autoDiscovery:
    enabled: true

broker:
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false

sasl:
  client:
    users: [ ""myuser"" ]
    passwords: [ ""mypassword"" ]

rbac:
  create: true

# siehe https://github.com/bitnami/charts/issues/19522
extraConfig: |
  deleteTopicEnable=true
  auto.create.topics.enable=false
  offsets.topic.replication.factor=1
  transaction.state.log.replication.factor=1

provisioning:
  enabled: true
  topics:
    - name: heartBeat
```

Wen I start my java application I do get this log output:

```
[2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.clients.producer.ProducerConfig.logAll - traceid= - ProducerConfig values:
    acks = -1
    auto.include.jmx.reporter = true
    batch.size = 16384
    bootstrap.servers = [localhost:9094]
    ...

org.springframework.kafka.support.serializer.JsonSerializer
    [2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector.init - traceid= - initializing Kafka metrics collector
    [2025-04-15 10:37:05,198] [main] severity=INFO - org.apache.kafka.clients.producer.KafkaProducer.configureTransactionState - traceid= - [Producer clientId=producer-1] Instantiated an idempotent producer.
    [2025-04-15 10:37:05,207] [main] severity=INFO - org.apache.kafka.common.security.authenticator.AbstractLogin.login - traceid= - Successfully logged in.
    [2025-04-15 10:37:05,213] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka version: 3.8.1
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka commitId: 70d6ff42debf7e17
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka startTimeMs: 1744706225213
    [2025-04-15 10:37:05,451] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.Metadata.update - traceid= - [Producer clientId=producer-1] Cluster ID: jg0sCtb1jEoEwK8lBIlDGj
    [2025-04-15 10:37:26,496] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.NetworkClient.handleDisconnections - traceid= - [Producer clientId=producer-1] Node 0 disconnected.
    [2025-04-15 10:37:26,497] [kafka-producer-network-thread | producer-1] severity=WARN - org.apache.kafka.clients.NetworkClient.processDisconnection - traceid= - [Producer clientId=producer-1] Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

After login it somehow tries to connect

```
Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

This does of course not work, since kafka is only available via `localhost:9094`. How can it setup-up my Client or Kafka to use the correct address to send the message?","kubernetes, apache-kafka, rancher-desktop",79575445.0,"There are two problems.

First there are breaking changes in the externalAccess section, and second as pointed out by @poisened_monkey the advertised listeners have to be configured.

The whole externAccess section has to be replace by this

```
externalAccess:
  enabled: true
  autoDiscovery:
    enabled: true
  broker:
    service:
      type: LoadBalancer
      ports:
        external: 9094
  controller:
    service:
      type: LoadBalancer
    containerPorts:
      external: 9094

defaultInitContainers:
  autoDiscovery:
    enabled: true

serviceAccount:
  create: true

rbac:
  create: true

listeners:
  advertisedListeners: CLIENT://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9092,INTERNAL://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9094,EXTERNAL://localhost:9094
```

The advertisedListeners list is sent to the client after successfull login. For the external client the correct address is localhost:9094.",2025-04-15T14:39:45,2025-04-15T08:49:38,"```yaml
externalAccess:
  enabled: true
  autoDiscovery:
    enabled: true
  broker:
    service:
      type: LoadBalancer
      ports:
        external: 9094
  controller:
    service:
      type: LoadBalancer
    containerPorts:
      external: 9094

defaultInitContainers:
  autoDiscovery:
    enabled: true

serviceAccount:
  create: true

rbac:
  create: true

listeners:
  advertisedListeners: CLIENT://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9092,INTERNAL://kafka01-controller-0.kafka01-controller-headless.default.svc.cluster.local:9094,EXTERNAL://localhost:9094
```

There are two problems.

First there are breaking changes in the externalAccess section, and second as pointed out by @poisened_monkey the advertised listeners have to be configured.

The whole externAccess section has to be replace by this

The advertisedListeners list is sent to the client after successfull login. For the external client the correct address is localhost:9094."
79574724,Unable to send Kafka Message after Login,"I have a single node kafka cluster set up with helm and kubernetes on [rancher desktop](https://rancherdesktop.io/). Everytime my java/spring-boot application starts, it cann log into kafka, start the producer but then fails to  send a message.

The kafka cluster is deployeyd with the help of the [bitnami helm script](https://github.com/bitnami/charts/tree/main/bitnami/kafka).

This is my value.yaml

```
controller:
  replicaCount: 1
  persistence:
    size: 1Gi
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false
  resources:
    limits:
      memory: 2Gi
  automountServiceAccountToken: true

externalAccess:
  enabled: true
  service:
    type: LoadBalancer
    ports:
      external: 9094
  autoDiscovery:
    enabled: true

broker:
  podSecurityContext:
    enabled: false
  containerSecurityContext:
    enabled: false

sasl:
  client:
    users: [ ""myuser"" ]
    passwords: [ ""mypassword"" ]

rbac:
  create: true

# siehe https://github.com/bitnami/charts/issues/19522
extraConfig: |
  deleteTopicEnable=true
  auto.create.topics.enable=false
  offsets.topic.replication.factor=1
  transaction.state.log.replication.factor=1

provisioning:
  enabled: true
  topics:
    - name: heartBeat
```

Wen I start my java application I do get this log output:

```
[2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.clients.producer.ProducerConfig.logAll - traceid= - ProducerConfig values:
    acks = -1
    auto.include.jmx.reporter = true
    batch.size = 16384
    bootstrap.servers = [localhost:9094]
    ...

org.springframework.kafka.support.serializer.JsonSerializer
    [2025-04-15 10:37:05,188] [main] severity=INFO - org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector.init - traceid= - initializing Kafka metrics collector
    [2025-04-15 10:37:05,198] [main] severity=INFO - org.apache.kafka.clients.producer.KafkaProducer.configureTransactionState - traceid= - [Producer clientId=producer-1] Instantiated an idempotent producer.
    [2025-04-15 10:37:05,207] [main] severity=INFO - org.apache.kafka.common.security.authenticator.AbstractLogin.login - traceid= - Successfully logged in.
    [2025-04-15 10:37:05,213] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka version: 3.8.1
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka commitId: 70d6ff42debf7e17
    [2025-04-15 10:37:05,214] [main] severity=INFO - org.apache.kafka.common.utils.AppInfoParser.<init> - traceid= - Kafka startTimeMs: 1744706225213
    [2025-04-15 10:37:05,451] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.Metadata.update - traceid= - [Producer clientId=producer-1] Cluster ID: jg0sCtb1jEoEwK8lBIlDGj
    [2025-04-15 10:37:26,496] [kafka-producer-network-thread | producer-1] severity=INFO - org.apache.kafka.clients.NetworkClient.handleDisconnections - traceid= - [Producer clientId=producer-1] Node 0 disconnected.
    [2025-04-15 10:37:26,497] [kafka-producer-network-thread | producer-1] severity=WARN - org.apache.kafka.clients.NetworkClient.processDisconnection - traceid= - [Producer clientId=producer-1] Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

After login it somehow tries to connect

```
Connection to node 0 (192.168.127.2/192.168.127.2:9094) could not be established. Node may not be available.
```

This does of course not work, since kafka is only available via `localhost:9094`. How can it setup-up my Client or Kafka to use the correct address to send the message?","kubernetes, apache-kafka, rancher-desktop",79574795.0,"Add to your `values.yaml`

```
controller:
  extraEnvVars:
    - name: KAFKA_CFG_ADVERTISED_LISTENERS
      value: PLAINTEXT://localhost:9094
    - name: KAFKA_CFG_LISTENERS
      value: PLAINTEXT://:9094
```",2025-04-15T09:23:34,2025-04-15T08:49:38,"```yaml
controller:
  extraEnvVars:
    - name: KAFKA_CFG_ADVERTISED_LISTENERS
      value: PLAINTEXT://localhost:9094
    - name: KAFKA_CFG_LISTENERS
      value: PLAINTEXT://:9094
```

Add to your `values.yaml`"
79574416,Unable to delete pod from EC2 instance,"I have an EKS cluster running, and I use an EC2 machine to submit jobs that get scheduled on the EKS cluster. The EKS cluster and EC2 machine are attached with an IAM role, which has access to multiple AWS accounts for business requirements. The IAM role is configured following this [AWS documentation](https://aws.amazon.com/blogs/containers/enabling-cross-account-access-to-amazon-eks-cluster-resources/).

I'm able to run all the `kubectl` commands from the EC2 except `kubectl delete pod <pod_name>` which results in the following error:

What could be missing here?

> Error from server (Forbidden): pods ""test-cronjob-29077280-s4gbn"" is forbidden: node ""EKSGetTokenAuth"" can only delete pods with spec.nodeName set to itself","amazon-web-services, kubernetes, amazon-ec2, amazon-iam, amazon-eks",79574426.0,"You need to ensure your IAM role (used by EC2) is correctly mapped to a Kubernetes user or group that has the correct RBAC permissions.

1.Check your IAM role ARN:

`aws sts get-caller-identity`

You’ll get something like:

`arn:aws:sts::123456789012:assumed-role/MyEKSRole/i-xxxxxxxxxxxx`

2.Update `aws-auth` `ConfigMap`:

Map the IAM role to a Kubernetes user or group:

```
mapRoles: |
  - rolearn: arn:aws:iam::123456789012:role/MyEKSRole
    username: ec2-user
    groups:
      - system:masters
```

You can edit the `ConfigMap` using:

`kubectl edit configmap aws-auth -n kube-system`",2025-04-15T05:27:01,2025-04-15T05:13:46,"```bash
aws sts get-caller-identity
```

You’ll get something like:

```bash
arn:aws:sts::123456789012:assumed-role/MyEKSRole/i-xxxxxxxxxxxx
```

```yaml
mapRoles: |
  - rolearn: arn:aws:iam::123456789012:role/MyEKSRole
    username: ec2-user
    groups:
      - system:masters
```

```bash
kubectl edit configmap aws-auth -n kube-system
```"
79563268,How to inject secrets into kube-prometheus-stack values.yaml (SMTP),"I have a deployment of kube-prometheus-stack (prometheus-community) and I am trying to inject secrets into the grafana values.yml specifically for the smtp configuration password.

I have created a secret resource in the same namespace as the prometheus/grafana deployment called ""grafana-secrets"", which contains the SENDGRID_API_KEY. I need to ""inject"" this into my values.yml file. Here is what I have;

```
grafana:
  grafana.ini:
    smtp:
      enabled: true
      host: smtp.sendgrid.net:587
      user: apikey
      password: ${SENDGRID_API_KEY}
      from_address: ""my-from-address""
      from_name: Grafana
      skip_verify: false
```","kubernetes, prometheus, grafana, sendgrid",79564400.0,"Just add `envFromSecret`

```
grafana:
  envFromSecret: grafana-secrets
  grafana.ini:
    smtp:
      enabled: true
      host: smtp.sendgrid.net:587
      user: apikey
      password: ${SENDGRID_API_KEY}
      from_address: ""my-from-address""
      from_name: Grafana
      skip_verify: false
```",2025-04-09T12:58:05,2025-04-09T00:55:50,"```text
Just add `envFromSecret`
```

```yaml
grafana:
  envFromSecret: grafana-secrets
  grafana.ini:
    smtp:
      enabled: true
      host: smtp.sendgrid.net:587
      user: apikey
      password: ${SENDGRID_API_KEY}
      from_address: ""my-from-address""
      from_name: Grafana
      skip_verify: false
```"
79563007,ActiveMQ Artemis does not display console when runs in K8S,"I deployed `apache/activemq-artemis:2.40.0-alpine` in k8s cluster. First run goes well, but when I open console I'm unable to view literally everything but white list:
[![enter image description here](https://i.sstatic.net/LR6hVtVd.png)](https://i.sstatic.net/LR6hVtVd.png)
[![enter image description here](https://i.sstatic.net/8HdEn9TK.png)](https://i.sstatic.net/8HdEn9TK.png)

Log:

```
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-amqp-protocol]. Adding protocol support for: AMQP                                                                       │
│ 2025-04-08 18:01:11,109 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-hornetq-protocol]. Adding protocol support for: HORNETQ                                                                 │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-mqtt-protocol]. Adding protocol support for: MQTT                                                                       │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-openwire-protocol]. Adding protocol support for: OPENWIRE                                                               │
│ 2025-04-08 18:01:11,110 INFO  [org.apache.activemq.artemis.core.server] AMQ221043: Protocol module found: [artemis-stomp-protocol]. Adding protocol support for: STOMP                                                                     │
│ 2025-04-08 18:01:11,298 INFO  [org.apache.activemq.artemis.core.server] AMQ221034: Waiting indefinitely to obtain primary lock                                                                                                             │
│ 2025-04-08 18:01:11,299 INFO  [org.apache.activemq.artemis.core.server] AMQ221035: Primary Server Obtained primary lock                                                                                                                    │
│ 2025-04-08 18:01:11,510 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address DLQ supporting [ANYCAST]                                                                                                              │
│ 2025-04-08 18:01:11,594 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue DLQ on address DLQ                                                                                                              │
│ 2025-04-08 18:01:11,804 INFO  [org.apache.activemq.artemis.core.server] AMQ221080: Deploying address ExpiryQueue supporting [ANYCAST]                                                                                                      │
│ 2025-04-08 18:01:11,805 INFO  [org.apache.activemq.artemis.core.server] AMQ221003: Deploying ANYCAST queue ExpiryQueue on address ExpiryQueue                                                                                              │
│ 2025-04-08 18:01:12,699 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61616 for protocols [CORE,MQTT,AMQP,STOMP,HORNETQ,OPENWIRE]                                                           │
│ 2025-04-08 18:01:12,700 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5445 for protocols [HORNETQ,STOMP]                                                                                    │
│ 2025-04-08 18:01:12,702 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:5672 for protocols [AMQP]                                                                                             │
│ 2025-04-08 18:01:12,703 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:1883 for protocols [MQTT]                                                                                             │
│ 2025-04-08 18:01:12,704 INFO  [org.apache.activemq.artemis.core.server] AMQ221020: Started EPOLL Acceptor at 0.0.0.0:61613 for protocols [STOMP]                                                                                           │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221007: Server is now active                                                                                                                                    │
│ 2025-04-08 18:01:12,706 INFO  [org.apache.activemq.artemis.core.server] AMQ221001: Apache ActiveMQ Artemis Message Broker version 2.40.0 [0.0.0.0, nodeID=7401e03f-14a3-11f0-ac6e-02d91668a613]                                            │
│ 2025-04-08 18:01:12,713 INFO  [org.apache.activemq.artemis] AMQ241003: Starting embedded web server                                                                                                                                        │
│ 2025-04-08 18:01:14,205 INFO  [io.hawt.HawtioContextListener] Initialising Hawtio services                                                                                                                                                 │
│ 2025-04-08 18:01:14,210 INFO  [io.hawt.jmx.JmxTreeWatcher] Welcome to Hawtio 4.2.0                                                                                                                                                         │
│ 2025-04-08 18:01:14,292 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Authentication throttling is enabled                                                                                                                          │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Starting Hawtio authentication filter, JAAS realm: ""activemq"" authorized role(s): ""amq"" role principal classes: ""org.apache.activemq.artemis.spi.core.securit │
│ 2025-04-08 18:01:14,390 INFO  [io.hawt.web.auth.AuthenticationConfiguration] Looking for OIDC configuration file in: /var/lib/artemis-instance/etc/hawtio-oidc.properties                                                                  │
│ 2025-04-08 18:01:14,505 INFO  [io.hawt.web.auth.ClientRouteRedirectFilter] Hawtio ClientRouteRedirectFilter is using 1800 sec. HttpSession timeout                                                                                         │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241001: HTTP Server started at http://0.0.0.0:8161                                                                                                                          │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241002: Artemis Jolokia REST API available at http://0.0.0.0:8161/console/jolokia                                                                                           │
│ 2025-04-08 18:01:14,611 INFO  [org.apache.activemq.artemis] AMQ241004: Artemis Console available at http://0.0.0.0:8161/console                                                                                                            │
│ 2025-04-08 18:01:28,287 INFO  [io.hawt.web.auth.keycloak.KeycloakServlet] Keycloak integration is disabled                                                                                                                                 │
│ 2025-04-08 18:01:34,108 INFO  [io.hawt.web.auth.LoginServlet] Hawtio login is using 1800 sec. HttpSession timeout                                                                                                                          │
│ 2025-04-08 18:01:34,401 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 18:01:47,631 INFO  [io.hawt.web.servlets.JolokiaConfiguredAgentServlet] Jolokia overridden property: [key=policyLocation, value=file:/var/lib/artemis-instance/./etc/jolokia-access.xml]                                        │
│ 2025-04-08 18:01:47,634 INFO  [io.hawt.web.proxy.ProxyServlet] Proxy servlet is disabled                                                                                                                                                   │
│ 2025-04-08 18:02:47,861 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis                                                                                                                                                     │
│ 2025-04-08 20:16:49,260 INFO  [io.hawt.web.auth.LoginServlet] Logging in user: artemis
```

moreover, in browser console i see the following error:
[![enter image description here](https://i.sstatic.net/3SckIxlD.png)](https://i.sstatic.net/3SckIxlD.png)

My current ingress configuration:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  labels:
    app.kubernetes.io/component: Ingress
  name: amq-artemis
  annotations:
    external-dns.alpha.kubernetes.io/hostname: hidden
    external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only
    cert-manager.io/cluster-issuer: hidden
    cert-manager.io/duration: 2160h
    cert-manager.io/renew-before: 720h
    nginx.ingress.kubernetes.io/keepalive_timeout: ""1200""
    nginx.ingress.kubernetes.io/proxy-body-size: ""250m""
    nginx.ingress.kubernetes.io/proxy-buffers-number: ""4 256k""
    nginx.ingress.kubernetes.io/proxy-buffering: 'on'
    nginx.ingress.kubernetes.io/proxy-buffer-size: ""128k""
    nginx.ingress.kubernetes.io/proxy-read-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-connect-timeout: ""300""
    nginx.ingress.kubernetes.io/proxy-send-timeout: ""300""
    nginx.ingress.kubernetes.io/backend-protocol: HTTP
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/affinity: ""cookie""
    nginx.ingress.kubernetes.io/session-cookie-name: ""amq-artemis""
    nginx.ingress.kubernetes.io/session-cookie-samesite: ""None""
    nginx.ingress.kubernetes.io/session-cookie-secure: ""true""
    nginx.ingress.kubernetes.io/session-cookie-path: ""/; Secure""
    nginx.ingress.kubernetes.io/app-root: /console/artemis
    nginx.ingress.kubernetes.io/cors-allow-methods: ""PUT, GET, POST, OPTIONS""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
    nginx.ingress.kubernetes.io/cors-allow-credentials: ""true""
    nginx.ingress.kubernetes.io/cors-allow-origin: ""*""
    nginx.ingress.kubernetes.io/cors-allow-headers: ""DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type""
    nginx.ingress.kubernetes.io/configuration-snippet: |
      proxy_busy_buffers_size   256k;
      client_body_buffer_size   10m;
      send_timeout              300;
spec:
  ingressClassName: nginx
  tls:
    - hosts:
        - hidden
      secretName: artemis-fqdn-cert
  rules:
    - host: hidden
      http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /jolokia
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /hawtio
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
        - path: /console
          pathType: Prefix
          backend:
            service:
              name: amq-artemis
              port:
                number: 8161
```

I run broker with the following parameters: `--relax-jolokia --name art --http-host 0.0.0.0 `

I have tried with various of ingress rules and annotations, but futile. Any idea what it could be?","java, kubernetes, activemq-artemis",79640383.0,"Example K8s configuration that worked for me:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: activemq
  name: activemq
  labels:
    app.kubernetes.io/name: activemq
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: activemq
  template:
    metadata:
      labels:
        app.kubernetes.io/name: activemq
    spec:
      containers:
      - name: activemq
        image: apache/activemq-artemis:2.41.0-alpine
        imagePullPolicy: Always
        ports:
          - containerPort: 61616
            name: messaging
          - containerPort: 8161
            name: http
        volumeMounts:
          - name: activemq-config
            mountPath:  /var/lib/artemis-instance/etc-override
      volumes:
        - name: activemq-config
          configMap:
            name: activemq-config
            items:
              - key: jolokia-access.xml
                path: jolokia-access.xml
```

```
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: activemq
  name: activemq-config
  labels:
    app.kubernetes.io/name: activemq
data:
  jolokia-access.xml: |
    <cors>
      <ignore-scheme/>
    </cors>
```",2025-05-27T11:27:45,2025-04-08T20:31:47,"```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  namespace: activemq
  name: activemq
  labels:
    app.kubernetes.io/name: activemq
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: activemq
  template:
    metadata:
      labels:
        app.kubernetes.io/name: activemq
    spec:
      containers:
      - name: activemq
        image: apache/activemq-artemis:2.41.0-alpine
        imagePullPolicy: Always
        ports:
          - containerPort: 61616
            name: messaging
          - containerPort: 8161
            name: http
        volumeMounts:
          - name: activemq-config
            mountPath:  /var/lib/artemis-instance/etc-override
      volumes:
        - name: activemq-config
          configMap:
            name: activemq-config
            items:
              - key: jolokia-access.xml
                path: jolokia-access.xml
```

Example K8s configuration that worked for me:

---

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: activemq
  name: activemq-config
  labels:
    app.kubernetes.io/name: activemq
data:
  jolokia-access.xml: |
    <cors>
      <ignore-scheme/>
    </cors>
```"
79562957,cant create service in kubernetes. - kubectl apply -f nginx-service.yaml,"Get the error, so could you please help, I am very new in Kubernetes. and gets the errors:

```
C:\Windows\system32>kubectl apply -f nginx-service.yaml
Error from server (BadRequest): error when creating ""nginx-service.yaml"":
Service in version ""v1"" cannot be handled as a Service:
strict decoding error: unknown field ""spec.ports[0].protocols""
```

```
apiVersion: v2
kind: Service
metadata:
  name: nginx-service
spec:
  selector:
    app: nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8080
```",kubernetes,79563089.0,"Two small things to fix.

`apiVersion` should be `v1`, not `v2`

```
apiVersion: v1
```

According to the error, in the port list, you should use `protocol`, not `protocols`. But in the snippet provided it seems you already fix it.",2025-04-08T21:32:06,2025-04-08T20:02:37,"```text
apiVersion should be v1, not v2
```

Two small things to fix.

According to the error, in the port list, you should use `protocol`, not `protocols`. But in the snippet provided it seems you already fix it.

```yaml
apiVersion: v1
```

`apiVersion` should be `v1`, not `v2`"
79559858,How to upgrade sidecar image without disrupting other containers in Kubernetes pod,"According to [sidecar containers docs](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#differences-from-init-containers):

> Changing the image of a sidecar container will not cause the Pod to restart, but will trigger a container restart.

Using their own [example application](https://kubernetes.io/docs/concepts/workloads/pods/sidecar-containers/#sidecar-example):

```
$ kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
      initContainers:
        - name: logshipper
          image: alpine:latest
          restartPolicy: Always
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
EOF
deployment.apps/myapp created
```

Replacing the sidecar's image causes the entire pod to be replaced:

```
$ kubectl set image deploy myapp logshipper=busybox:latest
deployment.apps/myapp image updated
$ kubectl get pod -l app=myapp
NAME                     READY   STATUS        RESTARTS   AGE
myapp-5cdcbc5cff-nv6gz   2/2     Running       0          6s
myapp-b5f9c8894-lkth4    2/2     Terminating   0          86s
```

So how does one upgrade a sidecar image without disrupting other containers running in the pod?

```
$ kubectl version
Client Version: v1.31.7
Kustomize Version: v5.4.2
Server Version: v1.30.9-gke.1046000
$ kubectl get --raw /metrics | grep SidecarContainers
kubernetes_feature_enabled{name=""SidecarContainers"",stage=""BETA""} 1
```","image, kubernetes, containers, sidecar",79572872.0,"While it is true that from v1.29 onwards there is a beta feature in which an init-Container will effectively become a sidecar container if it's `restartPolicy` is set to `always`, sidecars have been used as a second container within the same pod for quite a while:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
        - name: logshipper
          image: alpine:latest
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
```

Note the absence of the `initContainer`key. Let's check if that worked:

```
$ kubectl logs deployments/myapp -c logshipper
logging
logging
logging
[...]
```

## Deployment

Does it work with the original deployment?

```
$ kubectl logs deployments/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

With that being said, why does the deployment restart? Well, as David Maze correctly pointed out, you changed the template of the deployment, and hence kubernetes does what it is supposed to do: reconcile the differences.

## Pod

But how about we are creating not a deployment, but a pod?

```
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
    - name: myapp
      image: alpine:latest
      command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
      volumeMounts:
        - name: data
          mountPath: /opt
  initContainers:
    - name: logshipper
      image: alpine:3.18
      restartPolicy: Always
      command: ['sh', '-c', 'tail -F /opt/logs.txt']
      volumeMounts:
        - name: data
          mountPath: /opt
  volumes:
    - name: data
      emptyDir: {}
```

That gives us the expected log output:

```
$ kubectl logs pods/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

Now, if we change the image for the `logshipper` container to `alpine:3.20` and reapply the resource

```
$ kubectl apply -f pod.yaml
pod/myapp configured
```

the command `kubectl events --for pod/myapp` will show the expected behavior:

```
LAST SEEN               TYPE      REASON      OBJECT      MESSAGE
[...]
116s                    Normal    Pulled      Pod/myapp   Container image ""alpine:3.18"" already present on machine
116s                    Normal    Scheduled   Pod/myapp   Successfully assigned default/myapp to kind-cluster-control-plane
115s                    Normal    Pulling     Pod/myapp   Pulling image ""alpine:latest""
113s                    Normal    Created     Pod/myapp   Created container: myapp
113s                    Normal    Pulled      Pod/myapp   Successfully pulled image ""alpine:latest"" in 1.564s (1.565s including waiting). Image size: 3653068 bytes.
113s                    Normal    Started     Pod/myapp   Started container myapp
41s                     Normal    Killing     Pod/myapp   Init container logshipper definition changed
11s (x2 over 115s)      Normal    Created     Pod/myapp   Created container: logshipper
11s                     Normal    Pulled      Pod/myapp   Container image ""alpine:3.20"" already present on machine
10s (x2 over 115s)      Normal    Started     Pod/myapp   Started container logshipper
```

> Note that your output may vary.

## Conclusion

If you change the deployment template, you will trigger a reconciliation. That is very much expected behavior and your applications should account for that. Since hardly anyone will ever deploy pods manually, the feature that the main container does not restart if the sidecar is defined as an initContainer with restartPolicy set to always is plainly utterly useless for all practical purposes.",2025-04-14T10:06:14,2025-04-07T12:20:57,"```markdown
While it is true that from v1.29 onwards there is a beta feature in which an init-Container will effectively become a sidecar container if it's `restartPolicy` is set to `always`, sidecars have been used as a second container within the same pod for quite a while:
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
  labels:
    app: myapp
spec:
  replicas: 1
  selector:
    matchLabels:
      app: myapp
  template:
    metadata:
      labels:
        app: myapp
    spec:
      containers:
        - name: myapp
          image: alpine:latest
          command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
          volumeMounts:
            - name: data
              mountPath: /opt
        - name: logshipper
          image: alpine:latest
          command: ['sh', '-c', 'tail -F /opt/logs.txt']
          volumeMounts:
            - name: data
              mountPath: /opt
      volumes:
        - name: data
          emptyDir: {}
```

```markdown
Note the absence of the `initContainer`key. Let's check if that worked:
```

```bash
$ kubectl logs deployments/myapp -c logshipper
logging
logging
logging
[...]
```

```markdown
## Deployment

Does it work with the original deployment?
```

```bash
$ kubectl logs deployments/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

```markdown
With that being said, why does the deployment restart? Well, as David Maze correctly pointed out, you changed the template of the deployment, and hence kubernetes does what it is supposed to do: reconcile the differences.

## Pod

But how about we are creating not a deployment, but a pod?
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp
  labels:
    name: myapp
spec:
  containers:
    - name: myapp
      image: alpine:latest
      command: ['sh', '-c', 'while true; do echo ""logging"" >> /opt/logs.txt; sleep 1; done']
      volumeMounts:
        - name: data
          mountPath: /opt
  initContainers:
    - name: logshipper
      image: alpine:3.18
      restartPolicy: Always
      command: ['sh', '-c', 'tail -F /opt/logs.txt']
      volumeMounts:
        - name: data
          mountPath: /opt
  volumes:
    - name: data
      emptyDir: {}
```

```markdown
That gives us the expected log output:
```

```bash
$ kubectl logs pods/myapp -c logshipper
tail: can't open '/opt/logs.txt': No such file or directory
tail: /opt/logs.txt has appeared; following end of new file
logging
logging
logging
```

```markdown
Now, if we change the image for the `logshipper` container to `alpine:3.20` and reapply the resource
```

```bash
$ kubectl apply -f pod.yaml
pod/myapp configured
```

```markdown
the command `kubectl events --for pod/myapp` will show the expected behavior:
```

```bash
LAST SEEN               TYPE      REASON      OBJECT      MESSAGE
[...]
116s                    Normal    Pulled      Pod/myapp   Container image ""alpine:3.18"" already present on machine
116s                    Normal    Scheduled   Pod/myapp   Successfully assigned default/myapp to kind-cluster-control-plane
115s                    Normal    Pulling     Pod/myapp   Pulling image ""alpine:latest""
113s                    Normal    Created     Pod/myapp   Created container: myapp
113s                    Normal    Pulled      Pod/myapp   Successfully pulled image ""alpine:latest"" in 1.564s (1.565s including waiting). Image size: 3653068 bytes.
113s                    Normal    Started     Pod/myapp   Started container myapp
41s                     Normal    Killing     Pod/myapp   Init container logshipper definition changed
11s (x2 over 115s)      Normal    Created     Pod/myapp   Created container: logshipper
11s                     Normal    Pulled      Pod/myapp   Container image ""alpine:3.20"" already present on machine
10s (x2 over 115s)      Normal    Started     Pod/myapp   Started container logshipper
```

```markdown
> Note that your output may vary.

## Conclusion

If you change the deployment template, you will trigger a reconciliation. That is very much expected behavior and your applications should account for that. Since hardly anyone will ever deploy pods manually, the feature that the main container does not restart if the sidecar is defined as an initContainer with restartPolicy set to always is plainly utterly useless for all practical purposes.
```"
79554394,Running Ollama as a k8s STS with external script as entrypoint to load models,"I manage to run Ollama as a k8s STS. I am using it for Python Langchain LLM/RAG application. However the following Dockerfile `ENTRYPOINT` script which tries to pull a list of images exported as `MODELS` ENV from k8s STS manifest runs into problem. Dockerfile has the following `ENTRYPOINT` and `CMD`:

```
ENTRYPOINT [""/usr/local/bin/run.sh""]
CMD [""bash""]
```

`run.sh`:

```
#!/bin/bash
set -x
ollama serve&
sleep 10
models=""${MODELS//,/ }""
for i in ""${models[@]}""; do \
      echo model: $i  \
      ollama pull $i \
    done
```

k8s logs:

```
+ models=llama3.2
/usr/local/bin/run.sh: line 10: syntax error: unexpected end of file
```

David Maze's solution:

```
          lifecycle:
            postStart:
              exec:
                command:
                  - bash
                  - -c
                  - |
                    for i in $(seq 10); do
                      ollama ps && break
                      sleep 1
                    done
                    for model in ${MODELS//,/ }; do
                      ollama pull ""$model""
                    done
```

```
ollama-0          1/2     CrashLoopBackOff     4 (3s ago)        115s
ollama-1          1/2     CrashLoopBackOff     4 (1s ago)        115s
```

```
  Warning  FailedPostStartHook  106s (x3 over 2m14s)  kubelet            PostStartHook failed
```

```
$ k logs -fp ollama-0
Defaulted container ""ollama"" out of: ollama, fluentd
Error: unknown command ""ollama"" for ""ollama""
```

Update `Dockerfile`:

```
ENTRYPOINT [""/bin/ollama""]
#CMD [""bash""]
CMD [""ollama"", ""serve""]
```

I need the customized `Dockerfile` so that I could install Nvidia Container Toolkit.","kubernetes, dockerfile, py-langchain, ollama, docker-entrypoint",79555369.0,"At a mechanical level, the backslashes inside the `for` loop are causing problems.  This causes the shell to combine the lines together, so you get a single command `echo model: $i ollama pull $i done`, but there's not a standalone `done` command to terminate the loop.

The next problem you'll run into is that this entrypoint script is the only thing the container runs, and when this script exits, the container will exit as well.  It doesn't matter that you've started the Ollama server in the background.  If you wanted to run the container this way, you need to `wait` for the server to exit.  That would look something like

```
#!/bin/bash
ollama serve &
pid=$!                       # ADD: save the process ID of the server
sleep 10
models=""${MODELS//,/ }""
for i in ""${models[@]}""; do  # FIX: remove backslashes
  echo model: ""$i""
  ollama pull ""$i""
done
wait ""$pid""                  # ADD: keep the script running as long as the server is too
```

However, this model of starting a background process and then `wait`ing for it often isn't the best approach.  If the Pod gets shut down, for example, the termination signal will go to the wrapper script and not the Ollama server, and you won'd be able to have a clean shutdown.

In a Kubernetes context (you say you're running this in a StatefulSet) a [PostStart hook](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/) fits here.  This will let you run an unmodified image, but add your own script that runs at about the same time as the container startup.  In a Kubernetes manifest this might look like:

```
spec:
  template:
    spec:
      containers:
        - name: ollama
          image: ollama/ollama  # the unmodified upstream image
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                      for i in $(seq 10); do
                        ollama ps && break
                        sleep 1
                      done
                      for model in llama3.2; do
                        ollama pull ""$model""
                      done
```

This setup writes a shell script inline in the Kubernetes manifest.  It wraps it in `/bin/sh -c` to it can be run this way.  This uses an ""exec"" mechanism, so the script runs as a secondary process in the same container.  The first fragment waits up to 10 seconds for the server to be running, and the second is the loop to load the models.",2025-04-04T12:53:53,2025-04-04T03:19:47,"```bash
#!/bin/bash
ollama serve &
pid=$!                       # ADD: save the process ID of the server
sleep 10
models=""${MODELS//,/ }""
for i in ""${models[@]}""; do  # FIX: remove backslashes
  echo model: ""$i""
  ollama pull ""$i""
done
wait ""$pid""                  # ADD: keep the script running as long as the server is too
```

At a mechanical level, the backslashes inside the `for` loop are causing problems.  This causes the shell to combine the lines together, so you get a single command `echo model: $i ollama pull $i done`, but there's not a standalone `done` command to terminate the loop.

The next problem you'll run into is that this entrypoint script is the only thing the container runs, and when this script exits, the container will exit as well.  It doesn't matter that you've started the Ollama server in the background.  If you wanted to run the container this way, you need to `wait` for the server to exit.  That would look something like

However, this model of starting a background process and then `wait`ing for it often isn't the best approach.  If the Pod gets shut down, for example, the termination signal will go to the wrapper script and not the Ollama server, and you won'd be able to have a clean shutdown.

In a Kubernetes context (you say you're running this in a StatefulSet) a [PostStart hook](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/) fits here.  This will let you run an unmodified image, but add your own script that runs at about the same time as the container startup.  In a Kubernetes manifest this might look like:

```yaml
spec:
  template:
    spec:
      containers:
        - name: ollama
          image: ollama/ollama  # the unmodified upstream image
          lifecycle:
            postStart:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                      for i in $(seq 10); do
                        ollama ps && break
                        sleep 1
                      done
                      for model in llama3.2; do
                        ollama pull ""$model""
                      done
```

This setup writes a shell script inline in the Kubernetes manifest.  It wraps it in `/bin/sh -c` to it can be run this way.  This uses an ""exec"" mechanism, so the script runs as a secondary process in the same container.  The first fragment waits up to 10 seconds for the server to be running, and the second is the loop to load the models."
79553210,unable to connect to public internet when injecting istio proxy/istio envoy,"i have an EKS cluster version 1.30 running on which i have installed istio-base, istiod, istio-ingressgateway using terraform helm resource.I have not installed egress gateway. It is all fine untill i inject the istio envoy proxy to the application pods. At that time i am unable to make any ssl based requests.
I am attaching errors below which i encountered for resolving an s3 bucket, as you can see the domain name is pointing to `es.amazonaws.com` in output. [![enter image description here](https://i.sstatic.net/oTNmDdfA.png)](https://i.sstatic.net/oTNmDdfA.png)

i also have a basic alpine pod that installs certain packages in bootup(pod yaml below). this also crashes as the packages do not get instaled.

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    run: alpine
  name: alpine
spec:
  containers:
  - image: alpine
    name: alpine
    command: [""sh"",""-c"",""apk update && apk add aws-cli &&  apk add mysql-client && apk add openssh && sleep 365d"" ]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
```

how can i fix this issue, really appreciate the help.","kubernetes, istio, istio-gateway, istio-sidecar, istio-operator",79580462.0,"You can connect to an S3 bucket from your application using an Istio ServiceEntry and DestinationRule. In this example, I’m assuming the namespace is `default`. The S3 bucket endpoint format should look like:

```
<bucket-name>.s3.us-east-1.amazonaws.com
```

**Service Entry**

```
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: s3-access
spec:
  hosts:
  - s3.us-east-1.amazonaws.com
  - ""###bucket name###""
  location: MESH_EXTERNAL
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
```

**Destination rule**

```
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: s3-destination
spec:
  host: ###bucket name#
  trafficPolicy:
    tls:
      mode: SIMPLE
      insecureSkipVerify: true%
```

After creating the `ServiceEntry` and `DestinationRule`, your application pod should be able to connect to the S3 bucket.",2025-04-18T05:23:47,2025-04-03T14:21:45,"```text
<bucket-name>.s3.us-east-1.amazonaws.com
```

You can connect to an S3 bucket from your application using an Istio ServiceEntry and DestinationRule. In this example, I’m assuming the namespace is `default`. The S3 bucket endpoint format should look like:


```yaml
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: s3-access
spec:
  hosts:
  - s3.us-east-1.amazonaws.com
  - ""###bucket name###""
  location: MESH_EXTERNAL
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  resolution: DNS
```

**Service Entry**


```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: s3-destination
spec:
  host: ###bucket name#
  trafficPolicy:
    tls:
      mode: SIMPLE
      insecureSkipVerify: true%
```

**Destination rule**

After creating the `ServiceEntry` and `DestinationRule`, your application pod should be able to connect to the S3 bucket."
79552339,Path-based Routing for External Domains in Kubernetes with Istio/Gateway API,"We want to achieve path-based routing for external domains not owned by our Kubernetes cluster. We managed to configure routing successfully, but now we encounter a side-effect: workloads inside the cluster communicate using plain HTTP on port 443 to the external domain, resulting in SSL errors.

## Desired Behavior

- Requests to `www.example.com/graphql` are routed from the cluster ingress gateway to the external domain.
- Workloads within the service mesh can successfully query `https://graphql-api.mesh-external.example.com` using HTTPS without SSL issues.

## Current Approach

Our configuration uses Istio's `ServiceEntry`, `DestinationRule`, and the Gateway API's `HTTPRoute`:

```
apiVersion: networking.istio.io/v1
kind: ServiceEntry
metadata:
  name: www-example-com
spec:
  hosts:
  - graphql-api.mesh-external.example.com
  location: MESH_EXTERNAL
  ports:
  - name: https
    number: 443
    protocol: HTTPS
  resolution: DNS
---
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: www-example-com
spec:
  host: graphql-api.mesh-external.example.com
  trafficPolicy:
    portLevelSettings:
    - port:
        number: 443
      tls:
        mode: SIMPLE
        sni: graphql-api.mesh-external.example.com
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: www-example-com
spec:
  hostnames:
  - www.example.com
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: ingress
    namespace: gateway
  rules:
  - backendRefs:
    - group: networking.istio.io
      kind: Hostname
      name: graphql-api.mesh-external.example.com
      port: 443
      weight: 1
    matches:
    - path:
        type: PathPrefix
        value: /graphql
```

## Problem

- External requests through ingress work correctly, but internal mesh communication defaults to plain HTTP on port 443, causing SSL/TLS errors.

## Question

How can we configure Istio and Gateway API so that internal mesh workloads correctly perform HTTPS communication with the external domain while retaining proper path-based routing for ingress traffic?","kubernetes, istio",79552791.0,"You need to **modify** the **DestinationRule** to enforce [TLS settings](https://istio.io/latest/docs/reference/config/networking/destination-rule/#ClientTLSSettings) to make internal mesh workloads correctly perform HTTPS communication with the external domain while retaining proper path-based routing for ingress traffic without causing any SSL/TLS errors.

So you may need to update the **DestinationRule** by removing portLevelSettings as follows :

```
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: www-example-com
spec:
  host: graphql-api.mesh-external.example.com
  trafficPolicy:
    tls:
      mode: SIMPLE
      sni: graphql-api.mesh-external.example.com
```

For more information check this Isito [document](https://istio.io/latest/docs/concepts/traffic-management/#service-entries) and also go through this Medium [blog](https://harsh05.medium.com/understanding-ingress-gateway-in-istio-a-detailed-guide-9ee300b9da65) by Harsh, which might be helpful to resolve your issue.",2025-04-03T11:25:05,2025-04-03T08:11:15,"```yaml
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  name: www-example-com
spec:
  host: graphql-api.mesh-external.example.com
  trafficPolicy:
    tls:
      mode: SIMPLE
      sni: graphql-api.mesh-external.example.com
```

You need to **modify** the **DestinationRule** to enforce [TLS settings](https://istio.io/latest/docs/reference/config/networking/destination-rule/#ClientTLSSettings) to make internal mesh workloads correctly perform HTTPS communication with the external domain while retaining proper path-based routing for ingress traffic without causing any SSL/TLS errors.

So you may need to update the **DestinationRule** by removing portLevelSettings as follows :

For more information check this Isito [document](https://istio.io/latest/docs/concepts/traffic-management/#service-entries) and also go through this Medium [blog](https://harsh05.medium.com/understanding-ingress-gateway-in-istio-a-detailed-guide-9ee300b9da65) by Harsh, which might be helpful to resolve your issue."
79541591,How to add subpaths to url and delete subpaths to ingress,"I have a requirement to embed the dify app page into the web page.
Because of network and other reasons, I have to embed **additional sub-paths** in the original iframe src to route to different services.

I configure iframe and ingress as follows to access it normally.

```
<iframe
 src=""http://my.com/chatbot/abcdefg""
 style=""width: 100%; height: 100%; min-height: 700px""
 frameborder=""0""
 allow=""microphone"">
</iframe>
```

ingress

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /
            backend:
              serviceName: dify
              servicePort: 80
```

But when I try to add a subpath(proxy/dify) and configure the match rewrite in Ingress, an **error** occurs (default backend-404). From the developer tools, we can see an original request and a request without subpaths.

```
<iframe
 src=""http://my.com/proxy/dify/chatbot/abcdefg""
 style=""width: 100%; height: 100%; min-height: 700px""
 frameborder=""0""
 allow=""microphone"">
</iframe>
```

ingress

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /\$1
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/preserve-host: ""true""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /proxy/dify/(.*)
            backend:
              serviceName: dify
              servicePort: 80
```

I know there is a high probability of an error because I did not change the src etc inside the iframe. But I don't know any other way to achieve, looking forward to your help","kubernetes, iframe, kubernetes-ingress",79547153.0,"Try using regular expression `/proxy/dify(/|$)(.*)` with the rewrite target `/$2`. This will ensure that your every request will include the root path (/) after the ingress removes `/proxy/dify`. Consider adding `pathType` in your ingress paths as well.

Sample revised config:

```
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/preserve-host: ""true""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /proxy/dify(/|$)(.*)      #update to (/|$)(.*)
            backend:
            pathType: ImplementationSpecific      #added pathType
              serviceName: dify
              servicePort: 80
```

You can also refer to this [Rewrite](https://kubernetes.github.io/ingress-nginx/examples/rewrite/) documentation.",2025-03-31T19:55:49,2025-03-28T13:40:54,"```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  annotations:
    nginx.ingress.kubernetes.io/service-upstream: ""true""
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/preserve-host: ""true""
    nginx.ingress.kubernetes.io/enable-cors: ""true""
  name: proxy
spec:
  rules:
    - host: my.com
      http:
        paths:
          - path: /proxy/dify(/|$)(.*)      #update to (/|$)(.*)
            backend:
            pathType: ImplementationSpecific      #added pathType
              serviceName: dify
              servicePort: 80
```

Try using regular expression `/proxy/dify(/|$)(.*)` with the rewrite target `/$2`. This will ensure that your every request will include the root path (/) after the ingress removes `/proxy/dify`. Consider adding `pathType` in your ingress paths as well.

Sample revised config:

You can also refer to this [Rewrite](https://kubernetes.github.io/ingress-nginx/examples/rewrite/) documentation."
79540452,How to deploy a local image on K3D without pushing to a registry and upgrade Helm deployments locally?,"I have two questions regarding deploying a local Kubernetes cluster using K3D and Helm.

I have successfully built a local registry and cluster on K3D using the commands `k3d registry create registry.localhost -p 5000` and `k3d cluster create c1 --registry-use k3d-registry.localhost:5000`. I set the `imagePullPolicy` to `Always`, and it works.

However, I have to build the container, then push and pull it again (I use Helm) every time I want to test the service locally. To skip the push-pull process, I tried setting the `imagePullPolicy` to `Never` so that Helm would use the local container I just built. But I got failed `ErrImageNeverPull` like this:

```
NAME                                 READY   STATUS              RESTARTS   AGE
webtest-deployment-7fb8ccb485-7vpxz   0/1     ErrImageNeverPull   0          21s
```

So, how can I make the deployment successful without the push-pull process to the registry by setting imagePullPolicy to Never (just use the local image after it’s built)?

The second issue is that when I made changes or revisions to the project files, build a new Docker image and push it to the local registry. However, when I update the deployment using `helm upgrade <release> <chart>` or `helm upgrade <release> <chart> --force`, the changes do not take effect. Additionally, the pods are not replaced either before or after the upgrade. To apply the changes, I have to reinstall the package by running helm uninstall followed by helm install. Is this behavior common in Helm deployments, or am I missing a step to properly upgrade the service via Helm?

related question:
[Local Kubernetes Deployment using k3d - where should I push the docker images to?](https://stackoverflow.com/questions/73674229/local-kubernetes-deployment-using-k3d-where-should-i-push-the-docker-images-to)","docker, kubernetes, kubernetes-helm, docker-registry, k3d",79542100.0,"If I got you right you want to test custom images with existing helm charts without changing the helm chart or the hassle of setting up a registry and/or doing all the build/push/pull/imagePullSecrets stuff. This can be achieved using a clever combination of k3d and [tilt](https://tilt.dev)'s features and would go like this:

1. The image is built by tilt on every change in the context directory.
2. It is then automatically pushed by tilt into the registry created by k3d, which tilt auto-detects.
3. Then tilt ""injects"" the newly built image into the helm chart.

For the sake of this example, let's assume that you want to deploy an nginx image containing a custom web site for your company with the [bitnami helm chart for nginx](https://artifacthub.io/packages/helm/bitnami/nginx).

## Directory structure

```
.
├── Tiltfile
├── image
│   ├── Dockerfile
│   └── index.html
└── k3d.yaml
```

### `image/Dockerfile`

```
FROM bitnami/nginx:1.27.4-debian-12-r6
# Allow modifications to the image
USER 0
# Just an example for a custom image
ADD index.html /app/index.html
# Run nginx as a non-root user
USER 1001
```

Nothing much to see here. The HTML file is even more meaningless, so I leave it out.

### `k3d.yaml`

Also, not much of a surprise. However: tilt will automatically detect the registry created and be able to push images to it, so there is no need to adjust `insecure_registries` in your Docker settings. k3d in turn is able to pull images from said insecure registry, so we have the complete ""build->push->pull"" cycle.

```
apiVersion: k3d.io/v1alpha5
kind: Simple
metadata:
  name: demo # name that you want to give to your cluster (will still be prefixed with `k3d-`)
servers: 1 # same as `--servers 1`
agents: 1 # same as `--agents 1`
image: rancher/k3s:v1.29.15-k3s1 # same as `--image rancher/k3s:v1.29.15-k3s1`
ports:
  - port: 8080:80 # same as `--port '8080:80@loadbalancer'`
    nodeFilters:
      - loadbalancer
  - port: 8443:443 # same as `--port '8443:443@loadbalancer'`
    nodeFilters:
      - loadbalancer
registries: # define how registries should be created or used
  create: # creates a default registry to be used with the cluster; same as `--registry-create localregistry`
    name: localregistry
    host: ""0.0.0.0""
    hostPort: ""5000""
options:
  k3d: # k3d runtime settings
    wait: true # wait for cluster to be usable before returining; same as `--wait` (default: true)
    timeout: ""180s"" # wait timeout before aborting; same as `--timeout 60s`
  kubeconfig:
    updateDefaultKubeconfig: true # add new cluster to your default Kubeconfig; same as `--kubeconfig-update-default` (default: true)
    switchCurrentContext: true # also set current-context to the new cluster's context; same as `--kubeconfig-switch-context` (default: true)
```

### Tiltfile

```
# Build the custom image and push it to the local registry,
# which for k3d is autodetected by tilt.
# The image is built using the Dockerfile in the 'image' directory.
# Note that the docker image is rebuilt and the whole deployment starts over
# if the Dockerfile if any of the files in the `context` directory changes.
docker_build(""company/custom_nginx"",context=""image"")

# Load an extension to conveniently deal with the helm chart.
load(""ext://helm_resource"", ""helm_resource"", ""helm_repo"")

# We first need to load the helm repo
# and then we can load the helm resource.
helm_repo('bitnami',url=""https://charts.bitnami.com/bitnami"")

# Install the actual release.
# The image_deps are the images that are built before the helm resource is created.
# The image_keys are the keys that are used to inject the local image into the helm release.
helm_resource(
    'nginx-release',
    chart='bitnami/nginx',resource_deps=['bitnami'],
     flags=['--set=global.security.allowInsecureImages=true'],
    # THIS is where the magic happens:
    # 'helm_ressource'
    image_deps=['company/custom_nginx'],
    image_keys=[('image.registry', 'image.repository', 'image.tag')],
    )

```

## What is happening?

> Some parts of the following will be `[redacted]` for privacy reasons.

After [installing tilt](https://docs.tilt.dev/install.html), we can run `k3d cluster create --config k3d.yaml && tilt up` and watch the logs in tilt's UI.

1. Tiltfile is parsed

```
Loading Tiltfile at: [redacted]/Tiltfile
Successfully loaded Tiltfile (1.295717079s)
Auto-detected local registry from environment: &RegistryHosting{Host:localhost:5000,HostFromClusterNetwork:localregistry:5000,HostFromContainerRuntime:localregistry:5000,Help:https://k3d.io/stable/usage/registries/#using-a-local-registry,SingleName:,}
```

Note that tilt indeed detected the registry we just created.
2. The bitnami helm repo is added

```
Running cmd: helm repo add bitnami https://charts.bitnami.com/bitnami --force-update
""bitnami"" has been added to your repositories
```

Since we added the repo in the `resource_deps` of the `helm_resource`, the helm release will not be deployed before the helm repo was successfully added.
3. The helm release is deployed.

This is where it get's interesting. Since we declared the docker image `company/custom_nginx` in the `image_deps` of the helm resource and instructed the `helm_resource` where to use said image via `image_keys`, the image's values will be substituted:

```
STEP 1/3 — Building Dockerfile: [company/custom_nginx]
Building Dockerfile for platform linux/amd64:
[...]
STEP 2/3 — Pushing localhost:5000/company_custom_nginx:tilt-2de2a5b04212dc59
     Pushing with Docker client
     Authenticating to image repo: localhost:5000
     [...]
 STEP 3/3 — Deploying
      [...]
      Running cmd: ['helm', 'upgrade', '--install', '--set=global.security.allowInsecureImages=true', '--set', 'image.registry=localregistry:5000', '--set', 'image.repository=company_custom_nginx', '--set', 'image.tag=tilt-2de2a5b04212dc59', 'nginx-release', 'bitnami/nginx']
      Release ""nginx-release"" does not exist. Installing it now.
      NAME: nginx-release
      LAST DEPLOYED: Fri Mar 28 17:56:07 2025
      NAMESPACE: default
      STATUS: deployed
      REVISION: 1
      TEST SUITE: None
      NOTES:
      CHART NAME: nginx
      CHART VERSION: 19.0.3
      APP VERSION: 1.27.4
```

## Conclusion

Using tilt and some 4 lines of configuration, you not only can test your custom images easily, but can do so continuously, since the image will be rebuilt and redeployed each time there is a change in context directory of the image. And all this with two simple commands. Do not believe me? ""All"" the [code is available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/k3d-localregistry-tilt-79540452). Clone and try ;).",2025-03-28T17:46:15,2025-03-28T03:20:54,"```text
If I got you right you want to test custom images with existing helm charts without changing the helm chart or the hassle of setting up a registry and/or doing all the build/push/pull/imagePullSecrets stuff. This can be achieved using a clever combination of k3d and [tilt](https://tilt.dev)'s features and would go like this:

1. The image is built by tilt on every change in the context directory.
2. It is then automatically pushed by tilt into the registry created by k3d, which tilt auto-detects.
3. Then tilt ""injects"" the newly built image into the helm chart.

For the sake of this example, let's assume that you want to deploy an nginx image containing a custom web site for your company with the [bitnami helm chart for nginx](https://artifacthub.io/packages/helm/bitnami/nginx).

## Directory structure
```

Explanation: General descriptive text; no code here yet.

```bash
.
├── Tiltfile
├── image
│   ├── Dockerfile
│   └── index.html
└── k3d.yaml
```

Explanation: This is a directory tree listing, treated as a shell-style snippet.

```text
### `image/Dockerfile`
```

Explanation: Heading text, not code.

```dockerfile
FROM bitnami/nginx:1.27.4-debian-12-r6
# Allow modifications to the image
USER 0
# Just an example for a custom image
ADD index.html /app/index.html
# Run nginx as a non-root user
USER 1001
```

Explanation: Dockerfile code snippet.

```text
Nothing much to see here. The HTML file is even more meaningless, so I leave it out.

### `k3d.yaml`

Also, not much of a surprise. However: tilt will automatically detect the registry created and be able to push images to it, so there is no need to adjust `insecure_registries` in your Docker settings. k3d in turn is able to pull images from said insecure registry, so we have the complete ""build->push->pull"" cycle.
```

Explanation: Descriptive text around the k3d config.

```yaml
apiVersion: k3d.io/v1alpha5
kind: Simple
metadata:
  name: demo # name that you want to give to your cluster (will still be prefixed with `k3d-`)
servers: 1 # same as `--servers 1`
agents: 1 # same as `--agents 1`
image: rancher/k3s:v1.29.15-k3s1 # same as `--image rancher/k3s:v1.29.15-k3s1`
ports:
  - port: 8080:80 # same as `--port '8080:80@loadbalancer'`
    nodeFilters:
      - loadbalancer
  - port: 8443:443 # same as `--port '8443:443@loadbalancer'`
    nodeFilters:
      - loadbalancer
registries: # define how registries should be created or used
  create: # creates a default registry to be used with the cluster; same as `--registry-create localregistry`
    name: localregistry
    host: ""0.0.0.0""
    hostPort: ""5000""
options:
  k3d: # k3d runtime settings
    wait: true # wait for cluster to be usable before returining; same as `--wait` (default: true)
    timeout: ""180s"" # wait timeout before aborting; same as `--timeout 60s`
  kubeconfig:
    updateDefaultKubeconfig: true # add new cluster to your default Kubeconfig; same as `--kubeconfig-update-default` (default: true)
    switchCurrentContext: true # also set current-context to the new cluster's context; same as `--kubeconfig-switch-context` (default: true)
```

Explanation: YAML configuration snippet for k3d.

```text
### Tiltfile
```

Explanation: Heading text.

```python
# Build the custom image and push it to the local registry,
# which for k3d is autodetected by tilt.
# The image is built using the Dockerfile in the 'image' directory.
# Note that the docker image is rebuilt and the whole deployment starts over
# if the Dockerfile if any of the files in the `context` directory changes.
docker_build(""company/custom_nginx"",context=""image"")

# Load an extension to conveniently deal with the helm chart.
load(""ext://helm_resource"", ""helm_resource"", ""helm_repo"")

# We first need to load the helm repo
# and then we can load the helm resource.
helm_repo('bitnami',url=""https://charts.bitnami.com/bitnami"")

# Install the actual release.
# The image_deps are the images that are built before the helm resource is created.
# The image_keys are the keys that are used to inject the local image into the helm release.
helm_resource(
    'nginx-release',
    chart='bitnami/nginx',resource_deps=['bitnami'],
     flags=['--set=global.security.allowInsecureImages=true'],
    # THIS is where the magic happens:
    # 'helm_ressource'
    image_deps=['company/custom_nginx'],
    image_keys=[('image.registry', 'image.repository', 'image.tag')],
    )
```

Explanation: Tiltfile code (Starlark/Python-like) snippet.

```text
## What is happening?

> Some parts of the following will be `[redacted]` for privacy reasons.

After [installing tilt](https://docs.tilt.dev/install.html), we can run `k3d cluster create --config k3d.yaml && tilt up` and watch the logs in tilt's UI.

1. Tiltfile is parsed
```

Explanation: Descriptive text introducing log output.

```text
Loading Tiltfile at: [redacted]/Tiltfile
Successfully loaded Tiltfile (1.295717079s)
Auto-detected local registry from environment: &RegistryHosting{Host:localhost:5000,HostFromClusterNetwork:localregistry:5000,HostFromContainerRuntime:localregistry:5000,Help:https://k3d.io/stable/usage/registries/#using-a-local-registry,SingleName:,}
```

Explanation: Plain text log output snippet.

```text
Note that tilt indeed detected the registry we just created.
2. The bitnami helm repo is added
```

Explanation: Descriptive text.

```bash
Running cmd: helm repo add bitnami https://charts.bitnami.com/bitnami --force-update
""bitnami"" has been added to your repositories
```

Explanation: Shell/command output snippet.

```text
Since we added the repo in the `resource_deps` of the `helm_resource`, the helm release will not be deployed before the helm repo was successfully added.
3. The helm release is deployed.

This is where it get's interesting. Since we declared the docker image `company/custom_nginx` in the `image_deps` of the helm resource and instructed the `helm_resource` where to use said image via `image_keys`, the image's values will be substituted:
```

Explanation: Descriptive explanation text.

```text
STEP 1/3 — Building Dockerfile: [company/custom_nginx]
Building Dockerfile for platform linux/amd64:
[...]
STEP 2/3 — Pushing localhost:5000/company_custom_nginx:tilt-2de2a5b04212dc59
     Pushing with Docker client
     Authenticating to image repo: localhost:5000
     [...]
 STEP 3/3 — Deploying
      [...]
      Running cmd: ['helm', 'upgrade', '--install', '--set=global.security.allowInsecureImages=true', '--set', 'image.registry=localregistry:5000', '--set', 'image.repository=company_custom_nginx', '--set', 'image.tag=tilt-2de2a5b04212dc59', 'nginx-release', 'bitnami/nginx']
      Release ""nginx-release"" does not exist. Installing it now.
      NAME: nginx-release
      LAST DEPLOYED: Fri Mar 28 17:56:07 2025
      NAMESPACE: default
      STATUS: deployed
      REVISION: 1
      TEST SUITE: None
      NOTES:
      CHART NAME: nginx
      CHART VERSION: 19.0.3
      APP VERSION: 1.27.4
```

Explanation: Textual log/output snippet from the tooling.

```text
## Conclusion

Using tilt and some 4 lines of configuration, you not only can test your custom images easily, but can do so continuously, since the image will be rebuilt and redeployed each time there is a change in context directory of the image. And all this with two simple commands. Do not believe me? ""All"" the [code is available on GitHub](https://github.com/mwmahlberg/stackoverflow-answers/tree/main/k3d-localregistry-tilt-79540452). Clone and try ;).
```

Explanation: Final descriptive text, no code."
79536604,Java options within Kubernetes container,"I am working with Java application and I’m going to deploy it within container.
I have prepared Dockerfile with

`ENTRYPOINT [""java"", ""-jar"", ""java_j.jar""]`

in my Java application.
I have prepared some helm charts too.

Is it possible to use only one variable to specify all Java options interested by me in it to use it within container.args (Deployment.yaml)?

{root}/values.yaml:

```
TEST_JAVA_OPTS = ""-XX:+UseSerialGC""
TEST_JAVA_MEMORY_OPTS = ""-Xmx256m -XX:MetaspaceSize=64m""
{root}/templates/Deployment.yaml
```

{root}/templates/Deployment.yaml

```
...
spec:
   containers:
      - name: test-java-service
        command:
           - java
           - '{{ .Values.TEST_JAVA_MEMORY_OPTS }}'
           - '{{ .Values.TEST_JAVA_OPTS }}'
           - -jar
           - java_j.jar
...
```

For now it doesn’t work to me because each my application startup failes with `Improperly specified VM option`. I guess it tries to give java entire string as one java option. That is wrong of course.
My purpose is to avoid a lot of variables for each java option and to let change it in Deployment directly (I know that there is a possibility to set environment variables in Dockerfile at ENTRYPOINT part but let assume this option is disabled for us)

Kubernetes version: 1.28.12","java, kubernetes, kubernetes-helm",79536877.0,"In your Helm chart, you need to split out the different low-level JVM settings into individual items in the `command:` list.  The easiest way to do this is to make the Helm-level settings be a list of options, and then you can iterate over it.

```
# values.yaml
jvmOptions:
  - -XX:UseSerialGC
  - -Xmx256m
  - -XX:MetaspaceSize=64m
```

```
# templates/deployments.yaml
         command:
           - java
{{- range .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

Since `.Values.jvmOptions` is a list here, the template `range` construct loops through it, setting `.` to each item in turn.  In the example here, I use the `toJson` extension function to ensure each item is properly quoted as a string that fits on a single line.

Nothing would stop you from having multiple lists of option settings that you combined this way.

If you really want the JVM options as a space-separated string, then you need to split that string into words.  There is a [`splitList`](https://masterminds.github.io/sprig/string_slice.html) extension function (not mentioned in the Helm documentation but it's there) that can do this.

```
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```
# templates/deployments.yaml
         command:
           - java
{{- range splitList "" "" .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

The template part looks almost identical except for adding `splitList` in.  Note that this is a fairly naïve splitting; there's not going to be any support for quoting or embedding spaces inside a single option or any non-space whitespace.

Finally: note that the standard JVMs do support passing options in environment variables; see for example [What is the difference between JDK_JAVA_OPTIONS and JAVA_TOOL_OPTIONS when using Java 11?](https://stackoverflow.com/questions/52986487/what-is-the-difference-between-jdk-java-options-and-java-tool-options-when-using)  You could just set this environment variable without trying to reconstruct `command:`.  (IME if you have a choice, managing Kubernetes manifests tends to be easier if you can set environment variables as opposed to using command-line options.)

```
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```
# templates/deployments.yaml
         env:
{{- with .Values.jvmOptions }}
           - name: JDK_JAVA_OPTIONS
             value: {{ toJson . }}
{{- end }}
```",2025-03-26T17:57:47,2025-03-26T13:39:18,"```yaml
# values.yaml
jvmOptions:
  - -XX:UseSerialGC
  - -Xmx256m
  - -XX:MetaspaceSize=64m
```

In your Helm chart, you need to split out the different low-level JVM settings into individual items in the `command:` list.  The easiest way to do this is to make the Helm-level settings be a list of options, and then you can iterate over it.

```yaml
# templates/deployments.yaml
         command:
           - java
{{- range .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

Since `.Values.jvmOptions` is a list here, the template `range` construct loops through it, setting `.` to each item in turn.  In the example here, I use the `toJson` extension function to ensure each item is properly quoted as a string that fits on a single line.

Nothing would stop you from having multiple lists of option settings that you combined this way.

If you really want the JVM options as a space-separated string, then you need to split that string into words.  There is a [`splitList`](https://masterminds.github.io/sprig/string_slice.html) extension function (not mentioned in the Helm documentation but it's there) that can do this.

```yaml
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```yaml
# templates/deployments.yaml
         command:
           - java
{{- range splitList "" "" .Values.jvmOptions }}
           - {{ toJson . }}
{{- end }}
           - -jar
           - java_j.jar
```

The template part looks almost identical except for adding `splitList` in.  Note that this is a fairly naïve splitting; there's not going to be any support for quoting or embedding spaces inside a single option or any non-space whitespace.

Finally: note that the standard JVMs do support passing options in environment variables; see for example [What is the difference between JDK_JAVA_OPTIONS and JAVA_TOOL_OPTIONS when using Java 11?](https://stackoverflow.com/questions/52986487/what-is-the-difference-between-jdk-java-options-and-java-tool-options-when-using)  You could just set this environment variable without trying to reconstruct `command:`.  (IME if you have a choice, managing Kubernetes manifests tends to be easier if you can set environment variables as opposed to using command-line options.)

```yaml
# values.yaml
jvmOptions: ""-XX:UseSerialGC -Xmx256M -XX:MetaspaceSize=64m""
```

```yaml
# templates/deployments.yaml
         env:
{{- with .Values.jvmOptions }}
           - name: JDK_JAVA_OPTIONS
             value: {{ toJson . }}
{{- end }}
```"
79536604,Java options within Kubernetes container,"I am working with Java application and I’m going to deploy it within container.
I have prepared Dockerfile with

`ENTRYPOINT [""java"", ""-jar"", ""java_j.jar""]`

in my Java application.
I have prepared some helm charts too.

Is it possible to use only one variable to specify all Java options interested by me in it to use it within container.args (Deployment.yaml)?

{root}/values.yaml:

```
TEST_JAVA_OPTS = ""-XX:+UseSerialGC""
TEST_JAVA_MEMORY_OPTS = ""-Xmx256m -XX:MetaspaceSize=64m""
{root}/templates/Deployment.yaml
```

{root}/templates/Deployment.yaml

```
...
spec:
   containers:
      - name: test-java-service
        command:
           - java
           - '{{ .Values.TEST_JAVA_MEMORY_OPTS }}'
           - '{{ .Values.TEST_JAVA_OPTS }}'
           - -jar
           - java_j.jar
...
```

For now it doesn’t work to me because each my application startup failes with `Improperly specified VM option`. I guess it tries to give java entire string as one java option. That is wrong of course.
My purpose is to avoid a lot of variables for each java option and to let change it in Deployment directly (I know that there is a possibility to set environment variables in Dockerfile at ENTRYPOINT part but let assume this option is disabled for us)

Kubernetes version: 1.28.12","java, kubernetes, kubernetes-helm",79536853.0,"According to the [Kubernetes docs](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/), split the command array and the arguments array into `command` and `args` sections.

> When you create a Pod, you can define a command and arguments for the containers that run in the Pod. To define a command, include the `command` field in the configuration file. To define arguments for the command, include the `args` field in the configuration file. The command and arguments that you define cannot be changed after the Pod is created.
>
>
> The command and arguments that you define in the configuration file override the default command and arguments provided by the container image. If you define args, but do not define a command, the default command is used with your new arguments.

```
spec:
  containers:
    - name: test-java-service
      image: <your_image_name_here>
      command:
        - java
      args:
        - {{ .Values.TEST_JAVA_MEMORY_OPTS | quote }}
        - {{ .Values.TEST_JAVA_OPTS | quote }}
        - ""-jar""
        - java_j.jar
```

When Helm populates values, don't specify the quotes yourself, or else the values replacement string will be interpreted literally as that string.  Instead, pipe the Helm value to `quote`.  Place quotes around any value that could be interpreted specially in YAML, such as values with `-` characters, like your Java options.",2025-03-26T17:48:28,2025-03-26T13:39:18,"```yaml
spec:
  containers:
    - name: test-java-service
      image: <your_image_name_here>
      command:
        - java
      args:
        - {{ .Values.TEST_JAVA_MEMORY_OPTS | quote }}
        - {{ .Values.TEST_JAVA_OPTS | quote }}
        - ""-jar""
        - java_j.jar
```

According to the [Kubernetes docs](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/), split the command array and the arguments array into `command` and `args` sections.

> When you create a Pod, you can define a command and arguments for the containers that run in the Pod. To define a command, include the `command` field in the configuration file. To define arguments for the command, include the `args` field in the configuration file. The command and arguments that you define cannot be changed after the Pod is created.
>
>
> The command and arguments that you define in the configuration file override the default command and arguments provided by the container image. If you define args, but do not define a command, the default command is used with your new arguments.

When Helm populates values, don't specify the quotes yourself, or else the values replacement string will be interpreted literally as that string.  Instead, pipe the Helm value to `quote`.  Place quotes around any value that could be interpreted specially in YAML, such as values with `-` characters, like your Java options."
79536307,"How to expose ports in Minikube (Docker driver, Windows) without port-forward or minikube service?","Here’s your Stack Overflow question in English:

### How to expose ports in Minikube (Docker driver, Windows) without `port-forward` or `minikube service`?

I'm using Minikube with the Docker driver on Windows and want to expose ports for my services (React, ASP.NET API, MSSQL) **without manual commands** like `port-forward`, `tunnel`, `minikube service`, or `minikube start --ports`.

#### What I have:

- **Minikube (Docker driver) on Windows**
- **Applications running in Kubernetes:**
  - React (`NodePort 4200:30002`)
  - ASP.NET API (`NodePort 8084:30001`)
  - MSSQL (`NodePort 1433:30003`)
- **Requirement**: I want to expose ports **only through Kubernetes manifests** (Ingress, LoadBalancer, etc.), without running manual Minikube commands.

#### The problem:

According to the Kubernetes documentation:

> *""The network is limited if using the Docker driver on Darwin, Windows, or WSL, and the Node IP is not reachable directly.""*

This means I **cannot** simply rely on `NodePort` as I would on Linux.

#### The question:

How can I **automatically** expose ports in Minikube (Windows, Docker driver) using **only Kubernetes manifests**, without relying on `port-forward` or `minikube service`?

Additionally, how can I make this solution portable so that it works for both **development (Windows)** and **deployment (Linux)** environments?","docker, kubernetes, windows-subsystem-for-linux, kubernetes-ingress, minikube",79536765.0,"You can try exposing the service using a Kubernetes Service of type** NodePort** or **LoadBalancer**.

As per this GeeksforGeeks [document](https://www.geeksforgeeks.org/kubernetes-nodeport-service/).

NodePort service will expose the pods of one node to the other and also it will expose the pods to the outside of the cluster from where the users can access from the internet by using the IP address of node and port.

```
 apiVersion: v1
        kind: Service
        metadata:
          name: my-app-service
        spec:
          selector:
            app: my-app
          ports:
            - protocol: TCP
              port: 80
              targetPort: 8080
          type: LoadBalancer # or NodePort or Ingress
```

Also refer to this Minikube [Accessing apps](https://minikube.sigs.k8s.io/docs/handbook/accessing/) and official kubernetes document on [Set up Ingress on Minikube with the NGINX Ingress Controller](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/) for more information.",2025-03-26T17:12:38,2025-03-26T11:48:44,"```yaml
 apiVersion: v1
        kind: Service
        metadata:
          name: my-app-service
        spec:
          selector:
            app: my-app
          ports:
            - protocol: TCP
              port: 80
              targetPort: 8080
          type: LoadBalancer # or NodePort or Ingress
```

You can try exposing the service using a Kubernetes Service of type** NodePort** or **LoadBalancer**.

As per this GeeksforGeeks [document](https://www.geeksforgeeks.org/kubernetes-nodeport-service/).

NodePort service will expose the pods of one node to the other and also it will expose the pods to the outside of the cluster from where the users can access from the internet by using the IP address of node and port.

Also refer to this Minikube [Accessing apps](https://minikube.sigs.k8s.io/docs/handbook/accessing/) and official kubernetes document on [Set up Ingress on Minikube with the NGINX Ingress Controller](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/) for more information."
79533004,Issue in injecting environment variables during runtime in Vite + React,"I have created a react app using vite, where i have integrated azure sso, currently during local development I am utilizing the environment variables (client ID, tenantID, redirectURI) from .env.local file, which is working fine, but when I deploy it to Kubernetes the values are not getting burned into the variable during production, during local development in my main.jsx I am referencing the .env value using
`auth: { clientId: import.meta.env.VITE_APP_CLIENT_ID, authority: import.meta.env.VITE_APP_AUTHORITY, redirectUri: import.meta.env.VITE_APP_REDIRECT_URI, }`

for deployment I have a Kubernetes folder which contains dev folder (config-map.yml, micro-app.yml) containing the deployment code, in config-map.yml I have set the value of clientId,authority, redirectUri similar to .env file and the values are getting inserted into container, pods but not getting injected into the application's minified js, from R&D I understood that there is an issue with runtime injection.

any suggestions/method to solve this issue?","reactjs, kubernetes, deployment, vite",79534423.0,"Environment variables in files like .env.local (or injected via ConfigMaps) are only available at build time, not at runtime.

So even though your Kubernetes ConfigMap is correctly injecting the values into the container, your Vite app already had the environment variables ""burned in"" during the build step, which is why changes don’t reflect in the deployed JavaScript.

It happens because Vite replaces all import.meta.env.* values at build time.

To fix it:

Use a separate config.json (or similar) file that the app fetches at runtime, so you can inject values dynamically in Kubernetes.

1. Add a public/config.json file (empty placeholder for local dev):

`{ ""VITE_APP_CLIENT_ID"": ""local-client-id"", ""VITE_APP_AUTHORITY"": ""local-authority"",""VITE_APP_REDIRECT_URI"": ""http://localhost:3000""}`

1. In your app (e.g., main.jsx or a config loader):

const config = await fetch('/config.json').then(res => res.json());

```
const msalConfig = {
  auth: {
    clientId: config.VITE_APP_CLIENT_ID,
    authority: config.VITE_APP_AUTHORITY,
    redirectUri: config.VITE_APP_REDIRECT_URI,
  }
};
```

1. In your Kubernetes deployment:
Mount your dynamic config as a ConfigMap and inject it to /usr/share/nginx/html/config.json (or wherever your app is served).

```
volumeMounts:
  - name: config-volume
    mountPath: /usr/share/nginx/html/config.json
    subPath: config.json

volumes:
  - name: config-volume
    configMap:
      name: my-configmap
```",2025-03-25T17:23:19,2025-03-25T08:03:22,"`{ ""VITE_APP_CLIENT_ID"": ""local-client-id"", ""VITE_APP_AUTHORITY"": ""local-authority"",""VITE_APP_REDIRECT_URI"": ""http://localhost:3000""}`

Environment variables in files like .env.local (or injected via ConfigMaps) are only available at build time, not at runtime.

So even though your Kubernetes ConfigMap is correctly injecting the values into the container, your Vite app already had the environment variables ""burned in"" during the build step, which is why changes don’t reflect in the deployed JavaScript.

It happens because Vite replaces all import.meta.env.* values at build time.

To fix it:

Use a separate config.json (or similar) file that the app fetches at runtime, so you can inject values dynamically in Kubernetes.

1. Add a public/config.json file (empty placeholder for local dev):

1. In your app (e.g., main.jsx or a config loader):

```js
const config = await fetch('/config.json').then(res => res.json());
```

1. In your app (e.g., main.jsx or a config loader):

```js
const msalConfig = {
  auth: {
    clientId: config.VITE_APP_CLIENT_ID,
    authority: config.VITE_APP_AUTHORITY,
    redirectUri: config.VITE_APP_REDIRECT_URI,
  }
};
```

1. In your Kubernetes deployment:
Mount your dynamic config as a ConfigMap and inject it to /usr/share/nginx/html/config.json (or wherever your app is served).

```yaml
volumeMounts:
  - name: config-volume
    mountPath: /usr/share/nginx/html/config.json
    subPath: config.json

volumes:
  - name: config-volume
    configMap:
      name: my-configmap
```"
79528439,How to start a container in a kubernetes pod only after its proxy is running?,"I have a kubernetes cluster and a PostgreSQL Database running on Google Cloud.

The pod that has the problem is a cronjob with the following configuration:

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: taxiq-cronjob-reminder
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
    cloud.google.com/backend-config: '{""default"": ""taxiq-healthconfig""}'
spec:
  schedule: ""31 4 * * *""
  timeZone: ""Europe/Berlin""
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: taxiq-stage-gke
          initContainers:
            - image:  gcr.io/google.com/cloudsdktool/cloud-sdk:326.0.0-alpine
              name: workload-identity-initcontainer
              command:
              - '/bin/bash'
              - '-c'
              - ""curl -s -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1""
          containers:
          - name: cloud-sql-proxy
            image: gcr.io/cloudsql-docker/gce-proxy:1.33.5
            command:
              - ""/cloud_sql_proxy""
              - ""-instances=taxiq-stage-app:europe-west3:taxiq-stage=tcp:5432""
            securityContext:
              runAsNonRoot: true
          - name: taxiq-cronjob-reminder
            image: europe-west3-docker.pkg.dev/brantpoint-artifacts/taxiq/cronjob-reminder:beta09.7
            env:
              - name: PGURL
                valueFrom:
                  secretKeyRef:
                    name: secrets
                    key: PGURL
          restartPolicy: Never
```

I expect the proxy to start as well as the cronjob to run without errors.

But I get the following error:

```
failed to connect to `host=127.0.0.1 user=masterchief database=tasks_buildingblock`: dial error (dial tcp 127.0.0.1:5432: connect: connection refused)
```

I know this error is related to the cloud-sql-proxy not running (yet). I had removed it with `restartPolicy: OnFailure`.

But I can not use this restartPolicy value for the following reason:

The cronjob is supposed to send mails -> if I get a Failure for any other reason after some mails have already been sent, the cronjob will run again and send the mails multiple times, which might make customers/users unhappy

How can I ensure the cloud-sql-proxy is listening before the cronjob starts?","kubernetes, google-kubernetes-engine, kubernetes-cronjob, cloud-sql-proxy",79536576.0,"A mentioned in one of the comments, deploying the Cloud SQL Proxy using `initContainers` is the solution to start the Proxy as a [native sidecar](https://kubernetes.io/blog/2023/08/25/native-sidecar-containers/).

Also worth pointing out that you are using the old v1 Cloud SQL Proxy. It is recommended to [migrate to the new v2 Cloud SQL Proxy](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/main/migration-guide.md) to leverage both performance and reliability benefits.

There are examples of using the v2 Cloud SQL Proxy as a sidecar [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/tree/main/examples/k8s-sidecar) and example health check usage [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/b6ca9c52ca41dfd0ceaf5dce104a533410e6dfe0/examples/k8s-health-check/proxy_with_http_health_check.yaml#L129).

Your sample updated would look like the following:

```
apiVersion: batch/v1
kind: CronJob
metadata:
  name: taxiq-cronjob-reminder
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
    cloud.google.com/backend-config: '{""default"": ""taxiq-healthconfig""}'
spec:
  schedule: ""31 4 * * *""
  timeZone: ""Europe/Berlin""
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: taxiq-stage-gke
          initContainers:
            - image:  gcr.io/google.com/cloudsdktool/cloud-sdk:326.0.0-alpine
              name: workload-identity-initcontainer
              command:
              - '/bin/bash'
              - '-c'
              - ""curl -s -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1""
            - name: cloud-sql-proxy
              # v2 Cloud SQL Proxy image
              image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.15.2
              restartPolicy: Always
              env:
                - name: CSQL_PROXY_HEALTH_CHECK
                  value: ""true""
                - name: CSQL_PROXY_HTTP_PORT
                  value: ""9801""
                - name: CSQL_PROXY_HTTP_ADDRESS
                  value: 0.0.0.0
              startupProbe:
                failureThreshold: 60
                httpGet:
                  path: /startup
                  port: 9801
                  scheme: HTTP
                periodSeconds: 1
                successThreshold: 1
                timeoutSeconds: 10
              args:
                - ""--port=5432""
                - ""taxiq-stage-app:europe-west3:taxiq-stage""
              securityContext:
                runAsNonRoot: true
          containers:
          - name: taxiq-cronjob-reminder
            image: europe-west3-docker.pkg.dev/brantpoint-artifacts/taxiq/cronjob-reminder:beta09.7
            env:
              - name: PGURL
                valueFrom:
                  secretKeyRef:
                    name: secrets
                    key: PGURL
          restartPolicy: Never
```",2025-03-26T13:30:43,2025-03-23T04:17:51,"```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: taxiq-cronjob-reminder
  annotations:
    cloud.google.com/neg: '{""ingress"": true}'
    cloud.google.com/backend-config: '{""default"": ""taxiq-healthconfig""}'
spec:
  schedule: ""31 4 * * *""
  timeZone: ""Europe/Berlin""
  jobTemplate:
    spec:
      template:
        spec:
          serviceAccountName: taxiq-stage-gke
          initContainers:
            - image:  gcr.io/google.com/cloudsdktool/cloud-sdk:326.0.0-alpine
              name: workload-identity-initcontainer
              command:
              - '/bin/bash'
              - '-c'
              - ""curl -s -H 'Metadata-Flavor: Google' 'http://169.254.169.254/computeMetadata/v1/instance/service-accounts/default/token' --retry 30 --retry-connrefused --retry-max-time 30 > /dev/null || exit 1""
            - name: cloud-sql-proxy
              # v2 Cloud SQL Proxy image
              image: gcr.io/cloud-sql-connectors/cloud-sql-proxy:2.15.2
              restartPolicy: Always
              env:
                - name: CSQL_PROXY_HEALTH_CHECK
                  value: ""true""
                - name: CSQL_PROXY_HTTP_PORT
                  value: ""9801""
                - name: CSQL_PROXY_HTTP_ADDRESS
                  value: 0.0.0.0
              startupProbe:
                failureThreshold: 60
                httpGet:
                  path: /startup
                  port: 9801
                  scheme: HTTP
                periodSeconds: 1
                successThreshold: 1
                timeoutSeconds: 10
              args:
                - ""--port=5432""
                - ""taxiq-stage-app:europe-west3:taxiq-stage""
              securityContext:
                runAsNonRoot: true
          containers:
          - name: taxiq-cronjob-reminder
            image: europe-west3-docker.pkg.dev/brantpoint-artifacts/taxiq/cronjob-reminder:beta09.7
            env:
              - name: PGURL
                valueFrom:
                  secretKeyRef:
                    name: secrets
                    key: PGURL
          restartPolicy: Never
```

A mentioned in one of the comments, deploying the Cloud SQL Proxy using `initContainers` is the solution to start the Proxy as a [native sidecar](https://kubernetes.io/blog/2023/08/25/native-sidecar-containers/).

Also worth pointing out that you are using the old v1 Cloud SQL Proxy. It is recommended to [migrate to the new v2 Cloud SQL Proxy](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/main/migration-guide.md) to leverage both performance and reliability benefits.

There are examples of using the v2 Cloud SQL Proxy as a sidecar [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/tree/main/examples/k8s-sidecar) and example health check usage [here](https://github.com/GoogleCloudPlatform/cloud-sql-proxy/blob/b6ca9c52ca41dfd0ceaf5dce104a533410e6dfe0/examples/k8s-health-check/proxy_with_http_health_check.yaml#L129).

Your sample updated would look like the following:"
79526694,Airflow on Kubernetes with KubernetesExecutor only running one pod at a time,"I am running airflow on kubernetes with a `Chart.yaml` file:

```
apiVersion: v2
name: airflow
description: Umbrella chart for Airflow
type: application
version: 0.0.1
appVersion: ""2.1.2""
dependencies:
  - name: airflow
    alias: airflow
    version: 8.9.0
    repository: https://airflow-helm.github.io/charts
```

and a `values.yaml` file:

```
airflow:
  airflow:
    legacyCommands: false
    image:
      repository: apache/airflow
      tag: 2.8.4-python3.9
    executor: KubernetesExecutor
    fernetKey: ""7T512UXSSmBOkpWimFHIVb8jK6lfmSAvx4mO6Arehnc""
    webserverSecretKey: ""THIS IS UNSAFE!""
    config:
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: ""True""
      AIRFLOW__CORE__LOAD_EXAMPLES: ""True""
    users:
      - username: admin
        password: admin
        role: Admin
        email: tom.mclean@myemail.com
        firstName: admin
        lastName: admin
    connections: []
    variables: []
    pools: []
    extraPipPackages: []
    extraEnv: []
    extraVolumeMounts: []
    extraVolumes: []
    kubernetesPodTemplate:
      stringOverride: """"
      resources: {}
      extraPipPackages: []
      extraVolumeMounts: []
      extraVolumes: []
  scheduler:
    replicas: 1
    resources: {}
    logCleanup:
      enabled: true
      retentionMinutes: 21600
    livenessProbe:
      enabled: true
    taskCreationCheck:
      enabled: false
      thresholdSeconds: 300
      schedulerAgeBeforeCheck: 180
  web:
    replicas: 1
    resources: {}
    service:
      type: ClusterIP
      externalPort: 8080
    webserverConfig:
      stringOverride: |
        from airflow import configuration as conf
        from flask_appbuilder.security.manager import AUTH_DB

        # the SQLAlchemy connection string
        SQLALCHEMY_DATABASE_URI = conf.get(""core"", ""SQL_ALCHEMY_CONN"")

        # use embedded DB for auth
        AUTH_TYPE = AUTH_DB
      existingSecret: """"

  workers:
    enabled: false

  triggerer:
    enabled: true
    replicas: 1
    resources: {}
    capacity: 1000

  flower:
    enabled: false

  logs:
    path: /opt/airflow/logs
    persistence:
      enabled: false

  dags:
    path: /opt/airflow/dags
    persistence:
      enabled: false
    gitSync:
      enabled: true
      repo: ""https://tom.mclean:mypassword@dev.azure.com/MyOrg/MyOrg/_git/Airflow""
      branch: ""main""
      revision: ""HEAD""
      syncWait: 60
      depth: 1
      repoSubPath: ""dags""
      cloneDepth: 1

      httpSecret: ""airflow-http-git-secret""
      httpSecretUsernameKey: username
      httpSecretPasswordKey: password

  ingress:
    enabled: true

    web:
      host: airflow.mydomain.com
      annotations:
        kubernetes.io/ingress.class: alb
        alb.ingress.kubernetes.io/group.name: grafana
        alb.ingress.kubernetes.io/listen-ports: '[{""HTTP"": 80}, {""HTTPS"":443}]'
        alb.ingress.kubernetes.io/scheme: internet-facing
        alb.ingress.kubernetes.io/ssl-redirect: '443'
        alb.ingress.kubernetes.io/target-type: ip

  serviceAccount:
    create: true
    name: """"
    annotations: {}

  extraManifests: []

  pgbouncer:
    enabled: true
    resources: {}
    authType: md5

  postgresql:
    enabled: true
    persistence:
      enabled: true
      storagClass: """"
      size: 8Gi

  externalDatabase:
    type: postgres

  redis:
    enabled: false

  externalRedis:
    host: localhost
```

I then tried to run a job which had parallel tasks:

```
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import random
import time

def heavy_computation(task_number):
    """"""Simulates a computationally heavy task.""""""
    sleep_time = random.uniform(0, 1)  # Simulate varying computation times
    time.sleep(sleep_time)
    print(f""Task {task_number} completed after {sleep_time:.2f} seconds"")

# Define default args
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2024, 3, 21),
    'retries': 0,
}

# Define the DAG
with DAG(
    'parallel_computation_dag',
    default_args=default_args,
    schedule_interval=None,  # Manual trigger
    catchup=False,
    max_active_tasks=10,  # Allow multiple tasks to run in parallel
) as dag:

    tasks = [
        PythonOperator(
            task_id=f'heavy_task_{i}',
            python_callable=heavy_computation,
            op_kwargs={'task_number': i},
        ) for i in range(20)  # Creates 20 parallel tasks
    ]
```

However, only a single pod would run at a time, so the jobs would not run in parallel. Is there a way to change the config to allow multiple pods to run at the same time?

Thanks.","kubernetes, airflow, kubernetes-helm",79530847.0,"I suppose you use this [helm chart](https://airflow.apache.org/docs/helm-chart/stable/index.html)?
If so have a look at the [Parameter Reference](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html).
There are two specific values that set the replica count:

- [scheduler.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#scheduler): Airflow 2.0 allows users to run multiple schedulers. This feature is only recommended for MySQL 8+ and PostgreSQL
- [workers.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#workers): Number of Airflow Celery workers in StatefulSet.

As you want to run jobs in parallel, enabling workers and setting `replica` to >=2 deploys two workers so jobs can run in parallel.

Updated `values.yaml`:

```
airflow:
  workers:
    enabled: true
    replica: 2
```",2025-03-24T10:43:27,2025-03-21T22:48:05,"```yaml
airflow:
  workers:
    enabled: true
    replica: 2
```

I suppose you use this [helm chart](https://airflow.apache.org/docs/helm-chart/stable/index.html)?
If so have a look at the [Parameter Reference](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html).
There are two specific values that set the replica count:

- [scheduler.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#scheduler): Airflow 2.0 allows users to run multiple schedulers. This feature is only recommended for MySQL 8+ and PostgreSQL
- [workers.replicas](https://airflow.apache.org/docs/helm-chart/stable/parameters-ref.html#workers): Number of Airflow Celery workers in StatefulSet.

As you want to run jobs in parallel, enabling workers and setting `replica` to >=2 deploys two workers so jobs can run in parallel.

Updated `values.yaml`:"
79523213,NGINX reload triggered due to a change in configuration,"System :

- Ubuntu-24.04-noble-amd64

K8s :

- Client Version: v1.31.2
- Kustomize Version: v5.4.2
- Server Version: v1.31.7

ingress-nginx

- install with ""registry.k8s.io/ingress-nginx/controller:v1.12.0""
- version 1.12.0

My Ingress-nginx-controller always reload :
""NGINX reload triggered due to a change in configuration""

The current deployment file :

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
  uid: 7af65198-0f4a-4b53-9d48-b2c6f37ccddb
  resourceVersion: '195468'
  generation: 33
  creationTimestamp: '2025-03-19T10:12:31Z'
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.12.0
    k8slens-edit-resource-version: v1
  annotations:
    deployment.kubernetes.io/revision: '33'
    kubectl.kubernetes.io/last-applied-configuration: >
      {""apiVersion"":""apps/v1"",""kind"":""Deployment"",""metadata"":{""annotations"":{},""labels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx"",""app.kubernetes.io/part-of"":""ingress-nginx"",""app.kubernetes.io/version"":""1.12.0""},""name"":""ingress-nginx-controller"",""namespace"":""ingress-nginx""},""spec"":{""minReadySeconds"":0,""revisionHistoryLimit"":10,""selector"":{""matchLabels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx""}},""strategy"":{""rollingUpdate"":{""maxUnavailable"":1},""type"":""RollingUpdate""},""template"":{""metadata"":{""labels"":{""app.kubernetes.io/component"":""controller"",""app.kubernetes.io/instance"":""ingress-nginx"",""app.kubernetes.io/name"":""ingress-nginx"",""app.kubernetes.io/part-of"":""ingress-nginx"",""app.kubernetes.io/version"":""1.12.0""}},""spec"":{""containers"":[{""args"":[""/nginx-ingress-controller"",""--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller"",""--election-id=ingress-nginx-leader"",""--controller-class=k8s.io/ingress-nginx"",""--ingress-class=nginx"",""--configmap=$(POD_NAMESPACE)/ingress-nginx-controller"",""--validating-webhook=:8443"",""--validating-webhook-certificate=/usr/local/certificates/cert"",""--validating-webhook-key=/usr/local/certificates/key""],""env"":[{""name"":""POD_NAME"",""valueFrom"":{""fieldRef"":{""fieldPath"":""metadata.name""}}},{""name"":""POD_NAMESPACE"",""valueFrom"":{""fieldRef"":{""fieldPath"":""metadata.namespace""}}},{""name"":""LD_PRELOAD"",""value"":""/usr/local/lib/libmimalloc.so""}],""image"":""registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa"",""imagePullPolicy"":""IfNotPresent"",""lifecycle"":{""preStop"":{""exec"":{""command"":[""/wait-shutdown""]}}},""livenessProbe"":{""failureThreshold"":5,""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""periodSeconds"":10,""successThreshold"":1,""timeoutSeconds"":1},""name"":""controller"",""ports"":[{""containerPort"":80,""name"":""http"",""protocol"":""TCP""},{""containerPort"":443,""name"":""https"",""protocol"":""TCP""},{""containerPort"":8443,""name"":""webhook"",""protocol"":""TCP""}],""readinessProbe"":{""failureThreshold"":3,""httpGet"":{""path"":""/healthz"",""port"":10254,""scheme"":""HTTP""},""initialDelaySeconds"":10,""periodSeconds"":10,""successThreshold"":1,""timeoutSeconds"":1},""resources"":{""requests"":{""cpu"":""100m"",""memory"":""90Mi""}},""securityContext"":{""allowPrivilegeEscalation"":false,""capabilities"":{""add"":[""NET_BIND_SERVICE""],""drop"":[""ALL""]},""readOnlyRootFilesystem"":false,""runAsGroup"":82,""runAsNonRoot"":true,""runAsUser"":101,""seccompProfile"":{""type"":""RuntimeDefault""}},""volumeMounts"":[{""mountPath"":""/usr/local/certificates/"",""name"":""webhook-cert"",""readOnly"":true}]}],""dnsPolicy"":""ClusterFirst"",""nodeSelector"":{""kubernetes.io/os"":""linux""},""serviceAccountName"":""ingress-nginx"",""terminationGracePeriodSeconds"":300,""volumes"":[{""name"":""webhook-cert"",""secret"":{""secretName"":""ingress-nginx-admission""}}]}}}}
  selfLink: /apis/apps/v1/namespaces/ingress-nginx/deployments/ingress-nginx-controller
status:
  observedGeneration: 33
  replicas: 1
  updatedReplicas: 1
  unavailableReplicas: 1
  conditions:
    - type: Available
      status: 'True'
      lastUpdateTime: '2025-03-19T10:12:31Z'
      lastTransitionTime: '2025-03-19T10:12:31Z'
      reason: MinimumReplicasAvailable
      message: Deployment has minimum availability.
    - type: Progressing
      status: 'True'
      lastUpdateTime: '2025-03-20T10:46:08Z'
      lastTransitionTime: '2025-03-19T10:12:31Z'
      reason: NewReplicaSetAvailable
      message: >-
        ReplicaSet ""ingress-nginx-controller-8584ffb585"" has successfully
        progressed.
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      creationTimestamp: null
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/part-of: ingress-nginx
        app.kubernetes.io/version: 1.12.0
      annotations:
        kubectl.kubernetes.io/restartedAt: '2025-03-20T10:46:06Z'
    spec:
      volumes:
        - name: webhook-cert
          secret:
            secretName: ingress-nginx-admission
            defaultMode: 420
      containers:
        - name: controller
          image: >-
            registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
          args:
            - /nginx-ingress-controller
            - '--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller'
            - '--election-id=ingress-nginx-leader'
            - '--controller-class=k8s.io/ingress-nginx'
            - '--ingress-class=nginx'
            - '--configmap=$(POD_NAMESPACE)/ingress-nginx-controller'
            - '--validating-webhook=:8443'
            - '--validating-webhook-certificate=/usr/local/certificates/cert'
            - '--validating-webhook-key=/usr/local/certificates/key'
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
            - name: https
              containerPort: 443
              protocol: TCP
            - name: webhook
              containerPort: 8443
              protocol: TCP
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: LD_PRELOAD
              value: /usr/local/lib/libmimalloc.so
          resources:
            requests:
              cpu: 500m
              memory: 450Mi
          lifecycle:
            preStop:
              exec:
                command:
                  - /wait-shutdown
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
          imagePullPolicy: IfNotPresent
          securityContext:
            capabilities:
              add:
                - NET_BIND_SERVICE
              drop:
                - ALL
            runAsUser: 101
            runAsGroup: 82
            runAsNonRoot: true
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
            seccompProfile:
              type: RuntimeDefault
      restartPolicy: Always
      terminationGracePeriodSeconds: 300
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      serviceAccount: ingress-nginx
      securityContext: {}
      schedulerName: default-scheduler
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 25%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600
```

I can add file on demande.

I try to :

- change path to stock ssl keys
- restart auto certification
- restart deployment
- change allocation (cpu/memory)

I don't want helm install.","kubernetes, yaml, kubernetes-ingress, ubuntu-24.04",79526354.0,"By default, [reloading NGINX is necessary](https://kubernetes.github.io/ingress-nginx/how-it-works/#when-a-reload-is-required:%7E:text=The%20main%20implication%20of%20this%20requirement%20is%20the%20need%20to%20reload%20NGINX%20after%20any%20change%20in%20the%20configuration%20file.) after any configuration file changes. However, in certain situations, [reloads can be avoided](https://kubernetes.github.io/ingress-nginx/how-it-works/#avoiding-reloads), especially when there are changes to the endpoints, such as when a pod is started or replaced.

Since you’ve already tried a multiple of troubleshooting steps, and your ingress-nginx controller still constantly reloads, I suggest you try to identify the main cause of configuration changes that keeps triggering the reloads by carefully reviewing your logs:

First, increase your logging detail by editing the Ingress-Nginx Controller Deployment. Add the `-v `or `--v` flag to [increase verbosity](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#log-verbosity-level):

```
containers:

name: controller
image: registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
       args:
       /nginx-ingress-controller
       '--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--election-id=ingress-nginx-leader'
       '--controller-class=k8s.io/ingress-nginx'
       '--ingress-class=nginx'
       '--configmap=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--validating-webhook=:8443'
       '--validating-webhook-certificate=/usr/local/certificates/cert'
       '--validating-webhook-key=/usr/local/certificates/key'
       '-v=3' # Add this line
```

Second, check the controller logs by using:

```
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -f --all-containers
```

Look for messages before the “**NGINX reload triggered...**"" message. These messages will indicate which resource was modified.

Lastly, use the kubectl logs command with grep to filter the logs for relevant events:

```
kubectl logs -f ingress-nginx-controller-8584ffb585-abcd1 -n ingress-nginx | grep -E ""(Ingress|ConfigMap|Secret|update|change|reload)""
```

Once you are able to identify which resource changes are causing the reloads when you aren't explicitly making a change, it will help you take the appropriate action.

In addition, you might want to check this [thread](https://github.com/kubernetes/ingress-nginx/issues/10448#issuecomment-2350911143) for a possible workaround to your case.",2025-03-21T18:37:55,2025-03-20T14:13:00,"```yaml
containers:

name: controller
image: registry.k8s.io/ingress-nginx/controller:v1.12.0@sha256:e6b8de175acda6ca913891f0f727bca4527e797d52688cbe9fec9040d6f6b6fa
       args:
       /nginx-ingress-controller
       '--publish-service=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--election-id=ingress-nginx-leader'
       '--controller-class=k8s.io/ingress-nginx'
       '--ingress-class=nginx'
       '--configmap=$(POD_NAMESPACE)/ingress-nginx-controller'
       '--validating-webhook=:8443'
       '--validating-webhook-certificate=/usr/local/certificates/cert'
       '--validating-webhook-key=/usr/local/certificates/key'
       '-v=3' # Add this line
```

By default, [reloading NGINX is necessary](https://kubernetes.github.io/ingress-nginx/how-it-works/#when-a-reload-is-required:%7E:text=The%20main%20implication%20of%20this%20requirement%20is%20the%20need%20to%20reload%20NGINX%20after%20any%20change%20in%20the%20configuration%20file.) after any configuration file changes. However, in certain situations, [reloads can be avoided](https://kubernetes.github.io/ingress-nginx/how-it-works/#avoiding-reloads), especially when there are changes to the endpoints, such as when a pod is started or replaced.

Since you’ve already tried a multiple of troubleshooting steps, and your ingress-nginx controller still constantly reloads, I suggest you try to identify the main cause of configuration changes that keeps triggering the reloads by carefully reviewing your logs:

First, increase your logging detail by editing the Ingress-Nginx Controller Deployment. Add the `-v `or `--v` flag to [increase verbosity](https://kubernetes.io/docs/concepts/cluster-administration/system-logs/#log-verbosity-level):

---

```bash
kubectl logs -n ingress-nginx -l app.kubernetes.io/name=ingress-nginx -f --all-containers
```

Second, check the controller logs by using:

Look for messages before the “**NGINX reload triggered...**"" message. These messages will indicate which resource was modified.

---

```bash
kubectl logs -f ingress-nginx-controller-8584ffb585-abcd1 -n ingress-nginx | grep -E ""(Ingress|ConfigMap|Secret|update|change|reload)""
```

Lastly, use the kubectl logs command with grep to filter the logs for relevant events:

Once you are able to identify which resource changes are causing the reloads when you aren't explicitly making a change, it will help you take the appropriate action.

In addition, you might want to check this [thread](https://github.com/kubernetes/ingress-nginx/issues/10448#issuecomment-2350911143) for a possible workaround to your case."
79519594,Helm - Install WordPress Plugins from a Local Directory,"I'm seeking your help with a Helm-related issue (Kubernetes package manager).

I wanted to use Helm to deploy an instance of:

- WordPress
- MariaDB

I find it to be a very useful tool!

Specifically, I’m interested in how to declare a custom local path where `.zip` files of certain WordPress plugins are stored. These plugins should be installed during the Helm installation process.

Currently, in the `values.yaml` file (which centralizes the configuration for WP/MariaDB), there is a parameter called `wordpressPlugins` that allows plugin installation in two ways:

- Declaring the plugin name, which Helm then downloads from WordPress.org (requires internet access).
- Providing the URL of a public repository on GitHub.

I would like to know how to reference a local path instead. Any guidance would be greatly appreciated!

The goal is to automatically download and install WordPress plugins without requiring an internet connection.

I attempted to achieve this using the following approach, but I encountered a failure:

- I downloaded WordPress from Bitnami.
- Inside the downloaded WordPress directory, I created a folder named `plugins`.
- I placed the `.zip` files of the plugins I had downloaded into this folder.
- In the `values.yaml` file, under `wordpressPlugins`, I specified the path to the folder containing the `.zip` plugin files.
- I verified the read and write permissions of the folder.

After deploying everything, the WordPress pod enters a **CrashLoopBackOff** state.

The issue is caused by this approach because, if I hadn't specified anything in `wordpressPlugins`, WordPress would have started without any problems. So, the **CrashLoopBackOff** is triggered by an incorrect instruction.

There isn't much information available online about this. If you have any knowledge or experience with this, please share it with me. I would really appreciate it. Thank you!","wordpress, kubernetes, kubernetes-helm",79519800.0,"There is 2 way you can add custom path:

**Using Helm Values File:**

Add the custom path for plugin

```
wordpress:
  plugins:
    customPluginsPath: /path/to/custom/plugins/
```

**Using Config File:**

Step1:  Create the K8S for your Plugin

If you have `.zip` files for WordPress plugins stored locally, you can create a ConfigMap to store them in Kubernetes:

`kubectl create configmap wordpress-plugins --from-file=/path/to/custom/plugins/`

Step2:   Mount the ConfigMap to the WordPress Pod :

```
 extraVolumes:
  - name: plugins-volume
    configMap:
      name: wordpress-plugins  # Name of the ConfigMap

extraVolumeMounts:
  - name: plugins-volume
    mountPath: /var/www/html/wp-content/plugins  # Mount the plugins at the correct directory inside WordPress container
```

You may also want to run a `post-install` script to unzip the plugin files after WordPress is deployed. This can be achieved using an `initContainer`:

```
initContainers:
  - name: unzip-plugins
    image: busybox
    command: [""sh"", ""-c"", ""unzip /plugins/*.zip -d /var/www/html/wp-content/plugins""]
    volumeMounts:
      - name: plugins-volume
        mountPath: /plugins
      - name: wordpress-volume
        mountPath: /var/www/html/wp-content/plugins
```

Install Wordpress using helm chart :

`helm upgrade --install my-wordpress bitnami/wordpress -f values-wordpress.yaml`",2025-03-19T09:52:14,2025-03-19T08:40:47,"```yaml
wordpress:
  plugins:
    customPluginsPath: /path/to/custom/plugins/
```

There is 2 way you can add custom path:

**Using Helm Values File:**

Add the custom path for plugin

---

`kubectl create configmap wordpress-plugins --from-file=/path/to/custom/plugins/`

**Using Config File:**

Step1:  Create the K8S for your Plugin

If you have `.zip` files for WordPress plugins stored locally, you can create a ConfigMap to store them in Kubernetes:

---

```yaml
 extraVolumes:
  - name: plugins-volume
    configMap:
      name: wordpress-plugins  # Name of the ConfigMap

extraVolumeMounts:
  - name: plugins-volume
    mountPath: /var/www/html/wp-content/plugins  # Mount the plugins at the correct directory inside WordPress container
```

Step2:   Mount the ConfigMap to the WordPress Pod :

---

```yaml
initContainers:
  - name: unzip-plugins
    image: busybox
    command: [""sh"", ""-c"", ""unzip /plugins/*.zip -d /var/www/html/wp-content/plugins""]
    volumeMounts:
      - name: plugins-volume
        mountPath: /plugins
      - name: wordpress-volume
        mountPath: /var/www/html/wp-content/plugins
```

You may also want to run a `post-install` script to unzip the plugin files after WordPress is deployed. This can be achieved using an `initContainer`:

---

`helm upgrade --install my-wordpress bitnami/wordpress -f values-wordpress.yaml`

Install Wordpress using helm chart :"
79518543,How to expose resources under /.well-known/ with K8s?,"I need to expose some resources under `https://app.my-domain.net/.well-known/` using Kubernetes (Android `assetlinks.json` and `apple-app-site-association`).

These resources are packaged in a Nginx container. I created a K8s deployment, a K8s service, and tried the following ingress:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/app-root: /ui/
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - app.my-domain.net
      secretName: app-tls
  rules:
  - host: app.my-domain.net
    http:
      paths:
      - path: /.well-known
        pathType: Prefix
        backend:
          service:
            name: well-known-static-resources
            port:
              number: 80
```

But I got: `Warning: path /.well-known cannot be used with pathType Prefix`.

Reading the docs, [the dot in `/.well-known` seems incompatible with ingress path validation](https://kubernetes.github.io/ingress-nginx/faq/#validation-of-path).

But then, how should I route requests to the service for my `.well-known` resources? Or is there a better way to expose `.well-known` resources using K8s than ingress -> service -> pod -> Nginx container?","kubernetes, kubernetes-ingress, nginx-ingress",79518955.0,"I finally found a working solution with `pathType: ImplementationSpecific`.

Here is the modified yaml with usage of regexp and path rewrite I hadn't yet in the question, but now use on some other path:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/app-root: /ui/
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - app.my-domain.net
      secretName: app-tls
  rules:
  - host: app.my-domain.net
    http:
      paths:
      - path: /(\.well-known/.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: well-known-static-resources
            port:
              number: 80
```",2025-03-19T01:56:54,2025-03-18T19:59:13,"```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-production
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/app-root: /ui/
  name: app-ingress
  namespace: app
spec:
  ingressClassName: nginx
  tls:
    - hosts:
      - app.my-domain.net
      secretName: app-tls
  rules:
  - host: app.my-domain.net
    http:
      paths:
      - path: /(\.well-known/.*)
        pathType: ImplementationSpecific
        backend:
          service:
            name: well-known-static-resources
            port:
              number: 80
```

I finally found a working solution with `pathType: ImplementationSpecific`.

Here is the modified yaml with usage of regexp and path rewrite I hadn't yet in the question, but now use on some other path:"
79518515,otel-collector to scrap multiple pods,"I would like to use otel-collector to scrap multiple business pods.

This solution is already working for one (but just one) pod:

```
kubectl get pods

mycoolbusinesspod-7b4f8f4c4f-74sqq 1/1 Running 0
```

The concept is that there is one business pod which generates the metrics.

I expose a Kubernetes service svc for this business pod.

The collector is configured to scrap this only one endpoint and path.

```
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-conf
  labels:
    app: opentelemetry
    component: otel-collector-conf
data:
  otel-collector-config: |
    receivers:
      prometheus:
        config:
          scrape_configs:
            - job_name: ""jobname""
              scrape_interval: 5s
              metrics_path: '/actuator/prometheus'
              static_configs:
                - targets: [""mycoolbusinesspod-svc:8080""]

    processors:
      batch:

    exporters:
      prometheus:
        endpoint: ""localhost:8889""

    service:
      pipelines:
        metrics:
          receivers: [prometheus]
          processors: [batch]
          exporters: [prometheus]
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  labels:
    app: opentelemetry
    component: otel-collector
spec:
  selector:
    matchLabels:
      app: opentelemetry
      component: otel-collector
  minReadySeconds: 5
  progressDeadlineSeconds: 120
  replicas: 1
  template:
    metadata:
      labels:
        app: opentelemetry
        component: otel-collector
    spec:
      containers:
        - command:
            - ""/otelcol""
            - ""--config=/conf/otel-collector-config.yaml""
          image: otel/opentelemetry-collector:latest
          name: otel-collector
          resources:
            limits:
              cpu: 1
              memory: 2Gi
            requests:
              cpu: 200m
              memory: 400Mi
          ports:
            - containerPort: 55679 # Default endpoint for ZPages.
            - containerPort: 4317 # Default endpoint for OpenTelemetry receiver.
            - containerPort: 14250 # Default endpoint for Jaeger gRPC receiver.
            - containerPort: 14268 # Default endpoint for Jaeger HTTP receiver.
            - containerPort: 9411 # Default endpoint for Zipkin receiver.
            - containerPort: 8888  # Default endpoint for querying metrics.
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: GOMEMLIMIT
              value: 1600MiB
          volumeMounts:
            - name: otel-collector-config-vol
              mountPath: /conf
```

This is working, I can see the metrics.

However, now, I have multiple replicas.

```
kubectl get pods

mycoolbusinesspod-7b4f8f4c4f-lrphj 1/1 Running 0
mycoolbusinesspod-7b4f8f4c4f-n7v9h 1/1 Running 0
mycoolbusinesspod-7b4f8f4c4f-jght9 1/1 Running 0
```

Attempt 1:

I see the configuration accepts an array for ""target"". But I do not know what to input in the array.

Attempt 2:

I do some kind of load balancer as service to load balance between the multiple pods.

However, this will not get the metrics from the pods that were not load balanced.

How to configure Otel-collector to scrap metrics for all the business pods?","kubernetes, open-telemetry-collector, otel, otel-agent",79519046.0,"> I do some kind of load balancer as service to load balance between the multiple pods.
> However, this will not get the metrics from the pods that were not load balanced.

Since you're looking to scrape configuration on every pods, I think setting pod's annotation and use Prometheus [kubernetes service discovery](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#kubernetes_sd_config) to scrape pods metrics directly should be the way to go in microservices deployment.

If you'd scrape through k8s ClusterIP service the scrape request will get randomly distributed to every pods you deploy, which is undesirable since we want to know metrics of every pods.

I believe otel collector support this feature out of the box. [ref](https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/prometheusreceiver)

Sample configuration could be like this:

```
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
```

this configuration will make otel collector scrape metrics from any pods with annotations:

```
  prometheus.io/scrape: ""true""
  prometheus.io/path: ""/metrics""
  prometheus.io/port: ""8080""
```

while keeping `kubernetes_pod_name` and `kubernetes_namespace` value as metadata in labels.

You might need to set appropriate ClusterRole for your otel-collector ServiceAccount though.

something like:

```
rules:
- apiGroups: [""""]
  resources:
  - pods
  verbs: [""get"", ""list"", ""watch""]
- nonResourceURLs: [""/metrics""]
  verbs: [""get""]
```

should suffice.",2025-03-19T03:20:33,2025-03-18T19:38:47,"```text
I do some kind of load balancer as service to load balance between the multiple pods.
However, this will not get the metrics from the pods that were not load balanced.

Since you're looking to scrape configuration on every pods, I think setting pod's annotation and use Prometheus kubernetes service discovery to scrape pods metrics directly should be the way to go in microservices deployment.

If you'd scrape through k8s ClusterIP service the scrape request will get randomly distributed to every pods you deploy, which is undesirable since we want to know metrics of every pods.

I believe otel collector support this feature out of the box. ref

Sample configuration could be like this:
```

```yaml
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
        - role: pod
        relabel_configs:
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          action: keep
          regex: true
        - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
          action: replace
          target_label: __metrics_path__
          regex: (.+)
        - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
          action: replace
          regex: ([^:]+)(?::\d+)?;(\d+)
          replacement: $1:$2
          target_label: __address__
        - action: labelmap
          regex: __meta_kubernetes_pod_label_(.+)
        - source_labels: [__meta_kubernetes_namespace]
          action: replace
          target_label: kubernetes_namespace
        - source_labels: [__meta_kubernetes_pod_name]
          action: replace
          target_label: kubernetes_pod_name
```

```text
this configuration will make otel collector scrape metrics from any pods with annotations:
```

```yaml
  prometheus.io/scrape: ""true""
  prometheus.io/path: ""/metrics""
  prometheus.io/port: ""8080""
```

```text
while keeping kubernetes_pod_name and kubernetes_namespace value as metadata in labels.

You might need to set appropriate ClusterRole for your otel-collector ServiceAccount though.

something like:
```

```yaml
rules:
- apiGroups: [""""]
  resources:
  - pods
  verbs: [""get"", ""list"", ""watch""]
- nonResourceURLs: [""/metrics""]
  verbs: [""get""]
```

```text
should suffice.
```"
79516630,Using Promethus adapter as custom metrics server for HPA autoscaling,"I am trying to setup and use the Prometheus server and Prometheus adapter integration to replace the metrics-server in the local kubernetes cluster (built using kind) and use it to scale my HPA based on custom metrics.

I have 2 Promethus pod instances and 1 prometheus adapter deployed and running in the 'monitoring' namespace.

The Spring boot application deployment (to be scaled by HPA) is deployed and running in 'demo-config-app' namespace.

**Problem**: HPA (Horizontal Pod Autoscaler) is simply not able to fetch metrics from prometheus adapter which I intent to use as a replacement for K8S metrics-server.

Custom metrics query configured an Prometheus adapter ConfigMap is,

```
rules:
    - seriesQuery: 'http_server_requests_seconds_count{namespace!="""", service != """", uri = ""/""}'
      resources:
        overrides:
          namespace: {resource: ""namespace""}
          service: {resource: ""service""}
      name:
        matches: ""http_server_requests_seconds_count""
        as: ""http_server_requests_seconds_count""
      metricsQuery: sum(rate(<<.Series>>{<<.LabelMatchers>>,uri!~""/actuator/.*""}[15m]))
```

HPA Yaml manifest is as follows :

```
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2
metadata:
  name: demo-config-app
  namespace: dynamic-secrets-ns
spec:
  scaleTargetRef:
    # point the HPA at the sample application
    # you created above
    apiVersion: apps/v1
    kind: Deployment
    name: demo-config-watcher
  # autoscale between 1 and 10 replicas
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      metric:
        name: http_server_requests_seconds_count
      describedObject:
        apiVersion: v1
        kind: Service
        name: demo-config-watcher-svc-internal
      target:
        type: AverageValue
        averageValue: 10
```

Custom metrics, seems to have been correctly configured.
Executing the kubectl command,

```
    $ kubectl get --raw ""/apis/custom.metrics.k8s.io/v1beta2"" | jq

    OUTPUT:
        {
          ""kind"": ""APIResourceList"",
          ""apiVersion"": ""v1"",
          ""groupVersion"": ""custom.metrics.k8s.io/v1beta2"",
          ""resources"": [
            {
              ""name"": ""namespaces/http_server_requests_seconds_count"",
              ""singularName"": """",
              ""namespaced"": false,
              ""kind"": ""MetricValueList"",
              ""verbs"": [
                ""get""
              ]
            },
            {
              ""name"": ""services/http_server_requests_seconds_count"",
              ""singularName"": """",
              ""namespaced"": true,
              ""kind"": ""MetricValueList"",
              ""verbs"": [
                ""get""
              ]
            }
          ]
        }

```

Also When I execute the metrics query in prometheus console,

```
    sum(rate(http_server_requests_seconds_count{namespace=""dynamic-secrets-ns"",service=""demo-config-watcher-svc-internal"",uri!~""/actuator/.*""}[15m]))
```

I get an aggregated response value - 3.1471300541724765

***Following are the few points from my analysis of adapter logs :***

1. As soon as, Promethus adapter pod starts-up, it fires the following query,

```
http://prometheus-k8s.monitoring.svc:9090/api/v1/series?match%5B%5D=http_server_requests_seconds_count%7Bnamespace%21%3D%22%22%2C+service+%21%3D+%22%22%2C+uri+%3D+%22%2F%22%7D&start=1742277149.166
```

I tried executing the same query from an nginx pod in the same namespace as that of prometheus-adater (with the same ServiceAccount) and it gives me the following results:

```
{
   ""status"":""success"",
   ""data"":[
      {
         ""__name__"":""http_server_requests_seconds_count"",
         ""container"":""demo-config-watcher"",
         ""endpoint"":""http-internal"",
         ""error"":""none"",
         ""exception"":""none"",
         ""instance"":""10.244.2.104:8080"",
         ""job"":""demo-config-watcher-job"",
         ""method"":""GET"",
         ""namespace"":""dynamic-secrets-ns"",
         ""outcome"":""SUCCESS"",
         ""pod"":""demo-config-watcher-7dbb9b598b-k7cgj"",
         ""service"":""demo-config-watcher-svc-internal"",
         ""status"":""200"",
         ""uri"":""/""
      }
   ]
}
```

1. By increasing the verbosity of prometheus adapter logs, I can see following requests being repeatedly appearing in the log.
Not sure about the first GET request, where it is coming from.
The second request is clearly coming from HPA controller and it results in HTTP status 404. Not sure why ?

```
I0318 06:31:39.832124       1 round_trippers.go:553] POST https://10.96.0.1:443/apis/authorization.k8s.io/v1/subjectaccessreviews?timeout=10s 201 Created in 1 milliseconds
I0318 06:31:39.832343       1 handler.go:143] prometheus-metrics-adapter: GET ""/apis/custom.metrics.k8s.io/v1beta2/namespaces/dynamic-secrets-ns/services/demo-config-watcher-svc-internal/http_server_requests_seconds_count"" satisfied by gorestful with webservice /apis/custom.metrics.k8s.io
I0318 06:31:39.833331       1 api.go:88] GET http://prometheus-k8s.monitoring.svc:9090/api/v1/query?query=sum%28rate%28http_server_requests_seconds_count%7Bnamespace%3D%22dynamic-secrets-ns%22%2Cservice%3D%22demo-config-watcher-svc-internal%22%2Curi%21~%22%2Factuator%2F.%2A%22%7D%5B15m%5D%29%29&time=1742279499.832&timeout= 200 OK
E0318 06:31:39.833494       1 provider.go:186] None of the results returned by when fetching metric services/http_server_requests_seconds_count(namespaced) for ""dynamic-secrets-ns/demo-config-watcher-svc-internal"" matched the resource name
I0318 06:31:39.833600       1 httplog.go:132] ""HTTP"" verb=""GET"" URI=""/apis/custom.metrics.k8s.io/v1beta2/namespaces/dynamic-secrets-ns/services/demo-config-watcher-svc-internal/http_server_requests_seconds_count"" latency=""2.926569ms"" userAgent=""kube-controller-manager/v1.32.0 (linux/amd64) kubernetes/70d3cc9/system:serviceaccount:kube-system:horizontal-pod-autoscaler"" audit-ID=""8f71b62a-92bc-4f13-a409-01ec5b778429"" srcIP=""172.18.0.3:34574"" resp=404
```

HPA has following RBAC permissions configured,

```
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: ""2025-03-16T05:47:45Z""
  name: custom-metrics-getter
  resourceVersion: ""6381614""
  uid: 04106c39-be1f-4ee3-b2ab-cf863ef43aca
rules:
- apiGroups:
  - custom.metrics.k8s.io
  resources:
  - '*'
  verbs:
  - '*'

---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {""apiVersion"":""rbac.authorization.k8s.io/v1"",""kind"":""ClusterRoleBinding"",""metadata"":{""annotations"":{},""name"":""hpa-custom-metrics-getter""},""roleRef"":{""apiGroup"":""rbac.authorization.k8s.io"",""kind"":""ClusterRole"",""name"":""custom-metrics-getter""},""subjects"":[{""kind"":""ServiceAccount"",""name"":""horizontal-pod-autoscaler"",""namespace"":""kube-system""}]}
  creationTimestamp: ""2025-03-16T05:47:45Z""
  name: hpa-custom-metrics-getter
  resourceVersion: ""6381615""
  uid: c819798d-fdd0-47df-a8d1-55cff8101d84
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: custom-metrics-getter
subjects:
- kind: ServiceAccount
  name: horizontal-pod-autoscaler
  namespace: kube-system
```

Appreciate any help on how to take this forward, thanks in advance.","kubernetes, prometheus, prometheus-adapter",79519131.0,"Finally, The problem was with the metricsQuery configured in the adapter config.

```
 rules:
    - seriesQuery: 'http_server_requests_seconds_count{namespace!="""", pod != """"}'
      resources:
        overrides:
          namespace: {resource: ""namespace""}
          pod: {resource: ""pod""}
      name:
        matches: ""^(.*)_seconds_count""
        as: ""${1}_per_second""
      metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>,uri!~""/actuator/.*""}[2m])) by (pod)'
```

HPA:

```
---
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2
metadata:
  name: demo-http
  namespace: dynamic-secrets-ns
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-config-watcher
  minReplicas: 1
  maxReplicas: 10
  metrics:
  # use a ""Pods"" metric, which takes the average of the
  # given metric across all pods controlled by the autoscaling target
  - type: Pods
    pods:
      metric:
        # use the metric that you used above: pods/http_requests
        name: http_server_requests_per_second
      target:
       # We configured the HPA to scale Pods if the average of requests is greater than 10 per seconds.
        type: AverageValue
        averageValue: 10000m
```

Huge shoutout for a youtube video - [Anton's guide on K8S-Prometheus integration](https://www.youtube.com/watch?v=iodq-4srXA8&t=1125s)",2025-03-19T04:39:31,2025-03-18T07:23:48,"```yaml
 rules:
    - seriesQuery: 'http_server_requests_seconds_count{namespace!="""", pod != """"}'
      resources:
        overrides:
          namespace: {resource: ""namespace""}
          pod: {resource: ""pod""}
      name:
        matches: ""^(.*)_seconds_count""
        as: ""${1}_per_second""
      metricsQuery: 'sum(rate(<<.Series>>{<<.LabelMatchers>>,uri!~""/actuator/.*""}[2m])) by (pod)'
```

Finally, The problem was with the metricsQuery configured in the adapter config.

```yaml
---
kind: HorizontalPodAutoscaler
apiVersion: autoscaling/v2
metadata:
  name: demo-http
  namespace: dynamic-secrets-ns
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-config-watcher
  minReplicas: 1
  maxReplicas: 10
  metrics:
  # use a ""Pods"" metric, which takes the average of the
  # given metric across all pods controlled by the autoscaling target
  - type: Pods
    pods:
      metric:
        # use the metric that you used above: pods/http_requests
        name: http_server_requests_per_second
      target:
       # We configured the HPA to scale Pods if the average of requests is greater than 10 per seconds.
        type: AverageValue
        averageValue: 10000m
```

HPA:

Huge shoutout for a youtube video - [Anton's guide on K8S-Prometheus integration](https://www.youtube.com/watch?v=iodq-4srXA8&t=1125s)"
79516077,Cannot Connect my deployed kafka on Kubernetese with my spring boot application,"I created a Spring Boot application that uses Kafka, which I deployed on a Kubernetes cluster.

I am facing an error stating that the deployed Spring Boot application cannot resolve the bootstrap URLs inside the Kafka cluster.

I got this error when I tried to deploy my Spring Boot application:

```
rg.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:326) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:510) ~[spring-context-6.2.3.jar!/:6.2.3]
    at java.base/java.lang.Iterable.forEach(Iterable.java:75) ~[na:na]
    at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:295) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:240) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:1006) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:630) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1361) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1350) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at fr.formationacademy.scpiinvestpluspartner.ScpiInvestPlusPartnerApplication.main(ScpiInvestPlusPartnerApplication.java:10) ~[!/:0.0.1-SNAPSHOT]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:569) ~[na:na]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]

Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:265) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.ConsumerDelegateCreator.create(ConsumerDelegateCreator.java:65) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:600) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:595) ~[kafka-clients-3.8.1.jar!/:na]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer.<init>(DefaultKafkaConsumerFactory.java:498) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createRawConsumer(DefaultKafkaConsumerFactory.java:453) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:430) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumerWithAdjustedProperties(DefaultKafkaConsumerFactory.java:407) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:374) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumer(DefaultKafkaConsumerFactory.java:335) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.<init>(KafkaMessageListenerContainer.java:876) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer.doStart(KafkaMessageListenerContainer.java:387) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.ConcurrentMessageListenerContainer.doStart(ConcurrentMessageListenerContainer.java:264) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.startIfNecessary(KafkaListenerEndpointRegistry.java:436) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.start(KafkaListenerEndpointRegistry.java:382) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:323) ~[spring-context-6.2.3.jar!/:6.2.3]
    ... 20 common frames omitted

Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:103) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:62) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:58) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:184) ~[kafka-clients-3.8.1.jar!/:na]
    ... 37 common frames omitted
```

I configured Kafka inside Kubernetes using this configuration:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:6.1.1
          ports:
            - containerPort: 9092
            - containerPort: 9094
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

            - name: KAFKA_LISTENERS
              value: ""INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094""
            - name: KAFKA_ADVERTISED_LISTENERS
              value: ""INTERNAL://$(POD_NAME).kafka-headless:9092,OUTSIDE://$(POD_NAME).kafka-svc:9094""
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: ""INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT""
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: ""3""
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: ""INTERNAL""
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: ""zookeeper:2181""
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: ""false""

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
spec:
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094
  type: ClusterIP
```

And for zookeeper:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - name: zookeeper
          image: wurstmeister/zookeeper
          ports:
            - containerPort: 2181
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
spec:
  selector:
    app: zookeeper
  ports:
    - protocol: TCP
      port: 2181
      targetPort: 2181
```

My `application.yml`:

```
spring:
  application:
    version: 1.0.0
    name: scpi-invest-plus-partner
  kafka:
    bootstrap-servers: kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: ""*""
```","spring-boot, kubernetes, apache-kafka, spring-kafka",79518005.0,"After few modifications, I could resolve the error, but my topics are not recongnized by the application.

```
2025-03-18 15:37:45 [scpi-invest-plus-api] [int] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-scpi-partner-group-1, groupId=scpi-partner-group] Error while fetching metadata with correlation id 124 : {scpi-partner-response-topic=UNKNOWN_TOPIC_OR_PARTITION}
```

This is my configuration :

```
spring:
  application:
    name: scpi-invest-plus-api
    version: 1.0.0

  datasource:
    url: jdbc:postgresql://scpi-invest-db:5432/postgres
    username: postgres
    password: postgres
    driver-class-name: org.postgresql.Driver

  jpa:
    database: postgresql
    hibernate:
      ddl-auto: validate
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
  kafka:
    bootstrap-servers: kafka-headless.kafka:9092

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: ""*""

  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: https://keycloak.check-consulting.net/realms/master
          jwk-set-uri: https://keycloak.check-consulting.net/realms/master/protocol/openid-connect/certs

management:
  endpoints:
    web:
      exposure:
        include: health,prometheus
  endpoint:
    prometheus:
      enabled: true
  prometheus:
    metrics:
      export:
        enabled: true
```

The method where I send the message using Kafka:

```
public InvestmentDto saveInvestment(InvestmentDto investmentDto) throws GlobalException {
        log.info(""Début de la création d'un investissement."");

        if (investmentDto == null) {
            log.error(""L'objet InvestmentDto est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""InvestmentDto ne peut pas être null."");
        }

        if (investmentDto.getScpiId() == null) {
            log.error(""L'ID de la SCPI est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""L'ID de la SCPI ne peut pas être null."");
        }

        String email = userService.getEmail();
        log.info(""Récupération de l'email de l'utilisateur : {}"", email);

        ScpiDtoOut scpiDtoOut = scpiService.getScpiDetailsById(investmentDto.getScpiId());
        log.info(""Détails SCPI récupérés : {}"", scpiDtoOut);
        if (scpiDtoOut == null) {
            log.error(""SCPI non trouvée pour ID: {}"", investmentDto.getScpiId());
            throw new GlobalException(HttpStatus.NOT_FOUND, ""Aucune SCPI trouvée avec l'ID: "" + investmentDto.getScpiId());
        }
        log.info(""SCPI trouvée : {} - {}"", scpiDtoOut.getId(), scpiDtoOut.getName());

        Scpi scpiEntity = scpiRepository.findById(investmentDto.getScpiId())
                .orElseThrow(() -> new GlobalException(HttpStatus.NOT_FOUND, ""SCPI non trouvée""));

        Investment investment = investmentMapper.toEntity(investmentDto);
        investment.setInvestorId(email);
        investment.setInvestmentState(""En cours"");
        investment.setScpi(scpiEntity);

        Investment savedInvestment = investmentRepository.save(investment);
        log.info(""Investissement enregistré avec succès - ID: {}"", savedInvestment.getId());

        InvestmentKafkaDto kafkaDto = new InvestmentKafkaDto();
        InvestmentOutDto investmentOutDto = investmentMapper.toOutDto(savedInvestment);
        investmentOutDto.setId(savedInvestment.getId());
        kafkaDto.setInvestmentDto(investmentOutDto);
        kafkaDto.setInvestorEmail(email);
        kafkaDto.setScpi(scpiDtoOut);

        log.info(""Envoi la demande d'investissement au Bouchon pour Objet Traitement : {}"", kafkaDto);
        sendInvestment(kafkaDto);
        log.info(""Investissement envoyé avec succès à Kafka - ID: {}"", savedInvestment.getId());

        return investmentMapper.toDTO(savedInvestment);
    }
```

I also configured the topic :

```
import static fr.formationacademy.scpiinvestplusapi.utils.Constants.SCPI_REQUEST_TOPIC;

@Configuration
public class KafkaTopicConfig {
    @Bean
    public NewTopic getTopic() {
        return TopicBuilder.name(SCPI_REQUEST_TOPIC)
                .partitions(1)
                .replicas(1)
                .build();
    }
}
```",2025-03-18T15:43:42,2025-03-18T01:27:53,"```text
After few modifications, I could resolve the error, but my topics are not recongnized by the application.
```

```bash
2025-03-18 15:37:45 [scpi-invest-plus-api] [int] [org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] WARN  o.apache.kafka.clients.NetworkClient - [Consumer clientId=consumer-scpi-partner-group-1, groupId=scpi-partner-group] Error while fetching metadata with correlation id 124 : {scpi-partner-response-topic=UNKNOWN_TOPIC_OR_PARTITION}
```

```text
This is my configuration :
```

```yaml
spring:
  application:
    name: scpi-invest-plus-api
    version: 1.0.0

  datasource:
    url: jdbc:postgresql://scpi-invest-db:5432/postgres
    username: postgres
    password: postgres
    driver-class-name: org.postgresql.Driver

  jpa:
    database: postgresql
    hibernate:
      ddl-auto: validate
    properties:
      hibernate:
        dialect: org.hibernate.dialect.PostgreSQLDialect
  kafka:
    bootstrap-servers: kafka-headless.kafka:9092

    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.JsonDeserializer
      properties:
        spring.json.trusted.packages: ""*""

  security:
    oauth2:
      resourceserver:
        jwt:
          issuer-uri: https://keycloak.check-consulting.net/realms/master
          jwk-set-uri: https://keycloak.check-consulting.net/realms/master/protocol/openid-connect/certs

management:
  endpoints:
    web:
      exposure:
        include: health,prometheus
  endpoint:
    prometheus:
      enabled: true
  prometheus:
    metrics:
      export:
        enabled: true
```

```text
The method where I send the message using Kafka:
```

```java
public InvestmentDto saveInvestment(InvestmentDto investmentDto) throws GlobalException {
        log.info(""Début de la création d'un investissement."");

        if (investmentDto == null) {
            log.error(""L'objet InvestmentDto est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""InvestmentDto ne peut pas être null."");
        }

        if (investmentDto.getScpiId() == null) {
            log.error(""L'ID de la SCPI est null."");
            throw new GlobalException(HttpStatus.BAD_REQUEST, ""L'ID de la SCPI ne peut pas être null."");
        }

        String email = userService.getEmail();
        log.info(""Récupération de l'email de l'utilisateur : {}"", email);

        ScpiDtoOut scpiDtoOut = scpiService.getScpiDetailsById(investmentDto.getScpiId());
        log.info(""Détails SCPI récupérés : {}"", scpiDtoOut);
        if (scpiDtoOut == null) {
            log.error(""SCPI non trouvée pour ID: {}"", investmentDto.getScpiId());
            throw new GlobalException(HttpStatus.NOT_FOUND, ""Aucune SCPI trouvée avec l'ID: "" + investmentDto.getScpiId());
        }
        log.info(""SCPI trouvée : {} - {}"", scpiDtoOut.getId(), scpiDtoOut.getName());

        Scpi scpiEntity = scpiRepository.findById(investmentDto.getScpiId())
                .orElseThrow(() -> new GlobalException(HttpStatus.NOT_FOUND, ""SCPI non trouvée""));

        Investment investment = investmentMapper.toEntity(investmentDto);
        investment.setInvestorId(email);
        investment.setInvestmentState(""En cours"");
        investment.setScpi(scpiEntity);

        Investment savedInvestment = investmentRepository.save(investment);
        log.info(""Investissement enregistré avec succès - ID: {}"", savedInvestment.getId());

        InvestmentKafkaDto kafkaDto = new InvestmentKafkaDto();
        InvestmentOutDto investmentOutDto = investmentMapper.toOutDto(savedInvestment);
        investmentOutDto.setId(savedInvestment.getId());
        kafkaDto.setInvestmentDto(investmentOutDto);
        kafkaDto.setInvestorEmail(email);
        kafkaDto.setScpi(scpiDtoOut);

        log.info(""Envoi la demande d'investissement au Bouchon pour Objet Traitement : {}"", kafkaDto);
        sendInvestment(kafkaDto);
        log.info(""Investissement envoyé avec succès à Kafka - ID: {}"", savedInvestment.getId());

        return investmentMapper.toDTO(savedInvestment);
    }
```

```text
I also configured the topic :
```

```java
import static fr.formationacademy.scpiinvestplusapi.utils.Constants.SCPI_REQUEST_TOPIC;

@Configuration
public class KafkaTopicConfig {
    @Bean
    public NewTopic getTopic() {
        return TopicBuilder.name(SCPI_REQUEST_TOPIC)
                .partitions(1)
                .replicas(1)
                .build();
    }
}
```"
79516077,Cannot Connect my deployed kafka on Kubernetese with my spring boot application,"I created a Spring Boot application that uses Kafka, which I deployed on a Kubernetes cluster.

I am facing an error stating that the deployed Spring Boot application cannot resolve the bootstrap URLs inside the Kafka cluster.

I got this error when I tried to deploy my Spring Boot application:

```
rg.springframework.context.ApplicationContextException: Failed to start bean 'org.springframework.kafka.config.internalKafkaListenerEndpointRegistry'
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:326) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor$LifecycleGroup.start(DefaultLifecycleProcessor.java:510) ~[spring-context-6.2.3.jar!/:6.2.3]
    at java.base/java.lang.Iterable.forEach(Iterable.java:75) ~[na:na]
    at org.springframework.context.support.DefaultLifecycleProcessor.startBeans(DefaultLifecycleProcessor.java:295) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.onRefresh(DefaultLifecycleProcessor.java:240) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:1006) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:630) ~[spring-context-6.2.3.jar!/:6.2.3]
    at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:146) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:752) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:439) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:318) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1361) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at org.springframework.boot.SpringApplication.run(SpringApplication.java:1350) ~[spring-boot-3.4.3.jar!/:3.4.3]
    at fr.formationacademy.scpiinvestpluspartner.ScpiInvestPlusPartnerApplication.main(ScpiInvestPlusPartnerApplication.java:10) ~[!/:0.0.1-SNAPSHOT]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:na]
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77) ~[na:na]
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na]
    at java.base/java.lang.reflect.Method.invoke(Method.java:569) ~[na:na]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]
    at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40) ~[scpi-invest-plus-partner.jar:0.0.1-SNAPSHOT]

Caused by: org.apache.kafka.common.KafkaException: Failed to construct kafka consumer
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:265) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.ConsumerDelegateCreator.create(ConsumerDelegateCreator.java:65) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:600) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:595) ~[kafka-clients-3.8.1.jar!/:na]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer.<init>(DefaultKafkaConsumerFactory.java:498) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createRawConsumer(DefaultKafkaConsumerFactory.java:453) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:430) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumerWithAdjustedProperties(DefaultKafkaConsumerFactory.java:407) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createKafkaConsumer(DefaultKafkaConsumerFactory.java:374) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.core.DefaultKafkaConsumerFactory.createConsumer(DefaultKafkaConsumerFactory.java:335) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer$ListenerConsumer.<init>(KafkaMessageListenerContainer.java:876) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.KafkaMessageListenerContainer.doStart(KafkaMessageListenerContainer.java:387) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.ConcurrentMessageListenerContainer.doStart(ConcurrentMessageListenerContainer.java:264) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.listener.AbstractMessageListenerContainer.start(AbstractMessageListenerContainer.java:520) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.startIfNecessary(KafkaListenerEndpointRegistry.java:436) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.kafka.config.KafkaListenerEndpointRegistry.start(KafkaListenerEndpointRegistry.java:382) ~[spring-kafka-3.3.3.jar!/:3.3.3]
    at org.springframework.context.support.DefaultLifecycleProcessor.doStart(DefaultLifecycleProcessor.java:323) ~[spring-context-6.2.3.jar!/:6.2.3]
    ... 20 common frames omitted

Caused by: org.apache.kafka.common.config.ConfigException: No resolvable bootstrap urls given in bootstrap.servers
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:103) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:62) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.ClientUtils.parseAndValidateAddresses(ClientUtils.java:58) ~[kafka-clients-3.8.1.jar!/:na]
    at org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer.<init>(LegacyKafkaConsumer.java:184) ~[kafka-clients-3.8.1.jar!/:na]
    ... 37 common frames omitted
```

I configured Kafka inside Kubernetes using this configuration:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: kafka
spec:
  serviceName: kafka-headless
  replicas: 3
  selector:
    matchLabels:
      app: kafka
  template:
    metadata:
      labels:
        app: kafka
    spec:
      containers:
        - name: kafka
          image: confluentinc/cp-kafka:6.1.1
          ports:
            - containerPort: 9092
            - containerPort: 9094
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name

            - name: KAFKA_LISTENERS
              value: ""INTERNAL://0.0.0.0:9092,OUTSIDE://0.0.0.0:9094""
            - name: KAFKA_ADVERTISED_LISTENERS
              value: ""INTERNAL://$(POD_NAME).kafka-headless:9092,OUTSIDE://$(POD_NAME).kafka-svc:9094""
            - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP
              value: ""INTERNAL:PLAINTEXT,OUTSIDE:PLAINTEXT""
            - name: KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR
              value: ""3""
            - name: KAFKA_INTER_BROKER_LISTENER_NAME
              value: ""INTERNAL""
            - name: KAFKA_ZOOKEEPER_CONNECT
              value: ""zookeeper:2181""
            - name: KAFKA_AUTO_CREATE_TOPICS_ENABLE
              value: ""false""

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-headless
spec:
  clusterIP: None
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094

---
apiVersion: v1
kind: Service
metadata:
  name: kafka-svc
spec:
  selector:
    app: kafka
  ports:
    - name: internal-port
      protocol: TCP
      port: 9092
      targetPort: 9092
    - name: outside-port
      protocol: TCP
      port: 9094
      targetPort: 9094
  type: ClusterIP
```

And for zookeeper:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zookeeper
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zookeeper
  template:
    metadata:
      labels:
        app: zookeeper
    spec:
      containers:
        - name: zookeeper
          image: wurstmeister/zookeeper
          ports:
            - containerPort: 2181
---
apiVersion: v1
kind: Service
metadata:
  name: zookeeper
spec:
  selector:
    app: zookeeper
  ports:
    - protocol: TCP
      port: 2181
      targetPort: 2181
```

My `application.yml`:

```
spring:
  application:
    version: 1.0.0
    name: scpi-invest-plus-partner
  kafka:
    bootstrap-servers: kafka-0.kafka-headless:9092,kafka-1.kafka-headless:9092,kafka-2.kafka-headless:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer
    consumer:
      group-id: scpi-partner-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        spring.json.trusted.packages: ""*""
```","spring-boot, kubernetes, apache-kafka, spring-kafka",79516909.0,"i can see two problems in you application config.

1. beacuse you have headless svc you can just pass the service name and k8s dns will help you to resolveit
2. the client communication port as i see is 9094 not 9092 wich is internal brokers communication.

so the right config can look like:

```
spring:
  kafka:
    bootstrap-servers: kafka-svc:9094
```

also if the application and the kafka brokers are not in the same namespace use

```
spring:
  kafka:
    bootstrap-servers: kafka-svc.namespace-name.svc:9094
```",2025-03-18T09:30:35,2025-03-18T01:27:53,"```text
i can see two problems in you application config.

1. beacuse you have headless svc you can just pass the service name and k8s dns will help you to resolveit
2. the client communication port as i see is 9094 not 9092 wich is internal brokers communication.

so the right config can look like:
```

```yaml
spring:
  kafka:
    bootstrap-servers: kafka-svc:9094
```

```text
also if the application and the kafka brokers are not in the same namespace use
```

```yaml
spring:
  kafka:
    bootstrap-servers: kafka-svc.namespace-name.svc:9094
```"
79509708,Azure Kubernetes Services: LoadBalancer Inbound Connection Issues,"fairly new to Kubernetes in general but also Azure Kubernetes Services. I have a single cluster with a telemetry asterix adapter service/pod that is designed to ingest UDP data from ADSB sensors via a public IP circuit. I created a public IP and LoadBalancer service on my cluster in the same namespace using a generic YAML provided by Microsoft (modified slightly for this projects requirements) and deployed. Will post YAML below.

I am able to ping the public IP generated via the YAML and the circuit with the ADSB sensor has been set up via the IP provided via the contractor, but not seeing any packets in logs for my telemetry asterix adapter pod. I am using source port of 1025 and target port of 6000 and that is what the telemetry asterix adapter is using via NettyUDP. I believe the connection between the loadbalancer service, and that pod is set with the selector in the YAML.

Is there something that I am missing? I assume that the loadbalancer service is not connected to the desired pod as I don't see anything in the logs but am able to ping the IP.

```
kind: Service
apiVersion: v1
metadata:
  name: telemetry-asterix-adapter-svc
  namespace: utm
  uid: fac3e2f1-50e1-49f3-9624-2b49fe5bec39
  resourceVersion: '15394560'
  creationTimestamp: '2025-03-04T18:39:58Z'
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: >
      {""apiVersion"":""v1"",""kind"":""Service"",""metadata"":{""annotations"":{},""name"":""telemetry-asterix-adapter-svc"",""namespace"":""utm""},""spec"":{""loadBalancerSourceRanges"":[""71.###.###.###/32"",""71.###.###.###/32""],""ports"":[{""port"":1025,""protocol"":""UDP"",""targetPort"":6000}],""selector"":{""app"":""telemetry-asterix-adapter""},""type"":""LoadBalancer""}}
  finalizers:
    - service.kubernetes.io/load-balancer-cleanup
  managedFields:
    - manager: cloud-controller-manager
      operation: Update
      apiVersion: v1
      time: '2025-03-11T16:08:39Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:finalizers:
            .: {}
            v:""service.kubernetes.io/load-balancer-cleanup"": {}
        f:status:
          f:loadBalancer:
            f:ingress: {}
      subresource: status
    - manager: kubectl-client-side-apply
      operation: Update
      apiVersion: v1
      time: '2025-03-11T20:13:05Z'
      fieldsType: FieldsV1
      fieldsV1:
        f:metadata:
          f:annotations:
            .: {}
            f:kubectl.kubernetes.io/last-applied-configuration: {}
        f:spec:
          f:allocateLoadBalancerNodePorts: {}
          f:externalTrafficPolicy: {}
          f:internalTrafficPolicy: {}
          f:loadBalancerSourceRanges: {}
          f:ports:
            .: {}
            k:{""port"":1025,""protocol"":""UDP""}:
              .: {}
              f:port: {}
              f:protocol: {}
              f:targetPort: {}
          f:selector: {}
          f:sessionAffinity: {}
          f:type: {}
spec:
  ports:
    - protocol: UDP
      port: 1025
      targetPort: 6000
      nodePort: 31780
  selector:
    app: telemetry-asterix-adapter
  clusterIP: 10.0.203.107
  clusterIPs:
    - 10.0.203.107
  type: LoadBalancer
  sessionAffinity: None
  loadBalancerSourceRanges:
    - 71.###.###.###/32
    - 71.###.###.###/32
  externalTrafficPolicy: Cluster
  ipFamilies:
    - IPv4
  ipFamilyPolicy: SingleStack
  allocateLoadBalancerNodePorts: true
  internalTrafficPolicy: Cluster
status:
  loadBalancer:
    ingress:
      - ip: 62.##.##.###
        ipMode: VIP
```

Pod Manifest:

```
 Name:             telemetry-asterix-adapter-f8bb6f48d-2mqf6
Namespace:        utm
Priority:         0
Service Account:  default
Node:             aks-nodepool1-25615987-vmss000001/10.64.80.12
Start Time:       Thu, 13 Mar 2025 13:09:14 +0000
Labels:           app=telemetry-asterix-adapter
                  pod-template-hash=f8bb6f48d
Annotations:      kubectl.kubernetes.io/restartedAt: 2025-03-13T13:09:13Z
Status:           Running
IP:               10.64.82.134
IPs:
  IP:           10.64.82.134
Controlled By:  ReplicaSet/telemetry-asterix-adapter-f8bb6f48d
Containers:
  telemetry-asterix-adapter:
    Container ID:   containerd://88a01df213e0ec4732dee857798f61d73e9296b9f24ab4b1f61d7a6425c75e93
    Image:          crfusademousgv634.azurecr.us/utm-services/telemetry-asterix:3.5.0
    Image ID:       crfusademousgv634.azurecr.us/utm-services/telemetry-asterix@sha256:4c44d3b8946c6cecaa28d6637104b3f336776a4062f372a33a53238cec3a132f
    Ports:          6000/UDP, 8080/TCP
    Host Ports:     0/UDP, 0/TCP
    State:          Running
      Started:      Thu, 13 Mar 2025 13:09:15 +0000
    Ready:          True
    Restart Count:  0
    Limits:
      memory:  512Mi
    Requests:
      memory:  512Mi
    Environment Variables from:
      telemetry-asterix-adapter  ConfigMap  Optional: false
    Environment:                 <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-g6r4q (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       True
  ContainersReady             True
  PodScheduled                True
Volumes:
  kube-api-access-g6r4q:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/memory-pressure:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:                      <none>
```

Logs from pod that I am hoping to ingest UDP data with:

```
  .   ____          _            __ _ _
 /\\ / ___'_ __ _ _(_)_ __  __ _ \ \ \ \
( ( )\___ | '_ | '_| | '_ \/ _` | \ \ \ \
 \\/  ___)| |_)| | | | | || (_| |  ) ) ) )
  '  |____| .__|_| |_|_| |_\__, | / / / /
 =========|_|==============|___/=/_/_/_/
 :: Spring Boot ::                (v2.7.9)

2025-03-13 13:09:20.002  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Starting AsterixAdapterApp v3.5.0 using Java 11.0.16 on telemetry-asterix-adapter-f8bb6f48d-2mqf6 with PID 1 (/opt/adapter/adapter.jar started by ? in /opt/adapter)
2025-03-13 13:09:20.018 DEBUG 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Running with Spring Boot v2.7.9, Spring v5.3.25
2025-03-13 13:09:20.019  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : No active profile set, falling back to 1 default profile: ""default""
2025-03-13 13:09:26.636  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat initialized with port(s): 8080 (http)
2025-03-13 13:09:26.682  INFO 1 --- [           main] o.apache.catalina.core.StandardService   : Starting service [Tomcat]
2025-03-13 13:09:26.683  INFO 1 --- [           main] org.apache.catalina.core.StandardEngine  : Starting Servlet engine: [Apache Tomcat/9.0.71]
2025-03-13 13:09:26.968  INFO 1 --- [           main] a.c.c.C.[.[.[/telemetry-asterix-adapter] : Initializing Spring embedded WebApplicationContext
2025-03-13 13:09:26.969  INFO 1 --- [           main] w.s.c.ServletWebServerApplicationContext : Root WebApplicationContext: initialization completed in 6760 ms
2025-03-13 13:09:28.505  INFO 1 --- [           main] c.f.s.t.asterixadapter.grpc.GrpcClient   : Create gRPC client at address: telemetry-manager-ng.utm.svc.cluster.local:8081
2025-03-13 13:09:38.298  INFO 1 --- [           main] o.a.c.c.s.CamelHttpTransportServlet      : Initialized CamelHttpTransportServlet[name=CamelServlet, contextPath=/telemetry-asterix-adapter]
2025-03-13 13:09:38.304  INFO 1 --- [           main] o.s.b.w.embedded.tomcat.TomcatWebServer  : Tomcat started on port(s): 8080 (http) with context path '/telemetry-asterix-adapter'
2025-03-13 13:09:40.204  INFO 1 --- [           main] o.a.c.component.netty.NettyComponent     : Creating shared NettyConsumerExecutorGroup with 3 threads
2025-03-13 13:09:40.551  INFO 1 --- [           main] c.n.SingleUDPNettyServerBootstrapFactory : ConnectionlessBootstrap binding to 0.0.0.0:6000
2025-03-13 13:09:40.837  INFO 1 --- [           main] o.a.camel.component.netty.NettyConsumer  : Netty consumer bound to: 0.0.0.0:6000
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   : Routes startup (total:2 started:2)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   :     Started route1 (netty://UDP://0.0.0.0:6000)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   :     Started route2 (rest://post:telemetry)
2025-03-13 13:09:40.841  INFO 1 --- [           main] o.a.c.impl.engine.AbstractCamelContext   : Apache Camel 3.14.1 (camel-1) started in 2s483ms (build:178ms init:1s560ms start:745ms)
2025-03-13 13:09:40.980  INFO 1 --- [           main] c.f.s.t.a.AsterixAdapterApp              : Started AsterixAdapterApp in 23.126 seconds (JVM running for 25.782)
```

I have tried modifying the YAML and updating the loadbalancer service, removing the whitelist on source IP, tried sending test UDP packets via another device, some modification of NSGs...

Expecting to see at least some data in the logs for the pod showing incoming UDP packets.","azure, kubernetes, load-balancing, kubernetes-ingress, azure-aks",79552610.0,"Azure Load Balancer does not validate UDP health natively. Without a TCP port for health checks, the backend pool may be marked as unhealthy, even if the pod is up. UDP traffic is connectionless, so debugging requires low-level inspection or packet logging. The issue is the pod never actually received the UDP packets due to the above reason.

To overcome this, and to expose a UDP service behind AKS Load Balancer, you can expose a dummy TCP port (like 8080) on the pod. This allows the Azure Load Balancer to consider the backend healthy. Your actual UDP-based app can still bind to 6000 as usual. The TCP port (even if unused by your app) just ensures Azure forwards traffic to the pod.

```
ports:
  - containerPort: 6000
    protocol: UDP
  - containerPort: 8080
    protocol: TCP
```

LoadBalancer service YAML should expose both UDP and TCP

```
apiVersion: v1
kind: Service
metadata:
  name: telemetry-asterix-adapter-svc
  namespace: utm
spec:
  type: LoadBalancer
  selector:
    app: telemetry-asterix-adapter
  externalTrafficPolicy: Cluster
  ports:
    - name: udp-port
      protocol: UDP
      port: 1025
      targetPort: 6000
    - name: health-port
      protocol: TCP
      port: 8080
      targetPort: 8080
```

Since Netty logs may not show raw UDP activity easily, you can validate this using a simple Alpine pod with  `socat`

```
apiVersion: v1
kind: Pod
metadata:
  name: udp-echo-server
  namespace: udp-test
  labels:
    app: udp-echo
spec:
  containers:
  - name: udp-echo
    image: alpine
    command: [""/bin/sh""]
    args: [""-c"", ""apk add --no-cache socat && socat -v UDP-RECV:6000 STDOUT""]
    ports:
      - containerPort: 6000
        protocol: UDP
      - containerPort: 8080
        protocol: TCP
```

![enter image description here](https://i.imgur.com/nMwsJSe.png)

![enter image description here](https://i.imgur.com/qR7reaA.png)

![enter image description here](https://i.imgur.com/EqPffUq.png)

Then expose it with a Load Balancer service

![enter image description here](https://i.imgur.com/iFUQ2Z6.png)

and test the UDP ingestion as below-

```
kubectl run udp-client --rm -it --image=busybox --restart=Never --namespace=udp-test -- /bin/sh
echo ""hello after socat fix"" | nc -u <LB_PUBLIC_IP> 1025
```

![enter image description here](https://i.imgur.com/OGJRQVP.png)

you can confirm the message in pod logs using-

```
kubectl logs -n udp-test udp-echo-server
```

![enter image description here](https://i.imgur.com/11b13Hj.png)

Looks good.

Once you add the TCP port for health probes, UDP packets will start flowing through and your application received them without any other change as you can see in my example.",2025-04-03T10:07:24,2025-03-14T16:37:05,"```text
Azure Load Balancer does not validate UDP health natively. Without a TCP port for health checks, the backend pool may be marked as unhealthy, even if the pod is up. UDP traffic is connectionless, so debugging requires low-level inspection or packet logging. The issue is the pod never actually received the UDP packets due to the above reason.

To overcome this, and to expose a UDP service behind AKS Load Balancer, you can expose a dummy TCP port (like 8080) on the pod. This allows the Azure Load Balancer to consider the backend healthy. Your actual UDP-based app can still bind to 6000 as usual. The TCP port (even if unused by your app) just ensures Azure forwards traffic to the pod.
```

```yaml
ports:
  - containerPort: 6000
    protocol: UDP
  - containerPort: 8080
    protocol: TCP
```

```text
LoadBalancer service YAML should expose both UDP and TCP
```

```yaml
apiVersion: v1
kind: Service
metadata:
  name: telemetry-asterix-adapter-svc
  namespace: utm
spec:
  type: LoadBalancer
  selector:
    app: telemetry-asterix-adapter
  externalTrafficPolicy: Cluster
  ports:
    - name: udp-port
      protocol: UDP
      port: 1025
      targetPort: 6000
    - name: health-port
      protocol: TCP
      port: 8080
      targetPort: 8080
```

```text
Since Netty logs may not show raw UDP activity easily, you can validate this using a simple Alpine pod with  `socat`
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: udp-echo-server
  namespace: udp-test
  labels:
    app: udp-echo
spec:
  containers:
  - name: udp-echo
    image: alpine
    command: [""/bin/sh""]
    args: [""-c"", ""apk add --no-cache socat && socat -v UDP-RECV:6000 STDOUT""]
    ports:
      - containerPort: 6000
        protocol: UDP
      - containerPort: 8080
        protocol: TCP
```

```text
![enter image description here](https://i.imgur.com/nMwsJSe.png)

![enter image description here](https://i.imgur.com/qR7reaA.png)

![enter image description here](https://i.imgur.com/EqPffUq.png)

Then expose it with a Load Balancer service

![enter image description here](https://i.imgur.com/iFUQ2Z6.png)

and test the UDP ingestion as below-
```

```bash
kubectl run udp-client --rm -it --image=busybox --restart=Never --namespace=udp-test -- /bin/sh
echo ""hello after socat fix"" | nc -u <LB_PUBLIC_IP> 1025
```

```text
![enter image description here](https://i.imgur.com/OGJRQVP.png)

you can confirm the message in pod logs using-
```

```bash
kubectl logs -n udp-test udp-echo-server
```

```text
![enter image description here](https://i.imgur.com/11b13Hj.png)

Looks good.

Once you add the TCP port for health probes, UDP packets will start flowing through and your application received them without any other change as you can see in my example.
```"
79507486,Key-Vault auth issue with AKS &amp; external-secrets-operator,"I setup a simple setup of external-secret-operator and used a Managed Identity for authentication as shown in the documentation [here](https://external-secrets.io/v0.4.3/provider-azure-key-vault/#managed-identity-authentication).

I used the managed identity's Principal ID when setting in the SecretStore setup.

I setup the secret store and an External Secret (CRD's) and this is what I see in the External Secret (error):

> error processing spec.data[0] (key: my-secret), err:
> azure.BearerAuthorizer#WithAuthorization: Failed to refresh the Token
> for request to
> [https://my.vault.azure.net/secrets/my-secret/?api-version=7.0](https://my.vault.azure.net/secrets/my-secret/?api-version=7.0):
> StatusCode=400 -- Original Error: adal: Refresh request failed. Status
> Code = '400'. Response body:
> {""error"":""invalid_request"",""error_description"":""Identity not found""}
> Endpoint
> [http://xxx.xxx.xxx.xxx/metadata/identity/oauth2/token?api-version=2018-02-01&client_id=eeeeee-eeeeeee-eeeeeeee-eeeee-eeeeeeeeeeee&resource=https%3A%2F%2Fvault.azure.net](http://xxx.xxx.xxx.xxx/metadata/identity/oauth2/token?api-version=2018-02-01&client_id=eeeeee-eeeeeee-eeeeeeee-eeeee-eeeeeeeeeeee&resource=https%3A%2F%2Fvault.azure.net)","azure, kubernetes, azure-aks, azure-keyvault",79514444.0,"You error message indicates that External Secrets Operator (ESO) cannot authenticate with Azure Key Vault using Managed Identity. This typically happens due to incorrect identity configuration, missing role assignments, or specifying the wrong identity type. Check if your AKS is using system assigned or user assigned identity

```
az aks show --resource-group \<RESOURCE_GROUP\> --name \<AKS_CLUSTER_NAME\> --query identity
```

If you see type: ""SystemAssigned"", your AKS is using System Assigned Identity. If you see ""userAssignedIdentities"", it is using User Assigned Identity.

Please follow the below steps to set up external secrets operator with your azure key vault using managed identity on your AKS cluster.

Create your cluster with managed identity-

```
az aks create \
\--resource-group arkorg \
\--name myAKSCluster \
\--enable-managed-identity \
\--node-count 2 \
\--generate-ssh-keys
```

Create an Azure Key Vault

```
az keyvault create \
\--name arkoKeyVault \
\--resource-group arkorg \
\--location centralindia \
\--sku standard
```

![enter image description here](https://i.imgur.com/2HwZo9T.png)

Create a Managed Identity to authenticate with Key Vault

```
az identity create --name myIdentity --resource-group arkorg
```

and assign the ""Key Vault Administrator"" Role

```
az role assignment create \
\--assignee \<OBJECT_ID\> \
\--role ""Key Vault Administrator"" \
\--scope /subscriptions/abcdefghijk/resourceGroups/arkorg/providers/Microsoft.KeyVault/vaults/arkoKeyVault
```

Now store your secret. I am just using a sample one for example

```
az keyvault secret set --vault-name arkoKeyVault --name my-secret --value ""SuperSecretValue""
```

![enter image description here](https://i.imgur.com/Fd6I2KE.png)

Now that your Key Vault is ready, next will connect it to AKS using ESO.

```
helm repo add external-secrets https://charts.external-secrets.io
helm repo update
helm install external-secrets external-secrets/external-secrets \
\--namespace external-secrets \
\--create-namespace
```

![enter image description here](https://i.imgur.com/skVBA48.png)

Check it

```
kubectl get pods -n external-secrets
```

![enter image description here](https://i.imgur.com/op1KnJ2.png)

yup! working.

next create a Secret Store that connects to your Azure Key Vault.

```
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: keyvault-secretstore
  namespace: default
spec:
  provider:
    azurekv:
      authType: ManagedIdentity
      vaultUrl: ""https://arkokeyvault.vault.azure.net""
```

![enter image description here](https://i.imgur.com/GjPhLBz.png)

Done, now your Secret Store is now correctly configured to use System Assigned Identity and is in the Ready state.

Now, check if the ExternalSecret is syncing correctly

```
kubectl get externalsecret my-external-secret -o yaml
```

![enter image description here](https://i.imgur.com/D7VsVOk.png)

Check logs- `kubectl logs -n external-secrets deployment/external-secrets`

It's working. Below log snippet confirms that ESO successfully reconciled (synced) the secret from your Key Vault to your AKS.

![enter image description here](https://i.imgur.com/ZvCkHqi.png)

You can even run `kubectl get secrets my-kubernetes-secret -o yaml`

![enter image description here](https://i.imgur.com/k9UEO67.png)

You can even decode the secret to confirm the Value

```
kubectl get secret my-kubernetes-secret -o jsonpath=""{.data.my-secret}"" | base64 --decode
```

![enter image description here](https://i.imgur.com/ZiKE5Dl.png)

[![enter image description here](https://i.sstatic.net/eAlMRtmv.png)]

[![enter image description here](https://i.sstatic.net/871Kp6TK.png)](https://i.sstatic.net/871Kp6TK.png)

[![enter image description here](https://i.sstatic.net/CbBdv2Hr.png)]
[![enter image description here](https://i.sstatic.net/VCb4I00t.png)](https://i.sstatic.net/VCb4I00t.png)",2025-03-17T11:30:28,2025-03-13T19:52:31,"```bash
az aks show --resource-group \<RESOURCE_GROUP\> --name \<AKS_CLUSTER_NAME\> --query identity
```

You error message indicates that External Secrets Operator (ESO) cannot authenticate with Azure Key Vault using Managed Identity. This typically happens due to incorrect identity configuration, missing role assignments, or specifying the wrong identity type. Check if your AKS is using system assigned or user assigned identity

If you see type: ""SystemAssigned"", your AKS is using System Assigned Identity. If you see ""userAssignedIdentities"", it is using User Assigned Identity.

Please follow the below steps to set up external secrets operator with your azure key vault using managed identity on your AKS cluster.

Create your cluster with managed identity-

```bash
az aks create \
\--resource-group arkorg \
\--name myAKSCluster \
\--enable-managed-identity \
\--node-count 2 \
\--generate-ssh-keys
```

Create an Azure Key Vault

```bash
az keyvault create \
\--name arkoKeyVault \
\--resource-group arkorg \
\--location centralindia \
\--sku standard
```

![enter image description here](https://i.imgur.com/2HwZo9T.png)

Create a Managed Identity to authenticate with Key Vault

```bash
az identity create --name myIdentity --resource-group arkorg
```

and assign the ""Key Vault Administrator"" Role

```bash
az role assignment create \
\--assignee \<OBJECT_ID\> \
\--role ""Key Vault Administrator"" \
\--scope /subscriptions/abcdefghijk/resourceGroups/arkorg/providers/Microsoft.KeyVault/vaults/arkoKeyVault
```

Now store your secret. I am just using a sample one for example

```bash
az keyvault secret set --vault-name arkoKeyVault --name my-secret --value ""SuperSecretValue""
```

![enter image description here](https://i.imgur.com/Fd6I2KE.png)

Now that your Key Vault is ready, next will connect it to AKS using ESO.

```bash
helm repo add external-secrets https://charts.external-secrets.io
helm repo update
helm install external-secrets external-secrets/external-secrets \
\--namespace external-secrets \
\--create-namespace
```

![enter image description here](https://i.imgur.com/skVBA48.png)

Check it

```bash
kubectl get pods -n external-secrets
```

![enter image description here](https://i.imgur.com/op1KnJ2.png)

yup! working.

next create a Secret Store that connects to your Azure Key Vault.

```yaml
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: keyvault-secretstore
  namespace: default
spec:
  provider:
    azurekv:
      authType: ManagedIdentity
      vaultUrl: ""https://arkokeyvault.vault.azure.net""
```

![enter image description here](https://i.imgur.com/GjPhLBz.png)

Done, now your Secret Store is now correctly configured to use System Assigned Identity and is in the Ready state.

Now, check if the ExternalSecret is syncing correctly

```bash
kubectl get externalsecret my-external-secret -o yaml
```

![enter image description here](https://i.imgur.com/D7VsVOk.png)

Check logs- `kubectl logs -n external-secrets deployment/external-secrets`

It's working. Below log snippet confirms that ESO successfully reconciled (synced) the secret from your Key Vault to your AKS.

![enter image description here](https://i.imgur.com/ZvCkHqi.png)

You can even run `kubectl get secrets my-kubernetes-secret -o yaml`

![enter image description here](https://i.imgur.com/k9UEO67.png)

You can even decode the secret to confirm the Value

```bash
kubectl get secret my-kubernetes-secret -o jsonpath=""{.data.my-secret}"" | base64 --decode
```

![enter image description here](https://i.imgur.com/ZiKE5Dl.png)

[![enter image description here](https://i.sstatic.net/eAlMRtmv.png)]

[![enter image description here](https://i.sstatic.net/871Kp6TK.png)](https://i.sstatic.net/871Kp6TK.png)

[![enter image description here](https://i.sstatic.net/CbBdv2Hr.png)]
[![enter image description here](https://i.sstatic.net/VCb4I00t.png)](https://i.sstatic.net/VCb4I00t.png)"
79489621,How to Use spring-boot-starter-actuator Without spring-boot-starter-web for Health Checks and Prometheus Metrics?,"I have two services running in Kubernetes, and I need to configure:

```
1.  A health check (using httpGet)
2.  Prometheus metrics (via /actuator/prometheus)
```

My services use either:

```
•   org.springframework.cloud:spring-cloud-stream
•   org.springframework.grpc:spring-grpc-spring-boot-starter
```

I do not use spring-boot-starter-web and would prefer not to add it just for health checks and metrics.

How can I expose actuator endpoints
(/actuator/health, /actuator/prometheus) without adding spring-boot-starter-web? Are there alternative ways to achieve this in a lightweight manner?

Thanks in advance! Spring Boot 3.4.3","spring, spring-boot, kubernetes, kubernetes-helm, actuator",79561025.0,"Why not use `spring-boot-starter-actuator` dependency? That will give you the option of exposing plenty of endpoints.

As for `prometheus`, you will have to use `io.micrometer:micrometer-registry-prometheus` dependency in addition to enable `/actuator/prometheus`. It is plug-and-play so it will start exposing your JVM metrics.

The following config must be added in the base `application.yml` of the spring-boot service:

```
management:
  endpoints:
    web:
      exposure:
        include: * # health,prometheus
    health:
      show-details: always
```

You can be more granular on what endpoints you want to expose by replacing the `*`  with something more specific as per Spring's doco: [Endpoints :: Spring Boot](https://docs.spring.io/spring-boot/reference/actuator/endpoints.html)",2025-04-08T01:49:37,2025-03-06T14:07:59,"```yaml
management:
  endpoints:
    web:
      exposure:
        include: * # health,prometheus
    health:
      show-details: always
```

Why not use `spring-boot-starter-actuator` dependency? That will give you the option of exposing plenty of endpoints.

As for `prometheus`, you will have to use `io.micrometer:micrometer-registry-prometheus` dependency in addition to enable `/actuator/prometheus`. It is plug-and-play so it will start exposing your JVM metrics.

The following config must be added in the base `application.yml` of the spring-boot service:

You can be more granular on what endpoints you want to expose by replacing the `*`  with something more specific as per Spring's doco: [Endpoints :: Spring Boot](https://docs.spring.io/spring-boot/reference/actuator/endpoints.html)"
79486138,PostgreSQL database isn&#39;t created automatically in kubernetes deployment,"I want to deploy my postgreSQL database with kubernetes but the database is not created...I put POSTGRES_DB in env of the deployment.

the kubernetes deployment of postgres work very well but no ""qr_auth"" database created.

deployment:

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres-deployment
  labels:
    app: postgres
spec:
  serviceName: ""postgres-service""
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:17.4
        env:
          - name: POSTGRES_PASSWORD
            valueFrom:
              secretKeyRef:
                name: qr-auth-secret
                key: DB_PASSWORD
          - name: POSTGRES_USER
            valueFrom:
              secretKeyRef:
                name: qr-auth-secret
                key: DB_USER
          - name: POSTGRES_DB
            value: qr_auth
        resources:
          requests:
            memory: ""256Mi""
            cpu: ""250m""
          limits:
            memory: ""512Mi""
            cpu: ""1000m""
        ports:
        - containerPort: 5432
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 5Gi

---

apiVersion: v1
kind: Service
metadata:
  name: postgres-service
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
```

configmap:

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: qr-auth-config
data:
  DB_HOST: ""postgres-service""
  DB_PORT: ""5432""
  DB_NAME: ""qr_auth""
```

secret:

```
apiVersion: v1
kind: Secret
metadata:
  name: qr-auth-secret
type: Opaque
data:
  DB_USER: cG9zdGdyZXM=  # postgres en base64
  DB_PASSWORD: cm9vdA==  # root en base64
```","postgresql, kubernetes",79494560.0,"I solved the problem by changing:

```
volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 5Gi
```

to:

```
volumes:
            - name: postgres-storage
              persistentVolumeClaim:
                claimName: postgres-pvc
```

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgres
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```",2025-03-08T15:03:10,2025-03-05T10:22:57,"```yaml
volumeClaimTemplates:
  - metadata:
      name: postgres-storage
    spec:
      accessModes: [ ""ReadWriteOnce"" ]
      resources:
        requests:
          storage: 5Gi
```

I solved the problem by changing:

```yaml
volumes:
            - name: postgres-storage
              persistentVolumeClaim:
                claimName: postgres-pvc
```

to:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
spec:
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgres
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi
```"
79481161,OOM kills pod when setting the resource limits,"Below is the stateful-set that I use. If I run it in `minicube (with 2000M, 4Gi config)` without `resources.limits`, then it runs fine. But if I specify `resources.limits`, which are equal to the same number of resources that minikube can provide, then the pod either does not work, or I get an error like: `Unable to connect to the server: net/http: TLS handshake timeout`. Why is this happening if, logically, this pod should have a similar resource limit without specifying `resources.limits`?

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: cassandra
spec:
  serviceName: cassandra
  replicas: 1
  selector:
    matchLabels:
      app: cassandra
  template:
    metadata:
      labels:
        app: cassandra
    spec:
      containers:
        - name: cassandra
          image: sevabek/cassandra:latest
          ports:
            - containerPort: 9042
          volumeMounts:
            - mountPath: /var/lib/cassandra
              name: cassandra-storage

          livenessProbe:
            exec:
              command:
                - cqlsh
                - -e
                - ""SELECT release_version FROM system.local;""
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 2

          resources:
            requests:
              memory: ""3500Mi""
              cpu: ""1700m""
            limits:
              memory: ""4Gi""
              cpu: ""2000m""

  volumeClaimTemplates:
    - metadata:
        name: cassandra-storage
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 3Gi
```","kubernetes, cassandra",79508013.0,"I suspect the container is using more memory than you anticipated because you've configured the liveness probe to run `cqlsh`:

```
          livenessProbe:
            exec:
              command:
                - cqlsh
                - -e
                - ""SELECT release_version FROM system.local;""
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 2
```

`cqlsh` is a full-fledged Python application so it means that it consumes a significant amount of resources to run. It is a little excessive to use it just to check that Cassandra is ""alive"" every 30 seconds.

Cassandra is considered operational if it is listening for client connections on the CQL port (default is `9042`). If something goes wrong for whatever reason (disk failure for example), Cassandra will automatically stop accepting connections and shutdown the CQL port.

Instead of running a CQL `SELECT` statement through `cqlsh`, I would suggest using a low-level TCP check using Linux utilities like `netstat`:

```
$ netstat -ltn | grep 9042
```

If you use a lightweight liveness probe, the Cassandra containers should use significantly less resources. Cheers!",2025-03-14T02:06:57,2025-03-03T14:04:36,"```yaml
          livenessProbe:
            exec:
              command:
                - cqlsh
                - -e
                - ""SELECT release_version FROM system.local;""
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 2
```

I suspect the container is using more memory than you anticipated because you've configured the liveness probe to run `cqlsh`:

`cqlsh` is a full-fledged Python application so it means that it consumes a significant amount of resources to run. It is a little excessive to use it just to check that Cassandra is ""alive"" every 30 seconds.

Cassandra is considered operational if it is listening for client connections on the CQL port (default is `9042`). If something goes wrong for whatever reason (disk failure for example), Cassandra will automatically stop accepting connections and shutdown the CQL port.

Instead of running a CQL `SELECT` statement through `cqlsh`, I would suggest using a low-level TCP check using Linux utilities like `netstat`:

```bash
$ netstat -ltn | grep 9042
```

If you use a lightweight liveness probe, the Cassandra containers should use significantly less resources. Cheers!"
79471107,Is there a way to restrict access to directories within a Kubernetes container?,"If I have two groups that are **not root users** that will access a container's directory structure, is there a way to fine tune permissions such that **Group 1** can have WRITE permissions on /DIR1, but **Group 2** only has READ or even NO ACCESS permissions on /DIR1? Assuming that this **/DIR1 is NOT A MOUNTED VOLUME?**

Does the answer change if the directory IS a mounted volume?

I am unable to find an absolute answer online, but I think I might be touching on something called a security context, though I can't quite wrap my head around it, so I don't know if I am understanding it correctly as the examples always show a root, and a non-root user. But never two non-root users.

I have considered the following avenues:

- **RoleBindings**, but I am unable to find how I can limit or tweak something like the existing Read-Only role to point to specific directories? It seems to read K8 resources.
- I cannot completely remove all roles from **Group 2** as they will have to access the pods at some point to troubleshoot. Maybe.
- I know you can chmod / chown in the dockerfile during image build, but.... not sure how this would tie into users that log in and a variety of groups that may need to access the same directory. Like what if Group 1 and Group 3 need access? Can you chown 2 groups? Does it even work like that?","kubernetes, containers, rbac",79471131.0,"In your Dockerfile, create groups/users and set strict permissions:

```
RUN groupadd group1 && groupadd group2 && \
useradd -g group1 user1 && useradd -g group2 user2 && \
mkdir /DIR1 && \
chown user1:group1 /DIR1 && \  # Owned by user1 and group1
chmod 770 /DIR1  # rwx for owner/group, no access for others
```

In the pod’s YAML, set the runtime identity:

```
securityContext:
runAsUser: 1000
runAsGroup: 1000
```

Use fsGroup to set volume group:

```
securityContext:
fsGroup: 1000
```

(if you want to) Use an initContainer to fix permissions:

```
initContainers:
 - name: fix-permissions
   image: busybox
   command: [""sh"", ""-c"", ""chmod 770 /DIR1""]
   volumeMounts:
    - name: my-volume
      mountPath: /DIR1
```",2025-02-26T22:10:06,2025-02-26T21:53:07,"```bash
RUN groupadd group1 && groupadd group2 && \
useradd -g group1 user1 && useradd -g group2 user2 && \
mkdir /DIR1 && \
chown user1:group1 /DIR1 && \  # Owned by user1 and group1
chmod 770 /DIR1  # rwx for owner/group, no access for others
```
In your Dockerfile, create groups/users and set strict permissions:

```yaml
securityContext:
runAsUser: 1000
runAsGroup: 1000
```
In the pod’s YAML, set the runtime identity:

```yaml
securityContext:
fsGroup: 1000
```
Use fsGroup to set volume group:

```yaml
initContainers:
 - name: fix-permissions
   image: busybox
   command: [""sh"", ""-c"", ""chmod 770 /DIR1""]
   volumeMounts:
    - name: my-volume
      mountPath: /DIR1
```
(if you want to) Use an initContainer to fix permissions:"
79469513,How read a file from a pod in Azure Kubernetes Service (AKS) in a Pythonic way?,"I have a requirement to read a file which is located inside a particular folder in a pod in AKS.

My manual flow would be to:

1. exec into the pod with kubectl.
2. cd to the directory where the file is located.
3. cat the file to see it's contents.

I want to automate all this purely using python. I am able to do it with [subprocess](https://docs.python.org/3/library/subprocess.html) but that would work only on a machine which has azure and kubectl setup.

Thus, I am looking for a purely pythonic way of doing this. I have looked into the [Kubernetes client for Python](https://github.com/kubernetes-client/python) but I am not able to find a way to do everything which I listed above.","python, kubernetes, azure-aks",79488905.0,"To read a file which is located inside a particular folder in a pod in AKS via Python script, follow the below steps

Assuming you have a valid aks cluster up and running, deploy a pod with your desired file.

For example -

```
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: my-app
spec:
  containers:
  - name: my-container
    image: busybox
    command: [""/bin/sh"", ""-c"", ""echo 'Hello from AKS' > /data/file.txt && sleep 3600""]
    volumeMounts:
    - name: data-volume
      mountPath: ""/data""
  volumes:
  - name: data-volume
    emptyDir: {}
```

```
kubectl apply -f pod.yaml
kubectl get pods
```

![enter image description here](https://i.imgur.com/KtDWRDh.png)

![enter image description here](https://i.imgur.com/lNQeWzO.png)

This one says `Hello from AKS' and it should reflect the same when you read the file from the pod using python.

Install / update the necessary dependencies

`pip install kubernetes`

Here's the script-

```
from kubernetes import client, config, stream

def read_file_from_pod(namespace: str, pod_name: str, container_name: str, file_path: str) -> str:
    try:
        config.load_incluster_config()
    except config.config_exception.ConfigException:
        config.load_kube_config()

    api_instance = client.CoreV1Api()
    command = [""cat"", file_path]

    try:
        exec_response = stream.stream(
            api_instance.connect_get_namespaced_pod_exec,
            name=pod_name,
            namespace=namespace,
            command=command,
            container=container_name,
            stderr=True,
            stdin=False,
            stdout=True,
            tty=False,
        )
        return exec_response
    except Exception as e:
        return f""Error reading file from pod: {str(e)}""

if __name__ == ""__main__"":
    namespace = ""default""
    pod_name = ""my-pod""
    container_name = ""my-container""
    file_path = ""/data/file.txt""

    file_contents = read_file_from_pod(namespace, pod_name, container_name, file_path)
    print(""File Contents:"", file_contents)
```

Save and run the script. Now you can read a file from a pod in AKS in a Pythonic way.

![enter image description here](https://i.imgur.com/9WaMGqx.png)",2025-03-06T09:47:39,2025-02-26T11:19:18,"```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  labels:
    app: my-app
spec:
  containers:
  - name: my-container
    image: busybox
    command: [""/bin/sh"", ""-c"", ""echo 'Hello from AKS' > /data/file.txt && sleep 3600""]
    volumeMounts:
    - name: data-volume
      mountPath: ""/data""
  volumes:
  - name: data-volume
    emptyDir: {}
```

To read a file which is located inside a particular folder in a pod in AKS via Python script, follow the below steps

Assuming you have a valid aks cluster up and running, deploy a pod with your desired file.

For example -

```bash
kubectl apply -f pod.yaml
kubectl get pods
```

![enter image description here](https://i.imgur.com/KtDWRDh.png)

![enter image description here](https://i.imgur.com/lNQeWzO.png)

This one says `Hello from AKS' and it should reflect the same when you read the file from the pod using python.

```bash
pip install kubernetes
```

Install / update the necessary dependencies

`pip install kubernetes`

```python
from kubernetes import client, config, stream

def read_file_from_pod(namespace: str, pod_name: str, container_name: str, file_path: str) -> str:
    try:
        config.load_incluster_config()
    except config.config_exception.ConfigException:
        config.load_kube_config()

    api_instance = client.CoreV1Api()
    command = [""cat"", file_path]

    try:
        exec_response = stream.stream(
            api_instance.connect_get_namespaced_pod_exec,
            name=pod_name,
            namespace=namespace,
            command=command,
            container=container_name,
            stderr=True,
            stdin=False,
            stdout=True,
            tty=False,
        )
        return exec_response
    except Exception as e:
        return f""Error reading file from pod: {str(e)}""

if __name__ == ""__main__"":
    namespace = ""default""
    pod_name = ""my-pod""
    container_name = ""my-container""
    file_path = ""/data/file.txt""

    file_contents = read_file_from_pod(namespace, pod_name, container_name, file_path)
    print(""File Contents:"", file_contents)
```

Here's the script-

Save and run the script. Now you can read a file from a pod in AKS in a Pythonic way.

![enter image description here](https://i.imgur.com/9WaMGqx.png)"
79464712,Image name not resolving properly during Helm Upgrade/Install for Elastic Kibana: InvalidImageName error,"I am attempting to deploy Kibana to my Amazon EKS cluster via Jenkins and am encountering the error InvalidImageName and can't seem to figure out why the image name isn't resolving properly.

Inside my Jenkinsfile I believe i'm providing everything needed to the Helm Upgrade command so that it points to my private repository (Sonatype Nexus Repository). I am using a local copy of the Helm chart that exists in my project and I got it from the following URL: [https://helm.elastic.co/helm/kibana/kibana-8.5.1.tgz](https://helm.elastic.co/helm/kibana/kibana-8.5.1.tgz)

What I am noticing is that the image is being returned as `map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1` and I am unsure why the left hand side is an object/map? The right hand side is the default value for the image tag found in the values.yaml file of the Kibana Helm chart instead of the value I passed as an argument.

ElasticSearch doesn't seem to be giving me an issue and its deployed using the same loop so i'm not sure why Kibana is behaving differently.

When I look at the image within Nexus Repository it gives me the following docker command

```
docker pull bitnami/kibana:8-debian-12
```

The stage within Jenkins that performs this work has the following in it:

```
def helmCharts = [
    [image_repository:'bitnami/elasticsearch', image_tag:'8-debian-12', helm_release_name:'elasticsearch', helm_chart_directory:'charts/bitnami/elasticsearch',namespace:'logging'],
    [image_repository:'bitnami/kibana', image_tag:'8-debian-12', helm_release_name:'kibana', helm_chart_directory:'charts/bitnami/kibana', namespace:'logging'],
    // [image_repository:'bitnami/fluentd', image_tag:'', helm_release_name:'fluentd', helm_chart_directory:'charts/bitnami/fluentd'],
]

helmCharts.each { chart ->
    // Define the Helm command
    def helmCommand = """"""
        helm upgrade $chart.helm_release_name /workspace/$chart.helm_chart_directory \\
        --install \\
        --namespace $chart.namespace \\
        --create-namespace \\
        --cleanup-on-fail \\
        --timeout 2m0s \\
        --set image.registry=${DOCKER_REGISTRY} \\
        --set image.repository=$chart.image_repository \\
        --set image.tag=$chart.image_tag \\
        --set global.imagePullSecrets[0].name=${params.NEXUS_IMAGE_PULL_SECRET} \\
        --set global.defaultStorageClass=gp2 \\
        --set global.security.allowInsecureImages=true \\
        --kubeconfig /workspace/kubeconfig \\
        --debug
    """"""
    // Run Helm commands using Docker
    sh """"""
        docker run --rm \\
            -e AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID} \\
            -e AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY} \\
            -e AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION} \\
            -e HTTP_PROXY=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e HTTPS_PROXY=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e http_proxy=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -e https_proxy=http://${PROXY_USER}:${PROXY_PASS}@${PROXY_SERVER} \\
            -v ${JENKINS_WORKSPACE}:/workspace \\
            ${HELM_AWS_CLI_IMAGE} sh -c '${helmCommand}'
    """"""
}
```

The following is the output when looking at the pod that is giving me issues:

```
PS C:\Users\******> kubectl get pods -n logging
NAME                              READY   STATUS             RESTARTS   AGE
elasticsearch-master-0            0/1     Pending            0          2m55s
pre-install-kibana-kibana-jkj7h   0/1     InvalidImageName   0          2m51s
PS C:\Users\******> kubectl describe pod pre-install-kibana-kibana-jkj7h -n logging
Name:             pre-install-kibana-kibana-jkj7h
Namespace:        logging
Priority:         0
Service Account:  pre-install-kibana-kibana
Node:             ip-**-***-***-***.***-***-west-1.compute.internal/**.***.**.***
Start Time:       Mon, 24 Feb 2025 13:33:59 -0600
Labels:           batch.kubernetes.io/controller-uid=15cea76c-4fa1-4a12-b44b-0f81130a1b64
                  batch.kubernetes.io/job-name=pre-install-kibana-kibana
                  controller-uid=15cea76c-4fa1-4a12-b44b-0f81130a1b64
                  job-name=pre-install-kibana-kibana
Annotations:      <none>
Status:           Pending
IP:               **.***.**.***
IPs:
  IP:           **.***.**.***
Controlled By:  Job/pre-install-kibana-kibana
Containers:
  create-kibana-token:
    Container ID:
    Image:         map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1
    Image ID:
    Port:          <none>
    Host Port:     <none>
    Command:
      /usr/share/kibana/node/bin/node
    Args:
      /usr/share/kibana/helm-scripts/manage-es-token.js
      create
    State:          Waiting
      Reason:       InvalidImageName
    Ready:          False
    Restart Count:  0
    Environment:
      ELASTICSEARCH_USERNAME:                    <set to the key 'username' in secret 'elasticsearch-master-credentials'>  Optional: false
      ELASTICSEARCH_PASSWORD:                    <set to the key 'password' in secret 'elasticsearch-master-credentials'>  Optional: false
      ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES:  /usr/share/kibana/config/certs/ca.crt
    Mounts:
      /usr/share/kibana/config/certs from elasticsearch-certs (ro)
      /usr/share/kibana/helm-scripts from kibana-helm-scripts (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-lngm8 (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  elasticsearch-certs:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  elasticsearch-master-certs
    Optional:    false
  kibana-helm-scripts:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      kibana-kibana-helm-scripts
    Optional:  false
  kube-api-access-lngm8:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason         Age                   From               Message
  ----     ------         ----                  ----               -------
  Normal   Scheduled      3m16s                 default-scheduler  Successfully assigned logging/pre-install-kibana-kibana-jkj7h to ip-**-***-**-***.***-***-west-1.compute.internal
  Warning  Failed         66s (x12 over 3m16s)  kubelet            Error: InvalidImageName
  Warning  InspectFailed  52s (x13 over 3m16s)  kubelet            Failed to apply default image tag ""map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1"": couldn't parse image name ""map[registry:abc.xyz.com repository:bitnami/kibana tag:8-debian-12]:8.5.1"": invalid reference format
```

Any help would be greatly appreciated. Thank you

EDIT:
The following is what is inside the values.yaml file for Kibana with regards to the image

```
image: ""docker.elastic.co/kibana/kibana""
imageTag: ""8.5.1""
imagePullPolicy: ""IfNotPresent""
```

EDIT:
The following is taken from the deployment manifest with regards to the image

```
      containers:
      - name: kibana
        securityContext:
{{ toYaml .Values.securityContext | indent 10 }}
        image: ""{{ .Values.image }}:{{ .Values.imageTag }}""
        imagePullPolicy: ""{{ .Values.imagePullPolicy }}""
        env:
          {{- if .Values.elasticsearchURL }}
          - name: ELASTICSEARCH_URL
            value: ""{{ .Values.elasticsearchURL }}""
          {{- else if .Values.elasticsearchHosts }}
          - name: ELASTICSEARCH_HOSTS
            value: ""{{ .Values.elasticsearchHosts }}""
          {{- end }}
          - name: ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES
            value: ""{{ template ""kibana.home_dir"" . }}/config/certs/{{ .Values.elasticsearchCertificateAuthoritiesFile }}""
          - name: SERVER_HOST
            value: ""{{ .Values.serverHost }}""
          - name: ELASTICSEARCH_SERVICEACCOUNTTOKEN
            valueFrom:
              secretKeyRef:
                name: {{ template ""kibana.fullname"" . }}-es-token
                key: token
                optional: false
```","docker, kubernetes, kubernetes-helm, kibana",79467898.0,"When you set the image tag in the resulting YAML manifest

```
image: ""{{ .Values.image }}:{{ .Values.imageTag }}""
```

you expect `image` in the Helm values to be a string.  This is true in the default Helm values, but when you run the install command

```
helm upgrade ... \
  --set image.registry=${DOCKER_REGISTRY} \
  --set image.repository=$chart.image_repository \
  --set image.tag=$chart.image_tag \
  ...
```

that particular `--set` syntax turns `image` into an object, with embedded fields `registry`, `repository`, and `tag`.  What you're seeing in the output is a default Go-template serialization of string-keyed maps, which isn't usually useful in a Helm context.

Probably the easiest fix here is to change the pipeline code to match the structure that's in the Helm values

```
helm upgrade ... \
  --set image=""${DOCKER_REGISTRY}/$chart.image_repository"" \
  --set imageTag=$chart.image_tag \
  ...
```

It would also work to change the Helm template to match the values that are being passed in.  (Do one or the other, not both!)

```
{{- $i := .Values.image }}
image: ""{{ $i.registry }}/{{ $i.repository }}:{{ $i.tag }}""
```",2025-02-25T21:16:24,2025-02-24T20:20:52,"```yaml
image: ""{{ .Values.image }}:{{ .Values.imageTag }}""
```

you expect `image` in the Helm values to be a string.  This is true in the default Helm values, but when you run the install command

```bash
helm upgrade ... \
  --set image.registry=${DOCKER_REGISTRY} \
  --set image.repository=$chart.image_repository \
  --set image.tag=$chart.image_tag \
  ...
```

that particular `--set` syntax turns `image` into an object, with embedded fields `registry`, `repository`, and `tag`.  What you're seeing in the output is a default Go-template serialization of string-keyed maps, which isn't usually useful in a Helm context.

Probably the easiest fix here is to change the pipeline code to match the structure that's in the Helm values

```bash
helm upgrade ... \
  --set image=""${DOCKER_REGISTRY}/$chart.image_repository"" \
  --set imageTag=$chart.image_tag \
  ...
```

It would also work to change the Helm template to match the values that are being passed in.  (Do one or the other, not both!)

```gotemplate
{{- $i := .Values.image }}
image: ""{{ $i.registry }}/{{ $i.repository }}:{{ $i.tag }}""
```"
79464533,passing env variable to docker image from k8 secret store,"How to expand environment variables coming from  a secret store and pass them inside a docker container?. Said docker container does not have a shell, therefore it is not possible to run a script. This is the sample yaml file

```
        envFrom:
        - secretRef:
            name: secret
        command: [""my-command""]
        args:
          - ""--env=ENV1=${MY_ENV_VAR1}""
          - ""--env=env2=${MY_ENV_VAR2}""
```",kubernetes,79464639.0,"You can pass environment variables to your arguments by using parentheses `()` instead of braces `{}`

```
    envFrom:
    - secretRef:
        name: secret
    command: [""my-command""]
    args:
      - ""--env=ENV1=$(MY_ENV_VAR1)""
      - ""--env=env2=$(MY_ENV_VAR2)""
```

Kubernetes docs have an example here for reference: [https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)",2025-02-24T19:41:06,2025-02-24T18:41:21,"```yaml
    envFrom:
    - secretRef:
        name: secret
    command: [""my-command""]
    args:
      - ""--env=ENV1=$(MY_ENV_VAR1)""
      - ""--env=env2=$(MY_ENV_VAR2)""
```

You can pass environment variables to your arguments by using parentheses `()` instead of braces `{}`

Kubernetes docs have an example here for reference: [https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments](https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/#use-environment-variables-to-define-arguments)"
79454507,Kubernetes disabling pod auto-restart using yaml for pods created using deployment,"I want to disable the pod auto-restart in AKS using yaml file for pods created using kind=deployment.
From looking at the documentation it seems the restartPolicy can have value = ""Always"" only for the kind=deployment.
For kind=pod, the restartPolicy supports these value=""Always"", ""Never"", ""onFailure"". So by setting the restartPolicy=""Never"" we can disable a pod from restarting if crashes for some reason or there is a error on it.

But is there a way we can do it using kind=Deployment. That is disable auto restarting of Pods created using kind=Deployment.

I understand that with kind=Deployment, we are adding a deployment so ideally we would want pods to be restarted, but my requirement is for a lower testing env, where we would rather have a pod not restart and let devs look into the issue and fix it.

Any help is appreciated.","azure, kubernetes, azure-aks",79463029.0,"Deployments are designed to check the application stays available by automatically restarting failed pods, and it **cannot be disabled using a Deployment**

- The restartPolicy field is a standard part of the Pod specification. For Pods, the allowed values are ""Always,"" ""OnFailure,"" and ""Never."" When set to ""Never,"" Kubernetes will not restart the container if it fails.

```
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
spec:
  restartPolicy: Never
  containers:
  - name: debug-container
    image: your-image:tag
    command: [""sh"", ""-c"", ""exit 1""]
```

The container will exit and remain in a terminated state, allowing developers to inspect logs and diagnose issues without the Pod being automatically restarted.

![enter image description here](https://i.imgur.com/mL1siRk.png)

**Note:** While standalone Pods or Jobs honor the specified restartPolicy, Deployments always execute a restartPolicy of ""Always"" for their Pods.",2025-02-24T09:35:46,2025-02-20T12:26:56,"```yaml
apiVersion: v1
kind: Pod
metadata:
  name: debug-pod
spec:
  restartPolicy: Never
  containers:
  - name: debug-container
    image: your-image:tag
    command: [""sh"", ""-c"", ""exit 1""]
```

Deployments are designed to check the application stays available by automatically restarting failed pods, and it **cannot be disabled using a Deployment**

- The restartPolicy field is a standard part of the Pod specification. For Pods, the allowed values are ""Always,"" ""OnFailure,"" and ""Never."" When set to ""Never,"" Kubernetes will not restart the container if it fails.

The container will exit and remain in a terminated state, allowing developers to inspect logs and diagnose issues without the Pod being automatically restarted.

![enter image description here](https://i.imgur.com/mL1siRk.png)

**Note:** While standalone Pods or Jobs honor the specified restartPolicy, Deployments always execute a restartPolicy of ""Always"" for their Pods."
79448794,FATAL: password authentication failed for user &quot;postgres&quot; in Kubernetes,"I can connect the database through docker-compose.yml with its username as postgres and its password 111111 but I cannot handle with the process through Kubernetes with Postgres.

I got this error shown below

```
FATAL:  password authentication failed for user ""postgres""
DETAIL:  Connection matched file ""/var/lib/postgresql/data/pg_hba.conf"" line 128: ""host all all all scram-sha-256
```

How can I fix it?

Here is the **postgres-secret.yml**

```
apiVersion: v1
kind: Secret
metadata:
  name: postgres-secret
  namespace: default
type: Opaque
data:
  POSTGRES_USER: cG9zdGdyZXM=   # Base64 encoded ""postgres""
  POSTGRES_PASSWORD: MTExMTEx    # Base64 encoded ""111111""
```

Here is the **postgres-config.yml**

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
  namespace: default
data:
  POSTGRES_DB: ""weatherapianalysisdatabase""
  POSTGRES_PORT: ""5432""
```

Here is the **postgres-pv.yml**

```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: postgres-pv
  namespace: default
spec:
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/postgresql

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: default
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
```

Here is the **postgres-statefulset.yml**

```
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: default
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
        - name: postgres
          image: postgres:latest
          ports:
            - containerPort: 5432
          env:
            - name: POSTGRES_USER
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_USER
            - name: POSTGRES_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-secret
                  key: POSTGRES_PASSWORD
            - name: POSTGRES_DB
              valueFrom:
                configMapKeyRef:
                  name: postgres-config
                  key: POSTGRES_DB
          volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
  volumeClaimTemplates:
    - metadata:
        name: postgres-data
      spec:
        accessModes: [ ""ReadWriteOnce"" ]
        resources:
          requests:
            storage: 10Gi

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: default
spec:
  selector:
    app: postgres
  ports:
    - protocol: TCP
      port: 5432
      targetPort: 5432
  clusterIP: None
```

I just look through postgres pod inside

```
kubectl exec -it postgres-0 -n default -- /bin/bash

root@postgres-0:/# env | grep POSTGRES
POSTGRES_PASSWORD=111111
POSTGRES_USER=postgres
POSTGRES_DB=weatherapianalysisdatabase
```

Next I enter postgres-0 through bash

```
kubectl exec -it postgres-0 -n default -- /bin/bash
root@postgres-0:/# psql -h $(hostname -i) -U postgres
Password for user postgres:
psql: error: connection to server at ""10.244.0.62"", port 5432 failed: FATAL:  password authentication failed for user ""postgres""
```

I get the same error again.","postgresql, kubernetes, passwords",79452714.0,"After I defined `POSTGRES_INITDB_ARGS` in **postgres-statefulset.yml**, the issue disappeared.

Here is the **code** block shown below

```
- name: POSTGRES_INITDB_ARGS
  value: ""--auth-host=scram-sha-256""
```",2025-02-19T20:31:41,2025-02-18T15:36:04,"```yaml
- name: POSTGRES_INITDB_ARGS
  value: ""--auth-host=scram-sha-256""
```

After I defined `POSTGRES_INITDB_ARGS` in **postgres-statefulset.yml**, the issue disappeared.

Here is the **code** block shown below"
79433398,how can I inject a secret on my helm overlays?,"Im new with helm charts but I created a deployment template, the template will need to include 2 secrets, so, inside the deployment.yaml file I have this: (this is for 1 secret)

env:

```
{{- range $name, $value := .Values.env}}
 - name: {{ name }}
   value: ""{{ value }}""
{{- end }}
 - name: SECRET_PASSWORD
   valueFrom:
      secretKeyRef:
      name: {{ .Values.env.secret.secretPassword.name }}
      key: {{ .Values.env.secret.secretPassword.key }}
```

That is on the template, if I add example values for the secretKey I can do:

```
env:
  secret:
    secretPassword:
      name: passname
      key: passkey
```

However, I know there is a missing part, I saw I can also create a template for secrets, something like this:

```
apiVersion: bitnami.com/v1alpha1
kind: SealedSecret
metada:
   name: {{ .name }}
spec:
   encryptedData:
{{-range $key, $value := .encryptedData}}
{{$key}} : | -
{{$value}}
{{- end}}
```

what parts am I missing? did I understood it correctly? and how can I create an overlay for this? any tutorial or reference will be appreciated.","kubernetes, kubernetes-helm",79433714.0,"You have two different configuration structures for your environment variables.  If your Helm values say

```
env:
  SIMPLE_STRING_VALUE: string
  secret: { secretPassword: { ... } }
```

then the output will contain an additional entry for `secret` that might not make sense: you'll get a default Go serialization of the nested structure.

One option is just to skip over the special `secret` key in your loop

```
{{- range $name, $value := .Values.env}}
{{- if ne $name ""secret"" }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
{{- end }}
```

A better option would be to separate out the specific configuration for the value you're injecting as a Secret.

```
# values.yaml
env:
  SIMPLE_STRING_VALUE: string
secretPassword:
  name: secretName
  key: password
```

```
# deployment.yaml
{{- range $name, $value := .Values.env }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ .Values.secretPassword.name }}
      key: {{ .Values.secretPassword.key }}
```

I might go even further, though.  If the SealedSecret is generated in your chart, then its name doesn't need to be configurable.  In the Helm values, you can apply some structure around what you expect to be present, instead of allowing totally free-form data.  If the Helm values say

```
encryptedData:
  password: ""...""
```

and then the SealedSecret says specifically

```
metadata:
  name: {{ include ""mychart.name"" . }}
spec:
  encryptedData:
    password: {{ .Values.encryptedData.password }}
```

now you know the Secret name *and* the key within the Secret, and you don't need to make any of it configurable at all.

```
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ include ""mychart.name"" . }}
      key: password
```

Notice that this last block in the Deployment spec contains no references to `.Values`, and there is no provision for an administrator to change the Secret name (beyond the generic boilerplate settings for changing all of the object names) or the specific key; but since all of these objects are being created in the chart itself, an administrator also doesn't need to create them.",2025-02-12T16:01:39,2025-02-12T14:19:10,"```text
You have two different configuration structures for your environment variables.  If your Helm values say
```

```yaml
env:
  SIMPLE_STRING_VALUE: string
  secret: { secretPassword: { ... } }
```

```text
then the output will contain an additional entry for `secret` that might not make sense: you'll get a default Go serialization of the nested structure.

One option is just to skip over the special `secret` key in your loop
```

```gotemplate
{{- range $name, $value := .Values.env}}
{{- if ne $name ""secret"" }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
{{- end }}
```

```text
A better option would be to separate out the specific configuration for the value you're injecting as a Secret.
```

```yaml
# values.yaml
env:
  SIMPLE_STRING_VALUE: string
secretPassword:
  name: secretName
  key: password
```

```gotemplate
# deployment.yaml
{{- range $name, $value := .Values.env }}
- name: {{ $name }}
  value: {{ toJson $value }}
{{- end }}
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ .Values.secretPassword.name }}
      key: {{ .Values.secretPassword.key }}
```

```text
I might go even further, though.  If the SealedSecret is generated in your chart, then its name doesn't need to be configurable.  In the Helm values, you can apply some structure around what you expect to be present, instead of allowing totally free-form data.  If the Helm values say
```

```yaml
encryptedData:
  password: ""...""
```

```gotemplate
and then the SealedSecret says specifically
```

```gotemplate
metadata:
  name: {{ include ""mychart.name"" . }}
spec:
  encryptedData:
    password: {{ .Values.encryptedData.password }}
```

```text
now you know the Secret name *and* the key within the Secret, and you don't need to make any of it configurable at all.
```

```gotemplate
- name: SECRET_PASSWORD
  valueFrom:
    secretKeyRef:
      name: {{ include ""mychart.name"" . }}
      key: password
```

```text
Notice that this last block in the Deployment spec contains no references to `.Values`, and there is no provision for an administrator to change the Secret name (beyond the generic boilerplate settings for changing all of the object names) or the specific key; but since all of these objects are being created in the chart itself, an administrator also doesn't need to create them.
```"
79433005,Kubernetes PostStartHook fails with curl,"I am trying to get a postStart hook working in a container but it keeps failing. The error I get is the following:

```
kubelet[1057]: E0212 11:07:20.205922    1057 handlers.go:78] ""Exec lifecycle hook for Container in Pod failed"" err=<
kubelet[1057]:         command 'curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted' exited with 2: curl: (2) no URL specified
kubelet[1057]:         curl: try 'curl --help' or 'curl --manual' for more information
kubelet[1057]:  > execCommand=[curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted] containerName=""srsran-cu-du"" pod=""srsran/srsran-project-cudu-chart-78f658b865-pjvt2"" message=<
kubelet[1057]:         curl: (2) no URL specified
kubelet[1057]:         curl: try 'curl --help' or 'curl --manual' for more information
kubelet[1057]:  >
```

The hook in my manifest looks like this:

```
lifecycle:
  postStart:
    exec:
      command: [ ""curl"", ""-H"",  ""'Content-Type: application/json'"", ""-d"", ""'{ \""restarted\"": True}'"", ""-X"", ""POST http://localhost:5000/restarted"" ]
```

which renders to `curl -H 'Content-Type: application/json' -d '{ \""restarted\"": True}' -X POST http://localhost:5000/restarted`.

If I run the curl command as it renders in the container directly its working fine. But when running it via the posStart hook it doesn't work. What am I doing wrong?

I have tried replacing the `'` with `\\\""` but that also didnt work.","kubernetes, curl",79433074.0,"When you use an array-form `command:`, or pass a command as a container's `args:`, you need to pass exactly one shell word per YAML list item.  The most immediate cause of your error is that there are two words in the last list item, so `curl` interprets this as a single parameter requesting an HTTP method `POST http://...` including a space, and then there is no following parameter with the URL.

You will also possibly get an error from the `Content-Type:` header: because you have a set of single quotes inside the double-quoted YAML string, `curl` will see these quotes as part of the `-H` argument, and it may send an invalid HTTP header or reject the header syntax itself.

Splitting this out into one argument per word, using YAML block sequence syntax, and using only YAML quoting and only where required, I might write:

```
command:
  - curl
  - -H
  - 'Content-Type: application/json'  # quotes required, else `key: value` looks like a mapping
  - -d
  - '{ ""restarted"": true }'           # YAML single quoting; double quotes do not need to be escaped; some quoting required else this looks like a JSON object
  - -X
  - POST
  - http://localhost:5000/restarted   # note two separate words
```

Or you could repack this into an inline list (""flow sequence""), using the same quoting rules

```
command: [curl, -H, 'Content-Type: application/json', -d, '{ ""restarted"": true }', -X, POST, http://localhost:5000/restarted]
```

Again, note that there is only one set of quotes for the `Content-Type:` header, and that `POST` and `http://...` are separate list items.  You can quote the other words too if you'd like, but it's only required for the couple of things that could be mistaken for other YAML syntax.",2025-02-12T12:36:39,2025-02-12T12:15:33,"```text
When you use an array-form `command:`, or pass a command as a container's `args:`, you need to pass exactly one shell word per YAML list item.  The most immediate cause of your error is that there are two words in the last list item, so `curl` interprets this as a single parameter requesting an HTTP method `POST http://...` including a space, and then there is no following parameter with the URL.

You will also possibly get an error from the `Content-Type:` header: because you have a set of single quotes inside the double-quoted YAML string, `curl` will see these quotes as part of the `-H` argument, and it may send an invalid HTTP header or reject the header syntax itself.

Splitting this out into one argument per word, using YAML block sequence syntax, and using only YAML quoting and only where required, I might write:
```

```yaml
command:
  - curl
  - -H
  - 'Content-Type: application/json'  # quotes required, else `key: value` looks like a mapping
  - -d
  - '{ ""restarted"": true }'           # YAML single quoting; double quotes do not need to be escaped; some quoting required else this looks like a JSON object
  - -X
  - POST
  - http://localhost:5000/restarted   # note two separate words
```

```text
Or you could repack this into an inline list (""flow sequence""), using the same quoting rules
```

```yaml
command: [curl, -H, 'Content-Type: application/json', -d, '{ ""restarted"": true }', -X, POST, http://localhost:5000/restarted]
```

```text
Again, note that there is only one set of quotes for the `Content-Type:` header, and that `POST` and `http://...` are separate list items.  You can quote the other words too if you'd like, but it's only required for the couple of things that could be mistaken for other YAML syntax.
```"
79423739,ArgoCD not recognizing ApplicationSets,"I'm trying to wrap my head around Argo Application Sets, but I can't get my setup to work.

Here's my directory structure

```
.
├── kubernetes-deployments
│   └── core
│       ├── argo-cd
│       │   ├── Chart.yaml
│       │   └── values.yaml
│       └── cilium
│           ├── Chart.yaml
│           └── values.yaml
└── README.md
```

Here's my values file:

```
argo-cd:
  enabled: true
  dex:
    enabled: false
  notifications:
    enabled: false
  applicationSet:
    enabled: true
  server:
    extraArgs:
      - --insecure
  namespaceOverride: ""argo-cd""
  server:
    service:
      type: NodePort
      nodePort: 32080
applicationsets:
  core:
    goTemplate: true
    generators:
      - git:
          repoURL: https://mygitrepo.git
          revision: HEAD
          directories:
            - path: kubernetes-deployments/core/*
    template:
      metadata:
        name: '{{path.basename}}'
        labels: {}
      spec:
        project: default
        source:
          repoURL: https://mygitrepo.git
          targetRevision: HEAD
          path: ""{{ .path.path }}""
          helm: &appsets-helm
            valueFiles:
              - values.yaml
        destination: &appsets-destination
          server: https://kubernetes.default.svc
          namespace: ""{{ base .path.path }}""
        revisionHistoryLimit: 5
        syncPolicy:
          syncOptions: &appsets-sync-options
            - ApplyOutOfSyncOnly=true
            - CreateNamespace=true
            - RespectIgnoreDifferences=true
            - PruneLast=true
        ignoreDifferences: []
    syncPolicy:
      preserveResourcesOnDeletion: true
      applicationsSync: sync
```

Here's the chart file:

```
apiVersion: v2
description: A Helm chart for Argo CD, a declarative, GitOps continuous delivery tool for Kubernetes.
name: argo-cd
version: 7.8.2
home: https://github.com/argoproj/argo-helm
icon: https://argo-cd.readthedocs.io/en/stable/assets/logo.png
sources:
  - https://github.com/argoproj/argo-helm/tree/main/charts/argo-cd
  - https://github.com/argoproj/argo-cd
dependencies:
  - name: argo-cd
    version: 7.8.2
    repository: https://argoproj.github.io/argo-helm
    condition: argo-cd.enabled

  - name: argocd-apps
    version: 2.0.0
    repository: https://argoproj.github.io/argo-helm
    condition: argocd-apps.enabled
```

What I'm doing is applying the above values file. Argo CD gets deployed. I go through the initial setup of entering the admin password and connecting my GitHub repository. I don't see any apps in the Argo UI. Based on my directory structure above, I should see Cilium app, and the agro app, right?","kubernetes, argocd",79424969.0,"I figured this out. My values file wasn't structured properly. Here's the corrected values file:

```
argo-cd:
  enabled: true
  dex:
    enabled: false
  notifications:
    enabled: false
  applicationSet:
    enabled: true
  server:
    resources:
      limits:
        cpu: 250m
        memory: 128Mi
      requests:
        cpu: 25m
        memory: 48Mi
    extraArgs:
      - --insecure
  namespaceOverride: ""argocd""
  server:
    service:
      type: NodePort
      nodePort: 32080
argocd-apps:
  enabled: true
  applicationsets:
    core:
      goTemplate: true
      generators:
        - git:
            repoURL: REPO.git
            revision: HEAD
            directories:
              - path: kubernetes-deployments/core/*
      template:
        metadata:
          name: '{{path.basename}}'
          labels: {}
        spec:
          project: default
          source:
            repoURL: REPO.git
            targetRevision: HEAD
            path: ""{{ .path.path }}""
            helm: &appsets-helm
              valueFiles:
                - values.yaml
          destination: &appsets-destination
            server: https://kubernetes.default.svc
            namespace: ""{{ base .path.path }}""
          revisionHistoryLimit: 5
          syncPolicy:
            syncOptions: &appsets-sync-options
              - ApplyOutOfSyncOnly=true
              - CreateNamespace=true
              - RespectIgnoreDifferences=true
              - PruneLast=true
          ignoreDifferences: []
      syncPolicy:
        preserveResourcesOnDeletion: true
        applicationsSync: sync
```

What led me to this was the fact that when I was rendering locally, the applicationSets weren't being rendered, which is why they were appearing in the UI. After making the above change and rendering locally, my ApplicationSets are are now being rendered correctly and the applications are now appearing in the argo UI",2025-02-09T13:32:12,2025-02-08T18:45:55,"```yaml
argo-cd:
  enabled: true
  dex:
    enabled: false
  notifications:
    enabled: false
  applicationSet:
    enabled: true
  server:
    resources:
      limits:
        cpu: 250m
        memory: 128Mi
      requests:
        cpu: 25m
        memory: 48Mi
    extraArgs:
      - --insecure
  namespaceOverride: ""argocd""
  server:
    service:
      type: NodePort
      nodePort: 32080
argocd-apps:
  enabled: true
  applicationsets:
    core:
      goTemplate: true
      generators:
        - git:
            repoURL: REPO.git
            revision: HEAD
            directories:
              - path: kubernetes-deployments/core/*
      template:
        metadata:
          name: '{{path.basename}}'
          labels: {}
        spec:
          project: default
          source:
            repoURL: REPO.git
            targetRevision: HEAD
            path: ""{{ .path.path }}""
            helm: &appsets-helm
              valueFiles:
                - values.yaml
          destination: &appsets-destination
            server: https://kubernetes.default.svc
            namespace: ""{{ base .path.path }}""
          revisionHistoryLimit: 5
          syncPolicy:
            syncOptions: &appsets-sync-options
              - ApplyOutOfSyncOnly=true
              - CreateNamespace=true
              - RespectIgnoreDifferences=true
              - PruneLast=true
          ignoreDifferences: []
      syncPolicy:
        preserveResourcesOnDeletion: true
        applicationsSync: sync
```

I figured this out. My values file wasn't structured properly. Here's the corrected values file:

What led me to this was the fact that when I was rendering locally, the applicationSets weren't being rendered, which is why they were appearing in the UI. After making the above change and rendering locally, my ApplicationSets are are now being rendered correctly and the applications are now appearing in the argo UI"
79418570,Pods not able to communicate via service url (created with Kustomize),"I have a single node with microk8s running. And the [DNS plugin](https://microk8s.io/docs/addon-dns) is defenitly enabled. But still pods cannot communitcate via the services, direct access via pod IP is working.

I read in [kubernetes cannot ping another service](https://stackoverflow.com/questions/50852542/kubernetes-cannot-ping-another-service) that pinging a service doesn't work. Since the connection problem is with a Postgres container I'm testing with psql from inside another pod
:

```
psql -h service-name -U postgres -d db_name   # doesn't work with service name
psql -h 10.152.183.98 -U postgres -d db_name  # doesn't work with service ClusterIP
psql -h 10.1.100.73 -U postgres -d db_name    # but works with pod IP
```

If I do `nslookup service-name` the service IP is detected. But the problem seems to be to forward from the service to the pod. Not even from within the postgres pod itself a connection to the service works.","kubernetes, kustomize",79428382.0,"Damn I found the error. I set a commonLabel with Kustomize for all 4 services/deployments/pods of my stack.

```
commonLabels:
  app: myapp
```

That overwrote the app labels from all 4 services and the whole selector mechanism matching services to pods broke because of that. Removing the common Label app fixed it.",2025-02-10T21:47:59,2025-02-06T15:58:46,"```yaml
commonLabels:
  app: myapp
```

Damn I found the error. I set a commonLabel with Kustomize for all 4 services/deployments/pods of my stack.

That overwrote the app labels from all 4 services and the whole selector mechanism matching services to pods broke because of that. Removing the common Label app fixed it."
79414325,Seed MongoDB in local minikube cluster using skaffold,"I am using a skaffold to deploy mongodb to my local minikube cluster.

sample files below:

skaffold.yaml

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/mongodb.yaml""
    defaultNamespace: ""mongodb""
```

config/namespace.yaml

```
kind: Namespace
apiVersion: v1
metadata:
  name: mongodb
  labels:
    name: mongodb
```

config/mongodb.yaml

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb-mongo-depl
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb-mongo
  template:
    metadata:
      labels:
        app: mongodb-mongo
    spec:
      containers:
        - name: mongodb-mongo
          image: mongo
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb-mongo-srv
spec:
  selector:
    app: mongodb-mongo
  ports:
    - name: db
      protocol: TCP
      port: 27017
      targetPort: 27017
```

It successfully creates a mongodb instance in my minikube cluster.

I would also like to seed the db with some json data.

Is there are way to do this using skaffold ?

Update:

I have also created a configmap and a job to seed the database so my config is as follows:

skaffold.yaml

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/configmap.yaml""
    - ""config/mongodb.yaml""
    - ""config/mongo-seed-job.yaml""
    defaultNamespace: ""mongodb""
```

congigmap.yaml

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-data
  namespace: mongodb
data:
  init.json: |
    [{""name"":""Joe Smith"",""email"":""jsmith@gmail.com"",""age"":40,""admin"":false},{""name"":""Jen Ford"",""email"":""jford@gmail.com"",""age"":45,""admin"":true}]
```

mongo-seed-job.yaml

```
apiVersion: batch/v1
kind: Job
metadata:
  name: mongo-seed
spec:
  template:
    spec:
      containers:
      - name: seed
        image: mongo:latest
        command: [""sh"", ""-c"", ""mongoimport --uri mongodb://mongodb:27017/mydb --collection accounts --type json --file '/init.json' --jsonArray""]
        volumeMounts:
        - name: seed-data
          mountPath: /data
      volumes:
      - name: seed-data
        configMap:
          name: seed-data
          items:
          - key: init.json
            path: init.json
      restartPolicy: Never
```

now the mongo-seed pod wont start. I am getting ContainerCannotRun","mongodb, kubernetes, minikube, skaffold",79417726.0,"I managed to get it working. Here is the working code if anyone else needs it:

`skaffold.yaml`

```
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/mongodb-credentials.yaml""
    - ""config/configmap.yaml""
    - ""config/mongodb.yaml""
    - ""config/mongo-seed-job.yaml""
    defaultNamespace: ""mongodb""
```

`config/namespace.yaml`

```
kind: Namespace
apiVersion: v1
metadata:
  name: mongodb
  labels:
    name: mongodb
```

`config/mongodb-credentials.yaml`

Note: username: admin password: password

Please change this to whatever you want

```
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
```

`config/configmap.yaml`

```
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-data
data:
  init.json: |
    [{""name"":""Joe Smith"",""email"":""jsmith@gmail.com"",""age"":40,""admin"":false},{""name"":""Jen Ford"",""email"":""jford@gmail.com"",""age"":45,""admin"":true}]
```

`config/mongodb.yaml`

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: mongo:latest
          ports:
          - containerPort: 27017
          volumeMounts:
          - name: mongo-data
            mountPath: /data/db
          env:
            - name: MONGO_INITDB_ROOT_USERNAME
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: username
            - name: MONGO_INITDB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: password
      volumes:
      - name: mongo-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
spec:
  ports:
    - port: 27017
  selector:
    app: mongodb
```

`config/mongo-seed-job.yaml`

```
apiVersion: batch/v1
kind: Job
metadata:
  name: mongo-seed
spec:
  template:
    spec:
      initContainers:
      - name: init-copy
        image: busybox
        command: ['sh', '-c', 'cp /config/init.json /data/']
        volumeMounts:
        - name: config-volume
          mountPath: /config
        - name: data-volume
          mountPath: /data
      containers:
      - name: seed
        image: mongo:latest
        command: [""sh"", ""-c"", ""mongoimport --uri mongodb://$(MONGO_USERNAME):$(MONGO_PASSWORD)@mongodb:27017/mydb --collection accounts --type json --file /data/init.json --jsonArray --authenticationDatabase=admin""]
        env:
          - name: MONGO_USERNAME
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: username
          - name: MONGO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: password
        volumeMounts:
        - name: data-volume
          mountPath: /data
      restartPolicy: Never
      volumes:
      - name: config-volume
        configMap:
          name: seed-data
      - name: data-volume
        emptyDir: {}
```

If anyone has any alternate solutions it would be good to know.

Thanks @imran-premnawaz for your help",2025-02-06T11:29:25,2025-02-05T09:51:31,"```text
I managed to get it working. Here is the working code if anyone else needs it:
```

```text
`skaffold.yaml`
```

```yaml
apiVersion: skaffold/v2beta26
kind: Config

metadata:
  name: mongodb

deploy:
  kubectl:
    manifests:
    - ""config/namespace.yaml""
    - ""config/mongodb-credentials.yaml""
    - ""config/configmap.yaml""
    - ""config/mongodb.yaml""
    - ""config/mongo-seed-job.yaml""
    defaultNamespace: ""mongodb""
```

```text
`config/namespace.yaml`
```

```yaml
kind: Namespace
apiVersion: v1
metadata:
  name: mongodb
  labels:
    name: mongodb
```

```text
`config/mongodb-credentials.yaml`

Note: username: admin password: password

Please change this to whatever you want
```

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: mongodb-credentials
type: Opaque
data:
  username: YWRtaW4=
  password: cGFzc3dvcmQ=
```

```text
`config/configmap.yaml`
```

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: seed-data
data:
  init.json: |
    [{""name"":""Joe Smith"",""email"":""jsmith@gmail.com"",""age"":40,""admin"":false},{""name"":""Jen Ford"",""email"":""jford@gmail.com"",""age"":45,""admin"":true}]
```

```text
`config/mongodb.yaml`
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mongodb
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mongodb
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: mongo:latest
          ports:
          - containerPort: 27017
          volumeMounts:
          - name: mongo-data
            mountPath: /data/db
          env:
            - name: MONGO_INITDB_ROOT_USERNAME
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: username
            - name: MONGO_INITDB_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: mongodb-credentials
                  key: password
      volumes:
      - name: mongo-data
        emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: mongodb
spec:
  ports:
    - port: 27017
  selector:
    app: mongodb
```

```text
`config/mongo-seed-job.yaml`
```

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: mongo-seed
spec:
  template:
    spec:
      initContainers:
      - name: init-copy
        image: busybox
        command: ['sh', '-c', 'cp /config/init.json /data/']
        volumeMounts:
        - name: config-volume
          mountPath: /config
        - name: data-volume
          mountPath: /data
      containers:
      - name: seed
        image: mongo:latest
        command: [""sh"", ""-c"", ""mongoimport --uri mongodb://$(MONGO_USERNAME):$(MONGO_PASSWORD)@mongodb:27017/mydb --collection accounts --type json --file /data/init.json --jsonArray --authenticationDatabase=admin""]
        env:
          - name: MONGO_USERNAME
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: username
          - name: MONGO_PASSWORD
            valueFrom:
              secretKeyRef:
                name: mongodb-credentials
                key: password
        volumeMounts:
        - name: data-volume
          mountPath: /data
      restartPolicy: Never
      volumes:
      - name: config-volume
        configMap:
          name: seed-data
      - name: data-volume
        emptyDir: {}
```

```text
If anyone has any alternate solutions it would be good to know.

Thanks @imran-premnawaz for your help
```"
79411729,Kustomize patching multiple path with same value,"I am trying to see if there are other ways to run patches with multiple paths with the same value.

This is an example of my Kustomization where I am replacing it with the same value. Is there a way to have a variable that I can use to refer to replace it instead of typing the same value multiple times?

```
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: common-base
    patch: |-
      - op: replace
        path: /metadata/name
        value: ""svc1""
      - op: replace
        path: /metadata/labels/app
        value: ""svc1""
```","kubernetes, kustomize",79414257.0,"You can use **ConfigMaps** and **Secrets** to hold configuration or sensitive data that are used by other Kubernetes objects, such as **Pods**. The source of  ConfigMaps or Secrets are usually external to a cluster, such as a **.properties** file or an **SSH keyfile**. Kustomize has **secretGenerator** and **configMapGenerator**, which generate Secret and ConfigMap from files or literals.

To run patches with multiple paths with the same value,you need to store the value in a [ConfigMap or Secret](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) and reference it in your resources.

**Define a ConfigMap in your kustomization.yaml:**

```
configMapGenerator:
  - name: app-config
    literals:
      - appName=svc1   #(your path / value )
```

**Reference the ConfigMap in your patch:**

```
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: common-base
    patch: |-
      - op: replace
        path: /metadata/name
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
      - op: replace
        path: /metadata/labels/app
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
```

So by following the above process by referencing the **ConfigMap** in your patch you will be able to achieve patching multiple paths with the same value in the Kustomize and you can also use a  [strategic merge patch](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md) and  [JSON patch](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/jsonpatch.md) which might also help you to resolve your issue.

For more information check this [Github Link](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/patchMultipleObjects.md) which might be helpful for you.",2025-02-05T09:33:07,2025-02-04T12:42:24,"You can use **ConfigMaps** and **Secrets** to hold configuration or sensitive data that are used by other Kubernetes objects, such as **Pods**. The source of  ConfigMaps or Secrets are usually external to a cluster, such as a **.properties** file or an **SSH keyfile**. Kustomize has **secretGenerator** and **configMapGenerator**, which generate Secret and ConfigMap from files or literals.

To run patches with multiple paths with the same value,you need to store the value in a [ConfigMap or Secret](https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/) and reference it in your resources.

**Code snippet:**

```yaml
configMapGenerator:
  - name: app-config
    literals:
      - appName=svc1   #(your path / value )
```

**Textual explanation:**

**Define a ConfigMap in your kustomization.yaml:**

---

**Code snippet:**

```yaml
patches:
  - target:
      group: apps
      version: v1
      kind: Deployment
      name: common-base
    patch: |-
      - op: replace
        path: /metadata/name
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
      - op: replace
        path: /metadata/labels/app
        valueFrom:
          configMapKeyRef:
            name: app-config
            key: appName
```

**Textual explanation:**

**Reference the ConfigMap in your patch:**

So by following the above process by referencing the **ConfigMap** in your patch you will be able to achieve patching multiple paths with the same value in the Kustomize and you can also use a  [strategic merge patch](https://github.com/kubernetes/community/blob/master/contributors/devel/sig-api-machinery/strategic-merge-patch.md) and  [JSON patch](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/jsonpatch.md) which might also help you to resolve your issue.

For more information check this [Github Link](https://github.com/kubernetes-sigs/kustomize/blob/master/examples/patchMultipleObjects.md) which might be helpful for you."
79409619,Unable to connect to service in same namespace in kubernetes,"I have 2 apps. One is config-server and other is business-logic-app that consumes data from config-server. Both are running on same namespace in Kubernetes (kubectl on my laptop). However, am getting connection timed out exception when business-logic-app is connecting to config-server which is leading to livenessProbe and readinessProbe failures. What am I missing?

***config-server.yaml***

```
# Config server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-learning-config-server
  namespace: kubernetes-learning
  labels:
    app: kubernetes-learning-config-server
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: kubernetes-learning-config-server
  template:
    metadata:
      name: kubernetes-learning-config-server
      labels:
        app: kubernetes-learning-config-server
    spec:
      containers:
        - name: kubernetes-learning-config-server
          image: ghcr.io/kubernetes/learning.config-server
          imagePullPolicy: Always
          ports:
            - containerPort: 8888
              protocol: TCP
            - containerPort: 48888
              protocol: TCP
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: ""50""
            - name: BPL_DEBUG_ENABLED
              value: ""true""
            - name: BPL_DEBUG_PORT
              value: ""48888""
            - name: GITHUB_CONFIG_DATA_URL
              value: https://github.com/kubernetes/config-data
            - name: GITHUB_CONFIG_DATA_USERNAME
              value: github_user
            - name: GITHUB_CONFIG_DATA_PERSONAL_ACCESS_TOKEN
              value: github_sampletoken
          livenessProbe:
            httpGet:
              path: /alpha-app/local
              port: 8888
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /alpha-app/local
              port: 8888
            initialDelaySeconds: 30
            periodSeconds: 20
            timeoutSeconds: 10
            successThreshold: 1
            failureThreshold: 3

      restartPolicy: Always

# Expose Config server
---
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-learning-config-server
  labels:
    app: kubernetes-learning-config-server
spec:
  type: ClusterIP
  selector:
    app: kubernetes-learning-config-server
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8888
```

**alpha-app.yaml**

```
# app applications
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubernetes-learning-app
  namespace: kubernetes-learning
  labels:
    app: kubernetes-learning-app
spec:
  replicas: 1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
  selector:
    matchLabels:
      app: kubernetes-learning-app
  template:
    metadata:
      name: kubernetes-learning-app
      labels:
        app: kubernetes-learning-app
    spec:
      containers:
        - name: kubernetes-learning-alpha-app
          image: ghcr.io/kubernetes/learning.alpha-app
          imagePullPolicy: Always
          ports:
            - containerPort: 8441
              protocol: TCP
            - containerPort: 48441
              protocol: TCP
          env:
            - name: BPL_JVM_THREAD_COUNT
              value: ""50""
            - name: BPL_DEBUG_ENABLED
              value: ""true""
            - name: BPL_DEBUG_PORT
              value: ""48441""
            - name: SPRING_PROFILES_ACTIVE
              value: kube
            - name: SPRING_CLOUD_CONFIG_FAIL_FAST
              value: ""true""
            - name: SPRING_CLOUD_CONFIG_RETRY_INITIAL_INTERVAL
              value: ""1000""
            - name: SPRING_CLOUD_CONFIG_RETRY_MAX_INTERVAL
              value: ""10000""
            - name: SPRING_CLOUD_CONFIG_RETRY_MULTIPLIER
              value: ""2""
            - name: SPRING_CLOUD_CONFIG_RETRY_MAX_ATTEMPTS
              value: ""5""
            - name: SPRING_CLOUD_CONFIG_URI
              value: http://kubernetes-learning-config-server:8888
          livenessProbe:
            httpGet:
              path: /info
              port: 8441
            initialDelaySeconds: 60
            timeoutSeconds: 15
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /info
              port: 8441
            initialDelaySeconds: 60
            timeoutSeconds: 15
            periodSeconds: 30
            successThreshold: 1
            failureThreshold: 3

      restartPolicy: Always

# Expose Config server
---
apiVersion: v1
kind: Service
metadata:
  name: kubernetes-learning-app
  labels:
    app: kubernetes-learning-app
spec:
  type: ClusterIP
  selector:
    app: kubernetes-learning-app
  ports:
    - protocol: TCP
      port: 80
      targetPort: 8441

```

**exception**

```
Caused by: org.springframework.web.client.ResourceAccessException: I/O error on GET request for ""http://kubernetes-learning-config-server:8888/alpha-app/kube"": Connect timed out
    at org.springframework.web.client.RestTemplate.createResourceAccessException(RestTemplate.java:926) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:906) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.execute(RestTemplate.java:801) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.exchange(RestTemplate.java:683) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.cloud.config.client.ConfigServerConfigDataLoader.getRemoteEnvironment(ConfigServerConfigDataLoader.java:349) ~[spring-cloud-config-client-4.2.0.jar:4.2.0]
    at org.springframework.cloud.config.client.ConfigServerConfigDataLoader.doLoad(ConfigServerConfigDataLoader.java:130) ~[spring-cloud-config-client-4.2.0.jar:4.2.0]
    ... 37 common frames omitted
Caused by: java.net.SocketTimeoutException: Connect timed out
    at java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(Unknown Source) ~[na:na]
    at java.base/sun.nio.ch.NioSocketImpl.connect(Unknown Source) ~[na:na]
    at java.base/java.net.Socket.connect(Unknown Source) ~[na:na]
    at java.base/sun.net.NetworkClient.doConnect(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.openServer(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.openServer(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.<init>(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.New(Unknown Source) ~[na:na]
    at java.base/sun.net.www.http.HttpClient.New(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(Unknown Source) ~[na:na]
    at java.base/sun.net.www.protocol.http.HttpURLConnection.connect(Unknown Source) ~[na:na]
    at org.springframework.http.client.SimpleClientHttpRequest.executeInternal(SimpleClientHttpRequest.java:79) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.http.client.AbstractStreamingClientHttpRequest.executeInternal(AbstractStreamingClientHttpRequest.java:71) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.http.client.AbstractClientHttpRequest.execute(AbstractClientHttpRequest.java:81) ~[spring-web-6.2.1.jar:6.2.1]
    at org.springframework.web.client.RestTemplate.doExecute(RestTemplate.java:900) ~[spring-web-6.2.1.jar:6.2.1]
    ... 41 common frames omitted
```","spring-boot, kubernetes, kubectl, spring-cloud-config-server",79412097.0,"That is because the service is listening on port 80 that will be routed to port 8888 on the target pods.

```
ports:
  - protocol: TCP
    port: 80
    targetPort: 8888
```

Therefore, you need to point `kubernetes-learning-app` to `http://kubernetes-learning-config-server:80/alpha-app/kube`.

N.B. as mentioned by a comment to the question, `http://kubernetes-learning-config-server` works too because an `http://` URL without port number will default to port 80 which coincidentally what you have set as the service port, not because the service decides the port. Should you use another port like `8080`, the URL without a port number would not work.",2025-02-04T14:50:33,2025-02-03T17:56:40,"```yaml
ports:
  - protocol: TCP
    port: 80
    targetPort: 8888
```

That is because the service is listening on port 80 that will be routed to port 8888 on the target pods.

Therefore, you need to point `kubernetes-learning-app` to `http://kubernetes-learning-config-server:80/alpha-app/kube`.

N.B. as mentioned by a comment to the question, `http://kubernetes-learning-config-server` works too because an `http://` URL without port number will default to port 80 which coincidentally what you have set as the service port, not because the service decides the port. Should you use another port like `8080`, the URL without a port number would not work."
79402669,Istio TLS termination and mTLS,"I have a number of services in a k8s cluster with Istio.  I want the services to internally communicate with automatic mTLS and externally using a web-browser certificate from Let's Encrypt.

To accmplish the former, I have a peer authentication is the `istio-system` namespace:

```
apiVersion: security.istio.io/v1
kind: PeerAuthentication
metadata:
  name: peer-authentication
  namespace: istio-system
spec:
  mtls:
    mode: STRICT
```

This is working fine for internal communication (my service pods are installed with label `sidecar.istio.io/inject: ""true""`).

I have configured an ingress gateway and a gateway

```
apiVersion: networking.istio.io/v1
kind: Gateway
metadata:
  name: gateway
  namespace: istio-ingress
spec:
  selector:
    istio: gateway
  servers:
  - port:
      name: http
      number: 80
      protocol: HTTP
    hosts:
    - ""*.customer.ocs.nu""
    tls:
      httpsRedirect: true
  - port:
      name: https
      number: 443
      protocol: HTTPS
    hosts:
    - ""*.customer.ocs.nu""
    tls:
      credentialName: ""istio-ingress/star-customer-ocs-nu-crt""
      mode: SIMPLE
```

I have multiple applications I wish to expose; currently, I have this one:

```
apiVersion: networking.istio.io/v1
kind: VirtualService
metadata:
  name: application
  namespace: customer-application
spec:
  gateways:
  - istio-ingress/gateway
  - mesh
  hosts:
  - application.customer.ocs.nu
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: application.customer-application.svc.cluster.local
        port:
          number: 8000
```

(I've removed some irrelevant annotations + changed the name of the application + namespace, so typos in them are irrelevant)

Problem is now, I can connect on http:

```
# curl -kv http://application.customer.ocs.nu/
* Host application.customer.ocs.nu:80 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:80...
* Connected to application.customer.ocs.nu (IP) port 80
> GET / HTTP/1.1
> Host: application.customer.ocs.nu
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
< HTTP/1.1 301 Moved Permanently
< location: https://application.customer.ocs.nu/
< date: Fri, 31 Jan 2025 11:39:09 GMT
< server: istio-envoy
< content-length: 0
```

In the log, I see

```
[2025-01-31T11:39:09.561Z] ""GET / HTTP/1.1"" 301 - direct_response - ""-"" 0 0 0 - ""10.244.0.165"" ""curl/8.7.1"" ""b05ac233-eea7-46d6-9fee-e9f9c9cf8bb8"" ""application.customer.ocs.nu"" ""-"" - - 10.244.0.200:80 10.244.0.165:22533 - -
```

However, connecting via TLS, I get

```
# curl -kv https://application.customer.ocs.nu/
* Host application.customer.ocs.nu:443 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:443...
* Connected to application.customer.ocs.nu (IP) port 443
* ALPN: curl offers h2,http/1.1
* (304) (OUT), TLS handshake, Client hello (1):
* LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to application.customer.ocs.nu:443
* Closing connection
curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to application.customer.ocs.nu:443
```

and get nothing in the log.  Trying to do HTTP on the HTTPs port, I get

```
# curl -kv http://application.customer.ocs.nu:443/
* Host application.customer.ocs.nu:443 was resolved.
* IPv6: (none)
* IPv4: IP
*   Trying IP:443...
* Connected to application.customer.ocs.nu (IP) port 443
> GET / HTTP/1.1
> Host: application.customer.ocs.nu:443
> User-Agent: curl/8.7.1
> Accept: */*
>
* Request completely sent off
* Empty reply from server
* Closing connection
curl: (52) Empty reply from server
```

but this at least shows up in the log

```
[2025-01-31T11:43:01.227Z] ""- - -"" 0 NR filter_chain_not_found - ""-"" 0 0 0 - ""-"" ""-"" ""-"" ""-"" ""-"" - - 10.244.0.200:443 10.244.0.165:44729 - -
```

If I reconfigure the gateway + virtualservice to use HTTP, everything works as expected.  All Bing results suggest setting up redirection from HTTP to HTTPS, but I already have this.

`istioctl analyze -A` lists nothing significant (some services outside the mesh with illegal names + some namespaces without injection annotations), whereas I get

```
# istioctl pc secret istio-gateway-76676d4954-l5498.istio-ingress
RESOURCE NAME                                                 TYPE           STATUS      VALID CERT     SERIAL NUMBER                        NOT AFTER                NOT BEFORE
kubernetes://istio-ingress/star-customer-ocs-nu-crt                          WARMING     false
default                                                       Cert Chain     ACTIVE      true           12c998930e47b4c9df3f5ae259fb1a92     2025-02-01T03:04:23Z     2025-01-31T03:02:23Z
ROOTCA                                                        CA             ACTIVE      true           c6b587095c06abdabc53c84b1af924d3     2035-01-18T12:59:47Z     2025-01-20T12:59:47Z
```

The certificate is provisioned by Let's Encrypt using certbot with DNS authentication and is perfectly valid.  I assume the issue is that Istio is using its own CA for trust and does not trust my public certificate.

```
# kubectl get certificate -n istio-ingress
NAME                         READY   SECRET                           AGE
star-customer-ocs-nu         True    star-customer-ocs-nu-crt         23h
# kubectl get certificaterequest -n istio-ingress
NAME                           APPROVED   DENIED   READY   ISSUER             REQUESTER                                    AGE
star-customer-ocs-nu-1         True                True    letsencrypt-prod   system:serviceaccount:default:cert-manager   23h
```

Does anybody have an idea to work around this?  I'd prefer to use a Let's Encrypt certificate publicly (I don't want to issue certificated manually) without mTLS, and I'd prefer to use automatic internal mTLS.

E: Changing the tls section of the gateway to not include the namespace (while correct)

```
    tls:
      credentialName: star-customer-ocs-nu-crt
      mode: SIMPLE
```

At least shows the certificate as valid (pod name change due to a restart to make sure it picks it up):

```
# istioctl pc secret istio-gateway-76676d4954-8hhjl.istio-ingress
RESOURCE NAME                                   TYPE           STATUS     VALID CERT     SERIAL NUMBER                           NOT AFTER                NOT BEFORE
default                                         Cert Chain     ACTIVE     true           8101cda2556b2fd7c31872f9d013d72f        2025-02-01T12:31:02Z     2025-01-31T12:29:02Z
kubernetes://star-customer-ocs-nu-crt           Cert Chain     ACTIVE     true           4c8a2f7ccab5ff0c7aa61dd2a46aa9bef0b     2025-04-30T11:56:26Z     2025-01-30T11:56:27Z
ROOTCA                                          CA             ACTIVE     true           c6b587095c06abdabc53c84b1af924d3        2035-01-18T12:59:47Z     2025-01-20T12:59:47Z
```","kubernetes, istio, mtls, istio-gateway",79403088.0,"It was caused by an incorrect DestinationRule I wasn't thinking of:

```
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  labels:
    app: application
  name: application
  namespace: application-customer
spec:
  host: application
  subsets:
  - labels:
      app: application
    name: default
```

(the host should be `application.customer.ocs.nu`, not just `application`).",2025-01-31T14:45:00,2025-01-31T12:01:22,"```yaml
apiVersion: networking.istio.io/v1
kind: DestinationRule
metadata:
  labels:
    app: application
  name: application
  namespace: application-customer
spec:
  host: application
  subsets:
  - labels:
      app: application
    name: default
```

It was caused by an incorrect DestinationRule I wasn't thinking of:

(the host should be `application.customer.ocs.nu`, not just `application`)."
79402349,How to enable Client Certificate Validation for specific paths in Nginx Ingress Controller?,"I have applications deployed in Kubernetes using the Nginx Ingress Controller. I need to implement path-based Client Certificate Validation where:

- 'app.example.com/**auth**' -> path should **require** client certificates
- 'app.example.com/**tool**' -> path should **not require** client certificates

Currently, I'm using this annotation to enable/disable Client Certificate Validation (Authentication):
*nginx.ingress.kubernetes.io/auth-tls-verify-client: ""**on**""*

What I understand is client cert auth is a global configuration and it can not be configured for specific path.
for referece, see first few lines of the doc:

*[https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#client-certificate-authentication](https://github.com/kubernetes/ingress-nginx/blob/main/docs/user-guide/nginx-configuration/annotations.md#client-certificate-authentication)*

We also thought to use 2 ingress controller but both URLs have same domain so domain can only be resolved to any one Load Balancer IP of ingress controller service.

Please advise how can We enabled client cert validation on specific path?  We are also flexible to switch to some other Ingress controller.","kubernetes, nginx, kubernetes-ingress, nginx-ingress, client-certificates",79462733.0,"Simply use two separate ingress resources for two different paths:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auth-ingress
  annotations:
    nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
    nginx.ingress.kubernetes.io/auth-tls-secret: ""default/auth-secret""
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: ""1""
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tool-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /tool
        pathType: Prefix
        backend:
          service:
            name: tool-service
            port:
              number: 80
```

/auth path with have a block for cert validation and /tool path will bypass the validation.",2025-02-24T07:38:07,2025-01-31T09:53:20,"```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: auth-ingress
  annotations:
    nginx.ingress.kubernetes.io/auth-tls-verify-client: ""on""
    nginx.ingress.kubernetes.io/auth-tls-secret: ""default/auth-secret""
    nginx.ingress.kubernetes.io/auth-tls-verify-depth: ""1""
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /auth
        pathType: Prefix
        backend:
          service:
            name: auth-service
            port:
              number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: tool-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /tool
        pathType: Prefix
        backend:
          service:
            name: tool-service
            port:
              number: 80
```

Simply use two separate ingress resources for two different paths:

/auth path with have a block for cert validation and /tool path will bypass the validation."
79394276,How to limit the memory usage of a Python process?,"I'm trying to limit the memory usage of a Python service running in Kubernetes. I'm currently testing with Python 3.10 running in WSL2. I want the service to be aware of limitations set by Kubernetes, so it can throw a MemoryError when it's trying to allocate too much memory and handle that error in the code. I'm trying to set RLIMIT_AS as described [here](https://carlosbecker.com/posts/python-docker-limits/). Kubernetes has a limit of 500 MB for the process. But when I set the RLIMIT_AS to 500 MB, the service does not even start. I then wrote a very simple script and checked how low I can set the RLIMIT_AS.

Script:

```
import resource

limit = 1000 * 1024 * 1024  # 1000 MB
resource.setrlimit(resource.RLIMIT_AS, (limit, limit))
print(f""Setting memory limit to {limit} bytes."")
bytearray(1 * 1024 * 1024)  # Allocate 1 MB
print(""Successfully allocated memory."")
```

Output:

```
Setting memory limit to 1048576000 bytes.
Traceback (most recent call last):
  File ""/mnt/c/Users/xxx/foobar.py"", line 6, in <module>
    bytearray(1 * 1024 * 1024)  # Allocate 1 MB
MemoryError

Process finished with exit code 1
```

I have to set the limit to 1048 MB for the script to be successful. ""htop"" in WSL2 is showing me a VIRT of 1047 MB for the Python script, so it seems RLIMIT_AS has to be greater than VIRT, which is already more than 1 GB for the most simple script.

```
PID   USER PRI NI VIRT  RES   SHR   S CPU%▽ MEM% TIME+   Command
56232 xxx  20  0  1046M 81112 26156 S 0.0   0.2  0:00.10 /home/xxx/.virtualenvs/xxx/bin/python3 /mnt/c/Users/xxx/foobar.py
```

Why is VIRT so high? How can I limit the real memory usage of the process?","python, kubernetes, memory, windows-subsystem-for-linux, setrlimit",79394367.0,"### Why is VIRT usage so high?

VIRT includes all the memory that the process can access, not just the memory physically allocated (RES). It also includes:

- Mapped shared libraries.
- Reserved but unused memory regions.
- Memory-mapped files.
- The Python interpreter itself allocates a significant amount of memory during startup for internal data structures, libraries, and the garbage collector. This adds to the high VIRT.

As for How can I limit the real memory usage of the process?:

1. Use RLIMIT_RSS which is exactly physical memory usage, but it may not be available in modern system, it depends on platform.
2. Use RLIMIT_AS with additional space meant for libraries and other extras
3. Since you are in kubernetes you can use pod configuration:

```
resources:
  limits:
    memory: ""500Mi""
  requests:
    memory: ""500Mi""
```

this limits both swap,physical memory

4. Last, you can do a loop in async checking memory like:

```
process = psutil.Process(os.getpid())
mem_info = process.memory_info()
rss = mem_info.rss  # Resident Set Size (physical memory used)
```",2025-01-28T15:34:40,2025-01-28T15:04:51,"```text
### Why is VIRT usage so high?

VIRT includes all the memory that the process can access, not just the memory physically allocated (RES). It also includes:

- Mapped shared libraries.
- Reserved but unused memory regions.
- Memory-mapped files.
- The Python interpreter itself allocates a significant amount of memory during startup for internal data structures, libraries, and the garbage collector. This adds to the high VIRT.

As for How can I limit the real memory usage of the process?:

1. Use RLIMIT_RSS which is exactly physical memory usage, but it may not be available in modern system, it depends on platform.
2. Use RLIMIT_AS with additional space meant for libraries and other extras
3. Since you are in kubernetes you can use pod configuration:
```

```yaml
resources:
  limits:
    memory: ""500Mi""
  requests:
    memory: ""500Mi""
```

```text
this limits both swap,physical memory

4. Last, you can do a loop in async checking memory like:
```

```python
process = psutil.Process(os.getpid())
mem_info = process.memory_info()
rss = mem_info.rss  # Resident Set Size (physical memory used)
```"
79392767,helm remove double quote from arithmetic expression while templating to JSON,"1. helm configmap:

```
  apiVersion: v1
  kind: ConfigMap
  metadata:
      name: {{ .Release.Name }}-config
      {{- include ""commonMeta"" . | nindent 2 }}
  data:
      config.play.ts: |
        const config = {{ tpl (.Values.app.play | mustToPrettyJson) $ | indent 6 }}
        export default config;
```

1. values.yaml

```
   play:
      PORT: 1114
      PLAY_EXPIRES_IN_MS: 10 * 60 * 1000
      API_URL: ""https://stag.com/api""
```

1. helm template output:

```
  play:
      ""PORT"": 1114
      ""PLAY_EXPIRES_IN_MS"": ""10 * 60 * 1000""
      ""API_URL"": ""https://stag.com/api""
```

1. Expected output:

```
  play:
      ""PORT"": 1114
      ""PLAY_EXPIRES_IN_MS"": 10 * 60 * 1000
      ""API_URL"": ""https://stag.com/api""
```

While templating to JSON, it adds double quotes and displays as PLAY_EXPIRES_IN_MS: ""10 * 60 * 1000"". I need to pass the PLAY_EXPIRES_IN_MS value without double quote.

I tried with:

```
    {{ tpl (.Values.app.play | mustToPrettyJson) $ | indent 6 | replace ""\""10 * 60 * 1000\"""" ""10 * 60 * 1000"" }}
```

Problem is, value could be different in future so i need to have a dynamic logic remove double quote from any number as well as from any arithmetic expression.

This value ""PLAY_EXPIRES_IN_MS"": ""10 * 60 * 1000"", may get change to ""10 * 60 * 80 * 1000"".","kubernetes, kubernetes-helm",79393791.0,"I don't think this particular combination of automatic quoting and unquoting is possible.

JSON doesn't allow expressions.  `10 * 60 * 1000` is a valid Javascript expression, but if you were transporting that value in a JSON document, you'd have to first evaluate the expression and then include the result in the JSON; `""PLAY_EXPIRES_IN_MS"": 60000`.  In a Helm context, the thing this means is that `toJson` and its variants don't expect to generate expressions that shouldn't be quoted.

Both JSON and YAML have a basic notion of typing.  In YAML's standard rules, if a value has an unquoted value, it's a number if it can be parsed as a number and a string if not.  This means that, in your Helm values, `PLAY_EXPIRES_IN_MS` has a string value.  `toJson` will therefore serialize it as a string, including double quotes.

This setup doesn't have any way to automatically recognize that something isn't a number, but it is a Javascript expression that would produce a number.  If you can use any Javascript expression this gets even harder – is `Math.PI/2` a URL or an expression (both have dots and slashes)?

If you can't preëvaluate the millisecond value in your settings, I might just directly embed the Javascript fragment in the Helm values.  If you use YAML block-scalar syntax, you can embed a multi-line string in the values.

```
# values.yaml

app:
  # play holds a Javascript object that is the configuration.
  play: |
    {
      ""PORT"": 1114,
      ""PLAY_EXPIRES_IN_MS"": 10 * 60 * 1000,
      ""API_URL"": ""https://stag.com/api""
    }
```

```
# configmap.yaml
  data:
      config.play.ts: |
        const config = {{ tpl .Values.app.play $ | indent 8 | trim }};
        export default config;
```

(The ConfigMap is basically the same except it removes the `mustToPrettyJson call; I've also tweaked the indentation and added a cosmetic `trim`.)

If this is a fixed combination of settings, another is to handle each value separately.  This would let you manually handle the quoting for the option that needs it.

```
# values.yaml

app:
  play:
    port: 1114,
    playExpiresInMs: 10 * 60 * 1000,
    apiUrl: https://stag.com/api
```

```
# configmap.yaml
  data:
      config.play.ts: |
        const config = {
          ""PORT"": {{ .Values.app.play.port }},
          ""PLAY_EXPIRES_IN_MS"": {{ .Values.app.play.playExpiresInMs }},
          ""API_URL"": ""{{ .Values.app.play.apiUrl }}""
        };
        export default config;
```

Note here that I've explicitly quoted the last URL value (`{{ ...apiUrl | toJson }}` would have the same effect and be more robust), and I *haven't* quoted the preceding value even though it's internally a string type.",2025-01-28T12:22:22,2025-01-28T04:48:17,"```text
I don't think this particular combination of automatic quoting and unquoting is possible.

JSON doesn't allow expressions.  `10 * 60 * 1000` is a valid Javascript expression, but if you were transporting that value in a JSON document, you'd have to first evaluate the expression and then include the result in the JSON; `""PLAY_EXPIRES_IN_MS"": 60000`.  In a Helm context, the thing this means is that `toJson` and its variants don't expect to generate expressions that shouldn't be quoted.

Both JSON and YAML have a basic notion of typing.  In YAML's standard rules, if a value has an unquoted value, it's a number if it can be parsed as a number and a string if not.  This means that, in your Helm values, `PLAY_EXPIRES_IN_MS` has a string value.  `toJson` will therefore serialize it as a string, including double quotes.

This setup doesn't have any way to automatically recognize that something isn't a number, but it is a Javascript expression that would produce a number.  If you can use any Javascript expression this gets even harder – is `Math.PI/2` a URL or an expression (both have dots and slashes)?

If you can't preëvaluate the millisecond value in your settings, I might just directly embed the Javascript fragment in the Helm values.  If you use YAML block-scalar syntax, you can embed a multi-line string in the values.
```

```yaml
# values.yaml

app:
  # play holds a Javascript object that is the configuration.
  play: |
    {
      ""PORT"": 1114,
      ""PLAY_EXPIRES_IN_MS"": 10 * 60 * 1000,
      ""API_URL"": ""https://stag.com/api""
    }
```

```text
```

```yaml
# configmap.yaml
  data:
      config.play.ts: |
        const config = {{ tpl .Values.app.play $ | indent 8 | trim }};
        export default config;
```

```text
(The ConfigMap is basically the same except it removes the `mustToPrettyJson call; I've also tweaked the indentation and added a cosmetic `trim`.)

If this is a fixed combination of settings, another is to handle each value separately.  This would let you manually handle the quoting for the option that needs it.
```

```yaml
# values.yaml

app:
  play:
    port: 1114,
    playExpiresInMs: 10 * 60 * 1000,
    apiUrl: https://stag.com/api
```

```yaml
# configmap.yaml
  data:
      config.play.ts: |
        const config = {
          ""PORT"": {{ .Values.app.play.port }},
          ""PLAY_EXPIRES_IN_MS"": {{ .Values.app.play.playExpiresInMs }},
          ""API_URL"": ""{{ .Values.app.play.apiUrl }}""
        };
        export default config;
```

```text
Note here that I've explicitly quoted the last URL value (`{{ ...apiUrl | toJson }}` would have the same effect and be more robust), and I *haven't* quoted the preceding value even though it's internally a string type.
```"
79392070,ScalingModifiers not working in KEDA ScaledObject,"I am using KEDA scaledObject for scaling my pods based on the triggers. But I would like interrupt resources scaled by the triggers using ScalingModifiers if the Utilization is not enough. For example I have following two triggers for my scaled

```
triggers:
    - metadata:
        value: '75'
      metricType: Utilization
      name: ""one""
      type: cpu
    - metadata:
        desiredReplicas: '5'
        end: 20 8 * * *
        start: 10 8 * * *
      type: cron
```

So in above example the desiredReplicas will be 5 during 8.10am to 8.20am. But I would like to make sure if CPU Utilization is less than 75 during 8.10am to 8.20am then I would like to set desiredReplicas to 3. So I am trying to use following scalingModifier solution.

```
scalingModifiers:
        formula: ""one < 75 ? 1 : 0""
        target: ""3""
        activationTarget: ""1""
        metricType: ""Utilization""
```

But I get an error `error validating formula in ScalingModifiers invalid argument for float(one)`
I am not sure why it is giving error on my trigger name `one`. Even if I change the name of the trigger still it gives the same error.

Also if you have another solution for above use case feel free to suggest one. Your response is greatly appreciated.","kubernetes, keda, keda-scaledobject",79395161.0,"Based on the [Pull Request](https://github.com/kedacore/keda-docs/pull/1246) in Github community from KEDA releases v.2.13.0 concepts [casting 'float'](https://github.com/kedacore/keda-docs/commit/754943c60bcf0f90a20463d9805d2a299b0c12c2#diff-30ce2091937fb966578e23f7cbf86e44f757337047bda64841b891f43300428f) before returning the result is a must if a ternary operator result is ‘any’ as per [experimental scaling modifier](https://keda.sh/docs/2.12/concepts/scaling-deployments/#scaling-modifiers-experimental).

```
scalingModifiers:
    formula: ""float(one < 75 ? 1 : 0)""
    target: ""3""
    activationTarget: ""1""
    metricType: ""Utilization""
```

You may check your KEDA version using [kubectl command](https://kubernetes.io/docs/reference/kubectl/quick-reference/):

```
kubectl get deployment keda-operator -n keda -o=jsonpath='{.spec.template.spec.containers[0].image}'
```",2025-01-28T20:53:05,2025-01-27T20:11:25,"```yaml
scalingModifiers:
    formula: ""float(one < 75 ? 1 : 0)""
    target: ""3""
    activationTarget: ""1""
    metricType: ""Utilization""
```

Based on the [Pull Request](https://github.com/kedacore/keda-docs/pull/1246) in Github community from KEDA releases v.2.13.0 concepts [casting 'float'](https://github.com/kedacore/keda-docs/commit/754943c60bcf0f90a20463d9805d2a299b0c12c2#diff-30ce2091937fb966578e23f7cbf86e44f757337047bda64841b891f43300428f) before returning the result is a must if a ternary operator result is ‘any’ as per [experimental scaling modifier](https://keda.sh/docs/2.12/concepts/scaling-deployments/#scaling-modifiers-experimental).

```bash
kubectl get deployment keda-operator -n keda -o=jsonpath='{.spec.template.spec.containers[0].image}'
```

You may check your KEDA version using [kubectl command](https://kubernetes.io/docs/reference/kubectl/quick-reference/):"
79391422,How do I automatically retry a request in traefik when the downstream service isn&#39;t yet ready,"I've configured Traefik within a Kubernetes (k8s) cluster as the ingress. However, I have some legacy containers that are being exposed that don't behave as well as one would want from a modern containerised application. I would like to be able to configure a Traefik middleware such that it will retry when the downstream service isn't yet ready.","kubernetes, traefik, traefik-ingress, traefik-middleware, traefik-routers",79391423.0,"One would naively have expected the [retry middleware](https://doc.traefik.io/traefik/middlewares/http/retry/) to satisfy this requirement. Unfortunately (and by design) this does not work as it appears to receive a 503 status code from the backend service and, as is clearly stated in the documentation, treats any response whatsoever from downstream services as a non-retryable event.

To navigate around this, I used the [error middleware](https://doc.traefik.io/traefik/middlewares/http/errorpages/) instead. With this, I also provided a deployment (with associated service) in my cluster/namespace that was capable of serving a static html page that automatically refreshed e.g. some html that contained:

```
<meta http-equiv=""refresh"" content=""5"">
```

My middleware configuration looked like:

```
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: retry-on-503
spec:
  errors:
    status:
      - ""503""
    query: /retry.html
    service:
      name: staticsite
      port: 80
```

Whilst not ideal as the retry is exposed to the client, this works for my specific needs.",2025-01-27T16:08:49,2025-01-27T16:08:49,"```html
<meta http-equiv=""refresh"" content=""5"">
```

With this, I also provided a deployment (with associated service) in my cluster/namespace that was capable of serving a static html page that automatically refreshed e.g. some html that contained:

```yaml
apiVersion: traefik.io/v1alpha1
kind: Middleware
metadata:
  name: retry-on-503
spec:
  errors:
    status:
      - ""503""
    query: /retry.html
    service:
      name: staticsite
      port: 80
```

My middleware configuration looked like:"
79371532,Why throws the POD an InvocationTargetException,"I am new in Kubernetes and I want to run a basic Spring-Boot-application inside a namespace.

The Pod alsways tries to start and throws this Error:

> Exception in thread ""main"" java.lang.reflect.InvocationTargetException
> at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(Unknown
> Source)
> at java.base/java.lang.reflect.Method.invoke(Unknown Source)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:102)
> at org.springframework.boot.loader.launch.Launcher.launch(Launcher.java:64)
> at org.springframework.boot.loader.launch.JarLauncher.main(JarLauncher.java:40)
> Caused by: java.lang.reflect.InvocationTargetException

and this Error:

> > Caused by: java.lang.StackOverflowError
> > at java.base/java.lang.ThreadLocal.getCarrierThreadLocal(Unknown Source)
> > at java.base/java.lang.System$2.getCarrierThreadLocal(Unknown Source)
> > at java.base/jdk.internal.misc.CarrierThreadLocal.get(Unknown Source)
> > at java.base/sun.nio.fs.NativeBuffers.getNativeBufferFromCache(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.copyToNativeBuffer(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixNativeDispatcher.stat(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributes.get(Unknown Source)
> > at java.base/sun.nio.fs.UnixFileAttributeViews$Basic.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.UnixFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/sun.nio.fs.LinuxFileSystemProvider.readAttributes(Unknown
> > Source)
> > at java.base/java.nio.file.Files.readAttributes(Unknown Source)
> > at java.base/java.util.zip.ZipFile$Source.get(Unknown Source)
> > at java.base/java.util.zip.ZipFile$CleanableResource.(Unknown
> > Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.zip.ZipFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at java.base/java.util.jar.JarFile.(Unknown Source)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:141)
> > at org.springframework.boot.loader.jar.NestedJarFile.(NestedJarFile.java:124)
> > at org.springframework.boot.loader.net.protocol.jar.UrlNestedJarFile.(UrlNestedJarFile.java:42)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFileForNested(UrlJarFileFactory.java:86)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFileFactory.createJarFile(UrlJarFileFactory.java:55)
> > at org.springframework.boot.loader.net.protocol.jar.UrlJarFiles.getOrCreate(UrlJarFiles.java:72)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.connect(JarUrlConnection.java:289)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlConnection.getJarFile(JarUrlConnection.java:99)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.getJarFile(JarUrlClassLoader.java:188)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackage(JarUrlClassLoader.java:146)
> > at org.springframework.boot.loader.net.protocol.jar.JarUrlClassLoader.definePackageIfNecessary(JarUrlClassLoader.java:129)

I am using Helm as well, but with the helm files is everything fine.

My Dockerfile looks like that:

```
FROM /ubi8/minimum/java-21:8.10-1088-1-java21.0.5_11

COPY /target/*-spring-boot.jar app.jar

ENTRYPOINT [""java"", ""-Xms2G"", ""-Xmx2G"", ""-XX:+UseG1GC"", ""-XX:+ExitOnOutOfMemoryError"", ""-jar"", ""app.jar""]

LABEL COMMIT_ID=${COMMIT_ID}
```

And my POM looks like that:

```
<?xml version=""1.0"" encoding=""UTF-8""?>
<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd"">
    <modelVersion>4.0.0</modelVersion>

    <parent>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-parent</artifactId>
        <version>3.4.1</version>
        <relativePath/>
    </parent>

    <properties>
        <java.version>21</java.version>
        <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>

        <maven-compiler-plugin.version>3.13.0</maven-compiler-plugin.version>
        <maven-clean-plugin.version>3.4.0</maven-clean-plugin.version>
        <maven-surefire-plugin-version>3.5.2</maven-surefire-plugin-version>
        <maven-dependency-plugin.version>3.8.1</maven-dependency-plugin.version>
        <maven-spring-boot-plugin.version>3.4.0</maven-spring-boot-plugin.version>
    </properties>

    <dependencies>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-test</artifactId>
            <scope>test</scope>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-web</artifactId>
        </dependency>
        <dependency>
            <groupId>org.springframework.boot</groupId>
            <artifactId>spring-boot-starter-actuator</artifactId>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>${maven-compiler-plugin.version}</version>
                <configuration>
                    <release>${java.version}</release>
                    <parameters>true</parameters>
                </configuration>
            </plugin>
            <plugin>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-maven-plugin</artifactId>
                <version>${maven-spring-boot-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>repackage</goal>
                        </goals>
                        <configuration>
                            <classifier>spring-boot</classifier>
                        </configuration>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-dependency-plugin</artifactId>
                <version>${maven-dependency-plugin.version}</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>properties</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-surefire-plugin</artifactId>
                <version>${maven-surefire-plugin-version}</version>
            </plugin>
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-clean-plugin</artifactId>
                <version>${maven-clean-plugin.version}</version>
            </plugin>
        </plugins>
    </build>

    <profiles>
        <profile>
            <id>OWASP</id>
            <build>
                <plugins>
                    <plugin>
                        <groupId>org.owasp</groupId>
                        <artifactId>dependency-check-maven</artifactId>
                    </plugin>
                </plugins>
            </build>
        </profile>
    </profiles>
```

Thanks in advance!
Maybe somebody can help me :)","java, spring, spring-boot, docker, kubernetes",79436081.0,"In your Dockerfile you have set `-Xms2G` which mean your application needs to allocate '2G' of heap space to begin with at the time of initialization.

Accordingly, you must also configure equivalent or more `request memory` for your pod where your container will be deployed. Hence, in your `deployment.yaml` you need to configure the following:

```
....
resources:
      request:
        cpu: 0.1
        memory: 2G
....
```",2025-02-13T12:03:18,2025-01-20T13:49:57,"```yaml
....
resources:
      request:
        cpu: 0.1
        memory: 2G
....
```

In your Dockerfile you have set `-Xms2G` which mean your application needs to allocate '2G' of heap space to begin with at the time of initialization.

Accordingly, you must also configure equivalent or more `request memory` for your pod where your container will be deployed. Hence, in your `deployment.yaml` you need to configure the following:"
79369211,OpenTelemetry export to Prometheus – Unsupported compression: snappy (prometheusremotewrite),"The .NET OpenTelemetry.AutoInstrumentation package fails to export metrics to Prometheus, via an OpenTelemetry Collector (otel/opentelemetry-collector-contrib) due to snappy compression.

Prometheus OTLP endpoint `/api/v1/otlp/v1/metrics` throws `400 Bad Request`

```
unsupported compression: snappy. Only ""gzip"" or no compression supported
```

Full logs of OpenTelemetry Collector metrics requests:

1. INFO **debug**
2. ERROR **prometheusremotewrite**

```
2025-01-19T15:16:36.519Z    info    Metrics {""kind"": ""exporter"", ""data_type"": ""metrics"", ""name"": ""debug"", ""resource metrics"": 1, ""metrics"": 36, ""data points"": 150}
2025-01-19T15:16:36.526Z    error   internal/queue_sender.go:103    Exporting failed. Dropping data.    {""kind"": ""exporter"", ""data_type"": ""metrics"", ""name"": ""prometheusremotewrite"", ""error"": ""Permanent error: Permanent error: Permanent error: remote write returned HTTP status 400 Bad Request; err = %!w(<nil>): unsupported compression: snappy. Only \""gzip\"" or no compression supported\n"", ""dropped_items"": 150}
go.opentelemetry.io/collector/exporter/exporterhelper/internal.NewQueueSender.func1
    go.opentelemetry.io/collector/exporter@v0.116.0/exporterhelper/internal/queue_sender.go:103
go.opentelemetry.io/collector/exporter/internal/queue.(*Consumers[...]).Start.func1
    go.opentelemetry.io/collector/exporter@v0.116.0/internal/queue/consumers.go:43
```

`kube-prometheus-stack` includes configuration to open the OTLP endpoint: `/api/v1/otlp/v1/metrics`

```
prometheus:
  prometheusSpec:
    additionalArgs:
      - name: web.enable-otlp-receiver
        value: """"
```

OpenTelemetry Collector configuration:

```
# https://opentelemetry.io/docs/languages/js/exporters/#prometheus
apiVersion: opentelemetry.io/v1beta1
kind: OpenTelemetryCollector
metadata:
  name: ${NAME}
  namespace: ${NAMESPACE}
spec:
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318

    # https://github.com/open-telemetry/opentelemetry-helm-charts/issues/23#issuecomment-910885716
    processors:
      batch: {}
      memory_limiter:
        check_interval: 5s
        limit_percentage: 80
        spike_limit_percentage: 25

    exporters:
      debug:
        verbosity: basic
      prometheusremotewrite:
        endpoint: http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/otlp/v1/metrics
        tls:
          insecure: true

    service:
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug]
        metrics:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug, prometheusremotewrite]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [debug]
```

OpenTelemetry Instrumentation to automatically consume .NET metrics:

```
apiVersion: opentelemetry.io/v1alpha1
kind: Instrumentation
metadata:
  name: ${NAME}
  namespace: ${NAMESPACE}
spec:
  exporter:
    endpoint: http://${COLLECTOR_SERVICE}.${COLLECTOR_NAMESPACE}.svc.cluster.local:4318
  propagators:
    - tracecontext
    - baggage
  sampler:
    type: parentbased_traceidratio
    argument: ""1""
```

Kubernetes deployment of .NET container has an template metadata annotation:

```
instrumentation.opentelemetry.io/inject-dotnet: true
```","kubernetes, prometheus, open-telemetry, prometheus-operator, open-telemetry-collector",79369355.0,"In my Collector configuration, I am using the Prometheus RemoteWrite exporter, which pushes metrics via PRW, to the Prometheus OTLP endpoint. What I want to do, is pick one of those protocols and forget about the other.

1. If I want to push metrics via PRW, I update the endpoint to `http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write`
2. If I want to push metrics via OTLP, I replace my current exporter with the [OTLP exporter](https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter)

✅  PRW solution:

```
prometheus:
  prometheusSpec:
    enableRemoteWriteReceiver: true
    enableFeatures:
      - remote-write-receiver
```

and

```
prometheusremotewrite:
  endpoint: http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write
  tls:
    insecure: true
```",2025-01-19T16:50:33,2025-01-19T15:27:22,"```text
In my Collector configuration, I am using the Prometheus RemoteWrite exporter, which pushes metrics via PRW, to the Prometheus OTLP endpoint. What I want to do, is pick one of those protocols and forget about the other.

1. If I want to push metrics via PRW, I update the endpoint to `http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write`
2. If I want to push metrics via OTLP, I replace my current exporter with the [OTLP exporter](https://github.com/open-telemetry/opentelemetry-collector/tree/main/exporter/otlphttpexporter)

✅  PRW solution:
```

```yaml
prometheus:
  prometheusSpec:
    enableRemoteWriteReceiver: true
    enableFeatures:
      - remote-write-receiver
```

```text
and
```

```yaml
prometheusremotewrite:
  endpoint: http://${PROMETHEUS_SERVICE}.${PROMETHEUS_NAMESPACE}.svc.cluster.local:9090/api/v1/write
  tls:
    insecure: true
```"
79353181,How do I get a certificate (public and private key) into a windows container in AKS?,Given a **windows** container running inside Azure Kubernetes Service (AKS). How do I get a certificate (PFX) that I've stored in Azure Key Vault (AKV) stored in the local certificate store of the container?,"powershell, kubernetes, cryptography, azure-aks, azure-keyvault",79353182.0,"N.B. This assumes you've already successfully gotten AKS wired up and talking to AKV. Pause and start [elsewhere](https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver) if you've not successfully brought simple passwords across into the environment of your windows container yet.

The trick is to recognise that when you install a certificate (PFX) into keyvault this is accessed as two separate objects and you can get these pulled into the environment as a combined PEM if you setup your k8s secret provider appropriately.

First you must setup your k8s secrets to request it as an objecttype of 'secret' (not key or cert) e.g. :

```
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: sc-demo-keyvault-csi
spec:
  provider: azure
  parameters:
    usePodIdentity: ""false""
    useVMManagedIdentity: ""true""                                   # Set to true for using managed identity
    userAssignedIdentityID: <redacted>   # Set the clientID of the user-assigned managed identity to use
    keyvaultName: <redacted>                                     # Set to the name of your key vault
    objects:  |
      array:
        - |
          objectName: testcert           # keyvault secret name
          objectType: secret             # getting a cert as a secret returns the public & private key pair as a pem, a type of cert just returns the public key (https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/getting-certs-and-keys/)
    tenantId: <REDACTED>                # The tenant ID of the key vault
  secretObjects:
  - data:
    - key: secretcert
      objectName: testcert
    secretName: foosecret
    type: Opaque
```

Once this is done and you've mapped the secret through to your container as an environment variable in your deployment/pod description e.g.

```
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
        - name: test
          env:
            - name: SIGNING_KEYPAIR
              valueFrom:
                secretKeyRef:
                  name: foosecret
                  key: secretcert
          volumeMounts:
            - name : secrets-store01-inline
              mountPath: ""/mnt/secrets-store""
              readOnly: true
      volumes:
        - name: secrets-store01-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: 'sc-demo-keyvault-csi'
```

If you were to fire up your windows container at this point you'd find your environment contains a PEM file (I assume there's a potential issue here around the size of the certificates but not something I@ve run into.)

So then we just need to take that PEM, reconstruct into into a PFX file, load it into the certificate store in the container and apply the appropriate permissions.

Something like this works in powershell:

```
# Extract the keys from the environment variable
$matches = [regex]::match($Env:SIGNING_KEYPAIR,'(?smi)-----BEGIN PRIVATE KEY-----\s*(.+)-----END PRIVATE KEY-----\s*-----BEGIN CERTIFICATE-----\s*(.+)-----END CERTIFICATE-----')
$PRIVATE_KEY= $matches.Groups[1].Value
$PUBLIC_KEY= $matches.Groups[2].Value

# Write them out to a random file pair
$RANDOM_FILE= New-Guid
Out-File -FilePath ""$RANDOM_FILE.key"" -InputObject $PRIVATE_KEY
Out-File -FilePath ""$RANDOM_FILE.cer"" -InputObject $PUBLIC_KEY

# Create the PFX (the .key file will be attached as it shares the same filename)
& certutil -p ""ignored,$RANDOM_FILE"" -MergePFX ""$RANDOM_FILE.cer"" ""$RANDOM_FILE.pfx""  | Out-Null
$c= Import-PfxCertificate -Password (ConvertTo-SecureString -String ""$RANDOM_FILE"" -AsPlainText -Force) -FilePath ""$RANDOM_FILE.pfx"" -CertStoreLocation ""Cert:\LocalMachine\My""

# Cleanup the environment
# (doesn't really improve the security position, but I'd rather not have secrets in two places)
Remove-Item ""$RANDOM_FILE.*""
```

At this point you should have everything you need ($c.Thumbprint) to setup appropriate access to the private key as you would normally do.

The approach described here definitely works on containers based on mcr.microsoft.com/dotnet/framework/aspnet:4.8-windowsservercore-ltsc2019 . YMMV for containers based on other base containers.

edit: The mounting of the secrets described above is not required to get the secrets into the environment. Further as an alternative to grabbing the pair out of the environment, you can access the mounted secrets directly from c:\mnt.",2025-01-13T18:49:40,2025-01-13T18:49:40,"```text
N.B. This assumes you've already successfully gotten AKS wired up and talking to AKV. Pause and start [elsewhere](https://learn.microsoft.com/en-us/azure/aks/csi-secrets-store-driver) if you've not successfully brought simple passwords across into the environment of your windows container yet.

The trick is to recognise that when you install a certificate (PFX) into keyvault this is accessed as two separate objects and you can get these pulled into the environment as a combined PEM if you setup your k8s secret provider appropriately.

First you must setup your k8s secrets to request it as an objecttype of 'secret' (not key or cert) e.g. :
```

```yaml
apiVersion: secrets-store.csi.x-k8s.io/v1
kind: SecretProviderClass
metadata:
  name: sc-demo-keyvault-csi
spec:
  provider: azure
  parameters:
    usePodIdentity: ""false""
    useVMManagedIdentity: ""true""                                   # Set to true for using managed identity
    userAssignedIdentityID: <redacted>   # Set the clientID of the user-assigned managed identity to use
    keyvaultName: <redacted>                                     # Set to the name of your key vault
    objects:  |
      array:
        - |
          objectName: testcert           # keyvault secret name
          objectType: secret             # getting a cert as a secret returns the public & private key pair as a pem, a type of cert just returns the public key (https://azure.github.io/secrets-store-csi-driver-provider-azure/docs/configurations/getting-certs-and-keys/)
    tenantId: <REDACTED>                # The tenant ID of the key vault
  secretObjects:
  - data:
    - key: secretcert
      objectName: testcert
    secretName: foosecret
    type: Opaque
```

```text
Once this is done and you've mapped the secret through to your container as an environment variable in your deployment/pod description e.g.
```

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
        - name: test
          env:
            - name: SIGNING_KEYPAIR
              valueFrom:
                secretKeyRef:
                  name: foosecret
                  key: secretcert
          volumeMounts:
            - name : secrets-store01-inline
              mountPath: ""/mnt/secrets-store""
              readOnly: true
      volumes:
        - name: secrets-store01-inline
          csi:
            driver: secrets-store.csi.k8s.io
            readOnly: true
            volumeAttributes:
              secretProviderClass: 'sc-demo-keyvault-csi'
```

```text
If you were to fire up your windows container at this point you'd find your environment contains a PEM file (I assume there's a potential issue here around the size of the certificates but not something I@ve run into.)

So then we just need to take that PEM, reconstruct into into a PFX file, load it into the certificate store in the container and apply the appropriate permissions.

Something like this works in powershell:
```

```powershell
# Extract the keys from the environment variable
$matches = [regex]::match($Env:SIGNING_KEYPAIR,'(?smi)-----BEGIN PRIVATE KEY-----\s*(.+)-----END PRIVATE KEY-----\s*-----BEGIN CERTIFICATE-----\s*(.+)-----END CERTIFICATE-----')
$PRIVATE_KEY= $matches.Groups[1].Value
$PUBLIC_KEY= $matches.Groups[2].Value

# Write them out to a random file pair
$RANDOM_FILE= New-Guid
Out-File -FilePath ""$RANDOM_FILE.key"" -InputObject $PRIVATE_KEY
Out-File -FilePath ""$RANDOM_FILE.cer"" -InputObject $PUBLIC_KEY

# Create the PFX (the .key file will be attached as it shares the same filename)
& certutil -p ""ignored,$RANDOM_FILE"" -MergePFX ""$RANDOM_FILE.cer"" ""$RANDOM_FILE.pfx""  | Out-Null
$c= Import-PfxCertificate -Password (ConvertTo-SecureString -String ""$RANDOM_FILE"" -AsPlainText -Force) -FilePath ""$RANDOM_FILE.pfx"" -CertStoreLocation ""Cert:\LocalMachine\My""

# Cleanup the environment
# (doesn't really improve the security position, but I'd rather not have secrets in two places)
Remove-Item ""$RANDOM_FILE.*""
```

```text
At this point you should have everything you need ($c.Thumbprint) to setup appropriate access to the private key as you would normally do.

The approach described here definitely works on containers based on mcr.microsoft.com/dotnet/framework/aspnet:4.8-windowsservercore-ltsc2019 . YMMV for containers based on other base containers.

edit: The mounting of the secrets described above is not required to get the secrets into the environment. Further as an alternative to grabbing the pair out of the environment, you can access the mounted secrets directly from c:\mnt.
```"
79351984,Does all the kubectl commands executed using put params in concourse explicitly do readiness check?,"I m trying to deploy a docker container into Kubernetes using concourse ci put params, I could see after executing the kubectl command it explicitly checks for the readiness of all the others pods present in the same namespace. I don’t want to include the readiness check of all pods other than the pod I m trying to deploy to.

```
    type: kubernetes
    icon: kubernetes
    source:
      insecure_skip_tls_verify: false
      kubeconfig: {{kubernetes-config}}

  - name: deploy_capability_docker_kubernetes
    plan:
      - get: bufferautomationsourcecode
      - get: docker_hub_details_capability_development
        passed: [build_deploy_hub]
        trigger: true
      - get: version
      - get: concoursesourcecode
      - task: update_deploymentfile_git
        file: concoursesourcecode/task/kubernetes_fileupdate_task_parameterized1.yaml
        params:
          BRANCH: ""Dev""
          SOURCE_CODE: ""bufferautomationsourcecode""
      - put: kubernetes-cluster-deployment
        params:
          kubectl: config current-context
      - put: kubernetes-cluster-deployment
        params:
          kubectl: get pods -l app=video-buffer-detect-app -n videoautomationcapabilities
      - put: kubernetes-cluster-deployment
        params:
          kubectl: get pods -n videoautomationcapabilities --show-labels
      - put: kubernetes-cluster-deployment
        params:
          kubectl: apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
        ensure:
          do:
            - put: kubernetes-cluster-deployment
              params:
                kubectl: wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s || true
              ensure:
                do:
                  - put: kubernetes-cluster-deployment
                    params:
                      kubectl: rollout status deployment/video-buffer-detect-app --timeout=300s

+ kubectl config current-context
anvil-dev-01-videoautomationcapabilities
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl get pods -l app=video-buffer-detect-app -n videoautomationcapabilities
NAME                                       READY   STATUS    RESTARTS   AGE
video-buffer-detect-app-7cddd646cb-m589k   1/1     Running   0          15m
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
deployment.apps/video-buffer-detect-app configured
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

+ kubectl wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s
pod/video-buffer-detect-app-7cddd646cb-m589k condition met
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
+ kubectl rollout status deployment/video-buffer-detect-app --timeout=300s
deployment ""video-buffer-detect-app"" successfully rolled out
Waiting for pods to be ready for 30s (interval: 3s, selector: '')
Waiting for pods to be ready... (22/22)
```

There are total 22 pods present in the namespace Iam targeting buffer detect app pod, and has only 1 replica, despite of targeting the specific pod. I get these logs printed for every kubectl command I execute. I want to stop these readiness probe checking for other pods readiness. This is causing a false failure though the pod I targeted got deployed successfully and bc of other pod being in unready or crashed state showing a false failure in put step.

“waiting for pods to be ready for 30s ( interval:3s , selector: ‘ ‘)
waiting for pods to be ready… (22/22)","kubernetes, kubectl, concourse, concourse-pipeline, concourse-resource-types",79647430.0,"You're using a Kubernetes ""put"" step in Concourse CI like this:

```
- put: kubernetes-cluster-deployment
  params:
    kubectl: wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s || true
```

I already had a problem with the native put of concourse-CI and I went through a task finally tried something like:

```
- task: deploy-specific-pod
  config:
    platform: linux
    image_resource:
      type: registry-image
      source:
        repository: bitnami/kubectl
    inputs:
      - name: bufferautomationsourcecode
    params:
      KUBECONFIG: ((kubeconfig))
    run:
      path: sh
      args:
        - -exc
        - |
          echo ""$KUBECONFIG"" > /root/.kube/config
          kubectl apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
          kubectl wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s
          kubectl rollout status deployment/video-buffer-detect-app --timeout=300s
```",2025-06-01T10:45:08,2025-01-13T11:09:26,"```yaml
- put: kubernetes-cluster-deployment
  params:
    kubectl: wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s || true
```

You're using a Kubernetes ""put"" step in Concourse CI like this:

```yaml
- task: deploy-specific-pod
  config:
    platform: linux
    image_resource:
      type: registry-image
      source:
        repository: bitnami/kubectl
    inputs:
      - name: bufferautomationsourcecode
    params:
      KUBECONFIG: ((kubeconfig))
    run:
      path: sh
      args:
        - -exc
        - |
          echo ""$KUBECONFIG"" > /root/.kube/config
          kubectl apply -f bufferautomationsourcecode/kubernetes/Dev/Deployment.yaml -n videoautomationcapabilities
          kubectl wait --for=condition=Ready pod -l app=video-buffer-detect-app -n videoautomationcapabilities --timeout=300s
          kubectl rollout status deployment/video-buffer-detect-app --timeout=300s
```

I already had a problem with the native put of concourse-CI and I went through a task finally tried something like:"
79349874,NGINX Ingress Controller auth-url doesn&#39;t forward to the authentication service,"I have set up the NGINX Ingress Controller on my GKE cluster. I am trying to validate **`example.com`** before loading the page for the user. To achieve this, I created another service using FastAPI and deployed it to a subdomain. This service loads a page where the user can provide their credentials, and after a successful login, they should be redirected to example.com.

However, the `auth-url` annotation is not working for me. When I deployed this Ingress resource, it was supposed to forward the user to the domain `https://fastapi-auth.example.com/auth`, but nothing happened. The homepage simply loads without forcing the user to validate.

What could I be missing here?

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ms-ingress
  namespace: code-oss
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/add-headers: 'Cache-Control: max-age=31536000; public'
    nginx.ingress.kubernetes.io/enable-access-log: ""true""
    nginx.ingress.kubernetes.io/enable-debug: ""true""
    nginx.ingress.kubernetes.io/proxy-buffering: ""on""
    nginx.ingress.kubernetes.io/rewrite-target: /$1
    nginx.ingress.kubernetes.io/ssl-redirect: ""true""
    nginx.ingress.kubernetes.io/use-regex: ""true""
    nginx.ingress.kubernetes.io/auth-url: ""https://fastapi-auth.example.com/auth""

spec:
    ingressClassName: nginx
    tls:
    - hosts:
        - example.com
      secretName: ms-app-tls
    rules:
    - host: example.com
      http:
        paths:
        - path: /vfb-pod-one(/|$)(.*)
          pathType: ImplementationSpecific
          backend:
            service:
              name: vfb-pod-one
              port:
                number: 8000
        - path: /(.*)
          pathType: ImplementationSpecific
          backend:
            serviceName:  vfb-pod-one
            servicePortNumber: 8000
```","authentication, kubernetes, google-kubernetes-engine, nginx-ingress, ingress-controller",79417059.0,"Here is my solution that worked for me. I am only posting it so that people don’t think I’m doing this out of frustration.

```
nginx.ingress.kubernetes.io/auth-url: ""https://example.com/fast-api/login/check""
nginx.ingress.kubernetes.io/auth-signin: ""https://example.com/fast-api/auth?rd=$request_uri""
```",2025-02-06T07:20:07,2025-01-12T12:18:20,"```yaml
nginx.ingress.kubernetes.io/auth-url: ""https://example.com/fast-api/login/check""
nginx.ingress.kubernetes.io/auth-signin: ""https://example.com/fast-api/auth?rd=$request_uri""
```

Here is my solution that worked for me. I am only posting it so that people don’t think I’m doing this out of frustration."
79349279,Restrict external access to pod,"I have a helm chart configured with this service account:

```
apiVersion: v1
kind: Service
metadata:
  name: {{ include ""router.fullname"" . }}
  labels:
    {{- include ""router.labels"" . | nindent 4 }}
spec:a
  type: {{ .Values.service.type }}
  ports:
    - name: http
      nodePort: 30079 # Public port to access router resources. For example, http://<Kubernetes node IP>:30079
      protocol: TCP
      port: 80 # Will expose the kubernetes service within the cluster so communication between multiple different pods can happen and will redirect the request to TargetPort
      targetPort: 8180 # Microservice port. For router it's port 8180
  selector:
    {{- include ""router.selectorLabels"" . | nindent 4 }}
```

I need to access the pod only from internal pods. I would like to disable the public access. How I can implement this into the above configuration?","kubernetes, kubernetes-helm",79349830.0,"The `service: { type: }` controls this.  There are three [Service types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types), and `ClusterIP` is the one that's unreachable from outside the cluster.

With this setup, it should almost be enough to deploy with a Helm value setting that changes that setting

```
# deploy.yaml
service:
  type: ClusterIP
```

```
helm upgrade --install -f deploy.yaml ...
```

The one trick is that `nodePort:` isn't a valid setting for ClusterIP-type Services, so you also need to update your chart code to not deploy it.  (I'd also make the actual port number both optional and configurable.)

```
spec:
  type: {{ .Values.service.type }}
  ports:
    - name: http
{{- if and (ne .Values.service.type ""ClusterIP"") .Values.service.nodePort }}
      nodePort: {{ .Values.service.nodePort }}
{{- end }}
```",2025-01-12T11:42:02,2025-01-12T03:13:15,"```text
The `service: { type: }` controls this.  There are three [Service types](https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types), and `ClusterIP` is the one that's unreachable from outside the cluster.

With this setup, it should almost be enough to deploy with a Helm value setting that changes that setting
```

```yaml
# deploy.yaml
service:
  type: ClusterIP
```

```bash
helm upgrade --install -f deploy.yaml ...
```

```text
The one trick is that `nodePort:` isn't a valid setting for ClusterIP-type Services, so you also need to update your chart code to not deploy it.  (I'd also make the actual port number both optional and configurable.)
```

```yaml
spec:
  type: {{ .Values.service.type }}
  ports:
    - name: http
{{- if and (ne .Values.service.type ""ClusterIP"") .Values.service.nodePort }}
      nodePort: {{ .Values.service.nodePort }}
{{- end }}
```"
79342925,AWS EKS External DNS keeps deleting and recreating records,"I have an EKS cluster that uses external-dns controller to create DNS records in Route53 for ingresses. this has been working seamlessly until recently it started deleting and recreating sets of records causing the apps to go off and back online every minute.

here's an example of my ingress manifest:

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: test
  annotations:
    external-dns.alpha.kubernetes.io/hostname: stg.test.domain.com
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.name: ""staging-external""
    alb.ingress.kubernetes.io/listen-ports: '[{""HTTP"": 80}, {""HTTPS"": 443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
spec:
  ingressClassName: alb
  rules:
  - host: ""stg.test.domain.com""
    http:
      paths:
      - pathType: Prefix
        path: /
        backend:
          service:
            name: test-service. ##service name
            port:
              number: 80
```

*Edit*
External-dns pod logs

```
time=""2025-01-10T08:51:45Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:45Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzonename> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/service-name""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/notification-controller""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/source-controller""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/metrics-server""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/aws-load-balancer-webhook-service""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service external-secrets/external-secrets-webhook""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service flux-system/webhook-receiver""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service default/external-dns""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service default/kubernetes""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/eks-extension-metrics-api""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service kube-system/kube-dns""
time=""2025-01-10T08:51:46Z"" level=debug msg=""No endpoints could be generated from service namespace/servicename""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/service-name-ingress: [app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com [] app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Endpoints generated from ingress: namespace/servicename-ingress: [app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [] app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Removing duplicate endpoint app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com []""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app1.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app2.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app3.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app3-backend.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app4.domain.com 300 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app4.domain.com 300 IN A alb-FQDN.amazonaws.com [{alias true}], setting ttl=300""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Modifying endpoint: app5.domain.com 0 IN CNAME alb-FQDN.amazonaws.com [], setting alias=true""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzonename> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=info msg=""Applying provider record filter for domains: [domain.com. .domain.com.]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Refreshing zones list cache""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Considering zone: /hostedzone/<hostedzoneId> (domain: domain.com.)""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app1.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app1-backend.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app2.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app3.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app4.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=debug msg=""Adding cname-app5.domain.com. to zone domain.com. [Id: /hostedzone/<hostedzoneId>]""
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app3.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app3.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app2.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app2.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app3.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app2.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app1-backend.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app1.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app4.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE cname-app5.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1-backend.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1-backend.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app1.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app4.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app4.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app5.domain.com A"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""Desired change: CREATE app5.domain.com TXT"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
time=""2025-01-10T08:51:46Z"" level=info msg=""18 record(s) were successfully updated"" profile=default zoneID=/hostedzone/<hostedzoneId> zoneName=domain.com.
```

Just keeps repeating these actions","amazon-web-services, kubernetes, amazon-eks, amazon-route53, external-dns",79345499.0,"I figured out what was causing the problem.

So I have two almost identical clusters(Staging and Production), they both use the same hosted zone on Route53 in their external-dns controller so they both have access to all the records there. So the logs I wasn't checking were the logs on the external-dns controller on the production cluster which actually logged the DELETE events causing the staging cluster to continue recreating them.

This was fixed by adding the following argument to the external-dns deployment manifest to make sure each external-dns instance only has access to manage the records it created.

```
containers:
        - name: external-dns
          ## other config ...
          args:
            - --txt-owner-id=unique.staging.cluster.string.id
            ## other args ...
```

The *--txt-owner-id* argument gives each record a unique string Id with which it will be managed without conflict.

Thanks to everyone for their time and suggestions",2025-01-10T11:11:15,2025-01-09T14:13:32,"```yaml
containers:
        - name: external-dns
          ## other config ...
          args:
            - --txt-owner-id=unique.staging.cluster.string.id
            ## other args ...
```

I figured out what was causing the problem.

So I have two almost identical clusters(Staging and Production), they both use the same hosted zone on Route53 in their external-dns controller so they both have access to all the records there. So the logs I wasn't checking were the logs on the external-dns controller on the production cluster which actually logged the DELETE events causing the staging cluster to continue recreating them.

This was fixed by adding the following argument to the external-dns deployment manifest to make sure each external-dns instance only has access to manage the records it created.

The *--txt-owner-id* argument gives each record a unique string Id with which it will be managed without conflict.

Thanks to everyone for their time and suggestions"
